WARNING: crossbar sizes with different row annd column dimension not supported.

      ==> Arguments:
          batch_size: 1000
          dataset: cifar100
          savedir: /home/nano01/a/esoufler/activations/x64/rram/multiple_batches/
          model: resnet20
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mvm: True
          nideal: True
          mode: train
          input_size: None
          workers: 8
          gpus: 0,1,2,3
          experiment: 64x64
          batch_start: 0

      ==> Functional simulator configurations:
          weight_bits=16
          weight_bit_frac=12
          input_bits=16
          input_bit_frac=12
          xbar_row_size=64
          xbar_col_size=64
          tile_row=2
          tile_col=2
          bit_stream=1
          bit_slice=2
          adc_bit=14
          acm_bits=32
          acm_bit_frac=24
          mvm=True
          non-ideality=True
          
xbmodel=NN_model(
  (fc1): Linear(in_features=4160, out_features=500, bias=True)
  (relu1): ReLU(inplace=True)
  (do2): Dropout(p=0.5, inplace=False)
  (fc3): Linear(in_features=500, out_features=64, bias=True)
)
          
xbmodel_weight_path=../xb_models/XB_64_stream1slice207dropout50epochs.pth.tar
          inmax_test=1.2
          inmin_test=0.857


DEVICE: cuda
GPU Id(s) being used: 0,1,2,3
==> Building model and model_mvm for resnet20 ...
WARNING: crossbar sizes with different row annd column dimension not supported.
==> Initializing model parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Pretrained model accuracy: 69.5999984741211
Files already downloaded and verified
Saving activations to: /home/nano01/a/esoufler/activations/x64/rram/multiple_batches/cifar100/resnet20/train
Dry run for computing activation sizes...
Dry run finished...
relu1: torch.Size([250, 16, 32, 32])
relu2: torch.Size([250, 16, 32, 32])
relu3: torch.Size([250, 16, 32, 32])
relu4: torch.Size([250, 16, 32, 32])
relu5: torch.Size([250, 16, 32, 32])
relu6: torch.Size([250, 16, 32, 32])
relu7: torch.Size([250, 16, 32, 32])
relu8: torch.Size([250, 32, 16, 16])
relu9: torch.Size([250, 32, 16, 16])
relu10: torch.Size([250, 32, 16, 16])
relu11: torch.Size([250, 32, 16, 16])
relu12: torch.Size([250, 32, 16, 16])
relu13: torch.Size([250, 32, 16, 16])
relu14: torch.Size([250, 64, 8, 8])
relu15: torch.Size([250, 64, 8, 8])
relu16: torch.Size([250, 64, 8, 8])
relu17: torch.Size([250, 64, 8, 8])
relu18: torch.Size([250, 64, 8, 8])
relu19: torch.Size([250, 64, 8, 8])
fc: torch.Size([250, 100])
relu1: torch.Size([1000, 16, 32, 32])
relu2: torch.Size([1000, 16, 32, 32])
relu3: torch.Size([1000, 16, 32, 32])
relu4: torch.Size([1000, 16, 32, 32])
relu5: torch.Size([1000, 16, 32, 32])
relu6: torch.Size([1000, 16, 32, 32])
relu7: torch.Size([1000, 16, 32, 32])
relu8: torch.Size([1000, 32, 16, 16])
relu9: torch.Size([1000, 32, 16, 16])
relu10: torch.Size([1000, 32, 16, 16])
relu11: torch.Size([1000, 32, 16, 16])
relu12: torch.Size([1000, 32, 16, 16])
relu13: torch.Size([1000, 32, 16, 16])
relu14: torch.Size([1000, 64, 8, 8])
relu15: torch.Size([1000, 64, 8, 8])
relu16: torch.Size([1000, 64, 8, 8])
relu17: torch.Size([1000, 64, 8, 8])
relu18: torch.Size([1000, 64, 8, 8])
relu19: torch.Size([1000, 64, 8, 8])
fc: torch.Size([1000, 100])
Starting to save activations..
Batch IDx: 0 	 Time taken: 8m 12secs
Batch IDx: 1 	 Time taken: 8m 24secs
Batch IDx: 2 	 Time taken: 8m 24secs
Batch IDx: 3 	 Time taken: 8m 19secs
Batch IDx: 4 	 Time taken: 8m 20secs
Batch IDx: 5 	 Time taken: 8m 22secs
Batch IDx: 6 	 Time taken: 8m 21secs
Batch IDx: 7 	 Time taken: 8m 16secs
Batch IDx: 8 	 Time taken: 8m 24secs
Batch IDx: 9 	 Time taken: 8m 21secs
Batch IDx: 10 	 Time taken: 8m 26secs
Batch IDx: 11 	 Time taken: 8m 26secs
Batch IDx: 12 	 Time taken: 8m 14secs
Batch IDx: 13 	 Time taken: 8m 24secs
Batch IDx: 14 	 Time taken: 8m 21secs
Batch IDx: 15 	 Time taken: 8m 17secs
Batch IDx: 16 	 Time taken: 8m 24secs
Batch IDx: 17 	 Time taken: 8m 21secs
Batch IDx: 18 	 Time taken: 8m 18secs
Batch IDx: 19 	 Time taken: 8m 24secs
Batch IDx: 20 	 Time taken: 8m 22secs
Batch IDx: 21 	 Time taken: 8m 19secs
Batch IDx: 22 	 Time taken: 8m 21secs
Batch IDx: 23 	 Time taken: 8m 23secs
Batch IDx: 24 	 Time taken: 8m 23secs
Batch IDx: 25 	 Time taken: 8m 26secs
Batch IDx: 26 	 Time taken: 8m 22secs
Batch IDx: 27 	 Time taken: 8m 26secs
Batch IDx: 28 	 Time taken: 8m 21secs
Batch IDx: 29 	 Time taken: 8m 23secs
Batch IDx: 30 	 Time taken: 8m 18secs
Batch IDx: 31 	 Time taken: 8m 22secs
Batch IDx: 32 	 Time taken: 8m 26secs
Batch IDx: 33 	 Time taken: 8m 23secs
Batch IDx: 34 	 Time taken: 8m 27secs
Batch IDx: 35 	 Time taken: 8m 22secs
Batch IDx: 36 	 Time taken: 8m 21secs
Batch IDx: 37 	 Time taken: 8m 25secs
Batch IDx: 38 	 Time taken: 8m 28secs
Batch IDx: 39 	 Time taken: 8m 19secs
Batch IDx: 40 	 Time taken: 8m 25secs
Batch IDx: 41 	 Time taken: 8m 26secs
Batch IDx: 42 	 Time taken: 8m 23secs
Batch IDx: 43 	 Time taken: 8m 27secs
Batch IDx: 44 	 Time taken: 8m 27secs
Batch IDx: 45 	 Time taken: 8m 29secs
Batch IDx: 46 	 Time taken: 8m 27secs
Batch IDx: 47 	 Time taken: 8m 26secs
Batch IDx: 48 	 Time taken: 8m 32secs
Batch IDx: 49 	 Time taken: 8m 22secs
Done saving activations!
