
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 1.180 Prec@5 5.640 Loss 4.6211
Pre-trained Prec@1 with 5 layers frozen: 1.1799999475479126 	 Loss: 4.62109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 3.8555 (4.5161)	Prec@1 14.844 (5.859)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.1758 (3.9459)	Prec@1 31.250 (15.430)	
Epoch: [0][116/196]	LR: 0.002	Loss 2.6484 (3.5721)	Prec@1 46.094 (23.140)	
Epoch: [0][155/196]	LR: 0.002	Loss 2.4336 (3.3101)	Prec@1 44.141 (28.541)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.2871 (3.1148)	Prec@1 51.172 (32.788)	
Total train loss: 3.1135

 * Prec@1 53.600 Prec@5 83.450 Loss 2.1582
Best acc: 53.600
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.1055 (2.0490)	Prec@1 51.172 (57.041)	
Epoch: [1][77/196]	LR: 0.002	Loss 2.0020 (2.0085)	Prec@1 56.250 (57.602)	
Epoch: [1][116/196]	LR: 0.002	Loss 1.8105 (1.9581)	Prec@1 60.156 (58.754)	
Epoch: [1][155/196]	LR: 0.002	Loss 1.7715 (1.9236)	Prec@1 62.891 (59.330)	
Epoch: [1][194/196]	LR: 0.002	Loss 1.5898 (1.8917)	Prec@1 67.969 (59.924)	
Total train loss: 1.8916

 * Prec@1 62.650 Prec@5 89.570 Loss 1.7158
Best acc: 62.650
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 1.6680 (1.6364)	Prec@1 62.500 (65.585)	
Epoch: [2][77/196]	LR: 0.002	Loss 1.5664 (1.6166)	Prec@1 65.234 (65.961)	
Epoch: [2][116/196]	LR: 0.002	Loss 1.4941 (1.5904)	Prec@1 66.016 (66.446)	
Epoch: [2][155/196]	LR: 0.002	Loss 1.5703 (1.5741)	Prec@1 63.281 (66.729)	
Epoch: [2][194/196]	LR: 0.002	Loss 1.3760 (1.5564)	Prec@1 69.922 (67.113)	
Total train loss: 1.5567

 * Prec@1 66.840 Prec@5 91.390 Loss 1.5098
Best acc: 66.840
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.3701 (1.3987)	Prec@1 69.922 (71.384)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.3525 (1.3867)	Prec@1 75.391 (71.404)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.3721 (1.3846)	Prec@1 69.141 (71.194)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.3838 (1.3765)	Prec@1 66.797 (71.071)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.3154 (1.3669)	Prec@1 73.438 (71.192)	
Total train loss: 1.3669

 * Prec@1 69.500 Prec@5 92.350 Loss 1.3887
Best acc: 69.500
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.3887 (1.2571)	Prec@1 71.484 (74.069)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.2607 (1.2545)	Prec@1 73.438 (74.003)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.2334 (1.2505)	Prec@1 73.828 (74.015)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.3613 (1.2451)	Prec@1 68.359 (73.958)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.2041 (1.2382)	Prec@1 72.266 (74.004)	
Total train loss: 1.2384

 * Prec@1 71.080 Prec@5 93.100 Loss 1.3027
Best acc: 71.080
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.1055 (1.1435)	Prec@1 77.344 (76.893)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.1289 (1.1362)	Prec@1 72.656 (76.748)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.1318 (1.1337)	Prec@1 75.781 (76.639)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.1162 (1.1378)	Prec@1 75.781 (76.370)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.2383 (1.1355)	Prec@1 74.219 (76.370)	
Total train loss: 1.1356

 * Prec@1 72.070 Prec@5 93.350 Loss 1.2549
Best acc: 72.070
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.0078 (1.0460)	Prec@1 79.688 (78.926)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.1084 (1.0465)	Prec@1 75.391 (78.746)	
Epoch: [6][116/196]	LR: 0.002	Loss 0.9019 (1.0445)	Prec@1 82.812 (78.669)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.0166 (1.0500)	Prec@1 79.297 (78.425)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.0283 (1.0511)	Prec@1 76.953 (78.399)	
Total train loss: 1.0514

 * Prec@1 72.840 Prec@5 93.790 Loss 1.2109
Best acc: 72.840
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 0.9307 (0.9808)	Prec@1 82.422 (80.779)	
Epoch: [7][77/196]	LR: 0.002	Loss 1.1152 (0.9840)	Prec@1 73.828 (80.559)	
Epoch: [7][116/196]	LR: 0.002	Loss 0.8701 (0.9837)	Prec@1 82.422 (80.252)	
Epoch: [7][155/196]	LR: 0.002	Loss 0.9570 (0.9818)	Prec@1 81.250 (80.329)	
Epoch: [7][194/196]	LR: 0.002	Loss 1.0723 (0.9814)	Prec@1 75.391 (80.256)	
Total train loss: 0.9818

 * Prec@1 73.520 Prec@5 93.920 Loss 1.1719
Best acc: 73.520
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 0.9692 (0.9210)	Prec@1 78.906 (82.222)	
Epoch: [8][77/196]	LR: 0.002	Loss 0.9297 (0.9128)	Prec@1 80.859 (82.482)	
Epoch: [8][116/196]	LR: 0.002	Loss 0.9414 (0.9221)	Prec@1 82.812 (82.065)	
Epoch: [8][155/196]	LR: 0.002	Loss 0.8818 (0.9230)	Prec@1 85.156 (81.946)	
Epoch: [8][194/196]	LR: 0.002	Loss 0.9834 (0.9213)	Prec@1 80.859 (81.941)	
Total train loss: 0.9220

 * Prec@1 74.150 Prec@5 94.050 Loss 1.1562
Best acc: 74.150
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 0.7803 (0.8508)	Prec@1 85.547 (84.205)	
Epoch: [9][77/196]	LR: 0.002	Loss 0.8809 (0.8544)	Prec@1 81.250 (83.964)	
Epoch: [9][116/196]	LR: 0.002	Loss 0.7598 (0.8551)	Prec@1 85.938 (83.824)	
Epoch: [9][155/196]	LR: 0.002	Loss 0.9253 (0.8574)	Prec@1 81.641 (83.626)	
Epoch: [9][194/196]	LR: 0.002	Loss 0.8750 (0.8564)	Prec@1 83.594 (83.592)	
Total train loss: 0.8570

 * Prec@1 74.460 Prec@5 94.210 Loss 1.1309
Best acc: 74.460
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 0.7412 (0.8091)	Prec@1 89.453 (85.467)	
Epoch: [10][77/196]	LR: 0.0004	Loss 0.8809 (0.8130)	Prec@1 82.031 (85.111)	
Epoch: [10][116/196]	LR: 0.0004	Loss 0.7642 (0.8100)	Prec@1 85.938 (85.130)	
Epoch: [10][155/196]	LR: 0.0004	Loss 0.8701 (0.8047)	Prec@1 80.078 (85.216)	
Epoch: [10][194/196]	LR: 0.0004	Loss 0.8169 (0.8049)	Prec@1 86.328 (85.158)	
Total train loss: 0.8056

 * Prec@1 74.770 Prec@5 94.330 Loss 1.1357
Best acc: 74.770
--------------------------------------------------------------------------------
Epoch: [11][38/196]	LR: 0.0004	Loss 0.7876 (0.7711)	Prec@1 85.156 (86.118)	
Epoch: [11][77/196]	LR: 0.0004	Loss 0.8247 (0.7792)	Prec@1 82.422 (85.862)	
Epoch: [11][116/196]	LR: 0.0004	Loss 0.8330 (0.7891)	Prec@1 84.375 (85.677)	
Epoch: [11][155/196]	LR: 0.0004	Loss 0.8784 (0.7915)	Prec@1 82.812 (85.619)	
Epoch: [11][194/196]	LR: 0.0004	Loss 0.8247 (0.7919)	Prec@1 83.984 (85.633)	
Total train loss: 0.7922

 * Prec@1 74.680 Prec@5 94.280 Loss 1.1318
Best acc: 74.770
--------------------------------------------------------------------------------
Epoch: [12][38/196]	LR: 0.0004	Loss 0.8203 (0.7864)	Prec@1 85.938 (85.577)	
Epoch: [12][77/196]	LR: 0.0004	Loss 0.7988 (0.7850)	Prec@1 85.547 (86.053)	
Epoch: [12][116/196]	LR: 0.0004	Loss 0.7573 (0.7849)	Prec@1 85.156 (86.104)	
Epoch: [12][155/196]	LR: 0.0004	Loss 0.8242 (0.7865)	Prec@1 85.156 (86.020)	
Epoch: [12][194/196]	LR: 0.0004	Loss 0.8066 (0.7902)	Prec@1 87.891 (85.857)	
Total train loss: 0.7907

 * Prec@1 74.650 Prec@5 94.190 Loss 1.1328
Best acc: 74.770
--------------------------------------------------------------------------------
