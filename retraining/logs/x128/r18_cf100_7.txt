
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 1.290 Prec@5 5.130 Loss 4.6172
Pre-trained Prec@1 with 7 layers frozen: 1.2899999618530273 	 Loss: 4.6171875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 3.8887 (4.5732)	Prec@1 10.547 (4.617)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.1777 (4.0228)	Prec@1 32.031 (13.381)	
Epoch: [0][116/196]	LR: 0.002	Loss 2.8379 (3.6557)	Prec@1 36.719 (20.646)	
Epoch: [0][155/196]	LR: 0.002	Loss 2.5781 (3.3949)	Prec@1 43.750 (26.322)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.2402 (3.1939)	Prec@1 53.516 (30.571)	
Total train loss: 3.1929

 * Prec@1 51.380 Prec@5 82.120 Loss 2.2324
Best acc: 51.380
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.0469 (2.1253)	Prec@1 56.250 (55.168)	
Epoch: [1][77/196]	LR: 0.002	Loss 1.9697 (2.0884)	Prec@1 53.516 (55.399)	
Epoch: [1][116/196]	LR: 0.002	Loss 1.9082 (2.0471)	Prec@1 58.203 (56.384)	
Epoch: [1][155/196]	LR: 0.002	Loss 1.7920 (2.0058)	Prec@1 65.234 (57.254)	
Epoch: [1][194/196]	LR: 0.002	Loss 1.8408 (1.9710)	Prec@1 56.641 (57.857)	
Total train loss: 1.9710

 * Prec@1 61.060 Prec@5 88.420 Loss 1.7842
Best acc: 61.060
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 1.7500 (1.6997)	Prec@1 60.938 (64.123)	
Epoch: [2][77/196]	LR: 0.002	Loss 1.5996 (1.6764)	Prec@1 67.188 (64.638)	
Epoch: [2][116/196]	LR: 0.002	Loss 1.5615 (1.6638)	Prec@1 66.406 (64.503)	
Epoch: [2][155/196]	LR: 0.002	Loss 1.7764 (1.6492)	Prec@1 57.812 (64.781)	
Epoch: [2][194/196]	LR: 0.002	Loss 1.4893 (1.6335)	Prec@1 68.359 (65.128)	
Total train loss: 1.6337

 * Prec@1 65.150 Prec@5 90.520 Loss 1.5762
Best acc: 65.150
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.4365 (1.4735)	Prec@1 70.312 (68.720)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.4209 (1.4654)	Prec@1 68.750 (68.925)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.3301 (1.4521)	Prec@1 73.047 (69.154)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.3818 (1.4498)	Prec@1 72.266 (69.035)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.4678 (1.4386)	Prec@1 67.578 (69.347)	
Total train loss: 1.4388

 * Prec@1 67.940 Prec@5 91.630 Loss 1.4512
Best acc: 67.940
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.2412 (1.2937)	Prec@1 74.609 (73.147)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.3340 (1.2986)	Prec@1 72.266 (72.912)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.2549 (1.3068)	Prec@1 74.609 (72.523)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.2979 (1.3072)	Prec@1 70.312 (72.421)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.3359 (1.3043)	Prec@1 71.484 (72.422)	
Total train loss: 1.3044

 * Prec@1 69.700 Prec@5 92.350 Loss 1.3711
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.1211 (1.2053)	Prec@1 76.953 (75.531)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.2734 (1.2044)	Prec@1 72.656 (75.371)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.0693 (1.2027)	Prec@1 78.125 (75.344)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.1660 (1.1993)	Prec@1 76.562 (75.255)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.2168 (1.1968)	Prec@1 75.391 (75.036)	
Total train loss: 1.1973

 * Prec@1 70.810 Prec@5 92.750 Loss 1.3105
Best acc: 70.810
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.1543 (1.1124)	Prec@1 75.391 (77.194)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.1670 (1.1121)	Prec@1 77.734 (77.329)	
Epoch: [6][116/196]	LR: 0.002	Loss 1.1621 (1.1098)	Prec@1 74.219 (77.484)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.0723 (1.1138)	Prec@1 76.172 (77.199)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.1045 (1.1109)	Prec@1 78.125 (77.185)	
Total train loss: 1.1114

 * Prec@1 71.620 Prec@5 93.080 Loss 1.2734
Best acc: 71.620
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 1.0156 (1.0300)	Prec@1 80.469 (79.357)	
Epoch: [7][77/196]	LR: 0.002	Loss 1.0254 (1.0410)	Prec@1 80.469 (79.026)	
Epoch: [7][116/196]	LR: 0.002	Loss 1.1035 (1.0379)	Prec@1 77.344 (79.030)	
Epoch: [7][155/196]	LR: 0.002	Loss 0.9800 (1.0356)	Prec@1 78.516 (79.036)	
Epoch: [7][194/196]	LR: 0.002	Loss 1.0537 (1.0338)	Prec@1 75.781 (79.036)	
Total train loss: 1.0340

 * Prec@1 72.160 Prec@5 93.210 Loss 1.2393
Best acc: 72.160
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 0.9214 (0.9702)	Prec@1 83.984 (80.889)	
Epoch: [8][77/196]	LR: 0.002	Loss 0.9648 (0.9733)	Prec@1 83.203 (80.904)	
Epoch: [8][116/196]	LR: 0.002	Loss 0.9624 (0.9690)	Prec@1 82.031 (80.936)	
Epoch: [8][155/196]	LR: 0.002	Loss 0.9302 (0.9686)	Prec@1 82.422 (80.950)	
Epoch: [8][194/196]	LR: 0.002	Loss 1.0469 (0.9701)	Prec@1 76.562 (80.803)	
Total train loss: 0.9706

 * Prec@1 72.560 Prec@5 93.370 Loss 1.2158
Best acc: 72.560
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 0.9468 (0.9123)	Prec@1 81.641 (82.392)	
Epoch: [9][77/196]	LR: 0.002	Loss 0.9199 (0.9113)	Prec@1 83.203 (82.362)	
Epoch: [9][116/196]	LR: 0.002	Loss 0.8398 (0.9065)	Prec@1 84.766 (82.532)	
Epoch: [9][155/196]	LR: 0.002	Loss 0.9453 (0.9057)	Prec@1 80.078 (82.499)	
Epoch: [9][194/196]	LR: 0.002	Loss 0.8770 (0.9088)	Prec@1 82.031 (82.310)	
Total train loss: 0.9095

 * Prec@1 72.690 Prec@5 93.450 Loss 1.1943
Best acc: 72.690
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 0.8589 (0.8424)	Prec@1 82.422 (84.685)	
Epoch: [10][77/196]	LR: 0.0004	Loss 0.7842 (0.8411)	Prec@1 86.328 (84.706)	
Epoch: [10][116/196]	LR: 0.0004	Loss 0.8286 (0.8420)	Prec@1 87.500 (84.712)	
Epoch: [10][155/196]	LR: 0.0004	Loss 0.8096 (0.8441)	Prec@1 88.281 (84.610)	
Epoch: [10][194/196]	LR: 0.0004	Loss 0.8711 (0.8486)	Prec@1 83.203 (84.385)	
Total train loss: 0.8486

 * Prec@1 73.050 Prec@5 93.480 Loss 1.1914
Best acc: 73.050
--------------------------------------------------------------------------------
