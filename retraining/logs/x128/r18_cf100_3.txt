
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 0.970 Prec@5 5.100 Loss 4.6211
Pre-trained Prec@1 with 3 layers frozen: 0.9699999690055847 	 Loss: 4.62109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 3.9492 (4.6199)	Prec@1 12.891 (5.028)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.0000 (4.0247)	Prec@1 33.984 (14.333)	
Epoch: [0][116/196]	LR: 0.002	Loss 2.7656 (3.6307)	Prec@1 39.453 (21.938)	
Epoch: [0][155/196]	LR: 0.002	Loss 2.3848 (3.3447)	Prec@1 52.734 (28.017)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.1523 (3.1309)	Prec@1 55.859 (32.552)	
Total train loss: 3.1297

 * Prec@1 55.020 Prec@5 84.220 Loss 2.1055
Best acc: 55.020
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 1.8555 (2.0115)	Prec@1 63.672 (58.383)	
Epoch: [1][77/196]	LR: 0.002	Loss 2.0352 (1.9708)	Prec@1 53.906 (59.004)	
Epoch: [1][116/196]	LR: 0.002	Loss 1.8174 (1.9272)	Prec@1 59.766 (59.675)	
Epoch: [1][155/196]	LR: 0.002	Loss 1.7051 (1.8876)	Prec@1 64.844 (60.367)	
Epoch: [1][194/196]	LR: 0.002	Loss 1.6494 (1.8449)	Prec@1 64.062 (61.156)	
Total train loss: 1.8450

 * Prec@1 64.430 Prec@5 90.070 Loss 1.6484
Best acc: 64.430
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 1.6172 (1.5541)	Prec@1 66.797 (67.879)	
Epoch: [2][77/196]	LR: 0.002	Loss 1.5713 (1.5496)	Prec@1 66.016 (67.623)	
Epoch: [2][116/196]	LR: 0.002	Loss 1.5703 (1.5327)	Prec@1 63.672 (67.775)	
Epoch: [2][155/196]	LR: 0.002	Loss 1.4014 (1.5197)	Prec@1 71.094 (68.056)	
Epoch: [2][194/196]	LR: 0.002	Loss 1.3574 (1.5022)	Prec@1 73.438 (68.419)	
Total train loss: 1.5024

 * Prec@1 68.060 Prec@5 92.190 Loss 1.4502
Best acc: 68.060
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.4482 (1.3474)	Prec@1 67.969 (71.955)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.3223 (1.3363)	Prec@1 71.094 (71.975)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.3799 (1.3322)	Prec@1 70.703 (72.009)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.2891 (1.3225)	Prec@1 71.875 (72.063)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.2900 (1.3125)	Prec@1 68.750 (72.188)	
Total train loss: 1.3126

 * Prec@1 70.550 Prec@5 93.130 Loss 1.3330
Best acc: 70.550
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.1289 (1.2147)	Prec@1 76.953 (74.710)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.2002 (1.2112)	Prec@1 73.438 (74.669)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.1543 (1.2034)	Prec@1 73.828 (74.773)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.2393 (1.1945)	Prec@1 73.438 (74.920)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.1484 (1.1837)	Prec@1 75.000 (75.124)	
Total train loss: 1.1839

 * Prec@1 72.000 Prec@5 93.580 Loss 1.2500
Best acc: 72.000
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.0605 (1.0958)	Prec@1 78.125 (77.414)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.2783 (1.0973)	Prec@1 72.266 (77.434)	
Epoch: [5][116/196]	LR: 0.002	Loss 0.9067 (1.0865)	Prec@1 84.766 (77.648)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.0439 (1.0857)	Prec@1 77.734 (77.512)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.0518 (1.0824)	Prec@1 78.125 (77.414)	
Total train loss: 1.0826

 * Prec@1 73.190 Prec@5 93.910 Loss 1.2012
Best acc: 73.190
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 0.9116 (1.0111)	Prec@1 83.594 (79.557)	
Epoch: [6][77/196]	LR: 0.002	Loss 0.8848 (1.0011)	Prec@1 83.984 (79.808)	
Epoch: [6][116/196]	LR: 0.002	Loss 0.9497 (0.9988)	Prec@1 78.906 (79.768)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.1279 (1.0027)	Prec@1 76.562 (79.535)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.0537 (1.0015)	Prec@1 78.906 (79.469)	
Total train loss: 1.0020

 * Prec@1 73.820 Prec@5 94.040 Loss 1.1582
Best acc: 73.820
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 0.9326 (0.9454)	Prec@1 79.688 (80.769)	
Epoch: [7][77/196]	LR: 0.002	Loss 0.9648 (0.9359)	Prec@1 79.297 (81.350)	
Epoch: [7][116/196]	LR: 0.002	Loss 0.9346 (0.9339)	Prec@1 81.250 (81.350)	
Epoch: [7][155/196]	LR: 0.002	Loss 0.8501 (0.9338)	Prec@1 84.375 (81.270)	
Epoch: [7][194/196]	LR: 0.002	Loss 0.9150 (0.9320)	Prec@1 79.688 (81.216)	
Total train loss: 0.9322

 * Prec@1 74.410 Prec@5 94.300 Loss 1.1328
Best acc: 74.410
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 0.9360 (0.8861)	Prec@1 80.469 (82.792)	
Epoch: [8][77/196]	LR: 0.002	Loss 0.8325 (0.8701)	Prec@1 84.375 (83.153)	
Epoch: [8][116/196]	LR: 0.002	Loss 0.9736 (0.8685)	Prec@1 81.641 (83.233)	
Epoch: [8][155/196]	LR: 0.002	Loss 0.9121 (0.8699)	Prec@1 85.938 (83.153)	
Epoch: [8][194/196]	LR: 0.002	Loss 0.7871 (0.8687)	Prec@1 87.109 (83.127)	
Total train loss: 0.8688

 * Prec@1 74.770 Prec@5 94.290 Loss 1.1094
Best acc: 74.770
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 0.8521 (0.8126)	Prec@1 81.641 (84.335)	
Epoch: [9][77/196]	LR: 0.002	Loss 0.8022 (0.8206)	Prec@1 81.250 (84.165)	
Epoch: [9][116/196]	LR: 0.002	Loss 0.8252 (0.8171)	Prec@1 85.938 (84.251)	
Epoch: [9][155/196]	LR: 0.002	Loss 0.8242 (0.8157)	Prec@1 84.766 (84.495)	
Epoch: [9][194/196]	LR: 0.002	Loss 0.8359 (0.8152)	Prec@1 81.641 (84.409)	
Total train loss: 0.8153

 * Prec@1 75.200 Prec@5 94.460 Loss 1.0938
Best acc: 75.200
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 0.7183 (0.7521)	Prec@1 87.500 (86.238)	
Epoch: [10][77/196]	LR: 0.0004	Loss 0.8003 (0.7560)	Prec@1 83.984 (86.058)	
Epoch: [10][116/196]	LR: 0.0004	Loss 0.7783 (0.7569)	Prec@1 84.375 (86.021)	
Epoch: [10][155/196]	LR: 0.0004	Loss 0.7256 (0.7560)	Prec@1 87.891 (86.138)	
Epoch: [10][194/196]	LR: 0.0004	Loss 0.6885 (0.7558)	Prec@1 88.281 (86.180)	
Total train loss: 0.7564

 * Prec@1 75.240 Prec@5 94.510 Loss 1.0918
Best acc: 75.240
--------------------------------------------------------------------------------
