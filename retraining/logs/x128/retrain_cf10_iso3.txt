
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 91.190 Prec@5 99.480 Loss 0.3542
Pre-trained Prec@1 with 1 layers frozen: 91.18999481201172 	 Loss: 0.354248046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.1696 (0.2498)	Prec@1 94.531 (91.546)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.1908 (0.2313)	Prec@1 94.531 (92.072)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.1898 (0.2147)	Prec@1 96.094 (92.612)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.1749 (0.2007)	Prec@1 94.531 (93.091)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.1256 (0.1897)	Prec@1 96.094 (93.474)	
Total train loss: 0.1896

 * Prec@1 91.270 Prec@5 99.590 Loss 0.3069
Best acc: 91.270
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.0860 (0.1335)	Prec@1 97.656 (95.353)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.1155 (0.1324)	Prec@1 96.094 (95.438)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.1821 (0.1323)	Prec@1 95.312 (95.463)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.1025 (0.1304)	Prec@1 96.094 (95.485)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.1284 (0.1286)	Prec@1 96.094 (95.579)	
Total train loss: 0.1286

 * Prec@1 91.430 Prec@5 99.510 Loss 0.3059
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.0711 (0.1165)	Prec@1 98.438 (96.084)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.1157 (0.1158)	Prec@1 95.312 (96.009)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.1367 (0.1145)	Prec@1 95.312 (96.107)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.0705 (0.1136)	Prec@1 97.656 (96.064)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.1009 (0.1125)	Prec@1 95.312 (96.124)	
Total train loss: 0.1127

 * Prec@1 91.450 Prec@5 99.540 Loss 0.3005
Best acc: 91.450
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0567 (0.1113)	Prec@1 98.438 (96.184)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.0474 (0.1068)	Prec@1 99.219 (96.334)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.1575 (0.1065)	Prec@1 92.969 (96.388)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.1074 (0.1064)	Prec@1 96.094 (96.412)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.1405 (0.1056)	Prec@1 94.531 (96.408)	
Total train loss: 0.1058

 * Prec@1 91.320 Prec@5 99.540 Loss 0.3015
Best acc: 91.450
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.1002 (0.1129)	Prec@1 95.312 (96.124)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.1335 (0.1067)	Prec@1 95.312 (96.439)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.1025 (0.1054)	Prec@1 96.094 (96.474)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.1014 (0.1043)	Prec@1 96.094 (96.524)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.0981 (0.1026)	Prec@1 96.094 (96.577)	
Total train loss: 0.1027

 * Prec@1 91.360 Prec@5 99.540 Loss 0.3037
Best acc: 91.450
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0861 (0.0954)	Prec@1 96.094 (96.905)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.1252 (0.0943)	Prec@1 94.531 (96.990)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.1350 (0.0974)	Prec@1 94.531 (96.822)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0797 (0.0996)	Prec@1 97.656 (96.735)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0845 (0.0983)	Prec@1 96.875 (96.805)	
Total train loss: 0.0983

 * Prec@1 91.210 Prec@5 99.560 Loss 0.3032
Best acc: 91.450
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.1281 (0.0973)	Prec@1 97.656 (96.775)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.1371 (0.0970)	Prec@1 95.312 (96.810)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.1226 (0.0981)	Prec@1 96.094 (96.725)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0500 (0.0978)	Prec@1 98.438 (96.720)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0734 (0.0959)	Prec@1 97.656 (96.815)	
Total train loss: 0.0959

 * Prec@1 91.660 Prec@5 99.540 Loss 0.2993
Best acc: 91.660
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0927 (0.0984)	Prec@1 96.875 (96.795)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.1046 (0.0958)	Prec@1 96.875 (96.905)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0935 (0.0956)	Prec@1 96.875 (96.928)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0465 (0.0951)	Prec@1 99.219 (96.925)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.1011 (0.0935)	Prec@1 96.094 (96.973)	
Total train loss: 0.0936

 * Prec@1 91.490 Prec@5 99.530 Loss 0.3000
Best acc: 91.660
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1066 (0.0886)	Prec@1 95.312 (97.015)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.1250 (0.0920)	Prec@1 94.531 (96.925)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0921 (0.0914)	Prec@1 96.094 (96.935)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0532 (0.0901)	Prec@1 98.438 (97.013)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.1567 (0.0911)	Prec@1 96.875 (96.955)	
Total train loss: 0.0910

 * Prec@1 91.390 Prec@5 99.530 Loss 0.3027
Best acc: 91.660
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.1074 (0.0908)	Prec@1 96.875 (96.975)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0564 (0.0900)	Prec@1 98.438 (97.010)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.1095 (0.0906)	Prec@1 96.875 (97.022)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0497 (0.0906)	Prec@1 99.219 (97.020)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0823 (0.0902)	Prec@1 98.438 (97.015)	
Total train loss: 0.0904

 * Prec@1 91.710 Prec@5 99.560 Loss 0.2976
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.1129 (0.0899)	Prec@1 97.656 (96.975)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0986 (0.0874)	Prec@1 96.094 (97.075)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0667 (0.0867)	Prec@1 98.438 (97.222)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0831 (0.0877)	Prec@1 96.094 (97.163)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.1053 (0.0890)	Prec@1 96.875 (97.115)	
Total train loss: 0.0890

 * Prec@1 91.550 Prec@5 99.550 Loss 0.2991
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.1035 (0.0893)	Prec@1 95.312 (97.055)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0994 (0.0903)	Prec@1 97.656 (97.035)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.1467 (0.0906)	Prec@1 96.875 (97.085)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0306 (0.0886)	Prec@1 99.219 (97.168)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0739 (0.0886)	Prec@1 97.656 (97.153)	
Total train loss: 0.0886

 * Prec@1 91.530 Prec@5 99.540 Loss 0.3000
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.1138 (0.0881)	Prec@1 96.875 (97.316)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.1111 (0.0900)	Prec@1 96.875 (97.120)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0925 (0.0900)	Prec@1 96.094 (97.082)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.1548 (0.0885)	Prec@1 92.969 (97.118)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0978 (0.0888)	Prec@1 97.656 (97.113)	
Total train loss: 0.0889

 * Prec@1 91.420 Prec@5 99.540 Loss 0.3018
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.1349 (0.0907)	Prec@1 95.312 (96.915)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.1399 (0.0878)	Prec@1 95.312 (97.090)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.1262 (0.0895)	Prec@1 96.094 (97.025)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.1111 (0.0883)	Prec@1 94.531 (97.093)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0826 (0.0883)	Prec@1 97.656 (97.055)	
Total train loss: 0.0882

 * Prec@1 91.560 Prec@5 99.580 Loss 0.3000
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0739 (0.0919)	Prec@1 96.875 (96.905)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0397 (0.0874)	Prec@1 99.219 (97.201)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0750 (0.0867)	Prec@1 97.656 (97.236)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0843 (0.0876)	Prec@1 96.875 (97.130)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.1395 (0.0872)	Prec@1 95.312 (97.151)	
Total train loss: 0.0874

 * Prec@1 91.500 Prec@5 99.530 Loss 0.3018
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.1068 (0.0784)	Prec@1 96.875 (97.516)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0728 (0.0837)	Prec@1 96.875 (97.276)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0653 (0.0851)	Prec@1 97.656 (97.202)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.1306 (0.0855)	Prec@1 96.094 (97.231)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0739 (0.0863)	Prec@1 98.438 (97.204)	
Total train loss: 0.0864

 * Prec@1 91.650 Prec@5 99.580 Loss 0.2957
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0692 (0.0916)	Prec@1 97.656 (97.035)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.1323 (0.0908)	Prec@1 95.312 (97.005)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.1033 (0.0907)	Prec@1 95.312 (96.995)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0489 (0.0893)	Prec@1 98.438 (97.073)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0685 (0.0894)	Prec@1 96.875 (97.089)	
Total train loss: 0.0893

 * Prec@1 91.520 Prec@5 99.540 Loss 0.3018
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.1605 (0.0911)	Prec@1 94.531 (97.085)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0578 (0.0912)	Prec@1 96.875 (96.940)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0923 (0.0916)	Prec@1 96.094 (96.925)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0794 (0.0912)	Prec@1 96.094 (96.943)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0436 (0.0901)	Prec@1 98.438 (96.973)	
Total train loss: 0.0900

 * Prec@1 91.450 Prec@5 99.540 Loss 0.3010
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.1343 (0.0850)	Prec@1 96.094 (97.145)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0748 (0.0866)	Prec@1 97.656 (97.175)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0337 (0.0888)	Prec@1 100.000 (97.065)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0793 (0.0887)	Prec@1 97.656 (97.040)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.0466 (0.0887)	Prec@1 99.219 (97.047)	
Total train loss: 0.0887

 * Prec@1 91.610 Prec@5 99.560 Loss 0.2979
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0854 (0.0863)	Prec@1 97.656 (97.165)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0616 (0.0877)	Prec@1 98.438 (97.070)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0483 (0.0864)	Prec@1 99.219 (97.185)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.1003 (0.0875)	Prec@1 98.438 (97.100)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.1346 (0.0874)	Prec@1 94.531 (97.137)	
Total train loss: 0.0874

 * Prec@1 91.680 Prec@5 99.540 Loss 0.3015
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.1132 (0.0855)	Prec@1 97.656 (97.326)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0507 (0.0855)	Prec@1 99.219 (97.321)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0800 (0.0871)	Prec@1 96.875 (97.199)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0324 (0.0863)	Prec@1 99.219 (97.180)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0635 (0.0868)	Prec@1 97.656 (97.159)	
Total train loss: 0.0868

 * Prec@1 91.670 Prec@5 99.560 Loss 0.2988
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.1255 (0.0918)	Prec@1 96.875 (96.825)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0997 (0.0901)	Prec@1 96.094 (96.945)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.1232 (0.0873)	Prec@1 95.312 (97.082)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0692 (0.0890)	Prec@1 98.438 (97.015)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.1345 (0.0888)	Prec@1 96.094 (97.053)	
Total train loss: 0.0888

 * Prec@1 91.570 Prec@5 99.530 Loss 0.3000
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.1273 (0.0846)	Prec@1 96.094 (97.206)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.1174 (0.0884)	Prec@1 96.875 (97.085)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.1218 (0.0880)	Prec@1 95.312 (97.109)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0699 (0.0893)	Prec@1 96.094 (97.115)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0983 (0.0894)	Prec@1 95.312 (97.093)	
Total train loss: 0.0894

 * Prec@1 91.370 Prec@5 99.530 Loss 0.3003
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.1580 (0.0896)	Prec@1 92.188 (97.025)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0931 (0.0877)	Prec@1 96.875 (97.060)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.1539 (0.0886)	Prec@1 94.531 (97.069)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0889 (0.0884)	Prec@1 96.875 (97.068)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.0844 (0.0882)	Prec@1 97.656 (97.057)	
Total train loss: 0.0883

 * Prec@1 91.530 Prec@5 99.550 Loss 0.3020
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0777 (0.0830)	Prec@1 97.656 (97.396)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0650 (0.0858)	Prec@1 98.438 (97.231)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0743 (0.0877)	Prec@1 98.438 (97.112)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0856 (0.0878)	Prec@1 97.656 (97.128)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0472 (0.0874)	Prec@1 99.219 (97.145)	
Total train loss: 0.0877

 * Prec@1 91.640 Prec@5 99.530 Loss 0.2996
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0798 (0.0886)	Prec@1 96.875 (97.025)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0673 (0.0892)	Prec@1 98.438 (97.010)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0997 (0.0895)	Prec@1 95.312 (97.009)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0709 (0.0896)	Prec@1 98.438 (97.033)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0583 (0.0872)	Prec@1 98.438 (97.137)	
Total train loss: 0.0873

 * Prec@1 91.340 Prec@5 99.560 Loss 0.3066
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0824 (0.0883)	Prec@1 97.656 (97.165)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.1119 (0.0847)	Prec@1 96.094 (97.241)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0915 (0.0850)	Prec@1 96.094 (97.209)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0758 (0.0853)	Prec@1 96.875 (97.211)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0982 (0.0855)	Prec@1 96.094 (97.220)	
Total train loss: 0.0855

 * Prec@1 91.440 Prec@5 99.550 Loss 0.3003
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0876 (0.0908)	Prec@1 98.438 (97.115)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.1137 (0.0860)	Prec@1 94.531 (97.231)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.0687 (0.0859)	Prec@1 96.094 (97.212)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0626 (0.0860)	Prec@1 98.438 (97.180)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0869 (0.0859)	Prec@1 96.875 (97.196)	
Total train loss: 0.0858

 * Prec@1 91.590 Prec@5 99.560 Loss 0.2988
Best acc: 91.710
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.1362 (0.0874)	Prec@1 94.531 (97.145)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0967 (0.0895)	Prec@1 97.656 (97.070)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0851 (0.0902)	Prec@1 98.438 (97.052)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.1410 (0.0900)	Prec@1 97.656 (97.050)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0992 (0.0890)	Prec@1 95.312 (97.093)	
Total train loss: 0.0892

 * Prec@1 91.750 Prec@5 99.560 Loss 0.2974
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0949 (0.0880)	Prec@1 94.531 (97.065)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.1066 (0.0859)	Prec@1 96.094 (97.140)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0402 (0.0869)	Prec@1 99.219 (97.069)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.0957 (0.0864)	Prec@1 96.094 (97.095)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0755 (0.0875)	Prec@1 97.656 (97.043)	
Total train loss: 0.0876

 * Prec@1 91.390 Prec@5 99.550 Loss 0.3035
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.1218 (0.0896)	Prec@1 94.531 (96.995)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0514 (0.0877)	Prec@1 96.875 (97.075)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0490 (0.0879)	Prec@1 98.438 (97.085)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0834 (0.0879)	Prec@1 99.219 (97.113)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0535 (0.0868)	Prec@1 99.219 (97.155)	
Total train loss: 0.0867

 * Prec@1 91.420 Prec@5 99.560 Loss 0.3020
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.1438 (0.0916)	Prec@1 96.875 (96.895)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0428 (0.0915)	Prec@1 99.219 (96.960)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0520 (0.0886)	Prec@1 98.438 (97.072)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0532 (0.0863)	Prec@1 98.438 (97.145)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0601 (0.0861)	Prec@1 97.656 (97.157)	
Total train loss: 0.0861

 * Prec@1 91.490 Prec@5 99.540 Loss 0.3010
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0759 (0.0870)	Prec@1 98.438 (97.226)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.1167 (0.0868)	Prec@1 95.312 (97.256)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.1633 (0.0887)	Prec@1 94.531 (97.155)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.1454 (0.0889)	Prec@1 93.750 (97.133)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0464 (0.0885)	Prec@1 97.656 (97.117)	
Total train loss: 0.0885

 * Prec@1 91.590 Prec@5 99.560 Loss 0.3013
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0710 (0.0937)	Prec@1 99.219 (96.875)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0746 (0.0905)	Prec@1 98.438 (96.935)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.1451 (0.0908)	Prec@1 96.875 (96.952)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0769 (0.0907)	Prec@1 98.438 (96.993)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0490 (0.0897)	Prec@1 99.219 (97.097)	
Total train loss: 0.0897

 * Prec@1 91.600 Prec@5 99.550 Loss 0.3008
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.1089 (0.0925)	Prec@1 93.750 (96.795)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.1211 (0.0906)	Prec@1 94.531 (96.880)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0916 (0.0886)	Prec@1 97.656 (96.995)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0958 (0.0887)	Prec@1 96.875 (97.020)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0845 (0.0881)	Prec@1 96.875 (97.061)	
Total train loss: 0.0882

 * Prec@1 91.650 Prec@5 99.580 Loss 0.2986
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0908 (0.0835)	Prec@1 97.656 (97.256)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0933 (0.0848)	Prec@1 96.094 (97.216)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0538 (0.0839)	Prec@1 98.438 (97.239)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0856 (0.0848)	Prec@1 97.656 (97.221)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0891 (0.0856)	Prec@1 97.656 (97.173)	
Total train loss: 0.0856

 * Prec@1 91.520 Prec@5 99.540 Loss 0.3005
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.1045 (0.0904)	Prec@1 97.656 (97.045)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0745 (0.0913)	Prec@1 97.656 (96.925)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0827 (0.0866)	Prec@1 96.094 (97.132)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0634 (0.0872)	Prec@1 97.656 (97.105)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0814 (0.0873)	Prec@1 96.875 (97.065)	
Total train loss: 0.0873

 * Prec@1 91.550 Prec@5 99.560 Loss 0.3030
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.1213 (0.0850)	Prec@1 96.094 (97.095)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.0872 (0.0908)	Prec@1 96.094 (96.900)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.2155 (0.0907)	Prec@1 91.406 (96.962)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.1266 (0.0890)	Prec@1 95.312 (96.993)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0756 (0.0883)	Prec@1 98.438 (97.029)	
Total train loss: 0.0883

 * Prec@1 91.500 Prec@5 99.550 Loss 0.3010
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.1060 (0.0875)	Prec@1 96.875 (97.306)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.1073 (0.0874)	Prec@1 95.312 (97.221)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0665 (0.0891)	Prec@1 100.000 (97.085)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0359 (0.0879)	Prec@1 98.438 (97.158)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0984 (0.0878)	Prec@1 96.875 (97.141)	
Total train loss: 0.0878

 * Prec@1 91.530 Prec@5 99.550 Loss 0.2993
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0533 (0.0852)	Prec@1 99.219 (96.975)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.1528 (0.0865)	Prec@1 92.969 (97.055)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0517 (0.0874)	Prec@1 98.438 (97.092)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0692 (0.0874)	Prec@1 97.656 (97.113)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0709 (0.0875)	Prec@1 97.656 (97.105)	
Total train loss: 0.0877

 * Prec@1 91.560 Prec@5 99.540 Loss 0.2993
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.1217 (0.0870)	Prec@1 95.312 (97.125)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0973 (0.0867)	Prec@1 96.875 (97.105)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.1071 (0.0879)	Prec@1 98.438 (97.079)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.1149 (0.0886)	Prec@1 95.312 (97.065)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0612 (0.0874)	Prec@1 98.438 (97.129)	
Total train loss: 0.0874

 * Prec@1 91.500 Prec@5 99.550 Loss 0.3020
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0834 (0.0872)	Prec@1 96.875 (96.965)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0710 (0.0834)	Prec@1 97.656 (97.236)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0851 (0.0819)	Prec@1 96.875 (97.322)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0856 (0.0847)	Prec@1 97.656 (97.246)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.2260 (0.0859)	Prec@1 92.969 (97.192)	
Total train loss: 0.0859

 * Prec@1 91.560 Prec@5 99.550 Loss 0.3003
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0851 (0.0838)	Prec@1 97.656 (97.246)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0816 (0.0843)	Prec@1 96.094 (97.201)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0835 (0.0864)	Prec@1 97.656 (97.162)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.1129 (0.0871)	Prec@1 96.094 (97.105)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.1628 (0.0871)	Prec@1 92.188 (97.091)	
Total train loss: 0.0871

 * Prec@1 91.550 Prec@5 99.560 Loss 0.3013
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0737 (0.0937)	Prec@1 97.656 (96.915)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.1268 (0.0899)	Prec@1 94.531 (97.135)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0843 (0.0903)	Prec@1 96.875 (97.119)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0792 (0.0885)	Prec@1 98.438 (97.175)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0404 (0.0883)	Prec@1 99.219 (97.167)	
Total train loss: 0.0884

 * Prec@1 91.620 Prec@5 99.560 Loss 0.3008
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0915 (0.0876)	Prec@1 96.875 (97.055)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0833 (0.0894)	Prec@1 97.656 (97.010)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.0849 (0.0884)	Prec@1 96.094 (97.075)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0760 (0.0873)	Prec@1 97.656 (97.083)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0318 (0.0877)	Prec@1 99.219 (97.081)	
Total train loss: 0.0877

 * Prec@1 91.660 Prec@5 99.550 Loss 0.2998
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0707 (0.0818)	Prec@1 97.656 (97.466)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0696 (0.0863)	Prec@1 97.656 (97.236)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0551 (0.0868)	Prec@1 98.438 (97.212)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0462 (0.0867)	Prec@1 98.438 (97.168)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0651 (0.0868)	Prec@1 97.656 (97.149)	
Total train loss: 0.0869

 * Prec@1 91.560 Prec@5 99.580 Loss 0.2981
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0837 (0.0905)	Prec@1 97.656 (97.135)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0578 (0.0865)	Prec@1 97.656 (97.206)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0862 (0.0840)	Prec@1 98.438 (97.272)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0833 (0.0871)	Prec@1 95.312 (97.150)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.1017 (0.0867)	Prec@1 96.875 (97.171)	
Total train loss: 0.0868

 * Prec@1 91.630 Prec@5 99.530 Loss 0.3027
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0401 (0.0851)	Prec@1 100.000 (97.356)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0941 (0.0885)	Prec@1 95.312 (97.226)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.0831 (0.0881)	Prec@1 97.656 (97.212)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.1028 (0.0867)	Prec@1 96.875 (97.236)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.1296 (0.0873)	Prec@1 96.094 (97.214)	
Total train loss: 0.0874

 * Prec@1 91.650 Prec@5 99.540 Loss 0.3008
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0781 (0.0888)	Prec@1 97.656 (97.085)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.1285 (0.0889)	Prec@1 96.875 (97.085)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.1379 (0.0879)	Prec@1 95.312 (97.129)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.1062 (0.0883)	Prec@1 94.531 (97.093)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0732 (0.0878)	Prec@1 97.656 (97.133)	
Total train loss: 0.0877

 * Prec@1 91.470 Prec@5 99.560 Loss 0.2996
Best acc: 91.750
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0500 (0.0891)	Prec@1 99.219 (97.015)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.1521 (0.0879)	Prec@1 93.750 (97.105)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0815 (0.0877)	Prec@1 97.656 (97.109)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0555 (0.0873)	Prec@1 98.438 (97.140)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0510 (0.0862)	Prec@1 99.219 (97.196)	
Total train loss: 0.0862

 * Prec@1 91.650 Prec@5 99.560 Loss 0.2969
Best acc: 91.750
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 90.780 Prec@5 99.470 Loss 0.3716
Pre-trained Prec@1 with 3 layers frozen: 90.77999877929688 	 Loss: 0.37158203125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.0383 (0.0455)	Prec@1 98.438 (98.648)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.0516 (0.0487)	Prec@1 98.438 (98.508)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.0902 (0.0499)	Prec@1 96.094 (98.434)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.0168 (0.0500)	Prec@1 100.000 (98.432)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.0204 (0.0496)	Prec@1 100.000 (98.456)	
Total train loss: 0.0496

 * Prec@1 91.200 Prec@5 99.440 Loss 0.3491
Best acc: 91.200
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.0320 (0.0493)	Prec@1 100.000 (98.307)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.0385 (0.0490)	Prec@1 99.219 (98.357)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.0283 (0.0490)	Prec@1 99.219 (98.381)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.0607 (0.0486)	Prec@1 97.656 (98.400)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.0366 (0.0484)	Prec@1 98.438 (98.421)	
Total train loss: 0.0485

 * Prec@1 91.340 Prec@5 99.480 Loss 0.3445
Best acc: 91.340
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.0277 (0.0427)	Prec@1 100.000 (98.598)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.0381 (0.0438)	Prec@1 98.438 (98.608)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.0822 (0.0447)	Prec@1 98.438 (98.618)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.0510 (0.0451)	Prec@1 97.656 (98.573)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0309 (0.0451)	Prec@1 99.219 (98.574)	
Total train loss: 0.0452

 * Prec@1 91.280 Prec@5 99.490 Loss 0.3499
Best acc: 91.340
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0243 (0.0453)	Prec@1 99.219 (98.648)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.0261 (0.0463)	Prec@1 100.000 (98.548)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.0404 (0.0464)	Prec@1 98.438 (98.518)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0356 (0.0454)	Prec@1 99.219 (98.565)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.0270 (0.0447)	Prec@1 99.219 (98.616)	
Total train loss: 0.0447

 * Prec@1 91.300 Prec@5 99.480 Loss 0.3499
Best acc: 91.340
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.0253 (0.0441)	Prec@1 99.219 (98.698)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.0284 (0.0430)	Prec@1 100.000 (98.718)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.0382 (0.0437)	Prec@1 99.219 (98.648)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.0453 (0.0430)	Prec@1 98.438 (98.678)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.0201 (0.0430)	Prec@1 100.000 (98.692)	
Total train loss: 0.0430

 * Prec@1 91.390 Prec@5 99.490 Loss 0.3479
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0247 (0.0430)	Prec@1 98.438 (98.578)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0280 (0.0433)	Prec@1 98.438 (98.593)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0779 (0.0424)	Prec@1 97.656 (98.678)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0444 (0.0424)	Prec@1 99.219 (98.695)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0324 (0.0427)	Prec@1 98.438 (98.708)	
Total train loss: 0.0428

 * Prec@1 91.280 Prec@5 99.470 Loss 0.3455
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0389 (0.0377)	Prec@1 100.000 (98.998)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0341 (0.0383)	Prec@1 99.219 (98.948)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0577 (0.0408)	Prec@1 96.875 (98.815)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0295 (0.0403)	Prec@1 99.219 (98.823)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0547 (0.0403)	Prec@1 99.219 (98.826)	
Total train loss: 0.0403

 * Prec@1 91.120 Prec@5 99.450 Loss 0.3479
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0573 (0.0373)	Prec@1 98.438 (98.858)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0311 (0.0398)	Prec@1 100.000 (98.703)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0711 (0.0403)	Prec@1 96.875 (98.751)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0427 (0.0411)	Prec@1 99.219 (98.740)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0966 (0.0412)	Prec@1 95.312 (98.732)	
Total train loss: 0.0412

 * Prec@1 91.280 Prec@5 99.440 Loss 0.3445
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0397 (0.0378)	Prec@1 99.219 (98.988)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0579 (0.0396)	Prec@1 97.656 (98.818)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0338 (0.0397)	Prec@1 99.219 (98.798)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0276 (0.0404)	Prec@1 100.000 (98.758)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0290 (0.0408)	Prec@1 99.219 (98.734)	
Total train loss: 0.0408

 * Prec@1 91.210 Prec@5 99.490 Loss 0.3457
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0361 (0.0379)	Prec@1 99.219 (98.928)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0358 (0.0387)	Prec@1 98.438 (98.858)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.1029 (0.0394)	Prec@1 97.656 (98.801)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0672 (0.0390)	Prec@1 96.875 (98.838)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0404 (0.0393)	Prec@1 98.438 (98.824)	
Total train loss: 0.0392

 * Prec@1 91.270 Prec@5 99.440 Loss 0.3389
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0737 (0.0429)	Prec@1 98.438 (98.748)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0817 (0.0410)	Prec@1 96.875 (98.793)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0302 (0.0398)	Prec@1 99.219 (98.828)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0690 (0.0402)	Prec@1 96.875 (98.778)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0419 (0.0397)	Prec@1 97.656 (98.782)	
Total train loss: 0.0397

 * Prec@1 91.280 Prec@5 99.460 Loss 0.3425
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0502 (0.0400)	Prec@1 98.438 (98.878)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0508 (0.0392)	Prec@1 97.656 (98.883)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0203 (0.0384)	Prec@1 99.219 (98.925)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0528 (0.0389)	Prec@1 98.438 (98.906)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0283 (0.0391)	Prec@1 99.219 (98.892)	
Total train loss: 0.0391

 * Prec@1 91.310 Prec@5 99.410 Loss 0.3455
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0724 (0.0386)	Prec@1 98.438 (98.988)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0778 (0.0392)	Prec@1 97.656 (98.913)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0127 (0.0398)	Prec@1 100.000 (98.858)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0308 (0.0397)	Prec@1 99.219 (98.838)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0521 (0.0392)	Prec@1 97.656 (98.880)	
Total train loss: 0.0392

 * Prec@1 91.310 Prec@5 99.460 Loss 0.3455
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0375 (0.0394)	Prec@1 98.438 (98.908)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0343 (0.0384)	Prec@1 100.000 (98.903)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0591 (0.0385)	Prec@1 98.438 (98.902)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0379 (0.0385)	Prec@1 98.438 (98.873)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0338 (0.0387)	Prec@1 100.000 (98.864)	
Total train loss: 0.0387

 * Prec@1 91.330 Prec@5 99.450 Loss 0.3428
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0681 (0.0433)	Prec@1 98.438 (98.648)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0426 (0.0407)	Prec@1 97.656 (98.798)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0461 (0.0402)	Prec@1 97.656 (98.788)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0340 (0.0398)	Prec@1 99.219 (98.818)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0419 (0.0395)	Prec@1 99.219 (98.842)	
Total train loss: 0.0395

 * Prec@1 91.210 Prec@5 99.450 Loss 0.3491
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.0230 (0.0392)	Prec@1 100.000 (98.868)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0536 (0.0388)	Prec@1 97.656 (98.878)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0285 (0.0408)	Prec@1 99.219 (98.781)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0340 (0.0393)	Prec@1 99.219 (98.846)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0662 (0.0384)	Prec@1 97.656 (98.896)	
Total train loss: 0.0384

 * Prec@1 91.400 Prec@5 99.440 Loss 0.3425
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0635 (0.0366)	Prec@1 97.656 (98.968)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.0587 (0.0385)	Prec@1 97.656 (98.873)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.0410 (0.0388)	Prec@1 98.438 (98.872)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0371 (0.0395)	Prec@1 98.438 (98.828)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0180 (0.0399)	Prec@1 100.000 (98.824)	
Total train loss: 0.0399

 * Prec@1 91.150 Prec@5 99.470 Loss 0.3479
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0304 (0.0377)	Prec@1 99.219 (98.868)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0421 (0.0383)	Prec@1 97.656 (98.873)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0334 (0.0380)	Prec@1 99.219 (98.875)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0368 (0.0379)	Prec@1 99.219 (98.911)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0351 (0.0380)	Prec@1 98.438 (98.892)	
Total train loss: 0.0380

 * Prec@1 91.320 Prec@5 99.430 Loss 0.3474
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0128 (0.0409)	Prec@1 100.000 (98.718)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0774 (0.0388)	Prec@1 97.656 (98.818)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0771 (0.0375)	Prec@1 96.875 (98.908)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0192 (0.0381)	Prec@1 99.219 (98.921)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.0722 (0.0385)	Prec@1 96.094 (98.902)	
Total train loss: 0.0385

 * Prec@1 91.260 Prec@5 99.440 Loss 0.3462
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0699 (0.0376)	Prec@1 96.875 (98.888)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0577 (0.0385)	Prec@1 98.438 (98.838)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0306 (0.0380)	Prec@1 100.000 (98.865)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0230 (0.0384)	Prec@1 99.219 (98.853)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.0436 (0.0389)	Prec@1 99.219 (98.842)	
Total train loss: 0.0389

 * Prec@1 91.380 Prec@5 99.450 Loss 0.3428
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0440 (0.0437)	Prec@1 98.438 (98.708)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0370 (0.0423)	Prec@1 100.000 (98.743)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0390 (0.0399)	Prec@1 98.438 (98.855)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0208 (0.0384)	Prec@1 100.000 (98.921)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0318 (0.0385)	Prec@1 99.219 (98.892)	
Total train loss: 0.0385

 * Prec@1 91.400 Prec@5 99.430 Loss 0.3418
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.0212 (0.0391)	Prec@1 99.219 (98.948)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0298 (0.0373)	Prec@1 99.219 (99.018)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0261 (0.0371)	Prec@1 100.000 (99.022)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0673 (0.0373)	Prec@1 98.438 (99.001)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0714 (0.0378)	Prec@1 98.438 (98.974)	
Total train loss: 0.0378

 * Prec@1 91.300 Prec@5 99.480 Loss 0.3445
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0223 (0.0374)	Prec@1 100.000 (98.918)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.0648 (0.0379)	Prec@1 97.656 (98.903)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0478 (0.0382)	Prec@1 99.219 (98.882)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0453 (0.0390)	Prec@1 98.438 (98.841)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0206 (0.0389)	Prec@1 100.000 (98.844)	
Total train loss: 0.0389

 * Prec@1 91.250 Prec@5 99.430 Loss 0.3484
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0427 (0.0383)	Prec@1 98.438 (98.878)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0366 (0.0397)	Prec@1 99.219 (98.848)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0268 (0.0392)	Prec@1 99.219 (98.851)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0264 (0.0395)	Prec@1 100.000 (98.828)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.0345 (0.0395)	Prec@1 99.219 (98.848)	
Total train loss: 0.0395

 * Prec@1 91.360 Prec@5 99.440 Loss 0.3416
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0575 (0.0394)	Prec@1 96.875 (98.758)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0724 (0.0404)	Prec@1 97.656 (98.788)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0455 (0.0399)	Prec@1 99.219 (98.801)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0652 (0.0394)	Prec@1 97.656 (98.833)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0188 (0.0392)	Prec@1 100.000 (98.854)	
Total train loss: 0.0392

 * Prec@1 91.310 Prec@5 99.460 Loss 0.3416
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0209 (0.0403)	Prec@1 99.219 (98.798)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0412 (0.0414)	Prec@1 98.438 (98.763)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0692 (0.0405)	Prec@1 98.438 (98.818)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0359 (0.0394)	Prec@1 98.438 (98.861)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0458 (0.0394)	Prec@1 99.219 (98.856)	
Total train loss: 0.0395

 * Prec@1 91.120 Prec@5 99.450 Loss 0.3477
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0242 (0.0392)	Prec@1 99.219 (98.888)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0228 (0.0393)	Prec@1 99.219 (98.873)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0433 (0.0389)	Prec@1 97.656 (98.922)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0384 (0.0382)	Prec@1 98.438 (98.923)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0822 (0.0383)	Prec@1 96.094 (98.924)	
Total train loss: 0.0384

 * Prec@1 91.210 Prec@5 99.460 Loss 0.3462
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0583 (0.0380)	Prec@1 98.438 (98.918)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.0245 (0.0395)	Prec@1 100.000 (98.888)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.0582 (0.0398)	Prec@1 98.438 (98.825)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0190 (0.0385)	Prec@1 99.219 (98.886)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0356 (0.0389)	Prec@1 99.219 (98.856)	
Total train loss: 0.0389

 * Prec@1 91.270 Prec@5 99.420 Loss 0.3423
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0488 (0.0396)	Prec@1 97.656 (98.778)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0185 (0.0395)	Prec@1 99.219 (98.868)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0775 (0.0387)	Prec@1 98.438 (98.908)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0138 (0.0380)	Prec@1 100.000 (98.946)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0353 (0.0375)	Prec@1 100.000 (98.966)	
Total train loss: 0.0375

 * Prec@1 91.280 Prec@5 99.430 Loss 0.3433
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0389 (0.0381)	Prec@1 98.438 (98.868)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0224 (0.0380)	Prec@1 100.000 (98.958)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0564 (0.0393)	Prec@1 98.438 (98.858)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.0486 (0.0389)	Prec@1 99.219 (98.856)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0469 (0.0386)	Prec@1 99.219 (98.862)	
Total train loss: 0.0387

 * Prec@1 91.190 Prec@5 99.460 Loss 0.3406
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0402 (0.0343)	Prec@1 99.219 (99.058)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0515 (0.0373)	Prec@1 97.656 (98.903)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0169 (0.0371)	Prec@1 100.000 (98.922)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0327 (0.0379)	Prec@1 100.000 (98.878)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0307 (0.0380)	Prec@1 99.219 (98.868)	
Total train loss: 0.0382

 * Prec@1 91.360 Prec@5 99.470 Loss 0.3450
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0436 (0.0379)	Prec@1 98.438 (98.808)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0257 (0.0382)	Prec@1 99.219 (98.843)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0555 (0.0380)	Prec@1 98.438 (98.851)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0382 (0.0377)	Prec@1 98.438 (98.878)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0244 (0.0378)	Prec@1 100.000 (98.890)	
Total train loss: 0.0378

 * Prec@1 91.180 Prec@5 99.450 Loss 0.3438
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0192 (0.0347)	Prec@1 100.000 (99.169)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0517 (0.0369)	Prec@1 98.438 (99.003)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0282 (0.0373)	Prec@1 98.438 (98.955)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0616 (0.0380)	Prec@1 98.438 (98.896)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0258 (0.0384)	Prec@1 100.000 (98.892)	
Total train loss: 0.0383

 * Prec@1 91.310 Prec@5 99.450 Loss 0.3428
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0607 (0.0369)	Prec@1 97.656 (98.928)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0207 (0.0367)	Prec@1 100.000 (98.968)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0489 (0.0373)	Prec@1 98.438 (98.958)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0530 (0.0378)	Prec@1 99.219 (98.938)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0361 (0.0385)	Prec@1 99.219 (98.916)	
Total train loss: 0.0385

 * Prec@1 91.250 Prec@5 99.430 Loss 0.3442
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0494 (0.0397)	Prec@1 99.219 (98.868)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0703 (0.0390)	Prec@1 96.094 (98.888)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0643 (0.0386)	Prec@1 98.438 (98.915)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0645 (0.0387)	Prec@1 96.875 (98.886)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0331 (0.0392)	Prec@1 99.219 (98.848)	
Total train loss: 0.0393

 * Prec@1 91.400 Prec@5 99.450 Loss 0.3477
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0316 (0.0355)	Prec@1 99.219 (99.008)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0435 (0.0359)	Prec@1 98.438 (98.953)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0181 (0.0361)	Prec@1 100.000 (98.998)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.1064 (0.0369)	Prec@1 94.531 (98.936)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0315 (0.0374)	Prec@1 100.000 (98.904)	
Total train loss: 0.0376

 * Prec@1 91.310 Prec@5 99.460 Loss 0.3403
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0239 (0.0377)	Prec@1 100.000 (98.848)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0323 (0.0383)	Prec@1 99.219 (98.913)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0546 (0.0380)	Prec@1 96.875 (98.872)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0885 (0.0373)	Prec@1 96.094 (98.928)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0223 (0.0380)	Prec@1 100.000 (98.912)	
Total train loss: 0.0380

 * Prec@1 91.230 Prec@5 99.490 Loss 0.3467
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.0373 (0.0369)	Prec@1 98.438 (98.918)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.0537 (0.0381)	Prec@1 97.656 (98.883)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.0634 (0.0384)	Prec@1 97.656 (98.892)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0906 (0.0383)	Prec@1 96.094 (98.878)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0200 (0.0380)	Prec@1 100.000 (98.904)	
Total train loss: 0.0380

 * Prec@1 91.390 Prec@5 99.460 Loss 0.3423
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0518 (0.0392)	Prec@1 98.438 (98.748)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0294 (0.0396)	Prec@1 99.219 (98.848)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0764 (0.0399)	Prec@1 97.656 (98.831)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0183 (0.0389)	Prec@1 99.219 (98.858)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0316 (0.0382)	Prec@1 98.438 (98.888)	
Total train loss: 0.0382

 * Prec@1 91.320 Prec@5 99.410 Loss 0.3440
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0256 (0.0391)	Prec@1 99.219 (98.778)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.0257 (0.0378)	Prec@1 99.219 (98.828)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0268 (0.0388)	Prec@1 99.219 (98.828)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0597 (0.0386)	Prec@1 97.656 (98.851)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0214 (0.0385)	Prec@1 100.000 (98.878)	
Total train loss: 0.0385

 * Prec@1 91.380 Prec@5 99.470 Loss 0.3423
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0146 (0.0406)	Prec@1 100.000 (98.898)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0303 (0.0390)	Prec@1 99.219 (98.928)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.0250 (0.0385)	Prec@1 100.000 (98.925)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0381 (0.0387)	Prec@1 98.438 (98.908)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0231 (0.0381)	Prec@1 100.000 (98.922)	
Total train loss: 0.0381

 * Prec@1 91.410 Prec@5 99.440 Loss 0.3428
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0317 (0.0395)	Prec@1 100.000 (98.898)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0278 (0.0389)	Prec@1 98.438 (98.873)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0214 (0.0392)	Prec@1 100.000 (98.858)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0452 (0.0390)	Prec@1 99.219 (98.881)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.0265 (0.0385)	Prec@1 100.000 (98.908)	
Total train loss: 0.0385

 * Prec@1 91.280 Prec@5 99.470 Loss 0.3474
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0325 (0.0375)	Prec@1 99.219 (98.888)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0242 (0.0386)	Prec@1 99.219 (98.828)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0403 (0.0403)	Prec@1 98.438 (98.795)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0164 (0.0392)	Prec@1 100.000 (98.826)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0424 (0.0386)	Prec@1 99.219 (98.844)	
Total train loss: 0.0386

 * Prec@1 91.350 Prec@5 99.440 Loss 0.3438
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0342 (0.0380)	Prec@1 99.219 (98.948)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0219 (0.0407)	Prec@1 100.000 (98.848)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0362 (0.0398)	Prec@1 98.438 (98.895)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0299 (0.0392)	Prec@1 99.219 (98.878)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0298 (0.0385)	Prec@1 100.000 (98.888)	
Total train loss: 0.0385

 * Prec@1 91.210 Prec@5 99.480 Loss 0.3447
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0507 (0.0436)	Prec@1 99.219 (98.698)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0377 (0.0410)	Prec@1 99.219 (98.793)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.0770 (0.0411)	Prec@1 98.438 (98.791)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0734 (0.0401)	Prec@1 97.656 (98.826)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0444 (0.0394)	Prec@1 97.656 (98.846)	
Total train loss: 0.0395

 * Prec@1 91.240 Prec@5 99.490 Loss 0.3472
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0280 (0.0393)	Prec@1 99.219 (98.908)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0436 (0.0381)	Prec@1 98.438 (98.958)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0342 (0.0384)	Prec@1 99.219 (98.925)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0144 (0.0383)	Prec@1 100.000 (98.901)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0444 (0.0379)	Prec@1 99.219 (98.908)	
Total train loss: 0.0379

 * Prec@1 91.280 Prec@5 99.440 Loss 0.3459
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0197 (0.0372)	Prec@1 100.000 (98.938)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0289 (0.0374)	Prec@1 99.219 (98.928)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0286 (0.0385)	Prec@1 100.000 (98.888)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0247 (0.0382)	Prec@1 99.219 (98.911)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.0235 (0.0380)	Prec@1 99.219 (98.942)	
Total train loss: 0.0380

 * Prec@1 91.160 Prec@5 99.470 Loss 0.3428
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0569 (0.0362)	Prec@1 98.438 (98.818)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0391 (0.0385)	Prec@1 98.438 (98.798)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.0123 (0.0393)	Prec@1 100.000 (98.805)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0458 (0.0388)	Prec@1 99.219 (98.816)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0181 (0.0392)	Prec@1 100.000 (98.812)	
Total train loss: 0.0393

 * Prec@1 91.240 Prec@5 99.460 Loss 0.3467
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0371 (0.0412)	Prec@1 98.438 (98.728)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0263 (0.0385)	Prec@1 100.000 (98.848)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0323 (0.0391)	Prec@1 99.219 (98.835)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0486 (0.0393)	Prec@1 97.656 (98.846)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0202 (0.0385)	Prec@1 100.000 (98.890)	
Total train loss: 0.0384

 * Prec@1 91.400 Prec@5 99.450 Loss 0.3452
Best acc: 91.410
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0137 (0.0429)	Prec@1 100.000 (98.778)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0193 (0.0409)	Prec@1 99.219 (98.828)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0246 (0.0406)	Prec@1 99.219 (98.818)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0148 (0.0394)	Prec@1 100.000 (98.883)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0489 (0.0393)	Prec@1 98.438 (98.868)	
Total train loss: 0.0393

 * Prec@1 91.270 Prec@5 99.470 Loss 0.3501
Best acc: 91.410
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 90.130 Prec@5 99.400 Loss 0.3926
Pre-trained Prec@1 with 5 layers frozen: 90.12999725341797 	 Loss: 0.392578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.0295 (0.0537)	Prec@1 99.219 (98.127)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.0331 (0.0542)	Prec@1 99.219 (98.202)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.0655 (0.0531)	Prec@1 97.656 (98.180)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.0575 (0.0531)	Prec@1 97.656 (98.217)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.0456 (0.0543)	Prec@1 98.438 (98.183)	
Total train loss: 0.0543

 * Prec@1 90.940 Prec@5 99.440 Loss 0.3552
Best acc: 90.940
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.0485 (0.0490)	Prec@1 98.438 (98.377)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.0393 (0.0489)	Prec@1 99.219 (98.412)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.0470 (0.0492)	Prec@1 99.219 (98.394)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.0720 (0.0493)	Prec@1 97.656 (98.420)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.0314 (0.0498)	Prec@1 98.438 (98.407)	
Total train loss: 0.0499

 * Prec@1 91.260 Prec@5 99.450 Loss 0.3525
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.0297 (0.0449)	Prec@1 99.219 (98.588)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.0128 (0.0479)	Prec@1 100.000 (98.478)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.0346 (0.0480)	Prec@1 99.219 (98.438)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.1492 (0.0490)	Prec@1 95.312 (98.422)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0760 (0.0491)	Prec@1 96.875 (98.442)	
Total train loss: 0.0491

 * Prec@1 91.250 Prec@5 99.480 Loss 0.3491
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0728 (0.0507)	Prec@1 97.656 (98.448)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.0238 (0.0490)	Prec@1 98.438 (98.503)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.0344 (0.0487)	Prec@1 99.219 (98.468)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0492 (0.0486)	Prec@1 99.219 (98.480)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.0449 (0.0474)	Prec@1 97.656 (98.518)	
Total train loss: 0.0474

 * Prec@1 91.370 Prec@5 99.460 Loss 0.3499
Best acc: 91.370
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.0734 (0.0457)	Prec@1 98.438 (98.668)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.0854 (0.0461)	Prec@1 97.656 (98.543)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.0250 (0.0446)	Prec@1 100.000 (98.621)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.0742 (0.0463)	Prec@1 98.438 (98.543)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.0406 (0.0462)	Prec@1 99.219 (98.550)	
Total train loss: 0.0462

 * Prec@1 91.330 Prec@5 99.500 Loss 0.3491
Best acc: 91.370
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0880 (0.0479)	Prec@1 96.875 (98.588)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0537 (0.0453)	Prec@1 98.438 (98.678)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0349 (0.0455)	Prec@1 97.656 (98.661)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0390 (0.0449)	Prec@1 98.438 (98.665)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0251 (0.0456)	Prec@1 99.219 (98.640)	
Total train loss: 0.0456

 * Prec@1 91.310 Prec@5 99.500 Loss 0.3472
Best acc: 91.370
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0623 (0.0456)	Prec@1 97.656 (98.588)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0617 (0.0445)	Prec@1 96.875 (98.618)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0252 (0.0443)	Prec@1 100.000 (98.644)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0324 (0.0442)	Prec@1 99.219 (98.635)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0358 (0.0441)	Prec@1 98.438 (98.648)	
Total train loss: 0.0441

 * Prec@1 91.350 Prec@5 99.480 Loss 0.3496
Best acc: 91.370
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0260 (0.0447)	Prec@1 99.219 (98.798)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0339 (0.0447)	Prec@1 100.000 (98.678)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0317 (0.0436)	Prec@1 99.219 (98.671)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.1038 (0.0433)	Prec@1 96.875 (98.678)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0601 (0.0429)	Prec@1 97.656 (98.706)	
Total train loss: 0.0430

 * Prec@1 91.400 Prec@5 99.410 Loss 0.3496
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0283 (0.0396)	Prec@1 98.438 (98.808)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0177 (0.0427)	Prec@1 100.000 (98.648)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0504 (0.0421)	Prec@1 98.438 (98.685)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0193 (0.0422)	Prec@1 100.000 (98.690)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0236 (0.0419)	Prec@1 100.000 (98.736)	
Total train loss: 0.0419

 * Prec@1 91.310 Prec@5 99.450 Loss 0.3525
Best acc: 91.400
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0347 (0.0403)	Prec@1 99.219 (98.798)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0228 (0.0417)	Prec@1 100.000 (98.733)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0312 (0.0414)	Prec@1 99.219 (98.745)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0305 (0.0408)	Prec@1 99.219 (98.753)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0564 (0.0407)	Prec@1 97.656 (98.770)	
Total train loss: 0.0408

 * Prec@1 91.460 Prec@5 99.470 Loss 0.3472
Best acc: 91.460
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0397 (0.0459)	Prec@1 99.219 (98.468)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0574 (0.0446)	Prec@1 98.438 (98.573)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0438 (0.0431)	Prec@1 98.438 (98.648)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0802 (0.0430)	Prec@1 97.656 (98.665)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0175 (0.0426)	Prec@1 100.000 (98.694)	
Total train loss: 0.0426

 * Prec@1 91.340 Prec@5 99.440 Loss 0.3464
Best acc: 91.460
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0229 (0.0403)	Prec@1 99.219 (98.778)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0521 (0.0408)	Prec@1 98.438 (98.753)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0354 (0.0411)	Prec@1 98.438 (98.755)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0453 (0.0407)	Prec@1 98.438 (98.773)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0317 (0.0406)	Prec@1 99.219 (98.770)	
Total train loss: 0.0406

 * Prec@1 91.410 Prec@5 99.460 Loss 0.3455
Best acc: 91.460
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0590 (0.0435)	Prec@1 98.438 (98.618)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0999 (0.0424)	Prec@1 97.656 (98.738)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0439 (0.0427)	Prec@1 98.438 (98.731)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0406 (0.0422)	Prec@1 98.438 (98.756)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0148 (0.0417)	Prec@1 100.000 (98.786)	
Total train loss: 0.0418

 * Prec@1 91.260 Prec@5 99.480 Loss 0.3501
Best acc: 91.460
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0248 (0.0435)	Prec@1 100.000 (98.618)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0602 (0.0438)	Prec@1 98.438 (98.628)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0506 (0.0431)	Prec@1 99.219 (98.668)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0217 (0.0423)	Prec@1 100.000 (98.738)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0272 (0.0427)	Prec@1 100.000 (98.702)	
Total train loss: 0.0427

 * Prec@1 91.510 Prec@5 99.480 Loss 0.3494
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0219 (0.0413)	Prec@1 99.219 (98.728)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0367 (0.0424)	Prec@1 98.438 (98.623)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0124 (0.0415)	Prec@1 100.000 (98.725)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0864 (0.0423)	Prec@1 96.094 (98.708)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0180 (0.0419)	Prec@1 100.000 (98.706)	
Total train loss: 0.0419

 * Prec@1 91.470 Prec@5 99.470 Loss 0.3481
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.0280 (0.0366)	Prec@1 100.000 (99.048)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0415 (0.0395)	Prec@1 99.219 (98.848)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0182 (0.0404)	Prec@1 100.000 (98.801)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0388 (0.0410)	Prec@1 99.219 (98.783)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0448 (0.0412)	Prec@1 97.656 (98.760)	
Total train loss: 0.0412

 * Prec@1 91.400 Prec@5 99.450 Loss 0.3484
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0395 (0.0418)	Prec@1 99.219 (98.698)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.0559 (0.0420)	Prec@1 97.656 (98.723)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.0476 (0.0410)	Prec@1 98.438 (98.795)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0289 (0.0405)	Prec@1 98.438 (98.818)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0206 (0.0412)	Prec@1 100.000 (98.800)	
Total train loss: 0.0412

 * Prec@1 91.340 Prec@5 99.490 Loss 0.3479
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0717 (0.0459)	Prec@1 99.219 (98.638)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0264 (0.0421)	Prec@1 100.000 (98.778)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0179 (0.0411)	Prec@1 99.219 (98.815)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0337 (0.0400)	Prec@1 99.219 (98.828)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0086 (0.0409)	Prec@1 100.000 (98.788)	
Total train loss: 0.0410

 * Prec@1 91.460 Prec@5 99.420 Loss 0.3499
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0356 (0.0433)	Prec@1 100.000 (98.658)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0215 (0.0425)	Prec@1 100.000 (98.748)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0762 (0.0417)	Prec@1 97.656 (98.758)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0332 (0.0414)	Prec@1 99.219 (98.766)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.0235 (0.0412)	Prec@1 100.000 (98.772)	
Total train loss: 0.0411

 * Prec@1 91.240 Prec@5 99.480 Loss 0.3525
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0218 (0.0451)	Prec@1 100.000 (98.508)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0402 (0.0421)	Prec@1 98.438 (98.673)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0619 (0.0418)	Prec@1 97.656 (98.715)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0287 (0.0410)	Prec@1 99.219 (98.781)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.0166 (0.0410)	Prec@1 99.219 (98.762)	
Total train loss: 0.0410

 * Prec@1 91.450 Prec@5 99.420 Loss 0.3445
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0266 (0.0421)	Prec@1 100.000 (98.798)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0269 (0.0409)	Prec@1 100.000 (98.818)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0663 (0.0412)	Prec@1 97.656 (98.825)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0430 (0.0404)	Prec@1 98.438 (98.873)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0280 (0.0406)	Prec@1 100.000 (98.858)	
Total train loss: 0.0405

 * Prec@1 91.380 Prec@5 99.490 Loss 0.3481
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.0427 (0.0393)	Prec@1 99.219 (98.848)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0328 (0.0410)	Prec@1 99.219 (98.853)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0663 (0.0402)	Prec@1 98.438 (98.848)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0663 (0.0404)	Prec@1 97.656 (98.798)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0506 (0.0414)	Prec@1 98.438 (98.748)	
Total train loss: 0.0414

 * Prec@1 91.190 Prec@5 99.400 Loss 0.3479
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0165 (0.0417)	Prec@1 100.000 (98.718)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.0285 (0.0428)	Prec@1 99.219 (98.733)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0494 (0.0420)	Prec@1 98.438 (98.775)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0282 (0.0422)	Prec@1 99.219 (98.761)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0463 (0.0416)	Prec@1 99.219 (98.796)	
Total train loss: 0.0416

 * Prec@1 91.200 Prec@5 99.400 Loss 0.3486
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0282 (0.0412)	Prec@1 99.219 (98.788)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0440 (0.0398)	Prec@1 97.656 (98.808)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0340 (0.0385)	Prec@1 99.219 (98.858)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0383 (0.0393)	Prec@1 99.219 (98.816)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.1163 (0.0403)	Prec@1 94.531 (98.786)	
Total train loss: 0.0403

 * Prec@1 91.400 Prec@5 99.430 Loss 0.3508
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0340 (0.0427)	Prec@1 99.219 (98.628)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0515 (0.0417)	Prec@1 98.438 (98.698)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0587 (0.0420)	Prec@1 97.656 (98.705)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0440 (0.0413)	Prec@1 98.438 (98.771)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0294 (0.0410)	Prec@1 100.000 (98.782)	
Total train loss: 0.0410

 * Prec@1 91.260 Prec@5 99.430 Loss 0.3508
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0360 (0.0408)	Prec@1 98.438 (98.748)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0108 (0.0397)	Prec@1 100.000 (98.768)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0462 (0.0396)	Prec@1 100.000 (98.821)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0275 (0.0405)	Prec@1 99.219 (98.816)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0767 (0.0405)	Prec@1 96.875 (98.820)	
Total train loss: 0.0405

 * Prec@1 91.480 Prec@5 99.460 Loss 0.3472
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0499 (0.0400)	Prec@1 98.438 (98.758)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0491 (0.0411)	Prec@1 97.656 (98.778)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0542 (0.0412)	Prec@1 99.219 (98.765)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0380 (0.0399)	Prec@1 99.219 (98.836)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0319 (0.0408)	Prec@1 100.000 (98.794)	
Total train loss: 0.0408

 * Prec@1 91.420 Prec@5 99.450 Loss 0.3489
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0596 (0.0404)	Prec@1 98.438 (98.788)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.0634 (0.0416)	Prec@1 96.875 (98.768)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.0298 (0.0414)	Prec@1 99.219 (98.761)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0445 (0.0414)	Prec@1 98.438 (98.776)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0591 (0.0406)	Prec@1 98.438 (98.816)	
Total train loss: 0.0406

 * Prec@1 91.300 Prec@5 99.440 Loss 0.3459
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0193 (0.0423)	Prec@1 100.000 (98.728)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0391 (0.0402)	Prec@1 99.219 (98.813)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0812 (0.0396)	Prec@1 96.875 (98.851)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0516 (0.0394)	Prec@1 98.438 (98.851)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0334 (0.0401)	Prec@1 98.438 (98.790)	
Total train loss: 0.0401

 * Prec@1 91.250 Prec@5 99.410 Loss 0.3521
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0207 (0.0392)	Prec@1 100.000 (98.818)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0226 (0.0423)	Prec@1 100.000 (98.678)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0295 (0.0414)	Prec@1 99.219 (98.761)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.0532 (0.0420)	Prec@1 99.219 (98.745)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0431 (0.0415)	Prec@1 99.219 (98.764)	
Total train loss: 0.0416

 * Prec@1 91.320 Prec@5 99.460 Loss 0.3474
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0231 (0.0408)	Prec@1 99.219 (98.828)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0280 (0.0396)	Prec@1 100.000 (98.873)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0305 (0.0392)	Prec@1 99.219 (98.908)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0284 (0.0405)	Prec@1 98.438 (98.828)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0173 (0.0399)	Prec@1 100.000 (98.836)	
Total train loss: 0.0400

 * Prec@1 91.290 Prec@5 99.410 Loss 0.3462
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0086 (0.0401)	Prec@1 100.000 (98.898)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0208 (0.0400)	Prec@1 100.000 (98.858)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0345 (0.0408)	Prec@1 99.219 (98.761)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0634 (0.0410)	Prec@1 98.438 (98.758)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0569 (0.0414)	Prec@1 99.219 (98.756)	
Total train loss: 0.0413

 * Prec@1 91.500 Prec@5 99.470 Loss 0.3467
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0294 (0.0424)	Prec@1 99.219 (98.678)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0278 (0.0431)	Prec@1 99.219 (98.653)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0576 (0.0411)	Prec@1 98.438 (98.765)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0383 (0.0410)	Prec@1 99.219 (98.788)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0753 (0.0408)	Prec@1 96.875 (98.790)	
Total train loss: 0.0408

 * Prec@1 91.410 Prec@5 99.480 Loss 0.3479
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0460 (0.0409)	Prec@1 99.219 (98.838)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0191 (0.0415)	Prec@1 100.000 (98.763)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0120 (0.0410)	Prec@1 100.000 (98.765)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0662 (0.0397)	Prec@1 97.656 (98.851)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0432 (0.0400)	Prec@1 99.219 (98.840)	
Total train loss: 0.0400

 * Prec@1 91.450 Prec@5 99.440 Loss 0.3489
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0322 (0.0380)	Prec@1 98.438 (98.958)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0187 (0.0385)	Prec@1 100.000 (98.868)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0506 (0.0397)	Prec@1 99.219 (98.828)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0360 (0.0405)	Prec@1 99.219 (98.813)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0180 (0.0399)	Prec@1 100.000 (98.832)	
Total train loss: 0.0399

 * Prec@1 91.260 Prec@5 99.420 Loss 0.3479
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0192 (0.0393)	Prec@1 100.000 (98.838)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0529 (0.0403)	Prec@1 98.438 (98.818)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0758 (0.0400)	Prec@1 97.656 (98.855)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0617 (0.0404)	Prec@1 98.438 (98.826)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0304 (0.0403)	Prec@1 99.219 (98.816)	
Total train loss: 0.0403

 * Prec@1 91.520 Prec@5 99.450 Loss 0.3486
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0187 (0.0398)	Prec@1 100.000 (98.748)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0230 (0.0395)	Prec@1 99.219 (98.808)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0607 (0.0394)	Prec@1 98.438 (98.848)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0220 (0.0396)	Prec@1 100.000 (98.831)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0235 (0.0400)	Prec@1 99.219 (98.842)	
Total train loss: 0.0399

 * Prec@1 91.490 Prec@5 99.440 Loss 0.3462
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.0327 (0.0384)	Prec@1 100.000 (98.928)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.0334 (0.0395)	Prec@1 100.000 (98.863)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.0600 (0.0405)	Prec@1 97.656 (98.815)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0288 (0.0403)	Prec@1 99.219 (98.826)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.1057 (0.0402)	Prec@1 94.531 (98.830)	
Total train loss: 0.0403

 * Prec@1 91.510 Prec@5 99.480 Loss 0.3489
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0234 (0.0417)	Prec@1 100.000 (98.758)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0380 (0.0432)	Prec@1 99.219 (98.698)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0511 (0.0426)	Prec@1 99.219 (98.751)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0499 (0.0422)	Prec@1 97.656 (98.771)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0211 (0.0412)	Prec@1 100.000 (98.804)	
Total train loss: 0.0413

 * Prec@1 91.490 Prec@5 99.420 Loss 0.3484
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0306 (0.0450)	Prec@1 100.000 (98.648)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.0381 (0.0412)	Prec@1 98.438 (98.773)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0751 (0.0416)	Prec@1 98.438 (98.805)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0406 (0.0410)	Prec@1 99.219 (98.818)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0304 (0.0411)	Prec@1 100.000 (98.772)	
Total train loss: 0.0411

 * Prec@1 91.290 Prec@5 99.480 Loss 0.3499
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0378 (0.0391)	Prec@1 99.219 (98.868)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0538 (0.0407)	Prec@1 97.656 (98.798)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.0413 (0.0405)	Prec@1 99.219 (98.775)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0099 (0.0401)	Prec@1 100.000 (98.788)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0278 (0.0405)	Prec@1 100.000 (98.768)	
Total train loss: 0.0405

 * Prec@1 91.390 Prec@5 99.470 Loss 0.3469
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0405 (0.0393)	Prec@1 96.875 (98.808)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0338 (0.0406)	Prec@1 99.219 (98.718)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0456 (0.0395)	Prec@1 98.438 (98.775)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0579 (0.0405)	Prec@1 98.438 (98.791)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.0516 (0.0408)	Prec@1 97.656 (98.786)	
Total train loss: 0.0408

 * Prec@1 91.450 Prec@5 99.440 Loss 0.3472
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0260 (0.0402)	Prec@1 99.219 (98.908)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0363 (0.0402)	Prec@1 97.656 (98.873)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0106 (0.0401)	Prec@1 100.000 (98.825)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0264 (0.0399)	Prec@1 100.000 (98.838)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0291 (0.0397)	Prec@1 100.000 (98.858)	
Total train loss: 0.0397

 * Prec@1 91.230 Prec@5 99.460 Loss 0.3452
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0352 (0.0437)	Prec@1 99.219 (98.588)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0742 (0.0420)	Prec@1 97.656 (98.728)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0346 (0.0426)	Prec@1 98.438 (98.628)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0486 (0.0422)	Prec@1 99.219 (98.653)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0656 (0.0414)	Prec@1 96.875 (98.714)	
Total train loss: 0.0414

 * Prec@1 91.310 Prec@5 99.400 Loss 0.3494
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0151 (0.0372)	Prec@1 99.219 (98.918)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0430 (0.0386)	Prec@1 98.438 (98.903)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.0270 (0.0405)	Prec@1 99.219 (98.828)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0407 (0.0406)	Prec@1 99.219 (98.841)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0275 (0.0404)	Prec@1 100.000 (98.858)	
Total train loss: 0.0404

 * Prec@1 91.360 Prec@5 99.430 Loss 0.3481
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0241 (0.0422)	Prec@1 100.000 (98.728)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0277 (0.0437)	Prec@1 99.219 (98.643)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0631 (0.0410)	Prec@1 98.438 (98.761)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0482 (0.0415)	Prec@1 99.219 (98.740)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0354 (0.0412)	Prec@1 100.000 (98.778)	
Total train loss: 0.0412

 * Prec@1 91.400 Prec@5 99.460 Loss 0.3489
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0477 (0.0382)	Prec@1 98.438 (98.858)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0277 (0.0390)	Prec@1 99.219 (98.888)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0562 (0.0396)	Prec@1 97.656 (98.801)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0223 (0.0401)	Prec@1 99.219 (98.803)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.0579 (0.0401)	Prec@1 98.438 (98.792)	
Total train loss: 0.0402

 * Prec@1 91.510 Prec@5 99.500 Loss 0.3442
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0534 (0.0383)	Prec@1 97.656 (98.918)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0836 (0.0393)	Prec@1 96.875 (98.898)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.0406 (0.0407)	Prec@1 99.219 (98.841)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0447 (0.0407)	Prec@1 97.656 (98.821)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0239 (0.0409)	Prec@1 100.000 (98.804)	
Total train loss: 0.0409

 * Prec@1 91.460 Prec@5 99.430 Loss 0.3438
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0580 (0.0413)	Prec@1 98.438 (98.778)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0141 (0.0406)	Prec@1 100.000 (98.863)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0433 (0.0403)	Prec@1 97.656 (98.892)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0206 (0.0399)	Prec@1 99.219 (98.888)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0905 (0.0404)	Prec@1 96.094 (98.858)	
Total train loss: 0.0403

 * Prec@1 91.450 Prec@5 99.480 Loss 0.3499
Best acc: 91.520
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0544 (0.0429)	Prec@1 97.656 (98.688)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0590 (0.0421)	Prec@1 97.656 (98.783)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0193 (0.0412)	Prec@1 100.000 (98.805)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0707 (0.0411)	Prec@1 99.219 (98.806)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0362 (0.0404)	Prec@1 99.219 (98.832)	
Total train loss: 0.0404

 * Prec@1 91.350 Prec@5 99.500 Loss 0.3459
Best acc: 91.520
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 89.480 Prec@5 99.410 Loss 0.4285
Pre-trained Prec@1 with 7 layers frozen: 89.47999572753906 	 Loss: 0.428466796875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.1038 (0.0677)	Prec@1 97.656 (97.726)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.0592 (0.0674)	Prec@1 97.656 (97.761)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.0172 (0.0662)	Prec@1 100.000 (97.837)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.0397 (0.0641)	Prec@1 99.219 (97.894)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.1098 (0.0629)	Prec@1 96.094 (97.919)	
Total train loss: 0.0628

 * Prec@1 91.090 Prec@5 99.370 Loss 0.3584
Best acc: 91.090
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.0380 (0.0592)	Prec@1 98.438 (98.087)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.0787 (0.0578)	Prec@1 97.656 (98.102)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.0477 (0.0585)	Prec@1 98.438 (98.100)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.0472 (0.0580)	Prec@1 98.438 (98.109)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.1128 (0.0585)	Prec@1 95.312 (98.073)	
Total train loss: 0.0585

 * Prec@1 91.060 Prec@5 99.350 Loss 0.3572
Best acc: 91.090
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.0725 (0.0592)	Prec@1 96.094 (98.007)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.0640 (0.0572)	Prec@1 96.094 (98.132)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.0239 (0.0576)	Prec@1 100.000 (98.124)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.0408 (0.0578)	Prec@1 98.438 (98.135)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0503 (0.0567)	Prec@1 98.438 (98.191)	
Total train loss: 0.0567

 * Prec@1 91.170 Prec@5 99.410 Loss 0.3525
Best acc: 91.170
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0424 (0.0528)	Prec@1 98.438 (98.227)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.0296 (0.0528)	Prec@1 98.438 (98.257)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.1277 (0.0527)	Prec@1 96.094 (98.277)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0389 (0.0537)	Prec@1 99.219 (98.237)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.0688 (0.0536)	Prec@1 97.656 (98.265)	
Total train loss: 0.0536

 * Prec@1 91.150 Prec@5 99.400 Loss 0.3503
Best acc: 91.170
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.0386 (0.0527)	Prec@1 99.219 (98.317)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.0462 (0.0499)	Prec@1 97.656 (98.407)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.0822 (0.0502)	Prec@1 96.094 (98.361)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.0479 (0.0500)	Prec@1 98.438 (98.365)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.0256 (0.0507)	Prec@1 100.000 (98.369)	
Total train loss: 0.0509

 * Prec@1 91.200 Prec@5 99.310 Loss 0.3547
Best acc: 91.200
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0376 (0.0492)	Prec@1 98.438 (98.387)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0181 (0.0481)	Prec@1 99.219 (98.448)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0347 (0.0502)	Prec@1 98.438 (98.421)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0417 (0.0507)	Prec@1 99.219 (98.380)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0541 (0.0503)	Prec@1 96.875 (98.407)	
Total train loss: 0.0503

 * Prec@1 91.330 Prec@5 99.330 Loss 0.3530
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0424 (0.0544)	Prec@1 99.219 (98.207)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0520 (0.0517)	Prec@1 97.656 (98.367)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0532 (0.0507)	Prec@1 98.438 (98.381)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0847 (0.0493)	Prec@1 96.094 (98.432)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0724 (0.0490)	Prec@1 97.656 (98.452)	
Total train loss: 0.0490

 * Prec@1 91.260 Prec@5 99.410 Loss 0.3472
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0549 (0.0514)	Prec@1 97.656 (98.377)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0529 (0.0488)	Prec@1 97.656 (98.478)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0501 (0.0492)	Prec@1 98.438 (98.434)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0601 (0.0487)	Prec@1 97.656 (98.427)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0515 (0.0489)	Prec@1 98.438 (98.448)	
Total train loss: 0.0489

 * Prec@1 91.150 Prec@5 99.380 Loss 0.3545
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1284 (0.0523)	Prec@1 96.094 (98.317)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0403 (0.0490)	Prec@1 98.438 (98.422)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0323 (0.0490)	Prec@1 99.219 (98.441)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0387 (0.0487)	Prec@1 99.219 (98.463)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0529 (0.0480)	Prec@1 97.656 (98.494)	
Total train loss: 0.0480

 * Prec@1 91.230 Prec@5 99.460 Loss 0.3550
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0337 (0.0443)	Prec@1 98.438 (98.628)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0401 (0.0482)	Prec@1 100.000 (98.473)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0577 (0.0482)	Prec@1 96.875 (98.451)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0941 (0.0490)	Prec@1 97.656 (98.455)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0374 (0.0478)	Prec@1 98.438 (98.514)	
Total train loss: 0.0478

 * Prec@1 91.030 Prec@5 99.410 Loss 0.3552
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0345 (0.0440)	Prec@1 98.438 (98.668)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0338 (0.0441)	Prec@1 99.219 (98.703)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0440 (0.0452)	Prec@1 99.219 (98.634)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0462 (0.0457)	Prec@1 98.438 (98.643)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0359 (0.0464)	Prec@1 100.000 (98.630)	
Total train loss: 0.0465

 * Prec@1 91.120 Prec@5 99.410 Loss 0.3484
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0574 (0.0474)	Prec@1 98.438 (98.508)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0371 (0.0482)	Prec@1 98.438 (98.448)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0333 (0.0476)	Prec@1 99.219 (98.474)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0488 (0.0472)	Prec@1 98.438 (98.485)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0751 (0.0463)	Prec@1 96.094 (98.532)	
Total train loss: 0.0464

 * Prec@1 91.270 Prec@5 99.360 Loss 0.3535
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0467 (0.0395)	Prec@1 98.438 (98.868)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0380 (0.0450)	Prec@1 99.219 (98.633)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0937 (0.0449)	Prec@1 96.875 (98.631)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0999 (0.0458)	Prec@1 97.656 (98.605)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0333 (0.0455)	Prec@1 99.219 (98.600)	
Total train loss: 0.0456

 * Prec@1 91.160 Prec@5 99.400 Loss 0.3506
Best acc: 91.330
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0165 (0.0486)	Prec@1 100.000 (98.508)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0817 (0.0479)	Prec@1 97.656 (98.518)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0840 (0.0466)	Prec@1 97.656 (98.578)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0245 (0.0465)	Prec@1 99.219 (98.550)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0689 (0.0457)	Prec@1 97.656 (98.584)	
Total train loss: 0.0457

 * Prec@1 91.430 Prec@5 99.410 Loss 0.3503
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0256 (0.0477)	Prec@1 100.000 (98.558)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0588 (0.0472)	Prec@1 97.656 (98.533)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0779 (0.0477)	Prec@1 96.875 (98.521)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0451 (0.0473)	Prec@1 98.438 (98.523)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0308 (0.0466)	Prec@1 99.219 (98.588)	
Total train loss: 0.0466

 * Prec@1 91.160 Prec@5 99.400 Loss 0.3525
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.0753 (0.0484)	Prec@1 98.438 (98.538)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.1026 (0.0476)	Prec@1 96.875 (98.583)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0345 (0.0451)	Prec@1 99.219 (98.691)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0524 (0.0451)	Prec@1 99.219 (98.643)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0508 (0.0456)	Prec@1 99.219 (98.636)	
Total train loss: 0.0455

 * Prec@1 91.270 Prec@5 99.390 Loss 0.3489
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0847 (0.0438)	Prec@1 97.656 (98.848)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.0234 (0.0432)	Prec@1 99.219 (98.763)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.0370 (0.0445)	Prec@1 98.438 (98.718)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0417 (0.0448)	Prec@1 98.438 (98.693)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0077 (0.0450)	Prec@1 100.000 (98.652)	
Total train loss: 0.0450

 * Prec@1 91.180 Prec@5 99.480 Loss 0.3486
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0325 (0.0467)	Prec@1 98.438 (98.518)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0456 (0.0478)	Prec@1 98.438 (98.498)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0233 (0.0471)	Prec@1 100.000 (98.518)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0619 (0.0454)	Prec@1 96.094 (98.603)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0252 (0.0454)	Prec@1 100.000 (98.604)	
Total train loss: 0.0454

 * Prec@1 91.130 Prec@5 99.420 Loss 0.3523
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0298 (0.0470)	Prec@1 99.219 (98.548)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0280 (0.0463)	Prec@1 100.000 (98.648)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0298 (0.0471)	Prec@1 98.438 (98.558)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0184 (0.0469)	Prec@1 100.000 (98.578)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.1044 (0.0467)	Prec@1 95.312 (98.568)	
Total train loss: 0.0468

 * Prec@1 91.230 Prec@5 99.390 Loss 0.3484
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0629 (0.0451)	Prec@1 97.656 (98.658)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0460 (0.0443)	Prec@1 97.656 (98.668)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0349 (0.0453)	Prec@1 98.438 (98.631)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0246 (0.0455)	Prec@1 99.219 (98.620)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.0225 (0.0463)	Prec@1 100.000 (98.570)	
Total train loss: 0.0464

 * Prec@1 91.130 Prec@5 99.320 Loss 0.3555
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0371 (0.0493)	Prec@1 99.219 (98.578)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0654 (0.0473)	Prec@1 97.656 (98.588)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0306 (0.0468)	Prec@1 99.219 (98.628)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0227 (0.0461)	Prec@1 99.219 (98.645)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0495 (0.0461)	Prec@1 98.438 (98.590)	
Total train loss: 0.0462

 * Prec@1 91.170 Prec@5 99.360 Loss 0.3484
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.0224 (0.0418)	Prec@1 99.219 (98.768)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0369 (0.0438)	Prec@1 99.219 (98.678)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0873 (0.0456)	Prec@1 96.094 (98.574)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0181 (0.0469)	Prec@1 100.000 (98.528)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0308 (0.0464)	Prec@1 99.219 (98.550)	
Total train loss: 0.0465

 * Prec@1 91.040 Prec@5 99.420 Loss 0.3503
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0256 (0.0450)	Prec@1 99.219 (98.488)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.0376 (0.0445)	Prec@1 99.219 (98.538)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0154 (0.0453)	Prec@1 100.000 (98.588)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0900 (0.0451)	Prec@1 96.875 (98.628)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0573 (0.0447)	Prec@1 96.875 (98.634)	
Total train loss: 0.0448

 * Prec@1 91.220 Prec@5 99.370 Loss 0.3542
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0984 (0.0466)	Prec@1 96.094 (98.488)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0512 (0.0464)	Prec@1 98.438 (98.568)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0445 (0.0454)	Prec@1 98.438 (98.584)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0115 (0.0457)	Prec@1 100.000 (98.595)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.0587 (0.0449)	Prec@1 99.219 (98.654)	
Total train loss: 0.0450

 * Prec@1 91.270 Prec@5 99.330 Loss 0.3506
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0232 (0.0431)	Prec@1 99.219 (98.718)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0359 (0.0447)	Prec@1 98.438 (98.663)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0238 (0.0453)	Prec@1 99.219 (98.611)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0529 (0.0449)	Prec@1 99.219 (98.633)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0362 (0.0450)	Prec@1 99.219 (98.628)	
Total train loss: 0.0450

 * Prec@1 91.070 Prec@5 99.360 Loss 0.3481
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.1141 (0.0469)	Prec@1 95.312 (98.458)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0775 (0.0473)	Prec@1 96.875 (98.538)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.1306 (0.0477)	Prec@1 96.875 (98.544)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0282 (0.0467)	Prec@1 98.438 (98.563)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0196 (0.0464)	Prec@1 100.000 (98.554)	
Total train loss: 0.0465

 * Prec@1 91.210 Prec@5 99.400 Loss 0.3489
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0167 (0.0465)	Prec@1 100.000 (98.548)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0257 (0.0464)	Prec@1 98.438 (98.623)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0638 (0.0458)	Prec@1 99.219 (98.655)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0180 (0.0460)	Prec@1 100.000 (98.628)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0781 (0.0454)	Prec@1 96.875 (98.636)	
Total train loss: 0.0453

 * Prec@1 91.240 Prec@5 99.310 Loss 0.3530
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0424 (0.0433)	Prec@1 99.219 (98.778)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.0388 (0.0463)	Prec@1 98.438 (98.653)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.0285 (0.0458)	Prec@1 99.219 (98.624)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0498 (0.0469)	Prec@1 98.438 (98.575)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0274 (0.0462)	Prec@1 100.000 (98.590)	
Total train loss: 0.0462

 * Prec@1 91.110 Prec@5 99.400 Loss 0.3503
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0075 (0.0455)	Prec@1 100.000 (98.618)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0589 (0.0452)	Prec@1 97.656 (98.673)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0203 (0.0457)	Prec@1 100.000 (98.598)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0765 (0.0449)	Prec@1 97.656 (98.628)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0434 (0.0452)	Prec@1 98.438 (98.616)	
Total train loss: 0.0453

 * Prec@1 91.380 Prec@5 99.400 Loss 0.3511
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0123 (0.0438)	Prec@1 100.000 (98.638)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0302 (0.0442)	Prec@1 99.219 (98.658)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0384 (0.0447)	Prec@1 99.219 (98.661)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.1219 (0.0459)	Prec@1 95.312 (98.628)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0365 (0.0466)	Prec@1 99.219 (98.576)	
Total train loss: 0.0466

 * Prec@1 91.220 Prec@5 99.440 Loss 0.3469
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0738 (0.0449)	Prec@1 98.438 (98.618)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0350 (0.0441)	Prec@1 99.219 (98.668)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0247 (0.0449)	Prec@1 99.219 (98.638)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0235 (0.0444)	Prec@1 99.219 (98.670)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0555 (0.0448)	Prec@1 97.656 (98.654)	
Total train loss: 0.0447

 * Prec@1 91.260 Prec@5 99.320 Loss 0.3508
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0882 (0.0462)	Prec@1 97.656 (98.478)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0434 (0.0462)	Prec@1 98.438 (98.583)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0892 (0.0460)	Prec@1 96.875 (98.591)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0718 (0.0468)	Prec@1 95.312 (98.530)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0356 (0.0464)	Prec@1 99.219 (98.552)	
Total train loss: 0.0464

 * Prec@1 91.200 Prec@5 99.380 Loss 0.3508
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0909 (0.0499)	Prec@1 97.656 (98.397)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0490 (0.0489)	Prec@1 99.219 (98.448)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0312 (0.0469)	Prec@1 99.219 (98.498)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0185 (0.0463)	Prec@1 100.000 (98.523)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0248 (0.0453)	Prec@1 99.219 (98.574)	
Total train loss: 0.0453

 * Prec@1 91.310 Prec@5 99.330 Loss 0.3506
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0667 (0.0442)	Prec@1 97.656 (98.648)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0791 (0.0446)	Prec@1 96.875 (98.628)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0898 (0.0443)	Prec@1 96.875 (98.594)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0462 (0.0459)	Prec@1 99.219 (98.543)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0627 (0.0459)	Prec@1 98.438 (98.564)	
Total train loss: 0.0459

 * Prec@1 91.250 Prec@5 99.360 Loss 0.3481
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0595 (0.0411)	Prec@1 98.438 (98.888)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0243 (0.0431)	Prec@1 99.219 (98.768)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0394 (0.0440)	Prec@1 98.438 (98.685)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0373 (0.0441)	Prec@1 98.438 (98.675)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0565 (0.0447)	Prec@1 97.656 (98.658)	
Total train loss: 0.0448

 * Prec@1 91.130 Prec@5 99.390 Loss 0.3521
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0662 (0.0470)	Prec@1 98.438 (98.568)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0284 (0.0473)	Prec@1 100.000 (98.573)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0616 (0.0460)	Prec@1 97.656 (98.644)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0135 (0.0450)	Prec@1 100.000 (98.675)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0836 (0.0444)	Prec@1 97.656 (98.696)	
Total train loss: 0.0445

 * Prec@1 91.230 Prec@5 99.430 Loss 0.3499
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0248 (0.0436)	Prec@1 100.000 (98.768)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0250 (0.0436)	Prec@1 100.000 (98.743)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0770 (0.0454)	Prec@1 97.656 (98.628)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0180 (0.0448)	Prec@1 100.000 (98.620)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.1161 (0.0454)	Prec@1 96.875 (98.600)	
Total train loss: 0.0454

 * Prec@1 91.140 Prec@5 99.370 Loss 0.3518
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.0543 (0.0468)	Prec@1 98.438 (98.568)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.0393 (0.0462)	Prec@1 99.219 (98.533)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.0342 (0.0448)	Prec@1 100.000 (98.655)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0595 (0.0447)	Prec@1 96.875 (98.665)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0266 (0.0446)	Prec@1 99.219 (98.660)	
Total train loss: 0.0446

 * Prec@1 91.280 Prec@5 99.330 Loss 0.3508
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0369 (0.0441)	Prec@1 99.219 (98.618)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0466 (0.0446)	Prec@1 97.656 (98.563)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0168 (0.0452)	Prec@1 100.000 (98.558)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0419 (0.0451)	Prec@1 99.219 (98.558)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0518 (0.0447)	Prec@1 98.438 (98.592)	
Total train loss: 0.0446

 * Prec@1 91.070 Prec@5 99.470 Loss 0.3530
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0418 (0.0444)	Prec@1 98.438 (98.588)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.0692 (0.0452)	Prec@1 97.656 (98.638)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0421 (0.0454)	Prec@1 99.219 (98.574)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0184 (0.0450)	Prec@1 100.000 (98.615)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0390 (0.0448)	Prec@1 99.219 (98.632)	
Total train loss: 0.0449

 * Prec@1 91.380 Prec@5 99.390 Loss 0.3494
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0240 (0.0489)	Prec@1 98.438 (98.387)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0162 (0.0472)	Prec@1 100.000 (98.523)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.0314 (0.0454)	Prec@1 98.438 (98.561)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0270 (0.0439)	Prec@1 100.000 (98.605)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0309 (0.0438)	Prec@1 99.219 (98.638)	
Total train loss: 0.0438

 * Prec@1 91.270 Prec@5 99.350 Loss 0.3516
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0329 (0.0462)	Prec@1 99.219 (98.417)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0408 (0.0458)	Prec@1 98.438 (98.473)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0291 (0.0460)	Prec@1 99.219 (98.534)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0377 (0.0449)	Prec@1 99.219 (98.583)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.0245 (0.0449)	Prec@1 99.219 (98.574)	
Total train loss: 0.0450

 * Prec@1 91.270 Prec@5 99.400 Loss 0.3494
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0687 (0.0405)	Prec@1 96.875 (98.818)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0495 (0.0427)	Prec@1 98.438 (98.748)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0353 (0.0439)	Prec@1 99.219 (98.678)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0337 (0.0447)	Prec@1 99.219 (98.643)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0495 (0.0453)	Prec@1 98.438 (98.622)	
Total train loss: 0.0453

 * Prec@1 91.220 Prec@5 99.380 Loss 0.3489
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0609 (0.0450)	Prec@1 98.438 (98.638)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0242 (0.0452)	Prec@1 99.219 (98.623)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0416 (0.0446)	Prec@1 99.219 (98.658)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0461 (0.0445)	Prec@1 99.219 (98.690)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0356 (0.0442)	Prec@1 99.219 (98.698)	
Total train loss: 0.0442

 * Prec@1 91.410 Prec@5 99.350 Loss 0.3499
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0756 (0.0455)	Prec@1 98.438 (98.468)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0266 (0.0442)	Prec@1 99.219 (98.558)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.0245 (0.0462)	Prec@1 99.219 (98.544)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0353 (0.0458)	Prec@1 99.219 (98.575)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0112 (0.0456)	Prec@1 100.000 (98.596)	
Total train loss: 0.0457

 * Prec@1 91.200 Prec@5 99.390 Loss 0.3474
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0758 (0.0440)	Prec@1 96.875 (98.768)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0651 (0.0451)	Prec@1 97.656 (98.648)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0667 (0.0456)	Prec@1 96.875 (98.661)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0547 (0.0457)	Prec@1 99.219 (98.650)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0154 (0.0452)	Prec@1 100.000 (98.658)	
Total train loss: 0.0452

 * Prec@1 91.100 Prec@5 99.330 Loss 0.3521
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0265 (0.0440)	Prec@1 99.219 (98.738)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0600 (0.0448)	Prec@1 98.438 (98.718)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0611 (0.0444)	Prec@1 97.656 (98.671)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0703 (0.0439)	Prec@1 96.875 (98.670)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.0425 (0.0442)	Prec@1 99.219 (98.658)	
Total train loss: 0.0442

 * Prec@1 91.260 Prec@5 99.400 Loss 0.3503
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0680 (0.0457)	Prec@1 99.219 (98.708)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0670 (0.0448)	Prec@1 97.656 (98.698)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.0267 (0.0443)	Prec@1 100.000 (98.718)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0840 (0.0450)	Prec@1 97.656 (98.710)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0567 (0.0446)	Prec@1 96.875 (98.714)	
Total train loss: 0.0446

 * Prec@1 91.290 Prec@5 99.330 Loss 0.3503
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0271 (0.0455)	Prec@1 100.000 (98.618)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0420 (0.0463)	Prec@1 98.438 (98.638)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0630 (0.0453)	Prec@1 98.438 (98.634)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0291 (0.0456)	Prec@1 99.219 (98.618)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0668 (0.0449)	Prec@1 96.875 (98.646)	
Total train loss: 0.0449

 * Prec@1 91.160 Prec@5 99.360 Loss 0.3479
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0179 (0.0445)	Prec@1 100.000 (98.658)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0443 (0.0455)	Prec@1 98.438 (98.653)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0503 (0.0443)	Prec@1 98.438 (98.671)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0462 (0.0443)	Prec@1 98.438 (98.715)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0527 (0.0444)	Prec@1 98.438 (98.710)	
Total train loss: 0.0444

 * Prec@1 91.250 Prec@5 99.370 Loss 0.3547
Best acc: 91.430
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 86.060 Prec@5 98.980 Loss 0.5830
Pre-trained Prec@1 with 9 layers frozen: 86.05999755859375 	 Loss: 0.5830078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.1310 (0.1106)	Prec@1 95.312 (96.094)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.0558 (0.1073)	Prec@1 99.219 (96.224)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.1454 (0.1032)	Prec@1 95.312 (96.394)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.0410 (0.0992)	Prec@1 99.219 (96.514)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.1962 (0.0984)	Prec@1 96.094 (96.552)	
Total train loss: 0.0984

 * Prec@1 90.840 Prec@5 99.400 Loss 0.3657
Best acc: 90.840
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.0717 (0.0875)	Prec@1 97.656 (96.925)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.0825 (0.0874)	Prec@1 98.438 (96.910)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.1318 (0.0884)	Prec@1 94.531 (96.842)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.0919 (0.0868)	Prec@1 98.438 (96.928)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.0894 (0.0857)	Prec@1 97.656 (96.957)	
Total train loss: 0.0858

 * Prec@1 90.760 Prec@5 99.390 Loss 0.3662
Best acc: 90.840
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.0519 (0.0800)	Prec@1 98.438 (97.246)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.0587 (0.0788)	Prec@1 96.875 (97.241)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.0529 (0.0776)	Prec@1 96.875 (97.262)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.0679 (0.0796)	Prec@1 96.875 (97.206)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0979 (0.0802)	Prec@1 96.875 (97.218)	
Total train loss: 0.0802

 * Prec@1 90.890 Prec@5 99.420 Loss 0.3640
Best acc: 90.890
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0408 (0.0800)	Prec@1 98.438 (97.226)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.0650 (0.0784)	Prec@1 98.438 (97.251)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.1508 (0.0777)	Prec@1 92.188 (97.299)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0770 (0.0775)	Prec@1 97.656 (97.281)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.0930 (0.0755)	Prec@1 97.656 (97.368)	
Total train loss: 0.0755

 * Prec@1 90.990 Prec@5 99.430 Loss 0.3584
Best acc: 90.990
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.0503 (0.0695)	Prec@1 97.656 (97.626)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.0889 (0.0712)	Prec@1 96.094 (97.561)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.0608 (0.0722)	Prec@1 97.656 (97.496)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.1049 (0.0718)	Prec@1 96.094 (97.524)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.0823 (0.0720)	Prec@1 98.438 (97.538)	
Total train loss: 0.0721

 * Prec@1 91.050 Prec@5 99.400 Loss 0.3552
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0486 (0.0705)	Prec@1 98.438 (97.596)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0609 (0.0695)	Prec@1 98.438 (97.671)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0360 (0.0698)	Prec@1 99.219 (97.603)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0985 (0.0696)	Prec@1 96.094 (97.591)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0737 (0.0698)	Prec@1 96.094 (97.580)	
Total train loss: 0.0699

 * Prec@1 91.030 Prec@5 99.390 Loss 0.3535
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0384 (0.0710)	Prec@1 99.219 (97.546)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0320 (0.0683)	Prec@1 99.219 (97.671)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0468 (0.0662)	Prec@1 98.438 (97.763)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0961 (0.0671)	Prec@1 96.094 (97.729)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0886 (0.0677)	Prec@1 98.438 (97.732)	
Total train loss: 0.0677

 * Prec@1 91.070 Prec@5 99.410 Loss 0.3560
Best acc: 91.070
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0408 (0.0725)	Prec@1 99.219 (97.476)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0951 (0.0664)	Prec@1 96.094 (97.726)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0720 (0.0648)	Prec@1 97.656 (97.823)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0910 (0.0642)	Prec@1 97.656 (97.854)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0661 (0.0649)	Prec@1 97.656 (97.802)	
Total train loss: 0.0650

 * Prec@1 91.130 Prec@5 99.400 Loss 0.3574
Best acc: 91.130
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0674 (0.0677)	Prec@1 98.438 (97.666)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0525 (0.0645)	Prec@1 98.438 (97.852)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0415 (0.0643)	Prec@1 97.656 (97.860)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0812 (0.0654)	Prec@1 98.438 (97.832)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0705 (0.0655)	Prec@1 97.656 (97.792)	
Total train loss: 0.0655

 * Prec@1 91.150 Prec@5 99.370 Loss 0.3516
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0702 (0.0649)	Prec@1 97.656 (97.766)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0629 (0.0642)	Prec@1 97.656 (97.887)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0638 (0.0638)	Prec@1 98.438 (97.887)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0417 (0.0633)	Prec@1 98.438 (97.909)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0483 (0.0634)	Prec@1 99.219 (97.895)	
Total train loss: 0.0635

 * Prec@1 91.040 Prec@5 99.380 Loss 0.3550
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0341 (0.0640)	Prec@1 99.219 (97.877)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0403 (0.0607)	Prec@1 98.438 (97.962)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0492 (0.0592)	Prec@1 97.656 (97.983)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0456 (0.0607)	Prec@1 97.656 (97.917)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0403 (0.0611)	Prec@1 98.438 (97.925)	
Total train loss: 0.0610

 * Prec@1 91.070 Prec@5 99.360 Loss 0.3547
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0504 (0.0583)	Prec@1 99.219 (98.157)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0660 (0.0609)	Prec@1 97.656 (97.962)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.1057 (0.0619)	Prec@1 95.312 (97.913)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0392 (0.0626)	Prec@1 98.438 (97.934)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0891 (0.0617)	Prec@1 96.875 (98.001)	
Total train loss: 0.0616

 * Prec@1 90.950 Prec@5 99.390 Loss 0.3582
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0979 (0.0599)	Prec@1 95.312 (97.977)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0590 (0.0614)	Prec@1 97.656 (97.942)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0799 (0.0614)	Prec@1 96.875 (97.920)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0504 (0.0610)	Prec@1 96.875 (97.902)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0540 (0.0615)	Prec@1 98.438 (97.901)	
Total train loss: 0.0616

 * Prec@1 90.970 Prec@5 99.380 Loss 0.3547
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0429 (0.0605)	Prec@1 99.219 (98.097)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0303 (0.0630)	Prec@1 99.219 (97.912)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0831 (0.0605)	Prec@1 96.875 (98.003)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0512 (0.0614)	Prec@1 98.438 (97.989)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0695 (0.0615)	Prec@1 96.875 (97.963)	
Total train loss: 0.0615

 * Prec@1 91.010 Prec@5 99.400 Loss 0.3555
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0476 (0.0638)	Prec@1 98.438 (97.796)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0300 (0.0608)	Prec@1 99.219 (97.952)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0796 (0.0610)	Prec@1 97.656 (97.953)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0953 (0.0619)	Prec@1 96.094 (97.907)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0451 (0.0621)	Prec@1 98.438 (97.927)	
Total train loss: 0.0621

 * Prec@1 91.030 Prec@5 99.390 Loss 0.3533
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.0374 (0.0590)	Prec@1 99.219 (98.057)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0616 (0.0599)	Prec@1 97.656 (98.017)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0508 (0.0589)	Prec@1 98.438 (98.013)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0542 (0.0595)	Prec@1 98.438 (98.012)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0528 (0.0596)	Prec@1 97.656 (98.021)	
Total train loss: 0.0595

 * Prec@1 91.120 Prec@5 99.370 Loss 0.3521
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0767 (0.0619)	Prec@1 96.094 (97.837)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.0432 (0.0623)	Prec@1 99.219 (97.922)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.1238 (0.0628)	Prec@1 97.656 (97.907)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0940 (0.0625)	Prec@1 96.875 (97.874)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0609 (0.0614)	Prec@1 98.438 (97.915)	
Total train loss: 0.0614

 * Prec@1 91.020 Prec@5 99.340 Loss 0.3511
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0534 (0.0621)	Prec@1 98.438 (97.867)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0425 (0.0611)	Prec@1 97.656 (97.927)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0228 (0.0610)	Prec@1 99.219 (97.903)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0464 (0.0612)	Prec@1 97.656 (97.882)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0391 (0.0610)	Prec@1 98.438 (97.903)	
Total train loss: 0.0610

 * Prec@1 91.070 Prec@5 99.360 Loss 0.3542
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0608 (0.0607)	Prec@1 99.219 (97.997)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0292 (0.0607)	Prec@1 100.000 (97.982)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0583 (0.0603)	Prec@1 97.656 (98.020)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0542 (0.0599)	Prec@1 98.438 (98.029)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.0748 (0.0612)	Prec@1 98.438 (97.963)	
Total train loss: 0.0613

 * Prec@1 91.150 Prec@5 99.390 Loss 0.3552
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0258 (0.0623)	Prec@1 99.219 (98.017)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0440 (0.0605)	Prec@1 99.219 (98.027)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0408 (0.0603)	Prec@1 99.219 (97.983)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0297 (0.0603)	Prec@1 100.000 (97.997)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.0306 (0.0608)	Prec@1 100.000 (97.957)	
Total train loss: 0.0609

 * Prec@1 91.120 Prec@5 99.370 Loss 0.3547
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0983 (0.0678)	Prec@1 96.875 (97.756)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0795 (0.0660)	Prec@1 96.094 (97.706)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0849 (0.0633)	Prec@1 96.875 (97.850)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0311 (0.0622)	Prec@1 99.219 (97.887)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0529 (0.0628)	Prec@1 98.438 (97.871)	
Total train loss: 0.0629

 * Prec@1 91.020 Prec@5 99.360 Loss 0.3535
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.1356 (0.0601)	Prec@1 95.312 (97.967)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0961 (0.0606)	Prec@1 96.094 (98.002)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0511 (0.0598)	Prec@1 97.656 (97.977)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0723 (0.0599)	Prec@1 97.656 (97.987)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0754 (0.0607)	Prec@1 97.656 (97.959)	
Total train loss: 0.0609

 * Prec@1 90.950 Prec@5 99.360 Loss 0.3545
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0491 (0.0634)	Prec@1 99.219 (97.927)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.1404 (0.0634)	Prec@1 94.531 (97.781)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0360 (0.0617)	Prec@1 99.219 (97.837)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0190 (0.0608)	Prec@1 100.000 (97.899)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0276 (0.0606)	Prec@1 99.219 (97.913)	
Total train loss: 0.0606

 * Prec@1 91.100 Prec@5 99.370 Loss 0.3579
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.1013 (0.0626)	Prec@1 98.438 (97.877)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0633 (0.0597)	Prec@1 96.875 (98.032)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0765 (0.0595)	Prec@1 96.094 (98.037)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0380 (0.0613)	Prec@1 99.219 (97.949)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.0601 (0.0607)	Prec@1 96.875 (97.961)	
Total train loss: 0.0607

 * Prec@1 90.970 Prec@5 99.410 Loss 0.3552
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0362 (0.0602)	Prec@1 99.219 (98.147)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0666 (0.0618)	Prec@1 97.656 (98.097)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0685 (0.0615)	Prec@1 96.875 (98.050)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0287 (0.0613)	Prec@1 99.219 (97.989)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0390 (0.0611)	Prec@1 99.219 (98.015)	
Total train loss: 0.0611

 * Prec@1 91.080 Prec@5 99.440 Loss 0.3577
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0640 (0.0641)	Prec@1 98.438 (97.756)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0560 (0.0616)	Prec@1 98.438 (97.862)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0903 (0.0609)	Prec@1 97.656 (97.890)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0761 (0.0613)	Prec@1 97.656 (97.874)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0425 (0.0597)	Prec@1 98.438 (97.985)	
Total train loss: 0.0597

 * Prec@1 91.060 Prec@5 99.400 Loss 0.3538
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0264 (0.0641)	Prec@1 99.219 (97.877)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0609 (0.0617)	Prec@1 98.438 (97.967)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0528 (0.0622)	Prec@1 97.656 (97.890)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.1172 (0.0644)	Prec@1 95.312 (97.804)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0295 (0.0627)	Prec@1 99.219 (97.865)	
Total train loss: 0.0627

 * Prec@1 91.060 Prec@5 99.410 Loss 0.3550
Best acc: 91.150
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0462 (0.0609)	Prec@1 98.438 (97.957)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.0421 (0.0618)	Prec@1 98.438 (97.867)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.0290 (0.0600)	Prec@1 99.219 (97.993)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0298 (0.0606)	Prec@1 99.219 (97.969)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0336 (0.0606)	Prec@1 99.219 (98.001)	
Total train loss: 0.0607

 * Prec@1 91.260 Prec@5 99.390 Loss 0.3528
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0931 (0.0628)	Prec@1 95.312 (97.847)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.1013 (0.0636)	Prec@1 96.094 (97.817)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0587 (0.0618)	Prec@1 97.656 (97.923)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0555 (0.0610)	Prec@1 98.438 (97.957)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0902 (0.0608)	Prec@1 96.094 (97.963)	
Total train loss: 0.0608

 * Prec@1 91.060 Prec@5 99.380 Loss 0.3523
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0607 (0.0591)	Prec@1 96.875 (97.997)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0807 (0.0619)	Prec@1 96.094 (97.917)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0870 (0.0615)	Prec@1 96.875 (97.887)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.0357 (0.0606)	Prec@1 100.000 (97.924)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0678 (0.0613)	Prec@1 96.875 (97.911)	
Total train loss: 0.0614

 * Prec@1 91.070 Prec@5 99.390 Loss 0.3547
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0806 (0.0639)	Prec@1 97.656 (97.786)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0557 (0.0617)	Prec@1 97.656 (97.892)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0688 (0.0602)	Prec@1 97.656 (97.927)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0293 (0.0610)	Prec@1 100.000 (97.937)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0997 (0.0606)	Prec@1 96.875 (97.959)	
Total train loss: 0.0608

 * Prec@1 91.100 Prec@5 99.390 Loss 0.3567
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0543 (0.0627)	Prec@1 99.219 (97.917)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0537 (0.0622)	Prec@1 98.438 (97.952)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0301 (0.0623)	Prec@1 100.000 (97.953)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0341 (0.0603)	Prec@1 99.219 (98.039)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0723 (0.0606)	Prec@1 96.094 (98.009)	
Total train loss: 0.0606

 * Prec@1 91.250 Prec@5 99.400 Loss 0.3550
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0863 (0.0636)	Prec@1 96.094 (97.857)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0316 (0.0621)	Prec@1 100.000 (97.852)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0462 (0.0596)	Prec@1 98.438 (97.993)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0548 (0.0595)	Prec@1 99.219 (97.994)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0850 (0.0601)	Prec@1 97.656 (97.997)	
Total train loss: 0.0601

 * Prec@1 91.100 Prec@5 99.380 Loss 0.3560
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0457 (0.0609)	Prec@1 97.656 (97.977)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0519 (0.0611)	Prec@1 98.438 (97.952)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0788 (0.0611)	Prec@1 96.875 (97.937)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0804 (0.0602)	Prec@1 96.875 (98.009)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0992 (0.0603)	Prec@1 96.875 (97.985)	
Total train loss: 0.0603

 * Prec@1 91.090 Prec@5 99.350 Loss 0.3545
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0698 (0.0617)	Prec@1 99.219 (97.937)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0541 (0.0626)	Prec@1 98.438 (97.902)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0834 (0.0617)	Prec@1 96.875 (97.953)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0725 (0.0606)	Prec@1 96.094 (97.989)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0712 (0.0602)	Prec@1 96.094 (98.007)	
Total train loss: 0.0602

 * Prec@1 90.960 Prec@5 99.380 Loss 0.3606
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0572 (0.0597)	Prec@1 97.656 (97.997)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0470 (0.0594)	Prec@1 99.219 (98.067)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0577 (0.0599)	Prec@1 97.656 (98.060)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0886 (0.0601)	Prec@1 96.875 (98.059)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0555 (0.0601)	Prec@1 98.438 (98.071)	
Total train loss: 0.0601

 * Prec@1 91.080 Prec@5 99.370 Loss 0.3508
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0530 (0.0544)	Prec@1 97.656 (98.267)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0802 (0.0571)	Prec@1 97.656 (98.182)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0762 (0.0601)	Prec@1 96.875 (98.057)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0457 (0.0603)	Prec@1 98.438 (98.044)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0349 (0.0601)	Prec@1 99.219 (98.079)	
Total train loss: 0.0601

 * Prec@1 91.010 Prec@5 99.350 Loss 0.3547
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.0777 (0.0608)	Prec@1 96.875 (98.027)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.1295 (0.0618)	Prec@1 95.312 (97.992)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.0643 (0.0602)	Prec@1 99.219 (98.037)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0348 (0.0592)	Prec@1 99.219 (98.077)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0421 (0.0596)	Prec@1 98.438 (98.047)	
Total train loss: 0.0596

 * Prec@1 91.260 Prec@5 99.390 Loss 0.3538
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0710 (0.0636)	Prec@1 97.656 (97.907)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.1163 (0.0622)	Prec@1 96.094 (97.992)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0442 (0.0617)	Prec@1 99.219 (97.997)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0732 (0.0611)	Prec@1 98.438 (97.989)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0480 (0.0611)	Prec@1 97.656 (97.975)	
Total train loss: 0.0611

 * Prec@1 90.930 Prec@5 99.400 Loss 0.3572
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0557 (0.0587)	Prec@1 97.656 (98.037)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.0395 (0.0585)	Prec@1 99.219 (98.112)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0659 (0.0586)	Prec@1 97.656 (98.137)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0320 (0.0588)	Prec@1 99.219 (98.114)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0338 (0.0596)	Prec@1 98.438 (98.065)	
Total train loss: 0.0597

 * Prec@1 91.080 Prec@5 99.370 Loss 0.3547
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0277 (0.0635)	Prec@1 100.000 (97.877)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0398 (0.0639)	Prec@1 98.438 (97.811)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.1147 (0.0636)	Prec@1 96.094 (97.883)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0713 (0.0627)	Prec@1 96.875 (97.874)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0781 (0.0621)	Prec@1 95.312 (97.911)	
Total train loss: 0.0621

 * Prec@1 91.040 Prec@5 99.380 Loss 0.3542
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0685 (0.0577)	Prec@1 98.438 (98.097)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0666 (0.0581)	Prec@1 97.656 (98.147)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0603 (0.0589)	Prec@1 97.656 (98.094)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0651 (0.0591)	Prec@1 96.875 (98.092)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.0559 (0.0594)	Prec@1 98.438 (98.089)	
Total train loss: 0.0593

 * Prec@1 91.050 Prec@5 99.380 Loss 0.3540
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0385 (0.0604)	Prec@1 100.000 (98.087)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0870 (0.0573)	Prec@1 96.875 (98.182)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0360 (0.0584)	Prec@1 100.000 (98.084)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0357 (0.0589)	Prec@1 98.438 (98.027)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0374 (0.0590)	Prec@1 99.219 (98.039)	
Total train loss: 0.0592

 * Prec@1 91.120 Prec@5 99.390 Loss 0.3542
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0382 (0.0580)	Prec@1 98.438 (98.157)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0641 (0.0602)	Prec@1 97.656 (98.002)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0518 (0.0598)	Prec@1 98.438 (98.044)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0342 (0.0595)	Prec@1 99.219 (98.062)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0535 (0.0599)	Prec@1 98.438 (98.005)	
Total train loss: 0.0598

 * Prec@1 91.050 Prec@5 99.370 Loss 0.3555
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.1558 (0.0636)	Prec@1 93.750 (97.806)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0560 (0.0616)	Prec@1 98.438 (97.912)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.0384 (0.0600)	Prec@1 99.219 (97.987)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0501 (0.0596)	Prec@1 98.438 (98.014)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0498 (0.0608)	Prec@1 98.438 (97.961)	
Total train loss: 0.0609

 * Prec@1 91.120 Prec@5 99.420 Loss 0.3555
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0619 (0.0585)	Prec@1 99.219 (97.997)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0689 (0.0609)	Prec@1 98.438 (97.877)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0799 (0.0612)	Prec@1 96.875 (97.860)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0963 (0.0606)	Prec@1 95.312 (97.904)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0264 (0.0597)	Prec@1 99.219 (97.973)	
Total train loss: 0.0597

 * Prec@1 91.080 Prec@5 99.420 Loss 0.3511
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0372 (0.0604)	Prec@1 98.438 (98.007)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0536 (0.0612)	Prec@1 98.438 (97.972)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0412 (0.0600)	Prec@1 98.438 (97.970)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0255 (0.0606)	Prec@1 100.000 (97.969)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.0565 (0.0597)	Prec@1 98.438 (97.995)	
Total train loss: 0.0597

 * Prec@1 91.030 Prec@5 99.330 Loss 0.3552
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0334 (0.0611)	Prec@1 98.438 (97.867)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0634 (0.0612)	Prec@1 98.438 (97.922)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.0497 (0.0601)	Prec@1 98.438 (98.034)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0466 (0.0597)	Prec@1 99.219 (98.027)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0523 (0.0595)	Prec@1 97.656 (98.027)	
Total train loss: 0.0596

 * Prec@1 91.090 Prec@5 99.400 Loss 0.3547
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0780 (0.0643)	Prec@1 96.875 (97.907)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0433 (0.0609)	Prec@1 98.438 (98.052)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0450 (0.0598)	Prec@1 99.219 (98.097)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0410 (0.0595)	Prec@1 99.219 (98.102)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0517 (0.0597)	Prec@1 98.438 (98.075)	
Total train loss: 0.0597

 * Prec@1 91.110 Prec@5 99.360 Loss 0.3535
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.1053 (0.0629)	Prec@1 96.875 (97.817)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0426 (0.0616)	Prec@1 97.656 (97.867)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.1396 (0.0601)	Prec@1 96.094 (97.950)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.1263 (0.0616)	Prec@1 95.312 (97.919)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0552 (0.0613)	Prec@1 98.438 (97.923)	
Total train loss: 0.0612

 * Prec@1 91.200 Prec@5 99.370 Loss 0.3523
Best acc: 91.260
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 82.250 Prec@5 98.550 Loss 0.7559
Pre-trained Prec@1 with 11 layers frozen: 82.25 	 Loss: 0.755859375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.0996 (0.1247)	Prec@1 96.875 (95.723)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.1218 (0.1205)	Prec@1 94.531 (95.768)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.1881 (0.1188)	Prec@1 94.531 (95.830)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.0468 (0.1178)	Prec@1 99.219 (95.866)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.0860 (0.1134)	Prec@1 96.875 (96.054)	
Total train loss: 0.1133

 * Prec@1 90.660 Prec@5 99.390 Loss 0.3750
Best acc: 90.660
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.1544 (0.0963)	Prec@1 93.750 (96.715)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.0763 (0.0962)	Prec@1 97.656 (96.695)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.1343 (0.0993)	Prec@1 95.312 (96.528)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.1170 (0.0996)	Prec@1 95.312 (96.499)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.1090 (0.0987)	Prec@1 96.875 (96.577)	
Total train loss: 0.0987

 * Prec@1 90.820 Prec@5 99.440 Loss 0.3706
Best acc: 90.820
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.1094 (0.0931)	Prec@1 95.312 (96.625)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.1077 (0.0903)	Prec@1 95.312 (96.835)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.0936 (0.0903)	Prec@1 96.875 (96.818)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.0514 (0.0905)	Prec@1 99.219 (96.825)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0443 (0.0906)	Prec@1 98.438 (96.821)	
Total train loss: 0.0906

 * Prec@1 90.870 Prec@5 99.470 Loss 0.3635
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.0884 (0.0873)	Prec@1 96.094 (97.015)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.1199 (0.0868)	Prec@1 95.312 (97.070)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.0547 (0.0873)	Prec@1 99.219 (96.995)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0771 (0.0881)	Prec@1 97.656 (96.925)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.0756 (0.0867)	Prec@1 97.656 (96.977)	
Total train loss: 0.0868

 * Prec@1 91.050 Prec@5 99.420 Loss 0.3601
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.0952 (0.0868)	Prec@1 98.438 (97.015)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.1597 (0.0832)	Prec@1 95.312 (97.110)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.1335 (0.0838)	Prec@1 94.531 (97.052)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.1792 (0.0828)	Prec@1 92.188 (97.098)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.1019 (0.0818)	Prec@1 96.094 (97.145)	
Total train loss: 0.0817

 * Prec@1 91.010 Prec@5 99.430 Loss 0.3604
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0776 (0.0829)	Prec@1 97.656 (97.155)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0591 (0.0813)	Prec@1 98.438 (97.246)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0337 (0.0829)	Prec@1 99.219 (97.135)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.1017 (0.0816)	Prec@1 96.094 (97.211)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0721 (0.0817)	Prec@1 98.438 (97.188)	
Total train loss: 0.0817

 * Prec@1 91.030 Prec@5 99.440 Loss 0.3538
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0852 (0.0761)	Prec@1 97.656 (97.386)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0394 (0.0792)	Prec@1 99.219 (97.226)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.1085 (0.0790)	Prec@1 94.531 (97.239)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0522 (0.0771)	Prec@1 98.438 (97.318)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0726 (0.0770)	Prec@1 98.438 (97.334)	
Total train loss: 0.0771

 * Prec@1 90.970 Prec@5 99.460 Loss 0.3560
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0732 (0.0748)	Prec@1 97.656 (97.446)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0757 (0.0732)	Prec@1 97.656 (97.471)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0667 (0.0720)	Prec@1 96.875 (97.506)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0527 (0.0731)	Prec@1 97.656 (97.463)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.1732 (0.0727)	Prec@1 95.312 (97.478)	
Total train loss: 0.0727

 * Prec@1 90.980 Prec@5 99.450 Loss 0.3589
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0611 (0.0773)	Prec@1 98.438 (97.326)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0883 (0.0754)	Prec@1 96.094 (97.416)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0933 (0.0738)	Prec@1 96.875 (97.493)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0343 (0.0728)	Prec@1 99.219 (97.509)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0718 (0.0726)	Prec@1 97.656 (97.504)	
Total train loss: 0.0727

 * Prec@1 90.960 Prec@5 99.430 Loss 0.3582
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0750 (0.0705)	Prec@1 96.875 (97.616)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0471 (0.0683)	Prec@1 97.656 (97.711)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0735 (0.0712)	Prec@1 97.656 (97.610)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0536 (0.0710)	Prec@1 98.438 (97.601)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.1175 (0.0717)	Prec@1 97.656 (97.572)	
Total train loss: 0.0717

 * Prec@1 91.010 Prec@5 99.460 Loss 0.3569
Best acc: 91.050
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.1232 (0.0702)	Prec@1 95.312 (97.626)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0648 (0.0703)	Prec@1 98.438 (97.641)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0631 (0.0707)	Prec@1 98.438 (97.616)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.1027 (0.0692)	Prec@1 96.094 (97.681)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0370 (0.0689)	Prec@1 100.000 (97.680)	
Total train loss: 0.0692

 * Prec@1 91.160 Prec@5 99.420 Loss 0.3572
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0555 (0.0767)	Prec@1 98.438 (97.316)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0476 (0.0727)	Prec@1 99.219 (97.506)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0439 (0.0718)	Prec@1 98.438 (97.583)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0641 (0.0705)	Prec@1 97.656 (97.629)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0258 (0.0697)	Prec@1 100.000 (97.656)	
Total train loss: 0.0697

 * Prec@1 91.120 Prec@5 99.470 Loss 0.3557
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0572 (0.0697)	Prec@1 98.438 (97.716)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0997 (0.0715)	Prec@1 96.094 (97.546)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.1383 (0.0728)	Prec@1 93.750 (97.473)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0633 (0.0704)	Prec@1 96.094 (97.539)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0520 (0.0695)	Prec@1 98.438 (97.600)	
Total train loss: 0.0695

 * Prec@1 90.980 Prec@5 99.470 Loss 0.3503
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0537 (0.0736)	Prec@1 97.656 (97.516)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0571 (0.0722)	Prec@1 98.438 (97.496)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0378 (0.0708)	Prec@1 100.000 (97.563)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0544 (0.0702)	Prec@1 97.656 (97.601)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0727 (0.0700)	Prec@1 97.656 (97.610)	
Total train loss: 0.0700

 * Prec@1 90.950 Prec@5 99.470 Loss 0.3560
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0746 (0.0678)	Prec@1 97.656 (97.786)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0346 (0.0683)	Prec@1 98.438 (97.701)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0744 (0.0689)	Prec@1 96.094 (97.646)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0715 (0.0688)	Prec@1 97.656 (97.666)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0787 (0.0683)	Prec@1 96.875 (97.700)	
Total train loss: 0.0683

 * Prec@1 90.940 Prec@5 99.470 Loss 0.3569
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.0802 (0.0657)	Prec@1 97.656 (97.766)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0926 (0.0681)	Prec@1 96.094 (97.701)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0513 (0.0661)	Prec@1 99.219 (97.763)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.1483 (0.0668)	Prec@1 96.094 (97.771)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.0516 (0.0684)	Prec@1 98.438 (97.696)	
Total train loss: 0.0684

 * Prec@1 91.130 Prec@5 99.480 Loss 0.3521
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0622 (0.0678)	Prec@1 97.656 (97.706)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.1371 (0.0702)	Prec@1 93.750 (97.616)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.0919 (0.0688)	Prec@1 96.875 (97.673)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.0475 (0.0700)	Prec@1 98.438 (97.629)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0414 (0.0688)	Prec@1 99.219 (97.642)	
Total train loss: 0.0688

 * Prec@1 90.950 Prec@5 99.460 Loss 0.3582
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0572 (0.0718)	Prec@1 97.656 (97.576)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0432 (0.0682)	Prec@1 99.219 (97.666)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0804 (0.0698)	Prec@1 97.656 (97.606)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0613 (0.0697)	Prec@1 98.438 (97.614)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0689 (0.0699)	Prec@1 96.094 (97.616)	
Total train loss: 0.0699

 * Prec@1 91.050 Prec@5 99.470 Loss 0.3577
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0641 (0.0698)	Prec@1 97.656 (97.576)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.0851 (0.0687)	Prec@1 97.656 (97.676)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0452 (0.0687)	Prec@1 98.438 (97.696)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.1842 (0.0698)	Prec@1 93.750 (97.641)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.0312 (0.0694)	Prec@1 99.219 (97.680)	
Total train loss: 0.0694

 * Prec@1 91.050 Prec@5 99.480 Loss 0.3516
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0370 (0.0660)	Prec@1 100.000 (97.776)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0323 (0.0684)	Prec@1 99.219 (97.611)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.0843 (0.0683)	Prec@1 96.875 (97.650)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0365 (0.0683)	Prec@1 100.000 (97.674)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.0795 (0.0687)	Prec@1 97.656 (97.672)	
Total train loss: 0.0688

 * Prec@1 90.990 Prec@5 99.490 Loss 0.3528
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0628 (0.0695)	Prec@1 97.656 (97.556)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0542 (0.0693)	Prec@1 98.438 (97.561)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0518 (0.0693)	Prec@1 96.094 (97.579)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0681 (0.0701)	Prec@1 97.656 (97.576)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0790 (0.0688)	Prec@1 96.094 (97.620)	
Total train loss: 0.0688

 * Prec@1 91.140 Prec@5 99.480 Loss 0.3550
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.0682 (0.0669)	Prec@1 97.656 (97.686)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.0989 (0.0708)	Prec@1 96.875 (97.546)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0654 (0.0702)	Prec@1 99.219 (97.559)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0533 (0.0695)	Prec@1 98.438 (97.589)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0486 (0.0694)	Prec@1 98.438 (97.610)	
Total train loss: 0.0693

 * Prec@1 90.960 Prec@5 99.470 Loss 0.3564
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0459 (0.0709)	Prec@1 98.438 (97.606)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.0717 (0.0648)	Prec@1 96.875 (97.806)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0420 (0.0659)	Prec@1 98.438 (97.770)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0667 (0.0681)	Prec@1 96.875 (97.696)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.0656 (0.0679)	Prec@1 98.438 (97.682)	
Total train loss: 0.0679

 * Prec@1 91.070 Prec@5 99.470 Loss 0.3533
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0864 (0.0675)	Prec@1 96.875 (97.706)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.0952 (0.0669)	Prec@1 98.438 (97.741)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0576 (0.0679)	Prec@1 98.438 (97.686)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0644 (0.0678)	Prec@1 96.875 (97.679)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.0103 (0.0694)	Prec@1 100.000 (97.618)	
Total train loss: 0.0694

 * Prec@1 90.990 Prec@5 99.480 Loss 0.3530
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0726 (0.0706)	Prec@1 97.656 (97.776)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0570 (0.0690)	Prec@1 99.219 (97.786)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.0641 (0.0695)	Prec@1 98.438 (97.706)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0769 (0.0691)	Prec@1 96.094 (97.706)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0796 (0.0691)	Prec@1 95.312 (97.682)	
Total train loss: 0.0691

 * Prec@1 90.960 Prec@5 99.450 Loss 0.3569
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0161 (0.0653)	Prec@1 100.000 (97.796)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.1060 (0.0675)	Prec@1 95.312 (97.711)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0801 (0.0687)	Prec@1 97.656 (97.686)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.0494 (0.0687)	Prec@1 98.438 (97.671)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0776 (0.0678)	Prec@1 98.438 (97.700)	
Total train loss: 0.0679

 * Prec@1 91.100 Prec@5 99.470 Loss 0.3560
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0668 (0.0723)	Prec@1 97.656 (97.646)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0992 (0.0681)	Prec@1 96.094 (97.716)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0578 (0.0679)	Prec@1 96.094 (97.693)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0663 (0.0680)	Prec@1 98.438 (97.699)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0681 (0.0683)	Prec@1 97.656 (97.688)	
Total train loss: 0.0683

 * Prec@1 91.000 Prec@5 99.460 Loss 0.3547
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0234 (0.0713)	Prec@1 99.219 (97.566)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.0882 (0.0689)	Prec@1 96.094 (97.656)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.1797 (0.0689)	Prec@1 94.531 (97.603)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.0450 (0.0703)	Prec@1 100.000 (97.534)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0264 (0.0692)	Prec@1 100.000 (97.588)	
Total train loss: 0.0692

 * Prec@1 90.990 Prec@5 99.450 Loss 0.3552
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0458 (0.0695)	Prec@1 99.219 (97.546)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0229 (0.0699)	Prec@1 100.000 (97.616)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0764 (0.0695)	Prec@1 97.656 (97.630)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0530 (0.0684)	Prec@1 97.656 (97.671)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0804 (0.0689)	Prec@1 97.656 (97.674)	
Total train loss: 0.0688

 * Prec@1 90.990 Prec@5 99.490 Loss 0.3562
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.0324 (0.0737)	Prec@1 99.219 (97.546)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0959 (0.0715)	Prec@1 96.875 (97.561)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0599 (0.0708)	Prec@1 98.438 (97.596)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.1272 (0.0703)	Prec@1 95.312 (97.604)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0261 (0.0710)	Prec@1 98.438 (97.566)	
Total train loss: 0.0710

 * Prec@1 90.910 Prec@5 99.470 Loss 0.3538
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0765 (0.0715)	Prec@1 97.656 (97.486)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0371 (0.0703)	Prec@1 100.000 (97.641)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.1105 (0.0681)	Prec@1 96.094 (97.716)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.0712 (0.0682)	Prec@1 97.656 (97.694)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0624 (0.0676)	Prec@1 97.656 (97.698)	
Total train loss: 0.0676

 * Prec@1 90.960 Prec@5 99.490 Loss 0.3550
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0521 (0.0707)	Prec@1 97.656 (97.446)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.0317 (0.0681)	Prec@1 99.219 (97.631)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.0627 (0.0676)	Prec@1 96.875 (97.650)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0589 (0.0690)	Prec@1 98.438 (97.609)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0811 (0.0700)	Prec@1 97.656 (97.568)	
Total train loss: 0.0700

 * Prec@1 90.990 Prec@5 99.480 Loss 0.3572
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.0989 (0.0705)	Prec@1 96.875 (97.556)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0559 (0.0694)	Prec@1 98.438 (97.561)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0850 (0.0696)	Prec@1 95.312 (97.599)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0591 (0.0691)	Prec@1 96.875 (97.624)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0346 (0.0680)	Prec@1 100.000 (97.692)	
Total train loss: 0.0681

 * Prec@1 90.920 Prec@5 99.490 Loss 0.3547
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.0931 (0.0689)	Prec@1 95.312 (97.696)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0999 (0.0693)	Prec@1 96.875 (97.691)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0812 (0.0688)	Prec@1 97.656 (97.666)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.0433 (0.0687)	Prec@1 98.438 (97.671)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0570 (0.0696)	Prec@1 98.438 (97.604)	
Total train loss: 0.0696

 * Prec@1 90.930 Prec@5 99.460 Loss 0.3596
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0268 (0.0663)	Prec@1 99.219 (97.766)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0821 (0.0696)	Prec@1 96.875 (97.651)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0410 (0.0693)	Prec@1 100.000 (97.623)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0496 (0.0700)	Prec@1 97.656 (97.596)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0936 (0.0687)	Prec@1 97.656 (97.644)	
Total train loss: 0.0686

 * Prec@1 90.930 Prec@5 99.470 Loss 0.3552
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0678 (0.0649)	Prec@1 98.438 (97.726)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0315 (0.0681)	Prec@1 98.438 (97.696)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.0580 (0.0693)	Prec@1 98.438 (97.630)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0468 (0.0686)	Prec@1 98.438 (97.691)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0889 (0.0684)	Prec@1 96.875 (97.716)	
Total train loss: 0.0684

 * Prec@1 91.010 Prec@5 99.470 Loss 0.3569
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0652 (0.0668)	Prec@1 98.438 (97.596)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0735 (0.0688)	Prec@1 97.656 (97.596)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.0840 (0.0671)	Prec@1 98.438 (97.676)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.1011 (0.0673)	Prec@1 96.094 (97.706)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0845 (0.0688)	Prec@1 98.438 (97.642)	
Total train loss: 0.0689

 * Prec@1 90.930 Prec@5 99.440 Loss 0.3550
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.0659 (0.0629)	Prec@1 96.875 (97.746)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.0800 (0.0640)	Prec@1 96.875 (97.726)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.1383 (0.0650)	Prec@1 96.094 (97.673)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0702 (0.0663)	Prec@1 97.656 (97.649)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0793 (0.0670)	Prec@1 96.875 (97.634)	
Total train loss: 0.0671

 * Prec@1 90.960 Prec@5 99.480 Loss 0.3516
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0676 (0.0687)	Prec@1 98.438 (97.556)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0607 (0.0685)	Prec@1 97.656 (97.706)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0451 (0.0690)	Prec@1 97.656 (97.683)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0570 (0.0695)	Prec@1 98.438 (97.649)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0258 (0.0687)	Prec@1 99.219 (97.660)	
Total train loss: 0.0687

 * Prec@1 91.000 Prec@5 99.450 Loss 0.3562
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0687 (0.0723)	Prec@1 98.438 (97.506)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.1278 (0.0713)	Prec@1 96.094 (97.556)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0579 (0.0709)	Prec@1 98.438 (97.603)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0547 (0.0698)	Prec@1 99.219 (97.629)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0733 (0.0689)	Prec@1 96.875 (97.680)	
Total train loss: 0.0689

 * Prec@1 91.070 Prec@5 99.450 Loss 0.3562
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0643 (0.0723)	Prec@1 98.438 (97.436)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0782 (0.0696)	Prec@1 97.656 (97.586)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.0801 (0.0684)	Prec@1 98.438 (97.670)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0273 (0.0683)	Prec@1 100.000 (97.636)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0778 (0.0683)	Prec@1 97.656 (97.656)	
Total train loss: 0.0684

 * Prec@1 91.000 Prec@5 99.460 Loss 0.3564
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0632 (0.0612)	Prec@1 98.438 (97.957)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0645 (0.0646)	Prec@1 99.219 (97.786)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0828 (0.0681)	Prec@1 96.875 (97.640)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.0999 (0.0672)	Prec@1 97.656 (97.664)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.0659 (0.0672)	Prec@1 96.094 (97.670)	
Total train loss: 0.0672

 * Prec@1 91.080 Prec@5 99.490 Loss 0.3547
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0233 (0.0683)	Prec@1 99.219 (97.606)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0511 (0.0681)	Prec@1 97.656 (97.671)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0516 (0.0675)	Prec@1 99.219 (97.710)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0502 (0.0667)	Prec@1 98.438 (97.731)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0549 (0.0655)	Prec@1 98.438 (97.756)	
Total train loss: 0.0655

 * Prec@1 91.080 Prec@5 99.460 Loss 0.3535
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0753 (0.0674)	Prec@1 96.094 (97.686)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0551 (0.0660)	Prec@1 98.438 (97.756)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0897 (0.0671)	Prec@1 96.094 (97.746)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0706 (0.0679)	Prec@1 98.438 (97.676)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.1080 (0.0684)	Prec@1 96.094 (97.650)	
Total train loss: 0.0684

 * Prec@1 91.010 Prec@5 99.470 Loss 0.3567
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0512 (0.0694)	Prec@1 99.219 (97.516)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0492 (0.0681)	Prec@1 98.438 (97.521)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.1127 (0.0680)	Prec@1 96.094 (97.599)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.0757 (0.0670)	Prec@1 96.094 (97.661)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.0824 (0.0661)	Prec@1 96.875 (97.706)	
Total train loss: 0.0661

 * Prec@1 90.950 Prec@5 99.450 Loss 0.3589
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.0793 (0.0704)	Prec@1 96.875 (97.566)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.0785 (0.0689)	Prec@1 97.656 (97.646)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0847 (0.0659)	Prec@1 94.531 (97.743)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.1162 (0.0665)	Prec@1 97.656 (97.731)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0247 (0.0672)	Prec@1 100.000 (97.714)	
Total train loss: 0.0673

 * Prec@1 90.970 Prec@5 99.470 Loss 0.3579
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.0479 (0.0664)	Prec@1 98.438 (97.686)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0826 (0.0694)	Prec@1 96.875 (97.596)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.1075 (0.0691)	Prec@1 96.094 (97.610)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.0679 (0.0687)	Prec@1 96.875 (97.636)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.0353 (0.0686)	Prec@1 98.438 (97.650)	
Total train loss: 0.0686

 * Prec@1 91.020 Prec@5 99.480 Loss 0.3525
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.1137 (0.0652)	Prec@1 94.531 (97.827)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0554 (0.0662)	Prec@1 98.438 (97.811)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.1069 (0.0671)	Prec@1 96.875 (97.780)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0882 (0.0683)	Prec@1 96.094 (97.746)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0960 (0.0678)	Prec@1 96.875 (97.756)	
Total train loss: 0.0677

 * Prec@1 91.010 Prec@5 99.480 Loss 0.3545
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.1140 (0.0731)	Prec@1 96.094 (97.346)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0600 (0.0707)	Prec@1 96.875 (97.556)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0519 (0.0708)	Prec@1 98.438 (97.519)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0450 (0.0687)	Prec@1 99.219 (97.626)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0482 (0.0695)	Prec@1 98.438 (97.594)	
Total train loss: 0.0695

 * Prec@1 91.050 Prec@5 99.480 Loss 0.3550
Best acc: 91.160
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0440 (0.0690)	Prec@1 98.438 (97.636)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.1086 (0.0668)	Prec@1 96.094 (97.751)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0909 (0.0667)	Prec@1 97.656 (97.753)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0807 (0.0678)	Prec@1 96.875 (97.706)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0418 (0.0683)	Prec@1 99.219 (97.716)	
Total train loss: 0.0683

 * Prec@1 91.060 Prec@5 99.470 Loss 0.3540
Best acc: 91.160
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 78.730 Prec@5 97.980 Loss 0.9321
Pre-trained Prec@1 with 13 layers frozen: 78.72999572753906 	 Loss: 0.93212890625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.1776 (0.1595)	Prec@1 93.750 (94.661)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.1743 (0.1585)	Prec@1 92.188 (94.666)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.2532 (0.1547)	Prec@1 92.188 (94.782)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.1815 (0.1511)	Prec@1 95.312 (94.929)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.1890 (0.1496)	Prec@1 93.750 (94.982)	
Total train loss: 0.1495

 * Prec@1 89.950 Prec@5 99.340 Loss 0.3977
Best acc: 89.950
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.1388 (0.1449)	Prec@1 95.312 (95.032)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.1382 (0.1428)	Prec@1 92.969 (95.077)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.1500 (0.1354)	Prec@1 95.312 (95.299)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.0559 (0.1338)	Prec@1 98.438 (95.395)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.0993 (0.1303)	Prec@1 95.312 (95.529)	
Total train loss: 0.1303

 * Prec@1 90.190 Prec@5 99.340 Loss 0.3904
Best acc: 90.190
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.1191 (0.1149)	Prec@1 95.312 (95.793)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.1289 (0.1200)	Prec@1 95.312 (95.693)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.3359 (0.1211)	Prec@1 88.281 (95.680)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.1184 (0.1203)	Prec@1 96.875 (95.726)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.0742 (0.1198)	Prec@1 96.094 (95.803)	
Total train loss: 0.1198

 * Prec@1 90.210 Prec@5 99.300 Loss 0.3870
Best acc: 90.210
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.1348 (0.1053)	Prec@1 95.312 (96.494)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.1813 (0.1137)	Prec@1 92.969 (96.049)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.1024 (0.1137)	Prec@1 96.875 (96.044)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.0870 (0.1120)	Prec@1 98.438 (96.064)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.1307 (0.1129)	Prec@1 94.531 (96.036)	
Total train loss: 0.1129

 * Prec@1 90.040 Prec@5 99.310 Loss 0.3896
Best acc: 90.210
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.1484 (0.1025)	Prec@1 93.750 (96.494)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.0585 (0.1044)	Prec@1 98.438 (96.319)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.1261 (0.1072)	Prec@1 94.531 (96.194)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.1593 (0.1061)	Prec@1 95.312 (96.267)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.1055 (0.1070)	Prec@1 96.094 (96.264)	
Total train loss: 0.1070

 * Prec@1 90.140 Prec@5 99.330 Loss 0.3835
Best acc: 90.210
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0912 (0.1043)	Prec@1 95.312 (96.184)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0642 (0.1057)	Prec@1 99.219 (96.229)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0431 (0.1040)	Prec@1 100.000 (96.364)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0723 (0.1033)	Prec@1 97.656 (96.429)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.1259 (0.1031)	Prec@1 93.750 (96.434)	
Total train loss: 0.1033

 * Prec@1 90.380 Prec@5 99.330 Loss 0.3774
Best acc: 90.380
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0656 (0.1015)	Prec@1 97.656 (96.404)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0647 (0.1029)	Prec@1 97.656 (96.299)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.1398 (0.1020)	Prec@1 97.656 (96.398)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.1238 (0.1018)	Prec@1 95.312 (96.454)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.1481 (0.1008)	Prec@1 93.750 (96.494)	
Total train loss: 0.1009

 * Prec@1 90.280 Prec@5 99.380 Loss 0.3782
Best acc: 90.380
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0626 (0.0959)	Prec@1 98.438 (96.715)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0673 (0.1004)	Prec@1 97.656 (96.514)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.1187 (0.0974)	Prec@1 96.094 (96.595)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.1313 (0.0985)	Prec@1 95.312 (96.572)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.1124 (0.0970)	Prec@1 95.312 (96.653)	
Total train loss: 0.0971

 * Prec@1 90.360 Prec@5 99.450 Loss 0.3755
Best acc: 90.380
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0768 (0.0966)	Prec@1 98.438 (96.635)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.1046 (0.0945)	Prec@1 96.094 (96.670)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0920 (0.0940)	Prec@1 96.094 (96.685)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.1088 (0.0940)	Prec@1 97.656 (96.730)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.1054 (0.0951)	Prec@1 96.875 (96.719)	
Total train loss: 0.0952

 * Prec@1 90.380 Prec@5 99.360 Loss 0.3762
Best acc: 90.380
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.1469 (0.0911)	Prec@1 92.188 (96.915)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0536 (0.0898)	Prec@1 98.438 (96.860)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0406 (0.0947)	Prec@1 98.438 (96.701)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0597 (0.0937)	Prec@1 98.438 (96.745)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0358 (0.0928)	Prec@1 99.219 (96.759)	
Total train loss: 0.0927

 * Prec@1 90.500 Prec@5 99.360 Loss 0.3726
Best acc: 90.500
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0626 (0.0885)	Prec@1 97.656 (97.005)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0308 (0.0904)	Prec@1 99.219 (96.820)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.1338 (0.0891)	Prec@1 96.094 (96.905)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.1057 (0.0900)	Prec@1 96.875 (96.860)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.1371 (0.0908)	Prec@1 93.750 (96.841)	
Total train loss: 0.0907

 * Prec@1 90.620 Prec@5 99.410 Loss 0.3696
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0689 (0.0838)	Prec@1 97.656 (97.075)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0947 (0.0867)	Prec@1 96.094 (97.005)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0612 (0.0898)	Prec@1 98.438 (96.925)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0776 (0.0897)	Prec@1 96.094 (96.955)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0534 (0.0900)	Prec@1 99.219 (96.961)	
Total train loss: 0.0900

 * Prec@1 90.370 Prec@5 99.360 Loss 0.3750
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.1005 (0.0971)	Prec@1 98.438 (96.665)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0574 (0.0939)	Prec@1 98.438 (96.770)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0534 (0.0898)	Prec@1 98.438 (96.902)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.1028 (0.0900)	Prec@1 95.312 (96.862)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0458 (0.0919)	Prec@1 97.656 (96.809)	
Total train loss: 0.0920

 * Prec@1 90.440 Prec@5 99.350 Loss 0.3716
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.1235 (0.0951)	Prec@1 95.312 (96.625)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0485 (0.0900)	Prec@1 99.219 (96.895)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0938 (0.0902)	Prec@1 96.875 (96.902)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.1390 (0.0888)	Prec@1 92.969 (96.955)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.1312 (0.0908)	Prec@1 95.312 (96.869)	
Total train loss: 0.0908

 * Prec@1 90.340 Prec@5 99.380 Loss 0.3760
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.1010 (0.0875)	Prec@1 96.875 (97.005)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0556 (0.0897)	Prec@1 97.656 (96.930)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0812 (0.0903)	Prec@1 96.875 (96.898)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.1169 (0.0909)	Prec@1 94.531 (96.908)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0295 (0.0910)	Prec@1 99.219 (96.865)	
Total train loss: 0.0910

 * Prec@1 90.500 Prec@5 99.390 Loss 0.3733
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.1487 (0.0925)	Prec@1 96.094 (96.835)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0621 (0.0899)	Prec@1 96.875 (97.015)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.0537 (0.0889)	Prec@1 98.438 (96.985)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0765 (0.0901)	Prec@1 97.656 (96.938)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.1570 (0.0903)	Prec@1 93.750 (96.935)	
Total train loss: 0.0902

 * Prec@1 90.350 Prec@5 99.400 Loss 0.3779
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.0775 (0.0928)	Prec@1 96.875 (96.775)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.0815 (0.0915)	Prec@1 95.312 (96.790)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.1752 (0.0907)	Prec@1 92.969 (96.902)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.1163 (0.0909)	Prec@1 97.656 (96.855)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0506 (0.0905)	Prec@1 97.656 (96.875)	
Total train loss: 0.0905

 * Prec@1 90.400 Prec@5 99.400 Loss 0.3716
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.0581 (0.0998)	Prec@1 99.219 (96.765)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.0531 (0.0930)	Prec@1 98.438 (96.880)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.0796 (0.0912)	Prec@1 96.094 (96.982)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0827 (0.0893)	Prec@1 97.656 (96.998)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.0714 (0.0894)	Prec@1 98.438 (96.971)	
Total train loss: 0.0893

 * Prec@1 90.500 Prec@5 99.370 Loss 0.3704
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.0591 (0.0900)	Prec@1 96.875 (97.055)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.1209 (0.0907)	Prec@1 98.438 (96.870)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0660 (0.0906)	Prec@1 98.438 (96.902)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.0848 (0.0903)	Prec@1 96.875 (96.970)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.1165 (0.0898)	Prec@1 96.094 (96.965)	
Total train loss: 0.0898

 * Prec@1 90.400 Prec@5 99.400 Loss 0.3718
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.0909 (0.0880)	Prec@1 96.875 (96.855)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.0981 (0.0880)	Prec@1 96.875 (96.860)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.1035 (0.0898)	Prec@1 96.875 (96.852)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.1218 (0.0896)	Prec@1 96.094 (96.893)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.1030 (0.0900)	Prec@1 98.438 (96.909)	
Total train loss: 0.0900

 * Prec@1 90.250 Prec@5 99.340 Loss 0.3726
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0751 (0.0784)	Prec@1 97.656 (97.346)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.0536 (0.0858)	Prec@1 97.656 (97.030)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.0835 (0.0871)	Prec@1 99.219 (96.989)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.0851 (0.0876)	Prec@1 98.438 (96.990)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.0693 (0.0884)	Prec@1 97.656 (96.987)	
Total train loss: 0.0885

 * Prec@1 90.380 Prec@5 99.450 Loss 0.3757
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.1801 (0.0911)	Prec@1 94.531 (96.765)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.1396 (0.0908)	Prec@1 95.312 (96.880)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.0742 (0.0905)	Prec@1 96.875 (96.875)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0457 (0.0911)	Prec@1 97.656 (96.837)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.0341 (0.0904)	Prec@1 98.438 (96.871)	
Total train loss: 0.0904

 * Prec@1 90.380 Prec@5 99.370 Loss 0.3738
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.0580 (0.0952)	Prec@1 97.656 (96.835)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.0876 (0.0892)	Prec@1 96.094 (97.015)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.0815 (0.0897)	Prec@1 96.875 (96.972)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0841 (0.0886)	Prec@1 97.656 (96.993)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.1522 (0.0899)	Prec@1 92.969 (96.941)	
Total train loss: 0.0900

 * Prec@1 90.570 Prec@5 99.430 Loss 0.3726
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0891 (0.0902)	Prec@1 95.312 (96.785)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.1316 (0.0894)	Prec@1 93.750 (96.840)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0802 (0.0905)	Prec@1 97.656 (96.808)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.0403 (0.0907)	Prec@1 98.438 (96.832)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.1121 (0.0902)	Prec@1 96.094 (96.873)	
Total train loss: 0.0901

 * Prec@1 90.490 Prec@5 99.400 Loss 0.3713
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.0702 (0.0895)	Prec@1 96.875 (96.985)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.0903 (0.0915)	Prec@1 96.875 (96.975)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.1406 (0.0917)	Prec@1 95.312 (96.962)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.0963 (0.0896)	Prec@1 97.656 (97.050)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.0626 (0.0893)	Prec@1 98.438 (97.021)	
Total train loss: 0.0893

 * Prec@1 90.460 Prec@5 99.450 Loss 0.3701
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.0964 (0.0909)	Prec@1 97.656 (96.835)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0957 (0.0892)	Prec@1 97.656 (96.915)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.0708 (0.0886)	Prec@1 96.875 (96.918)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.1459 (0.0880)	Prec@1 96.094 (96.950)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.0625 (0.0888)	Prec@1 98.438 (96.943)	
Total train loss: 0.0887

 * Prec@1 90.320 Prec@5 99.390 Loss 0.3745
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.0911 (0.0925)	Prec@1 96.875 (96.685)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.0829 (0.0884)	Prec@1 96.094 (96.930)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.0898 (0.0901)	Prec@1 96.094 (96.895)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.0865 (0.0890)	Prec@1 96.094 (96.888)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.0608 (0.0899)	Prec@1 98.438 (96.881)	
Total train loss: 0.0898

 * Prec@1 90.510 Prec@5 99.420 Loss 0.3728
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.0917 (0.0827)	Prec@1 96.875 (97.346)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.1299 (0.0886)	Prec@1 96.094 (97.080)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.1027 (0.0899)	Prec@1 96.094 (96.958)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.1066 (0.0909)	Prec@1 96.875 (96.908)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.0983 (0.0896)	Prec@1 96.875 (96.929)	
Total train loss: 0.0896

 * Prec@1 90.310 Prec@5 99.430 Loss 0.3726
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.0673 (0.0902)	Prec@1 98.438 (96.785)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0931 (0.0876)	Prec@1 96.094 (96.895)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.0692 (0.0878)	Prec@1 98.438 (96.895)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.0516 (0.0891)	Prec@1 99.219 (96.867)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0661 (0.0890)	Prec@1 96.875 (96.905)	
Total train loss: 0.0890

 * Prec@1 90.460 Prec@5 99.360 Loss 0.3752
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.1500 (0.0865)	Prec@1 93.750 (96.925)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.0600 (0.0864)	Prec@1 96.875 (96.985)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.0554 (0.0867)	Prec@1 97.656 (96.978)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.1550 (0.0870)	Prec@1 94.531 (97.010)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.0586 (0.0873)	Prec@1 98.438 (96.977)	
Total train loss: 0.0874

 * Prec@1 90.380 Prec@5 99.410 Loss 0.3726
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.1726 (0.0926)	Prec@1 92.188 (96.845)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.0371 (0.0928)	Prec@1 99.219 (96.805)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0537 (0.0897)	Prec@1 98.438 (96.948)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.1064 (0.0894)	Prec@1 96.094 (96.983)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0853 (0.0891)	Prec@1 96.094 (96.985)	
Total train loss: 0.0893

 * Prec@1 90.610 Prec@5 99.460 Loss 0.3713
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.0162 (0.0893)	Prec@1 100.000 (96.925)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.1214 (0.0899)	Prec@1 96.875 (96.910)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.1168 (0.0919)	Prec@1 94.531 (96.802)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.0649 (0.0904)	Prec@1 96.875 (96.878)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.0322 (0.0906)	Prec@1 99.219 (96.877)	
Total train loss: 0.0905

 * Prec@1 90.530 Prec@5 99.470 Loss 0.3682
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.1187 (0.0937)	Prec@1 95.312 (96.895)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.0626 (0.0916)	Prec@1 97.656 (96.875)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0743 (0.0910)	Prec@1 96.094 (96.875)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.0654 (0.0910)	Prec@1 99.219 (96.860)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0888 (0.0906)	Prec@1 97.656 (96.883)	
Total train loss: 0.0907

 * Prec@1 90.370 Prec@5 99.460 Loss 0.3682
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.1185 (0.0901)	Prec@1 96.094 (96.815)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.0699 (0.0881)	Prec@1 98.438 (96.965)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.0817 (0.0894)	Prec@1 97.656 (96.882)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.1235 (0.0906)	Prec@1 95.312 (96.805)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.1399 (0.0894)	Prec@1 97.656 (96.909)	
Total train loss: 0.0893

 * Prec@1 90.480 Prec@5 99.440 Loss 0.3743
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.1129 (0.0882)	Prec@1 95.312 (96.795)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.0595 (0.0897)	Prec@1 97.656 (96.840)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.0690 (0.0898)	Prec@1 97.656 (96.915)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.0821 (0.0901)	Prec@1 97.656 (96.933)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0993 (0.0884)	Prec@1 96.094 (96.979)	
Total train loss: 0.0885

 * Prec@1 90.500 Prec@5 99.360 Loss 0.3745
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.0366 (0.0922)	Prec@1 99.219 (96.825)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.0420 (0.0914)	Prec@1 100.000 (96.835)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.1643 (0.0913)	Prec@1 95.312 (96.902)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.0708 (0.0902)	Prec@1 99.219 (96.943)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.0599 (0.0906)	Prec@1 98.438 (96.925)	
Total train loss: 0.0906

 * Prec@1 90.460 Prec@5 99.360 Loss 0.3765
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0760 (0.0867)	Prec@1 96.875 (97.065)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.0639 (0.0893)	Prec@1 97.656 (96.930)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.1212 (0.0901)	Prec@1 96.875 (96.868)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.0709 (0.0889)	Prec@1 96.875 (96.908)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0897 (0.0894)	Prec@1 97.656 (96.893)	
Total train loss: 0.0896

 * Prec@1 90.350 Prec@5 99.380 Loss 0.3735
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.1230 (0.0957)	Prec@1 94.531 (96.645)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.2046 (0.0912)	Prec@1 95.312 (96.890)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.1416 (0.0890)	Prec@1 94.531 (97.005)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.0887 (0.0876)	Prec@1 97.656 (97.043)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0822 (0.0880)	Prec@1 96.875 (97.011)	
Total train loss: 0.0881

 * Prec@1 90.390 Prec@5 99.400 Loss 0.3708
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.0712 (0.0897)	Prec@1 96.875 (97.035)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0444 (0.0910)	Prec@1 99.219 (96.965)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0917 (0.0904)	Prec@1 98.438 (97.002)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.0834 (0.0887)	Prec@1 96.875 (97.033)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0490 (0.0879)	Prec@1 98.438 (97.025)	
Total train loss: 0.0878

 * Prec@1 90.330 Prec@5 99.400 Loss 0.3708
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.0605 (0.0889)	Prec@1 97.656 (96.985)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.0677 (0.0865)	Prec@1 97.656 (96.960)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.0784 (0.0885)	Prec@1 97.656 (96.908)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.0767 (0.0886)	Prec@1 97.656 (96.933)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.0726 (0.0890)	Prec@1 97.656 (96.907)	
Total train loss: 0.0890

 * Prec@1 90.440 Prec@5 99.400 Loss 0.3718
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.0957 (0.0956)	Prec@1 95.312 (96.695)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.0497 (0.0902)	Prec@1 99.219 (97.020)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.1213 (0.0901)	Prec@1 95.312 (96.995)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0937 (0.0899)	Prec@1 96.875 (97.023)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.0718 (0.0887)	Prec@1 96.094 (97.045)	
Total train loss: 0.0887

 * Prec@1 90.510 Prec@5 99.360 Loss 0.3738
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0754 (0.0875)	Prec@1 96.875 (97.035)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.0588 (0.0840)	Prec@1 97.656 (97.175)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.0388 (0.0857)	Prec@1 99.219 (97.145)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.1438 (0.0876)	Prec@1 94.531 (97.075)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.1129 (0.0882)	Prec@1 96.875 (97.017)	
Total train loss: 0.0882

 * Prec@1 90.490 Prec@5 99.340 Loss 0.3708
Best acc: 90.620
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.0657 (0.0893)	Prec@1 97.656 (96.925)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.0546 (0.0887)	Prec@1 99.219 (96.960)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.0268 (0.0875)	Prec@1 100.000 (97.012)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0709 (0.0882)	Prec@1 97.656 (96.975)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.0525 (0.0887)	Prec@1 98.438 (96.971)	
Total train loss: 0.0888

 * Prec@1 90.690 Prec@5 99.450 Loss 0.3699
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.0723 (0.0922)	Prec@1 96.094 (96.875)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.0958 (0.0882)	Prec@1 96.875 (96.980)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0442 (0.0878)	Prec@1 97.656 (97.002)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.0491 (0.0873)	Prec@1 99.219 (96.958)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0962 (0.0877)	Prec@1 96.875 (96.949)	
Total train loss: 0.0878

 * Prec@1 90.500 Prec@5 99.430 Loss 0.3691
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.0867 (0.0914)	Prec@1 96.094 (96.865)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0934 (0.0909)	Prec@1 96.094 (96.950)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.1159 (0.0890)	Prec@1 94.531 (96.995)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.1373 (0.0892)	Prec@1 96.875 (96.985)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.1075 (0.0885)	Prec@1 96.094 (96.975)	
Total train loss: 0.0885

 * Prec@1 90.550 Prec@5 99.390 Loss 0.3706
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.1257 (0.0901)	Prec@1 96.875 (97.025)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.1008 (0.0884)	Prec@1 96.875 (97.035)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.0248 (0.0879)	Prec@1 99.219 (97.022)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0551 (0.0866)	Prec@1 98.438 (97.060)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.0524 (0.0875)	Prec@1 97.656 (97.041)	
Total train loss: 0.0875

 * Prec@1 90.530 Prec@5 99.340 Loss 0.3718
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.1068 (0.0886)	Prec@1 96.875 (96.915)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.0525 (0.0876)	Prec@1 98.438 (96.910)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.0767 (0.0862)	Prec@1 96.875 (96.962)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.1023 (0.0864)	Prec@1 98.438 (97.010)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.1294 (0.0879)	Prec@1 93.750 (96.935)	
Total train loss: 0.0880

 * Prec@1 90.520 Prec@5 99.480 Loss 0.3691
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.0496 (0.0836)	Prec@1 98.438 (97.125)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.0857 (0.0867)	Prec@1 96.875 (96.990)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.1011 (0.0881)	Prec@1 96.094 (96.968)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.1312 (0.0882)	Prec@1 95.312 (96.935)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.0751 (0.0886)	Prec@1 96.875 (96.893)	
Total train loss: 0.0886

 * Prec@1 90.460 Prec@5 99.430 Loss 0.3716
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.0956 (0.0887)	Prec@1 96.094 (96.835)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.0876 (0.0862)	Prec@1 96.875 (96.970)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.0711 (0.0867)	Prec@1 97.656 (97.032)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.0558 (0.0886)	Prec@1 97.656 (96.933)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0590 (0.0889)	Prec@1 98.438 (96.943)	
Total train loss: 0.0889

 * Prec@1 90.520 Prec@5 99.370 Loss 0.3704
Best acc: 90.690
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0293 (0.0881)	Prec@1 99.219 (97.055)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0480 (0.0873)	Prec@1 98.438 (97.025)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.0699 (0.0863)	Prec@1 98.438 (97.045)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0721 (0.0883)	Prec@1 97.656 (97.030)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.0882 (0.0876)	Prec@1 96.875 (97.031)	
Total train loss: 0.0876

 * Prec@1 90.530 Prec@5 99.410 Loss 0.3699
Best acc: 90.690
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 75.210 Prec@5 97.830 Loss 1.0303
Pre-trained Prec@1 with 15 layers frozen: 75.20999908447266 	 Loss: 1.0302734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.2925 (0.3648)	Prec@1 89.844 (88.812)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.3411 (0.3441)	Prec@1 89.844 (89.458)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.4133 (0.3260)	Prec@1 88.281 (89.800)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.1462 (0.3139)	Prec@1 95.312 (90.139)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.2517 (0.3027)	Prec@1 93.750 (90.465)	
Total train loss: 0.3028

 * Prec@1 87.400 Prec@5 99.240 Loss 0.5156
Best acc: 87.400
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.3279 (0.2416)	Prec@1 94.531 (91.967)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.2168 (0.2443)	Prec@1 92.188 (91.872)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.2018 (0.2420)	Prec@1 92.969 (91.987)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.2305 (0.2368)	Prec@1 94.531 (92.208)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.2244 (0.2331)	Prec@1 94.531 (92.328)	
Total train loss: 0.2333

 * Prec@1 87.920 Prec@5 99.360 Loss 0.4849
Best acc: 87.920
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.2330 (0.2261)	Prec@1 92.188 (92.548)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.2089 (0.2169)	Prec@1 92.188 (92.673)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.2073 (0.2131)	Prec@1 90.625 (92.795)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.1151 (0.2130)	Prec@1 97.656 (92.834)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.2030 (0.2110)	Prec@1 95.312 (92.893)	
Total train loss: 0.2111

 * Prec@1 88.170 Prec@5 99.300 Loss 0.4648
Best acc: 88.170
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.2341 (0.2019)	Prec@1 92.969 (93.169)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.1759 (0.2021)	Prec@1 94.531 (93.164)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.2754 (0.1999)	Prec@1 93.750 (93.266)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.1576 (0.1994)	Prec@1 95.312 (93.272)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.2385 (0.1964)	Prec@1 90.625 (93.291)	
Total train loss: 0.1964

 * Prec@1 88.350 Prec@5 99.400 Loss 0.4656
Best acc: 88.350
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.1891 (0.1791)	Prec@1 92.969 (94.030)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.1648 (0.1849)	Prec@1 95.312 (93.660)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.2036 (0.1860)	Prec@1 92.969 (93.623)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.2079 (0.1843)	Prec@1 93.750 (93.747)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.1054 (0.1828)	Prec@1 96.094 (93.770)	
Total train loss: 0.1828

 * Prec@1 88.400 Prec@5 99.450 Loss 0.4465
Best acc: 88.400
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.2032 (0.1865)	Prec@1 93.750 (93.740)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0891 (0.1790)	Prec@1 96.094 (93.985)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.1428 (0.1797)	Prec@1 93.750 (93.954)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.2091 (0.1795)	Prec@1 92.969 (93.918)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0939 (0.1768)	Prec@1 97.656 (93.960)	
Total train loss: 0.1768

 * Prec@1 88.400 Prec@5 99.430 Loss 0.4446
Best acc: 88.400
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.1493 (0.1693)	Prec@1 94.531 (94.111)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.2139 (0.1652)	Prec@1 91.406 (94.156)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.1427 (0.1706)	Prec@1 95.312 (94.000)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.1771 (0.1694)	Prec@1 93.750 (94.066)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.1725 (0.1691)	Prec@1 92.188 (94.085)	
Total train loss: 0.1692

 * Prec@1 88.710 Prec@5 99.420 Loss 0.4409
Best acc: 88.710
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.1418 (0.1658)	Prec@1 96.875 (94.401)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.2476 (0.1628)	Prec@1 90.625 (94.426)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.1375 (0.1620)	Prec@1 95.312 (94.448)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.1052 (0.1631)	Prec@1 97.656 (94.394)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.1565 (0.1630)	Prec@1 95.312 (94.399)	
Total train loss: 0.1631

 * Prec@1 88.870 Prec@5 99.520 Loss 0.4321
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1995 (0.1646)	Prec@1 93.750 (94.010)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.1782 (0.1616)	Prec@1 93.750 (94.296)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.1398 (0.1606)	Prec@1 95.312 (94.314)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.2103 (0.1586)	Prec@1 93.750 (94.371)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2070 (0.1583)	Prec@1 93.750 (94.427)	
Total train loss: 0.1583

 * Prec@1 88.610 Prec@5 99.470 Loss 0.4392
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.1851 (0.1518)	Prec@1 95.312 (94.902)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.1461 (0.1532)	Prec@1 92.969 (94.772)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.1487 (0.1539)	Prec@1 96.094 (94.681)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0959 (0.1527)	Prec@1 94.531 (94.674)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.1423 (0.1529)	Prec@1 94.531 (94.637)	
Total train loss: 0.1530

 * Prec@1 88.850 Prec@5 99.470 Loss 0.4253
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.1182 (0.1523)	Prec@1 96.094 (94.772)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.1236 (0.1488)	Prec@1 95.312 (94.787)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.1289 (0.1472)	Prec@1 95.312 (94.909)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.2092 (0.1487)	Prec@1 93.750 (94.859)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.1023 (0.1487)	Prec@1 96.094 (94.874)	
Total train loss: 0.1487

 * Prec@1 88.810 Prec@5 99.480 Loss 0.4285
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.1846 (0.1528)	Prec@1 94.531 (94.561)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.1044 (0.1494)	Prec@1 96.094 (94.772)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.1716 (0.1506)	Prec@1 92.188 (94.718)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.1312 (0.1512)	Prec@1 94.531 (94.681)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.1290 (0.1512)	Prec@1 96.094 (94.724)	
Total train loss: 0.1513

 * Prec@1 88.720 Prec@5 99.450 Loss 0.4329
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2079 (0.1479)	Prec@1 92.188 (94.752)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.1740 (0.1489)	Prec@1 94.531 (94.782)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.1128 (0.1465)	Prec@1 94.531 (94.915)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.1176 (0.1484)	Prec@1 95.312 (94.814)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.2529 (0.1487)	Prec@1 91.406 (94.836)	
Total train loss: 0.1488

 * Prec@1 88.710 Prec@5 99.490 Loss 0.4329
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.1764 (0.1468)	Prec@1 93.750 (94.902)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0759 (0.1467)	Prec@1 97.656 (94.907)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.1638 (0.1480)	Prec@1 95.312 (94.905)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.1152 (0.1480)	Prec@1 96.875 (94.852)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0684 (0.1466)	Prec@1 98.438 (94.862)	
Total train loss: 0.1466

 * Prec@1 88.770 Prec@5 99.470 Loss 0.4285
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.1989 (0.1520)	Prec@1 92.969 (94.942)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.1189 (0.1498)	Prec@1 96.094 (94.867)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.1434 (0.1483)	Prec@1 93.750 (94.835)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.1997 (0.1484)	Prec@1 92.969 (94.829)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.1313 (0.1494)	Prec@1 95.312 (94.758)	
Total train loss: 0.1494

 * Prec@1 88.670 Prec@5 99.470 Loss 0.4321
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.1459 (0.1477)	Prec@1 93.750 (94.621)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.0745 (0.1481)	Prec@1 97.656 (94.762)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.1195 (0.1469)	Prec@1 95.312 (94.815)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.0997 (0.1496)	Prec@1 96.094 (94.764)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.1615 (0.1491)	Prec@1 95.312 (94.762)	
Total train loss: 0.1491

 * Prec@1 88.870 Prec@5 99.490 Loss 0.4272
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.2225 (0.1474)	Prec@1 92.188 (94.972)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.1345 (0.1504)	Prec@1 94.531 (94.917)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.1963 (0.1491)	Prec@1 92.969 (94.865)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.1437 (0.1498)	Prec@1 93.750 (94.762)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.0812 (0.1494)	Prec@1 96.875 (94.746)	
Total train loss: 0.1494

 * Prec@1 88.660 Prec@5 99.450 Loss 0.4290
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.1094 (0.1507)	Prec@1 96.094 (94.772)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.1674 (0.1453)	Prec@1 93.750 (94.897)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.1072 (0.1444)	Prec@1 96.094 (94.985)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.0954 (0.1458)	Prec@1 96.094 (94.919)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.1248 (0.1470)	Prec@1 93.750 (94.880)	
Total train loss: 0.1471

 * Prec@1 88.820 Prec@5 99.450 Loss 0.4290
Best acc: 88.870
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.1404 (0.1585)	Prec@1 95.312 (94.481)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.1294 (0.1547)	Prec@1 95.312 (94.636)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.0843 (0.1496)	Prec@1 96.875 (94.845)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.1625 (0.1493)	Prec@1 92.188 (94.824)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.1111 (0.1480)	Prec@1 96.875 (94.830)	
Total train loss: 0.1480

 * Prec@1 88.900 Prec@5 99.470 Loss 0.4260
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.2048 (0.1552)	Prec@1 92.188 (94.641)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.1765 (0.1494)	Prec@1 95.312 (94.907)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.1046 (0.1489)	Prec@1 97.656 (94.872)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.0915 (0.1487)	Prec@1 95.312 (94.869)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.1412 (0.1496)	Prec@1 94.531 (94.832)	
Total train loss: 0.1497

 * Prec@1 88.830 Prec@5 99.470 Loss 0.4268
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.0896 (0.1486)	Prec@1 96.094 (94.832)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.1609 (0.1471)	Prec@1 93.750 (94.922)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.1447 (0.1491)	Prec@1 94.531 (94.865)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.1099 (0.1463)	Prec@1 96.875 (94.972)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.1914 (0.1475)	Prec@1 95.312 (94.934)	
Total train loss: 0.1475

 * Prec@1 88.730 Prec@5 99.480 Loss 0.4309
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.3047 (0.1408)	Prec@1 85.156 (95.092)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.1393 (0.1459)	Prec@1 96.875 (94.932)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.1287 (0.1471)	Prec@1 96.094 (94.935)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.0969 (0.1471)	Prec@1 96.875 (94.947)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.1332 (0.1470)	Prec@1 93.750 (94.956)	
Total train loss: 0.1470

 * Prec@1 88.730 Prec@5 99.500 Loss 0.4309
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.1748 (0.1451)	Prec@1 95.312 (94.992)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.1858 (0.1477)	Prec@1 92.969 (94.867)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.2129 (0.1479)	Prec@1 89.844 (94.888)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.0915 (0.1488)	Prec@1 95.312 (94.824)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.1238 (0.1473)	Prec@1 93.750 (94.850)	
Total train loss: 0.1473

 * Prec@1 88.860 Prec@5 99.480 Loss 0.4272
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.0995 (0.1421)	Prec@1 94.531 (95.132)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.2583 (0.1478)	Prec@1 91.406 (94.887)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.0891 (0.1459)	Prec@1 97.656 (94.919)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.1231 (0.1479)	Prec@1 95.312 (94.857)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.1440 (0.1478)	Prec@1 94.531 (94.874)	
Total train loss: 0.1478

 * Prec@1 88.830 Prec@5 99.470 Loss 0.4280
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.1162 (0.1488)	Prec@1 96.875 (94.772)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.1421 (0.1456)	Prec@1 93.750 (94.877)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.2739 (0.1454)	Prec@1 90.625 (94.892)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.2063 (0.1477)	Prec@1 91.406 (94.862)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.1209 (0.1485)	Prec@1 96.094 (94.828)	
Total train loss: 0.1483

 * Prec@1 88.580 Prec@5 99.500 Loss 0.4333
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.2494 (0.1380)	Prec@1 92.188 (95.042)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.0907 (0.1434)	Prec@1 98.438 (94.947)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.1215 (0.1452)	Prec@1 95.312 (94.932)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.1761 (0.1482)	Prec@1 92.188 (94.882)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.1030 (0.1468)	Prec@1 95.312 (94.940)	
Total train loss: 0.1471

 * Prec@1 88.890 Prec@5 99.470 Loss 0.4292
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.1320 (0.1436)	Prec@1 95.312 (94.992)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.1123 (0.1412)	Prec@1 95.312 (95.077)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.1284 (0.1462)	Prec@1 95.312 (94.952)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.1654 (0.1458)	Prec@1 94.531 (95.022)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.2340 (0.1471)	Prec@1 91.406 (94.938)	
Total train loss: 0.1470

 * Prec@1 88.740 Prec@5 99.460 Loss 0.4321
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.1190 (0.1498)	Prec@1 95.312 (94.942)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.1278 (0.1508)	Prec@1 96.094 (94.877)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.1300 (0.1493)	Prec@1 95.312 (94.875)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.2257 (0.1475)	Prec@1 93.750 (94.927)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.1115 (0.1454)	Prec@1 97.656 (94.964)	
Total train loss: 0.1454

 * Prec@1 88.900 Prec@5 99.430 Loss 0.4280
Best acc: 88.900
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.1837 (0.1518)	Prec@1 93.750 (94.421)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.0876 (0.1517)	Prec@1 97.656 (94.556)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.2727 (0.1472)	Prec@1 92.188 (94.832)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.1746 (0.1477)	Prec@1 93.750 (94.834)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.0800 (0.1474)	Prec@1 98.438 (94.846)	
Total train loss: 0.1475

 * Prec@1 88.940 Prec@5 99.440 Loss 0.4260
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.1807 (0.1510)	Prec@1 95.312 (94.671)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.1406 (0.1437)	Prec@1 95.312 (94.997)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.1183 (0.1445)	Prec@1 96.094 (94.902)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.1556 (0.1466)	Prec@1 94.531 (94.894)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.2211 (0.1471)	Prec@1 92.969 (94.868)	
Total train loss: 0.1472

 * Prec@1 88.900 Prec@5 99.460 Loss 0.4260
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.0636 (0.1499)	Prec@1 98.438 (94.792)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.1953 (0.1473)	Prec@1 93.750 (94.817)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.0836 (0.1468)	Prec@1 96.875 (94.835)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.1639 (0.1454)	Prec@1 92.969 (94.889)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.0754 (0.1455)	Prec@1 99.219 (94.914)	
Total train loss: 0.1455

 * Prec@1 88.790 Prec@5 99.480 Loss 0.4280
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.1342 (0.1482)	Prec@1 95.312 (94.892)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.1367 (0.1462)	Prec@1 96.094 (94.922)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.1669 (0.1478)	Prec@1 91.406 (94.872)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.2764 (0.1476)	Prec@1 92.969 (94.924)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.1575 (0.1472)	Prec@1 95.312 (94.914)	
Total train loss: 0.1471

 * Prec@1 88.780 Prec@5 99.480 Loss 0.4297
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.1287 (0.1416)	Prec@1 96.094 (95.333)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.2134 (0.1443)	Prec@1 92.969 (95.032)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.0707 (0.1483)	Prec@1 98.438 (94.929)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.1202 (0.1456)	Prec@1 96.875 (94.992)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.0748 (0.1459)	Prec@1 96.875 (94.930)	
Total train loss: 0.1461

 * Prec@1 88.650 Prec@5 99.480 Loss 0.4258
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.1636 (0.1493)	Prec@1 92.969 (94.832)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.1179 (0.1468)	Prec@1 96.094 (94.857)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.1578 (0.1471)	Prec@1 92.969 (94.785)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.2141 (0.1468)	Prec@1 90.625 (94.794)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.0640 (0.1476)	Prec@1 98.438 (94.790)	
Total train loss: 0.1474

 * Prec@1 88.700 Prec@5 99.450 Loss 0.4297
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.0930 (0.1478)	Prec@1 98.438 (95.062)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.1504 (0.1461)	Prec@1 94.531 (94.902)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.1840 (0.1462)	Prec@1 94.531 (94.875)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.2169 (0.1456)	Prec@1 93.750 (94.849)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.0564 (0.1460)	Prec@1 97.656 (94.876)	
Total train loss: 0.1459

 * Prec@1 88.810 Prec@5 99.470 Loss 0.4258
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.1818 (0.1488)	Prec@1 93.750 (94.822)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.1328 (0.1461)	Prec@1 96.094 (94.937)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.1857 (0.1454)	Prec@1 95.312 (94.995)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.2391 (0.1442)	Prec@1 90.625 (95.047)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.1682 (0.1445)	Prec@1 93.750 (95.008)	
Total train loss: 0.1445

 * Prec@1 88.930 Prec@5 99.460 Loss 0.4280
Best acc: 88.940
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.0918 (0.1430)	Prec@1 96.094 (94.922)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.1151 (0.1446)	Prec@1 96.094 (94.952)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.1290 (0.1455)	Prec@1 96.875 (95.009)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.1016 (0.1463)	Prec@1 95.312 (94.989)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.0989 (0.1469)	Prec@1 96.875 (94.956)	
Total train loss: 0.1468

 * Prec@1 88.960 Prec@5 99.470 Loss 0.4258
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.2045 (0.1457)	Prec@1 93.750 (94.902)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.1613 (0.1454)	Prec@1 93.750 (94.897)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.0764 (0.1450)	Prec@1 97.656 (94.892)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.1997 (0.1467)	Prec@1 92.188 (94.797)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.0989 (0.1461)	Prec@1 96.094 (94.806)	
Total train loss: 0.1460

 * Prec@1 88.890 Prec@5 99.460 Loss 0.4265
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.1454 (0.1450)	Prec@1 93.750 (94.842)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.1946 (0.1475)	Prec@1 92.188 (94.762)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.0685 (0.1461)	Prec@1 97.656 (94.848)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.1775 (0.1468)	Prec@1 95.312 (94.909)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.0665 (0.1448)	Prec@1 98.438 (94.968)	
Total train loss: 0.1447

 * Prec@1 88.850 Prec@5 99.450 Loss 0.4229
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.1064 (0.1418)	Prec@1 96.875 (95.112)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.1000 (0.1435)	Prec@1 96.875 (94.952)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.1152 (0.1458)	Prec@1 95.312 (94.885)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.1992 (0.1459)	Prec@1 92.969 (94.889)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.1539 (0.1463)	Prec@1 93.750 (94.854)	
Total train loss: 0.1462

 * Prec@1 88.800 Prec@5 99.490 Loss 0.4272
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.1240 (0.1356)	Prec@1 96.094 (95.172)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.2408 (0.1420)	Prec@1 89.844 (94.942)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.1808 (0.1455)	Prec@1 94.531 (94.835)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.0983 (0.1443)	Prec@1 96.875 (94.894)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.1164 (0.1441)	Prec@1 96.094 (94.978)	
Total train loss: 0.1439

 * Prec@1 88.870 Prec@5 99.470 Loss 0.4268
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.0775 (0.1516)	Prec@1 96.875 (94.742)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.1167 (0.1497)	Prec@1 95.312 (94.732)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.1359 (0.1447)	Prec@1 96.875 (94.909)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.3081 (0.1469)	Prec@1 89.062 (94.864)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.1804 (0.1463)	Prec@1 91.406 (94.856)	
Total train loss: 0.1462

 * Prec@1 88.940 Prec@5 99.490 Loss 0.4285
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.1128 (0.1466)	Prec@1 96.875 (94.782)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.1267 (0.1489)	Prec@1 93.750 (94.702)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.1809 (0.1488)	Prec@1 92.969 (94.758)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.0869 (0.1472)	Prec@1 97.656 (94.797)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.1581 (0.1476)	Prec@1 94.531 (94.828)	
Total train loss: 0.1475

 * Prec@1 88.820 Prec@5 99.460 Loss 0.4260
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.1141 (0.1451)	Prec@1 96.875 (95.092)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.1079 (0.1448)	Prec@1 96.094 (95.142)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.0792 (0.1455)	Prec@1 95.312 (95.099)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.1804 (0.1471)	Prec@1 93.750 (94.997)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.0588 (0.1458)	Prec@1 99.219 (94.978)	
Total train loss: 0.1458

 * Prec@1 88.880 Prec@5 99.450 Loss 0.4236
Best acc: 88.960
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.2214 (0.1565)	Prec@1 95.312 (94.661)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.0402 (0.1485)	Prec@1 100.000 (94.912)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.1022 (0.1470)	Prec@1 96.875 (94.868)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.1133 (0.1441)	Prec@1 96.875 (94.974)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.1345 (0.1451)	Prec@1 96.094 (94.938)	
Total train loss: 0.1450

 * Prec@1 89.000 Prec@5 99.460 Loss 0.4216
Best acc: 89.000
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.2211 (0.1421)	Prec@1 92.188 (95.012)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.2227 (0.1443)	Prec@1 92.188 (94.932)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.1517 (0.1460)	Prec@1 94.531 (94.905)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.0914 (0.1445)	Prec@1 96.094 (94.977)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.1823 (0.1449)	Prec@1 94.531 (94.950)	
Total train loss: 0.1449

 * Prec@1 88.860 Prec@5 99.440 Loss 0.4265
Best acc: 89.000
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.1864 (0.1427)	Prec@1 95.312 (94.942)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.1201 (0.1427)	Prec@1 96.875 (94.992)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.1010 (0.1440)	Prec@1 96.094 (94.952)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.1031 (0.1424)	Prec@1 97.656 (95.032)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.2410 (0.1437)	Prec@1 92.969 (95.016)	
Total train loss: 0.1438

 * Prec@1 88.850 Prec@5 99.500 Loss 0.4336
Best acc: 89.000
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.2241 (0.1500)	Prec@1 92.188 (94.621)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.1827 (0.1462)	Prec@1 95.312 (94.867)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.1611 (0.1437)	Prec@1 92.969 (95.022)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.0956 (0.1464)	Prec@1 94.531 (94.932)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.1621 (0.1454)	Prec@1 91.406 (94.990)	
Total train loss: 0.1452

 * Prec@1 88.850 Prec@5 99.480 Loss 0.4277
Best acc: 89.000
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.2356 (0.1394)	Prec@1 92.969 (95.302)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.1660 (0.1454)	Prec@1 94.531 (95.087)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.1506 (0.1467)	Prec@1 92.969 (95.029)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.1896 (0.1453)	Prec@1 94.531 (95.065)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.0584 (0.1430)	Prec@1 99.219 (95.158)	
Total train loss: 0.1430

 * Prec@1 88.930 Prec@5 99.470 Loss 0.4204
Best acc: 89.000
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.0967 (0.1421)	Prec@1 97.656 (95.172)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.0812 (0.1403)	Prec@1 97.656 (95.177)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.1339 (0.1425)	Prec@1 94.531 (95.122)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.0762 (0.1440)	Prec@1 96.094 (95.102)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.2607 (0.1438)	Prec@1 92.969 (95.086)	
Total train loss: 0.1439

 * Prec@1 88.730 Prec@5 99.460 Loss 0.4316
Best acc: 89.000
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 74.150 Prec@5 97.770 Loss 1.0439
Pre-trained Prec@1 with 17 layers frozen: 74.1500015258789 	 Loss: 1.0439453125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.3779 (0.4646)	Prec@1 89.844 (86.649)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.3120 (0.4438)	Prec@1 89.844 (87.074)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.2966 (0.4178)	Prec@1 90.625 (87.413)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.4263 (0.4083)	Prec@1 89.062 (87.730)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.3804 (0.3956)	Prec@1 87.500 (88.001)	
Total train loss: 0.3952

 * Prec@1 86.310 Prec@5 98.980 Loss 0.5601
Best acc: 86.310
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.3958 (0.3154)	Prec@1 85.156 (89.804)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.2384 (0.3104)	Prec@1 92.188 (89.954)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.2803 (0.3164)	Prec@1 91.406 (89.777)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.3633 (0.3149)	Prec@1 85.938 (89.851)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.2793 (0.3176)	Prec@1 91.406 (89.792)	
Total train loss: 0.3174

 * Prec@1 86.830 Prec@5 99.010 Loss 0.5303
Best acc: 86.830
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.3064 (0.2882)	Prec@1 89.844 (90.304)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.2507 (0.2867)	Prec@1 90.625 (90.465)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.3469 (0.2940)	Prec@1 88.281 (90.301)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.3425 (0.2914)	Prec@1 91.406 (90.350)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.2822 (0.2896)	Prec@1 90.625 (90.407)	
Total train loss: 0.2895

 * Prec@1 86.980 Prec@5 99.110 Loss 0.5146
Best acc: 86.980
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4026 (0.2762)	Prec@1 88.281 (90.735)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.2583 (0.2664)	Prec@1 91.406 (90.946)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.1483 (0.2688)	Prec@1 96.094 (90.986)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.2742 (0.2736)	Prec@1 90.625 (90.918)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.2367 (0.2741)	Prec@1 92.969 (90.925)	
Total train loss: 0.2741

 * Prec@1 87.230 Prec@5 99.140 Loss 0.4949
Best acc: 87.230
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.2471 (0.2697)	Prec@1 90.625 (90.835)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.2961 (0.2712)	Prec@1 89.844 (90.875)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.2395 (0.2653)	Prec@1 92.969 (91.126)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.2793 (0.2614)	Prec@1 92.188 (91.256)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.2549 (0.2634)	Prec@1 91.406 (91.192)	
Total train loss: 0.2635

 * Prec@1 87.180 Prec@5 99.170 Loss 0.4949
Best acc: 87.230
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.2040 (0.2598)	Prec@1 91.406 (91.086)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.2349 (0.2597)	Prec@1 92.969 (91.171)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.3015 (0.2554)	Prec@1 89.844 (91.293)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.1564 (0.2550)	Prec@1 94.531 (91.369)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.1520 (0.2515)	Prec@1 93.750 (91.492)	
Total train loss: 0.2515

 * Prec@1 87.400 Prec@5 99.320 Loss 0.4834
Best acc: 87.400
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.1982 (0.2434)	Prec@1 93.750 (91.907)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3469 (0.2464)	Prec@1 88.281 (91.762)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.2856 (0.2478)	Prec@1 91.406 (91.607)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.2532 (0.2462)	Prec@1 90.625 (91.662)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.2864 (0.2459)	Prec@1 92.188 (91.701)	
Total train loss: 0.2459

 * Prec@1 87.490 Prec@5 99.340 Loss 0.4709
Best acc: 87.490
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2747 (0.2425)	Prec@1 93.750 (91.837)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.2460 (0.2444)	Prec@1 92.188 (91.822)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.1082 (0.2414)	Prec@1 96.875 (91.847)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.1790 (0.2417)	Prec@1 92.188 (91.824)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.3438 (0.2390)	Prec@1 88.281 (91.861)	
Total train loss: 0.2392

 * Prec@1 87.530 Prec@5 99.390 Loss 0.4612
Best acc: 87.530
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1362 (0.2465)	Prec@1 96.094 (91.917)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.1234 (0.2390)	Prec@1 97.656 (91.867)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.2410 (0.2367)	Prec@1 89.844 (91.967)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.2041 (0.2344)	Prec@1 92.188 (92.045)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2371 (0.2330)	Prec@1 90.625 (92.061)	
Total train loss: 0.2329

 * Prec@1 87.600 Prec@5 99.290 Loss 0.4661
Best acc: 87.600
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.1853 (0.2282)	Prec@1 94.531 (92.208)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.2494 (0.2282)	Prec@1 91.406 (92.288)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.1418 (0.2293)	Prec@1 95.312 (92.294)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.1370 (0.2261)	Prec@1 96.094 (92.348)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.1827 (0.2273)	Prec@1 94.531 (92.300)	
Total train loss: 0.2276

 * Prec@1 87.500 Prec@5 99.340 Loss 0.4585
Best acc: 87.600
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.2795 (0.2255)	Prec@1 94.531 (92.087)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.1400 (0.2292)	Prec@1 95.312 (92.107)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2949 (0.2241)	Prec@1 89.844 (92.311)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.1606 (0.2248)	Prec@1 93.750 (92.265)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.2539 (0.2247)	Prec@1 90.625 (92.288)	
Total train loss: 0.2248

 * Prec@1 87.520 Prec@5 99.320 Loss 0.4668
Best acc: 87.600
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.1226 (0.2272)	Prec@1 96.094 (91.867)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.1786 (0.2244)	Prec@1 92.188 (92.097)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2063 (0.2271)	Prec@1 94.531 (92.111)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.2561 (0.2248)	Prec@1 89.844 (92.265)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.1669 (0.2234)	Prec@1 96.094 (92.300)	
Total train loss: 0.2234

 * Prec@1 87.380 Prec@5 99.340 Loss 0.4585
Best acc: 87.600
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2622 (0.2232)	Prec@1 92.188 (92.358)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.2261 (0.2207)	Prec@1 89.844 (92.308)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2700 (0.2209)	Prec@1 92.969 (92.298)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.2355 (0.2215)	Prec@1 89.844 (92.355)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.2322 (0.2240)	Prec@1 92.969 (92.322)	
Total train loss: 0.2240

 * Prec@1 87.680 Prec@5 99.390 Loss 0.4590
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.2927 (0.2298)	Prec@1 89.062 (92.047)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.1761 (0.2279)	Prec@1 93.750 (92.152)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3040 (0.2254)	Prec@1 90.625 (92.318)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.1912 (0.2240)	Prec@1 95.312 (92.388)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.2167 (0.2228)	Prec@1 91.406 (92.418)	
Total train loss: 0.2227

 * Prec@1 87.600 Prec@5 99.290 Loss 0.4636
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2036 (0.2210)	Prec@1 94.531 (92.378)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3252 (0.2258)	Prec@1 89.062 (92.293)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.1010 (0.2219)	Prec@1 98.438 (92.411)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.1399 (0.2225)	Prec@1 94.531 (92.355)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3276 (0.2229)	Prec@1 89.062 (92.326)	
Total train loss: 0.2231

 * Prec@1 87.610 Prec@5 99.340 Loss 0.4668
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.3184 (0.2177)	Prec@1 91.406 (92.588)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.2778 (0.2240)	Prec@1 88.281 (92.298)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.2130 (0.2240)	Prec@1 90.625 (92.304)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.1107 (0.2220)	Prec@1 96.094 (92.348)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.3530 (0.2243)	Prec@1 89.062 (92.292)	
Total train loss: 0.2244

 * Prec@1 87.580 Prec@5 99.260 Loss 0.4644
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.1315 (0.2231)	Prec@1 94.531 (92.458)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.3008 (0.2203)	Prec@1 87.500 (92.378)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3853 (0.2225)	Prec@1 89.062 (92.321)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.1805 (0.2227)	Prec@1 93.750 (92.270)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.2086 (0.2213)	Prec@1 92.188 (92.386)	
Total train loss: 0.2212

 * Prec@1 87.510 Prec@5 99.380 Loss 0.4590
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.2365 (0.2332)	Prec@1 91.406 (92.067)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.2642 (0.2259)	Prec@1 87.500 (92.167)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.2974 (0.2256)	Prec@1 91.406 (92.211)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.1664 (0.2264)	Prec@1 92.188 (92.215)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.2043 (0.2258)	Prec@1 92.188 (92.244)	
Total train loss: 0.2258

 * Prec@1 87.570 Prec@5 99.360 Loss 0.4641
Best acc: 87.680
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.1788 (0.2203)	Prec@1 92.188 (92.318)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.1663 (0.2163)	Prec@1 96.094 (92.578)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.2057 (0.2196)	Prec@1 92.969 (92.331)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.1772 (0.2223)	Prec@1 93.750 (92.245)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.2015 (0.2227)	Prec@1 92.188 (92.296)	
Total train loss: 0.2227

 * Prec@1 87.720 Prec@5 99.350 Loss 0.4624
Best acc: 87.720
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.1902 (0.2183)	Prec@1 93.750 (92.488)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.2468 (0.2241)	Prec@1 93.750 (92.383)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.1484 (0.2215)	Prec@1 95.312 (92.435)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.1755 (0.2238)	Prec@1 93.750 (92.345)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.3003 (0.2226)	Prec@1 89.062 (92.424)	
Total train loss: 0.2226

 * Prec@1 87.760 Prec@5 99.350 Loss 0.4565
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.2377 (0.2133)	Prec@1 91.406 (92.738)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.2751 (0.2105)	Prec@1 89.062 (92.803)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.2037 (0.2162)	Prec@1 94.531 (92.652)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.2568 (0.2215)	Prec@1 92.188 (92.495)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.1888 (0.2216)	Prec@1 94.531 (92.484)	
Total train loss: 0.2215

 * Prec@1 87.610 Prec@5 99.390 Loss 0.4592
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.1443 (0.2170)	Prec@1 93.750 (92.638)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.2598 (0.2187)	Prec@1 90.625 (92.598)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.2124 (0.2202)	Prec@1 92.188 (92.535)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.2437 (0.2207)	Prec@1 91.406 (92.458)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.2476 (0.2218)	Prec@1 89.062 (92.316)	
Total train loss: 0.2221

 * Prec@1 87.540 Prec@5 99.360 Loss 0.4604
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.1356 (0.2206)	Prec@1 93.750 (92.458)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.1975 (0.2156)	Prec@1 95.312 (92.633)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.1296 (0.2215)	Prec@1 94.531 (92.488)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.1980 (0.2207)	Prec@1 92.969 (92.480)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.2844 (0.2196)	Prec@1 89.844 (92.476)	
Total train loss: 0.2199

 * Prec@1 87.580 Prec@5 99.360 Loss 0.4617
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.2568 (0.2229)	Prec@1 90.625 (92.348)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.1509 (0.2289)	Prec@1 95.312 (91.977)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.1821 (0.2258)	Prec@1 92.188 (92.154)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.1362 (0.2226)	Prec@1 94.531 (92.270)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.3057 (0.2223)	Prec@1 90.625 (92.294)	
Total train loss: 0.2221

 * Prec@1 87.670 Prec@5 99.390 Loss 0.4592
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.2520 (0.2185)	Prec@1 92.969 (92.598)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.3247 (0.2255)	Prec@1 88.281 (92.408)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.2781 (0.2237)	Prec@1 91.406 (92.334)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.2002 (0.2208)	Prec@1 90.625 (92.420)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.2262 (0.2208)	Prec@1 93.750 (92.406)	
Total train loss: 0.2209

 * Prec@1 87.660 Prec@5 99.350 Loss 0.4641
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.2338 (0.2259)	Prec@1 89.844 (91.967)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.3401 (0.2273)	Prec@1 85.938 (92.117)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.2502 (0.2227)	Prec@1 90.625 (92.348)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.1938 (0.2209)	Prec@1 91.406 (92.390)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.2109 (0.2211)	Prec@1 91.406 (92.358)	
Total train loss: 0.2212

 * Prec@1 87.640 Prec@5 99.360 Loss 0.4568
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.1610 (0.2208)	Prec@1 94.531 (92.288)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.2178 (0.2193)	Prec@1 92.969 (92.423)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.2355 (0.2172)	Prec@1 93.750 (92.488)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.3369 (0.2198)	Prec@1 88.281 (92.413)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.1732 (0.2199)	Prec@1 94.531 (92.420)	
Total train loss: 0.2199

 * Prec@1 87.630 Prec@5 99.440 Loss 0.4592
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.1840 (0.2174)	Prec@1 95.312 (92.588)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.1675 (0.2163)	Prec@1 95.312 (92.548)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.1256 (0.2205)	Prec@1 96.094 (92.495)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.1647 (0.2201)	Prec@1 94.531 (92.508)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.1816 (0.2202)	Prec@1 93.750 (92.518)	
Total train loss: 0.2203

 * Prec@1 87.730 Prec@5 99.390 Loss 0.4580
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.1849 (0.2188)	Prec@1 92.969 (92.248)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.2886 (0.2192)	Prec@1 89.062 (92.338)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.2311 (0.2169)	Prec@1 91.406 (92.448)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.3457 (0.2168)	Prec@1 88.281 (92.468)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.1523 (0.2194)	Prec@1 94.531 (92.412)	
Total train loss: 0.2195

 * Prec@1 87.530 Prec@5 99.400 Loss 0.4612
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.2354 (0.2312)	Prec@1 92.188 (92.117)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.3689 (0.2206)	Prec@1 89.062 (92.388)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.2499 (0.2219)	Prec@1 91.406 (92.334)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.2449 (0.2202)	Prec@1 92.969 (92.435)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.1809 (0.2209)	Prec@1 92.188 (92.414)	
Total train loss: 0.2211

 * Prec@1 87.650 Prec@5 99.330 Loss 0.4609
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.2175 (0.2245)	Prec@1 93.750 (92.378)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.2615 (0.2214)	Prec@1 90.625 (92.388)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.1168 (0.2189)	Prec@1 96.875 (92.508)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.2396 (0.2196)	Prec@1 91.406 (92.395)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.2296 (0.2208)	Prec@1 92.969 (92.382)	
Total train loss: 0.2208

 * Prec@1 87.550 Prec@5 99.400 Loss 0.4585
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.1711 (0.2176)	Prec@1 92.188 (92.438)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.3052 (0.2209)	Prec@1 89.062 (92.333)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.1475 (0.2189)	Prec@1 95.312 (92.495)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.1576 (0.2173)	Prec@1 94.531 (92.506)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.1686 (0.2198)	Prec@1 93.750 (92.424)	
Total train loss: 0.2200

 * Prec@1 87.460 Prec@5 99.310 Loss 0.4680
Best acc: 87.760
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.2416 (0.2232)	Prec@1 90.625 (92.208)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.1803 (0.2213)	Prec@1 92.969 (92.343)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.1322 (0.2206)	Prec@1 97.656 (92.421)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.1879 (0.2181)	Prec@1 94.531 (92.470)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.1718 (0.2183)	Prec@1 95.312 (92.518)	
Total train loss: 0.2182

 * Prec@1 87.800 Prec@5 99.340 Loss 0.4568
Best acc: 87.800
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.1847 (0.2019)	Prec@1 94.531 (92.939)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.2302 (0.2054)	Prec@1 89.844 (92.929)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.2590 (0.2136)	Prec@1 91.406 (92.665)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.2057 (0.2147)	Prec@1 95.312 (92.566)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.2878 (0.2187)	Prec@1 91.406 (92.442)	
Total train loss: 0.2187

 * Prec@1 87.560 Prec@5 99.400 Loss 0.4553
Best acc: 87.800
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.2424 (0.2253)	Prec@1 90.625 (92.228)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.1151 (0.2207)	Prec@1 96.875 (92.373)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.1422 (0.2202)	Prec@1 96.094 (92.445)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.2411 (0.2189)	Prec@1 93.750 (92.475)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.1714 (0.2193)	Prec@1 96.094 (92.440)	
Total train loss: 0.2195

 * Prec@1 87.720 Prec@5 99.340 Loss 0.4590
Best acc: 87.800
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.1737 (0.2206)	Prec@1 89.062 (92.468)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.1938 (0.2209)	Prec@1 94.531 (92.378)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.3628 (0.2210)	Prec@1 89.062 (92.368)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.1764 (0.2218)	Prec@1 91.406 (92.263)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.2600 (0.2199)	Prec@1 91.406 (92.398)	
Total train loss: 0.2199

 * Prec@1 87.570 Prec@5 99.340 Loss 0.4585
Best acc: 87.800
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.2083 (0.2173)	Prec@1 93.750 (92.538)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.2664 (0.2153)	Prec@1 91.406 (92.563)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.1400 (0.2194)	Prec@1 93.750 (92.465)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.1415 (0.2177)	Prec@1 96.094 (92.536)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.1622 (0.2194)	Prec@1 95.312 (92.516)	
Total train loss: 0.2196

 * Prec@1 87.890 Prec@5 99.310 Loss 0.4597
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.2573 (0.2172)	Prec@1 90.625 (92.578)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.3757 (0.2208)	Prec@1 85.938 (92.323)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.2515 (0.2210)	Prec@1 92.188 (92.284)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.2328 (0.2173)	Prec@1 92.969 (92.403)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.2512 (0.2188)	Prec@1 96.875 (92.432)	
Total train loss: 0.2188

 * Prec@1 87.580 Prec@5 99.360 Loss 0.4590
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.1316 (0.2226)	Prec@1 94.531 (92.368)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.0969 (0.2152)	Prec@1 96.875 (92.718)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.2502 (0.2167)	Prec@1 91.406 (92.535)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.2098 (0.2161)	Prec@1 92.969 (92.581)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.3076 (0.2205)	Prec@1 91.406 (92.436)	
Total train loss: 0.2205

 * Prec@1 87.740 Prec@5 99.310 Loss 0.4590
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.1150 (0.2060)	Prec@1 96.094 (92.879)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.1678 (0.2117)	Prec@1 94.531 (92.578)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.1476 (0.2179)	Prec@1 94.531 (92.471)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.1874 (0.2179)	Prec@1 92.188 (92.458)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.2144 (0.2195)	Prec@1 92.188 (92.416)	
Total train loss: 0.2193

 * Prec@1 87.670 Prec@5 99.370 Loss 0.4622
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.1533 (0.2289)	Prec@1 96.094 (92.107)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.2052 (0.2234)	Prec@1 90.625 (92.238)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.2852 (0.2221)	Prec@1 89.844 (92.281)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.1559 (0.2187)	Prec@1 95.312 (92.448)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.2451 (0.2186)	Prec@1 92.969 (92.472)	
Total train loss: 0.2185

 * Prec@1 87.640 Prec@5 99.320 Loss 0.4600
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.2930 (0.2098)	Prec@1 88.281 (92.628)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.1130 (0.2104)	Prec@1 97.656 (92.608)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.2507 (0.2162)	Prec@1 95.312 (92.565)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.2412 (0.2160)	Prec@1 89.062 (92.548)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.2922 (0.2184)	Prec@1 89.062 (92.492)	
Total train loss: 0.2184

 * Prec@1 87.620 Prec@5 99.370 Loss 0.4597
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.1276 (0.2121)	Prec@1 93.750 (92.628)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.2429 (0.2182)	Prec@1 90.625 (92.398)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.1544 (0.2201)	Prec@1 92.969 (92.401)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.1819 (0.2199)	Prec@1 93.750 (92.415)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.2827 (0.2188)	Prec@1 89.062 (92.460)	
Total train loss: 0.2187

 * Prec@1 87.740 Prec@5 99.370 Loss 0.4561
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.1195 (0.2208)	Prec@1 96.094 (92.598)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.1602 (0.2179)	Prec@1 95.312 (92.523)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.1962 (0.2160)	Prec@1 91.406 (92.565)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.2930 (0.2203)	Prec@1 89.844 (92.430)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.2668 (0.2178)	Prec@1 92.188 (92.530)	
Total train loss: 0.2178

 * Prec@1 87.600 Prec@5 99.350 Loss 0.4592
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.2773 (0.2231)	Prec@1 89.844 (92.248)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.2556 (0.2175)	Prec@1 88.281 (92.513)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.2112 (0.2155)	Prec@1 89.844 (92.615)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.2150 (0.2171)	Prec@1 92.969 (92.513)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.2981 (0.2176)	Prec@1 89.844 (92.536)	
Total train loss: 0.2177

 * Prec@1 87.770 Prec@5 99.380 Loss 0.4597
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.3464 (0.2078)	Prec@1 88.281 (92.698)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.2169 (0.2185)	Prec@1 90.625 (92.313)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.1650 (0.2154)	Prec@1 93.750 (92.508)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.1847 (0.2141)	Prec@1 94.531 (92.583)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.3276 (0.2173)	Prec@1 88.281 (92.426)	
Total train loss: 0.2172

 * Prec@1 87.730 Prec@5 99.380 Loss 0.4568
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.1705 (0.2095)	Prec@1 95.312 (93.069)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.2123 (0.2138)	Prec@1 92.969 (92.733)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.2878 (0.2134)	Prec@1 88.281 (92.652)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.1775 (0.2148)	Prec@1 93.750 (92.601)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.1937 (0.2170)	Prec@1 92.188 (92.536)	
Total train loss: 0.2171

 * Prec@1 87.630 Prec@5 99.420 Loss 0.4597
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.1510 (0.2261)	Prec@1 96.875 (91.957)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.1692 (0.2239)	Prec@1 95.312 (92.147)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.1614 (0.2173)	Prec@1 92.969 (92.431)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.2786 (0.2177)	Prec@1 89.844 (92.398)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.2620 (0.2170)	Prec@1 89.062 (92.498)	
Total train loss: 0.2174

 * Prec@1 87.460 Prec@5 99.410 Loss 0.4524
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.1917 (0.2070)	Prec@1 94.531 (92.819)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.2018 (0.2166)	Prec@1 92.969 (92.543)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.2585 (0.2152)	Prec@1 90.625 (92.645)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.1232 (0.2156)	Prec@1 95.312 (92.638)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.1357 (0.2169)	Prec@1 94.531 (92.614)	
Total train loss: 0.2167

 * Prec@1 87.660 Prec@5 99.390 Loss 0.4585
Best acc: 87.890
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.2314 (0.2206)	Prec@1 90.625 (92.378)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.2026 (0.2182)	Prec@1 92.188 (92.373)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.1776 (0.2205)	Prec@1 92.188 (92.394)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.1616 (0.2191)	Prec@1 94.531 (92.458)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.2915 (0.2191)	Prec@1 89.844 (92.436)	
Total train loss: 0.2191

 * Prec@1 87.690 Prec@5 99.360 Loss 0.4573
Best acc: 87.890
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 73.620 Prec@5 97.920 Loss 1.0078
Pre-trained Prec@1 with 19 layers frozen: 73.6199951171875 	 Loss: 1.0078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.6406 (0.5462)	Prec@1 81.250 (83.694)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.4033 (0.5276)	Prec@1 85.938 (84.175)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.3894 (0.5147)	Prec@1 89.844 (84.368)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6226 (0.5139)	Prec@1 81.250 (84.465)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.7393 (0.5090)	Prec@1 79.688 (84.537)	
Total train loss: 0.5090

 * Prec@1 84.080 Prec@5 98.930 Loss 0.6577
Best acc: 84.080
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.3669 (0.4701)	Prec@1 92.188 (85.717)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5044 (0.4664)	Prec@1 82.812 (85.552)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5005 (0.4751)	Prec@1 85.938 (85.337)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4980 (0.4725)	Prec@1 82.812 (85.387)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.4055 (0.4690)	Prec@1 88.281 (85.411)	
Total train loss: 0.4687

 * Prec@1 84.540 Prec@5 98.990 Loss 0.6177
Best acc: 84.540
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5977 (0.4579)	Prec@1 82.031 (85.677)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5752 (0.4625)	Prec@1 82.031 (85.437)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.3262 (0.4585)	Prec@1 86.719 (85.647)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.3716 (0.4547)	Prec@1 88.281 (85.762)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.4949 (0.4513)	Prec@1 81.250 (85.831)	
Total train loss: 0.4514

 * Prec@1 84.810 Prec@5 98.960 Loss 0.6030
Best acc: 84.810
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.3005 (0.4314)	Prec@1 87.500 (86.298)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6172 (0.4390)	Prec@1 80.469 (86.063)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.4192 (0.4419)	Prec@1 87.500 (85.914)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.4500 (0.4391)	Prec@1 84.375 (86.023)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.3962 (0.4371)	Prec@1 88.281 (86.126)	
Total train loss: 0.4373

 * Prec@1 85.020 Prec@5 98.990 Loss 0.5903
Best acc: 85.020
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4421 (0.4320)	Prec@1 82.031 (86.228)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.3542 (0.4437)	Prec@1 89.062 (85.938)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4260 (0.4306)	Prec@1 83.594 (86.185)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.4587 (0.4273)	Prec@1 83.594 (86.341)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.4304 (0.4288)	Prec@1 85.156 (86.308)	
Total train loss: 0.4287

 * Prec@1 85.090 Prec@5 99.030 Loss 0.5693
Best acc: 85.090
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4023 (0.4360)	Prec@1 86.719 (85.998)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3906 (0.4254)	Prec@1 88.281 (86.403)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.4038 (0.4234)	Prec@1 85.938 (86.485)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.3342 (0.4207)	Prec@1 89.844 (86.538)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.6260 (0.4182)	Prec@1 83.594 (86.645)	
Total train loss: 0.4185

 * Prec@1 84.790 Prec@5 99.010 Loss 0.5859
Best acc: 85.090
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.5811 (0.4320)	Prec@1 79.688 (86.268)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.4355 (0.4167)	Prec@1 80.469 (86.498)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3657 (0.4203)	Prec@1 90.625 (86.445)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.3799 (0.4135)	Prec@1 85.938 (86.609)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.4006 (0.4138)	Prec@1 87.500 (86.562)	
Total train loss: 0.4139

 * Prec@1 85.100 Prec@5 99.020 Loss 0.5737
Best acc: 85.100
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.5000 (0.4170)	Prec@1 82.812 (86.508)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3496 (0.4182)	Prec@1 88.281 (86.443)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.3140 (0.4110)	Prec@1 91.406 (86.642)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.5273 (0.4117)	Prec@1 83.594 (86.591)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2705 (0.4100)	Prec@1 90.625 (86.633)	
Total train loss: 0.4106

 * Prec@1 85.050 Prec@5 99.110 Loss 0.5698
Best acc: 85.100
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.2235 (0.3984)	Prec@1 93.750 (86.979)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.2661 (0.4106)	Prec@1 90.625 (86.754)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.2505 (0.4063)	Prec@1 91.406 (86.842)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.3291 (0.4076)	Prec@1 89.062 (86.821)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2444 (0.4083)	Prec@1 92.188 (86.765)	
Total train loss: 0.4085

 * Prec@1 85.080 Prec@5 98.980 Loss 0.5679
Best acc: 85.100
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.5220 (0.4115)	Prec@1 82.812 (86.478)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3965 (0.4149)	Prec@1 89.062 (86.483)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3418 (0.4076)	Prec@1 87.500 (86.619)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.2186 (0.4038)	Prec@1 90.625 (86.829)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3984 (0.4032)	Prec@1 86.719 (86.883)	
Total train loss: 0.4035

 * Prec@1 85.170 Prec@5 99.000 Loss 0.5566
Best acc: 85.170
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4167 (0.4111)	Prec@1 88.281 (86.649)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3938 (0.4020)	Prec@1 85.938 (86.869)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.4080 (0.4025)	Prec@1 88.281 (86.862)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4875 (0.3984)	Prec@1 83.594 (86.969)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3652 (0.4014)	Prec@1 85.156 (86.873)	
Total train loss: 0.4016

 * Prec@1 85.050 Prec@5 98.970 Loss 0.5630
Best acc: 85.170
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.2744 (0.3949)	Prec@1 87.500 (87.169)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4812 (0.4091)	Prec@1 89.062 (86.669)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3975 (0.4058)	Prec@1 88.281 (86.719)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.3125 (0.3996)	Prec@1 92.188 (86.929)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.4045 (0.4023)	Prec@1 82.812 (86.819)	
Total train loss: 0.4024

 * Prec@1 85.160 Prec@5 98.950 Loss 0.5615
Best acc: 85.170
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3057 (0.3997)	Prec@1 87.500 (86.859)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3945 (0.3972)	Prec@1 88.281 (86.809)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.3315 (0.4014)	Prec@1 88.281 (86.802)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3152 (0.4023)	Prec@1 87.500 (86.759)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.4475 (0.4014)	Prec@1 87.500 (86.839)	
Total train loss: 0.4012

 * Prec@1 84.930 Prec@5 99.080 Loss 0.5605
Best acc: 85.170
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4641 (0.4085)	Prec@1 85.156 (86.799)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4836 (0.4025)	Prec@1 86.719 (86.944)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4585 (0.4043)	Prec@1 84.375 (86.946)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3584 (0.4072)	Prec@1 88.281 (86.894)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.5254 (0.4030)	Prec@1 85.156 (87.003)	
Total train loss: 0.4030

 * Prec@1 85.230 Prec@5 99.040 Loss 0.5635
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.3093 (0.4055)	Prec@1 89.062 (86.959)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3679 (0.4006)	Prec@1 88.281 (86.904)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.2854 (0.4012)	Prec@1 90.625 (86.906)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.4502 (0.3980)	Prec@1 82.812 (86.937)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.2423 (0.4016)	Prec@1 90.625 (86.855)	
Total train loss: 0.4017

 * Prec@1 85.040 Prec@5 99.010 Loss 0.5630
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.3545 (0.3971)	Prec@1 91.406 (86.929)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.2822 (0.4033)	Prec@1 91.406 (86.789)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.4639 (0.3986)	Prec@1 85.156 (86.979)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.3406 (0.4017)	Prec@1 87.500 (86.879)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.3083 (0.4001)	Prec@1 87.500 (86.929)	
Total train loss: 0.4002

 * Prec@1 85.040 Prec@5 98.980 Loss 0.5605
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.4094 (0.3941)	Prec@1 85.156 (86.969)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.4670 (0.3967)	Prec@1 82.812 (86.904)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3684 (0.3973)	Prec@1 89.844 (86.932)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.2949 (0.3982)	Prec@1 89.844 (86.974)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.5171 (0.4009)	Prec@1 82.031 (86.967)	
Total train loss: 0.4008

 * Prec@1 85.230 Prec@5 99.050 Loss 0.5591
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.4438 (0.4093)	Prec@1 87.500 (86.719)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.3501 (0.4061)	Prec@1 90.625 (86.784)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.4944 (0.4070)	Prec@1 83.594 (86.735)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.1721 (0.4005)	Prec@1 94.531 (86.984)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.3940 (0.4018)	Prec@1 86.719 (86.863)	
Total train loss: 0.4018

 * Prec@1 85.030 Prec@5 99.050 Loss 0.5615
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.4148 (0.3922)	Prec@1 85.938 (87.190)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.4368 (0.3991)	Prec@1 84.375 (87.054)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.3657 (0.4020)	Prec@1 87.500 (86.852)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.4548 (0.4030)	Prec@1 82.031 (86.816)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.4084 (0.4027)	Prec@1 87.500 (86.803)	
Total train loss: 0.4023

 * Prec@1 85.220 Prec@5 99.010 Loss 0.5562
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.4807 (0.4020)	Prec@1 84.375 (87.039)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.3943 (0.4079)	Prec@1 85.938 (86.934)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.3713 (0.4057)	Prec@1 85.156 (86.902)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.3052 (0.4038)	Prec@1 88.281 (86.866)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.5347 (0.4022)	Prec@1 85.938 (86.817)	
Total train loss: 0.4024

 * Prec@1 85.190 Prec@5 99.020 Loss 0.5630
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.3608 (0.3969)	Prec@1 87.500 (87.039)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.5825 (0.3953)	Prec@1 85.156 (86.964)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.3389 (0.4004)	Prec@1 90.625 (86.896)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.4121 (0.4043)	Prec@1 85.938 (86.819)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.4990 (0.4023)	Prec@1 82.812 (86.813)	
Total train loss: 0.4026

 * Prec@1 85.170 Prec@5 98.980 Loss 0.5635
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.4138 (0.3933)	Prec@1 86.719 (87.420)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.3057 (0.3908)	Prec@1 89.062 (87.149)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.2747 (0.3980)	Prec@1 92.188 (87.009)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.4133 (0.4002)	Prec@1 85.938 (86.907)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.2942 (0.3990)	Prec@1 89.844 (86.951)	
Total train loss: 0.3991

 * Prec@1 84.930 Prec@5 98.970 Loss 0.5732
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.3176 (0.4020)	Prec@1 92.188 (87.079)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.3374 (0.3977)	Prec@1 89.844 (87.089)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.5083 (0.4036)	Prec@1 85.156 (86.946)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.2959 (0.4023)	Prec@1 89.062 (86.912)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.4348 (0.4046)	Prec@1 85.156 (86.887)	
Total train loss: 0.4045

 * Prec@1 85.190 Prec@5 99.030 Loss 0.5640
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.3550 (0.4083)	Prec@1 87.500 (86.418)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.5576 (0.4051)	Prec@1 83.594 (86.759)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.4211 (0.4043)	Prec@1 89.062 (86.839)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.2661 (0.4009)	Prec@1 89.844 (86.879)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.3716 (0.4015)	Prec@1 86.719 (86.865)	
Total train loss: 0.4016

 * Prec@1 85.150 Prec@5 99.060 Loss 0.5591
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.6533 (0.3891)	Prec@1 82.031 (87.260)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.2949 (0.3964)	Prec@1 91.406 (87.094)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.3086 (0.4041)	Prec@1 87.500 (86.859)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.2957 (0.4032)	Prec@1 89.062 (86.932)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.2747 (0.4033)	Prec@1 90.625 (86.943)	
Total train loss: 0.4031

 * Prec@1 85.020 Prec@5 99.020 Loss 0.5615
Best acc: 85.230
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.2893 (0.4072)	Prec@1 89.844 (86.719)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.3469 (0.4072)	Prec@1 88.281 (86.829)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.5435 (0.4046)	Prec@1 86.719 (86.765)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.5864 (0.4040)	Prec@1 83.594 (86.796)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.5010 (0.4017)	Prec@1 85.156 (86.843)	
Total train loss: 0.4014

 * Prec@1 85.260 Prec@5 99.000 Loss 0.5601
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.4109 (0.3870)	Prec@1 82.812 (87.470)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.5283 (0.3967)	Prec@1 80.469 (87.225)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.3899 (0.3992)	Prec@1 83.594 (87.073)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.3982 (0.4005)	Prec@1 86.719 (86.932)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.5923 (0.4032)	Prec@1 81.250 (86.861)	
Total train loss: 0.4031

 * Prec@1 84.940 Prec@5 98.980 Loss 0.5601
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.4812 (0.3987)	Prec@1 86.719 (87.089)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.3093 (0.3933)	Prec@1 89.844 (87.174)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.3604 (0.3959)	Prec@1 89.062 (87.046)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.3301 (0.3999)	Prec@1 88.281 (86.939)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.3914 (0.4017)	Prec@1 88.281 (86.899)	
Total train loss: 0.4017

 * Prec@1 85.220 Prec@5 98.970 Loss 0.5493
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.3684 (0.4079)	Prec@1 86.719 (86.989)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.4475 (0.3986)	Prec@1 85.156 (87.200)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.4021 (0.3980)	Prec@1 85.156 (87.129)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.3640 (0.4005)	Prec@1 85.938 (87.007)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.4084 (0.4014)	Prec@1 89.062 (86.991)	
Total train loss: 0.4015

 * Prec@1 85.230 Prec@5 98.980 Loss 0.5479
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.6611 (0.3884)	Prec@1 83.594 (87.410)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.4014 (0.3962)	Prec@1 85.156 (87.260)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.5293 (0.3983)	Prec@1 84.375 (87.179)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.3945 (0.3982)	Prec@1 89.062 (87.137)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.5225 (0.3987)	Prec@1 82.031 (87.109)	
Total train loss: 0.3989

 * Prec@1 85.110 Prec@5 99.010 Loss 0.5669
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.4934 (0.4091)	Prec@1 85.156 (86.669)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.5396 (0.4082)	Prec@1 82.031 (86.589)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.3804 (0.4052)	Prec@1 87.500 (86.822)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.4316 (0.4044)	Prec@1 85.156 (86.907)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.4612 (0.4011)	Prec@1 82.031 (87.005)	
Total train loss: 0.4013

 * Prec@1 85.120 Prec@5 99.070 Loss 0.5767
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.4968 (0.3801)	Prec@1 85.156 (87.310)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.4858 (0.3924)	Prec@1 84.375 (87.350)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.4971 (0.3990)	Prec@1 85.938 (87.136)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.3726 (0.3998)	Prec@1 88.281 (86.997)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.3237 (0.4015)	Prec@1 86.719 (86.961)	
Total train loss: 0.4015

 * Prec@1 85.130 Prec@5 99.080 Loss 0.5547
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.3438 (0.4058)	Prec@1 86.719 (86.829)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.2019 (0.4000)	Prec@1 92.188 (86.889)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.2468 (0.4018)	Prec@1 92.188 (86.882)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.4233 (0.4044)	Prec@1 87.500 (86.754)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.4202 (0.4025)	Prec@1 85.156 (86.927)	
Total train loss: 0.4024

 * Prec@1 85.150 Prec@5 98.930 Loss 0.5527
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.4744 (0.3981)	Prec@1 81.250 (86.939)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.4727 (0.4000)	Prec@1 83.594 (86.869)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.4473 (0.4052)	Prec@1 85.156 (86.806)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.2573 (0.4035)	Prec@1 92.969 (86.877)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.2418 (0.4022)	Prec@1 92.188 (86.855)	
Total train loss: 0.4019

 * Prec@1 85.230 Prec@5 99.030 Loss 0.5581
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4812 (0.4095)	Prec@1 84.375 (86.448)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4434 (0.4122)	Prec@1 85.156 (86.654)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.5063 (0.4058)	Prec@1 86.719 (86.876)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.1826 (0.3996)	Prec@1 92.969 (86.959)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.4736 (0.4024)	Prec@1 89.062 (86.861)	
Total train loss: 0.4024

 * Prec@1 85.040 Prec@5 99.010 Loss 0.5605
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4792 (0.3792)	Prec@1 82.031 (87.250)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.2534 (0.3922)	Prec@1 92.188 (87.079)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.4543 (0.4042)	Prec@1 85.938 (86.902)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.3325 (0.4039)	Prec@1 89.844 (86.932)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.3660 (0.4018)	Prec@1 87.500 (86.973)	
Total train loss: 0.4019

 * Prec@1 85.230 Prec@5 98.970 Loss 0.5664
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.3694 (0.4052)	Prec@1 89.062 (86.769)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.3667 (0.4007)	Prec@1 85.938 (87.019)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.3169 (0.3998)	Prec@1 91.406 (86.932)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3447 (0.3999)	Prec@1 88.281 (86.937)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.2852 (0.3978)	Prec@1 90.625 (86.969)	
Total train loss: 0.3980

 * Prec@1 85.110 Prec@5 99.030 Loss 0.5576
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.2644 (0.3945)	Prec@1 92.188 (87.500)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.5156 (0.4100)	Prec@1 83.594 (87.034)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.6167 (0.4077)	Prec@1 78.906 (87.046)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.3992 (0.4002)	Prec@1 90.625 (87.172)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.4194 (0.4010)	Prec@1 85.156 (87.131)	
Total train loss: 0.4009

 * Prec@1 85.120 Prec@5 99.080 Loss 0.5615
Best acc: 85.260
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.2876 (0.4231)	Prec@1 89.062 (86.368)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.4805 (0.4133)	Prec@1 85.156 (86.619)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.2230 (0.4026)	Prec@1 91.406 (86.929)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.3066 (0.4026)	Prec@1 86.719 (86.889)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.3962 (0.4003)	Prec@1 86.719 (86.955)	
Total train loss: 0.4003

 * Prec@1 85.280 Prec@5 99.010 Loss 0.5659
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.5425 (0.3995)	Prec@1 85.156 (86.799)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.3018 (0.3900)	Prec@1 90.625 (87.039)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.5420 (0.3981)	Prec@1 82.031 (86.886)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.3447 (0.4015)	Prec@1 89.844 (86.871)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.5259 (0.4008)	Prec@1 83.594 (86.889)	
Total train loss: 0.4007

 * Prec@1 85.220 Prec@5 98.960 Loss 0.5571
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.4287 (0.3995)	Prec@1 88.281 (87.169)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.2336 (0.3977)	Prec@1 92.188 (87.139)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.2607 (0.3999)	Prec@1 90.625 (87.123)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.5283 (0.4028)	Prec@1 85.156 (86.969)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.3359 (0.4030)	Prec@1 91.406 (87.011)	
Total train loss: 0.4031

 * Prec@1 85.200 Prec@5 99.000 Loss 0.5576
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.4414 (0.4111)	Prec@1 85.156 (86.679)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.3655 (0.4111)	Prec@1 87.500 (86.829)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.4639 (0.4085)	Prec@1 87.500 (86.866)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.4319 (0.4034)	Prec@1 85.938 (86.937)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.3372 (0.4033)	Prec@1 89.062 (86.993)	
Total train loss: 0.4035

 * Prec@1 85.080 Prec@5 99.080 Loss 0.5586
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.5601 (0.3996)	Prec@1 81.250 (86.949)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.5098 (0.4130)	Prec@1 84.375 (86.644)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.3591 (0.4074)	Prec@1 87.500 (86.862)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.3503 (0.4006)	Prec@1 89.844 (87.109)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.2325 (0.4024)	Prec@1 92.188 (86.993)	
Total train loss: 0.4021

 * Prec@1 84.990 Prec@5 99.070 Loss 0.5620
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.4072 (0.3886)	Prec@1 86.719 (87.200)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.3159 (0.3966)	Prec@1 90.625 (87.024)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.3613 (0.3979)	Prec@1 88.281 (87.066)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.3484 (0.4010)	Prec@1 88.281 (86.994)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.2981 (0.4021)	Prec@1 89.844 (86.911)	
Total train loss: 0.4021

 * Prec@1 85.260 Prec@5 98.970 Loss 0.5586
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.4553 (0.4008)	Prec@1 82.812 (87.069)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4377 (0.4008)	Prec@1 87.500 (86.974)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.4885 (0.4006)	Prec@1 86.719 (86.966)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.3799 (0.4016)	Prec@1 86.719 (86.889)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.2998 (0.4018)	Prec@1 89.844 (86.859)	
Total train loss: 0.4019

 * Prec@1 85.150 Prec@5 98.990 Loss 0.5591
Best acc: 85.280
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.5068 (0.4174)	Prec@1 82.812 (87.009)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.3550 (0.4136)	Prec@1 85.938 (87.024)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.4104 (0.4058)	Prec@1 84.375 (86.989)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3918 (0.4037)	Prec@1 89.062 (87.017)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.4307 (0.4033)	Prec@1 86.719 (87.039)	
Total train loss: 0.4034

 * Prec@1 85.290 Prec@5 99.060 Loss 0.5571
Best acc: 85.290
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.4692 (0.4082)	Prec@1 84.375 (86.819)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.2852 (0.4035)	Prec@1 89.062 (86.974)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.3584 (0.4009)	Prec@1 91.406 (87.016)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.4360 (0.3996)	Prec@1 88.281 (86.969)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.5903 (0.3999)	Prec@1 84.375 (86.987)	
Total train loss: 0.3998

 * Prec@1 85.100 Prec@5 98.990 Loss 0.5615
Best acc: 85.290
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.4536 (0.4144)	Prec@1 85.156 (86.528)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.2798 (0.4019)	Prec@1 88.281 (86.949)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.3110 (0.3989)	Prec@1 86.719 (86.972)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.4185 (0.4023)	Prec@1 83.594 (86.884)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.3059 (0.3996)	Prec@1 89.062 (86.989)	
Total train loss: 0.3994

 * Prec@1 85.030 Prec@5 99.100 Loss 0.5645
Best acc: 85.290
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.5508 (0.4087)	Prec@1 82.031 (86.498)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.4023 (0.4020)	Prec@1 88.281 (86.679)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.4912 (0.3970)	Prec@1 86.719 (86.886)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.2944 (0.4003)	Prec@1 90.625 (86.746)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.3413 (0.4024)	Prec@1 86.719 (86.767)	
Total train loss: 0.4025

 * Prec@1 85.320 Prec@5 99.040 Loss 0.5591
Best acc: 85.320
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.5181 (0.4111)	Prec@1 79.688 (86.268)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.4451 (0.4167)	Prec@1 82.812 (86.263)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.2637 (0.4099)	Prec@1 90.625 (86.532)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.4019 (0.4016)	Prec@1 85.938 (86.869)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.4314 (0.4000)	Prec@1 87.500 (86.823)	
Total train loss: 0.4004

 * Prec@1 85.200 Prec@5 98.970 Loss 0.5635
Best acc: 85.320
--------------------------------------------------------------------------------
