
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 0.920 Prec@5 4.120 Loss 4.6289
Pre-trained Prec@1 with 15 layers frozen: 0.9199999570846558 	 Loss: 4.62890625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 4.3242 (4.7929)	Prec@1 7.422 (3.065)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.9453 (4.4435)	Prec@1 13.672 (6.671)	
Epoch: [0][116/196]	LR: 0.002	Loss 3.7539 (4.2195)	Prec@1 12.891 (9.455)	
Epoch: [0][155/196]	LR: 0.002	Loss 3.4961 (4.0703)	Prec@1 23.047 (11.596)	
Epoch: [0][194/196]	LR: 0.002	Loss 3.3867 (3.9549)	Prec@1 25.781 (13.452)	
Total train loss: 3.9542

 * Prec@1 23.420 Prec@5 49.750 Loss 3.4023
Best acc: 23.420
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 3.1719 (3.2899)	Prec@1 31.641 (26.122)	
Epoch: [1][77/196]	LR: 0.002	Loss 3.0605 (3.2419)	Prec@1 32.031 (27.339)	
Epoch: [1][116/196]	LR: 0.002	Loss 3.1895 (3.2096)	Prec@1 29.297 (28.118)	
Epoch: [1][155/196]	LR: 0.002	Loss 3.1934 (3.1843)	Prec@1 26.953 (28.556)	
Epoch: [1][194/196]	LR: 0.002	Loss 3.0156 (3.1611)	Prec@1 35.547 (29.056)	
Total train loss: 3.1610

 * Prec@1 30.960 Prec@5 59.200 Loss 3.0898
Best acc: 30.960
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 2.9160 (2.9410)	Prec@1 37.500 (34.505)	
Epoch: [2][77/196]	LR: 0.002	Loss 2.9922 (2.9126)	Prec@1 33.984 (35.577)	
Epoch: [2][116/196]	LR: 0.002	Loss 2.8652 (2.9031)	Prec@1 35.156 (35.687)	
Epoch: [2][155/196]	LR: 0.002	Loss 2.8496 (2.8887)	Prec@1 37.109 (35.925)	
Epoch: [2][194/196]	LR: 0.002	Loss 2.8926 (2.8738)	Prec@1 35.156 (36.200)	
Total train loss: 2.8732

 * Prec@1 35.110 Prec@5 63.910 Loss 2.9102
Best acc: 35.110
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 2.5762 (2.7076)	Prec@1 51.172 (40.114)	
Epoch: [3][77/196]	LR: 0.002	Loss 2.8379 (2.7007)	Prec@1 36.719 (40.385)	
Epoch: [3][116/196]	LR: 0.002	Loss 2.6035 (2.6918)	Prec@1 42.578 (40.672)	
Epoch: [3][155/196]	LR: 0.002	Loss 2.6230 (2.6888)	Prec@1 45.312 (40.825)	
Epoch: [3][194/196]	LR: 0.002	Loss 2.6895 (2.6817)	Prec@1 41.016 (40.968)	
Total train loss: 2.6817

 * Prec@1 37.440 Prec@5 66.320 Loss 2.7910
Best acc: 37.440
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 2.5469 (2.5603)	Prec@1 43.359 (44.121)	
Epoch: [4][77/196]	LR: 0.002	Loss 2.5898 (2.5526)	Prec@1 41.016 (43.980)	
Epoch: [4][116/196]	LR: 0.002	Loss 2.5703 (2.5544)	Prec@1 44.531 (43.954)	
Epoch: [4][155/196]	LR: 0.002	Loss 2.6250 (2.5475)	Prec@1 42.969 (43.985)	
Epoch: [4][194/196]	LR: 0.002	Loss 2.5762 (2.5410)	Prec@1 40.234 (44.197)	
Total train loss: 2.5410

 * Prec@1 39.110 Prec@5 68.030 Loss 2.7051
Best acc: 39.110
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 2.4355 (2.4153)	Prec@1 45.312 (48.538)	
Epoch: [5][77/196]	LR: 0.002	Loss 2.4336 (2.4189)	Prec@1 44.141 (47.892)	
Epoch: [5][116/196]	LR: 0.002	Loss 2.4883 (2.4236)	Prec@1 43.750 (47.599)	
Epoch: [5][155/196]	LR: 0.002	Loss 2.4297 (2.4237)	Prec@1 45.703 (47.423)	
Epoch: [5][194/196]	LR: 0.002	Loss 2.4453 (2.4219)	Prec@1 49.219 (47.438)	
Total train loss: 2.4222

 * Prec@1 40.200 Prec@5 69.510 Loss 2.6426
Best acc: 40.200
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 2.2344 (2.3091)	Prec@1 55.078 (51.562)	
Epoch: [6][77/196]	LR: 0.002	Loss 2.3789 (2.3217)	Prec@1 48.438 (50.676)	
Epoch: [6][116/196]	LR: 0.002	Loss 2.4043 (2.3240)	Prec@1 42.188 (50.197)	
Epoch: [6][155/196]	LR: 0.002	Loss 2.2773 (2.3223)	Prec@1 47.266 (50.090)	
Epoch: [6][194/196]	LR: 0.002	Loss 2.3887 (2.3225)	Prec@1 47.266 (50.066)	
Total train loss: 2.3228

 * Prec@1 41.140 Prec@5 69.850 Loss 2.6113
Best acc: 41.140
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 2.3066 (2.2336)	Prec@1 52.344 (52.464)	
Epoch: [7][77/196]	LR: 0.002	Loss 2.2695 (2.2317)	Prec@1 48.047 (52.604)	
Epoch: [7][116/196]	LR: 0.002	Loss 2.2012 (2.2319)	Prec@1 55.078 (52.487)	
Epoch: [7][155/196]	LR: 0.002	Loss 2.2910 (2.2305)	Prec@1 52.734 (52.504)	
Epoch: [7][194/196]	LR: 0.002	Loss 2.3711 (2.2312)	Prec@1 46.094 (52.366)	
Total train loss: 2.2314

 * Prec@1 41.790 Prec@5 71.090 Loss 2.5625
Best acc: 41.790
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 2.1445 (2.1349)	Prec@1 53.125 (55.709)	
Epoch: [8][77/196]	LR: 0.002	Loss 2.1270 (2.1392)	Prec@1 53.906 (55.484)	
Epoch: [8][116/196]	LR: 0.002	Loss 2.0664 (2.1472)	Prec@1 56.250 (55.138)	
Epoch: [8][155/196]	LR: 0.002	Loss 2.1250 (2.1509)	Prec@1 55.078 (54.938)	
Epoch: [8][194/196]	LR: 0.002	Loss 2.0762 (2.1511)	Prec@1 57.812 (54.864)	
Total train loss: 2.1515

 * Prec@1 42.600 Prec@5 71.600 Loss 2.5215
Best acc: 42.600
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 1.9736 (2.0608)	Prec@1 60.547 (57.372)	
Epoch: [9][77/196]	LR: 0.002	Loss 1.9561 (2.0664)	Prec@1 63.672 (57.277)	
Epoch: [9][116/196]	LR: 0.002	Loss 2.1738 (2.0691)	Prec@1 54.297 (57.008)	
Epoch: [9][155/196]	LR: 0.002	Loss 2.0957 (2.0717)	Prec@1 52.734 (56.914)	
Epoch: [9][194/196]	LR: 0.002	Loss 2.0898 (2.0760)	Prec@1 55.469 (56.739)	
Total train loss: 2.0763

 * Prec@1 43.060 Prec@5 71.850 Loss 2.5039
Best acc: 43.060
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 1.9893 (2.0026)	Prec@1 58.203 (59.505)	
Epoch: [10][77/196]	LR: 0.0004	Loss 2.0684 (1.9891)	Prec@1 55.469 (59.906)	
Epoch: [10][116/196]	LR: 0.0004	Loss 2.0000 (1.9882)	Prec@1 57.812 (59.652)	
Epoch: [10][155/196]	LR: 0.0004	Loss 1.9990 (1.9849)	Prec@1 59.766 (59.718)	
Epoch: [10][194/196]	LR: 0.0004	Loss 2.0137 (1.9840)	Prec@1 62.500 (59.746)	
Total train loss: 1.9840

 * Prec@1 43.390 Prec@5 71.830 Loss 2.5000
Best acc: 43.390
--------------------------------------------------------------------------------
Epoch: [11][38/196]	LR: 0.0004	Loss 2.0059 (1.9641)	Prec@1 58.203 (60.367)	
Epoch: [11][77/196]	LR: 0.0004	Loss 1.9424 (1.9599)	Prec@1 66.406 (60.807)	
Epoch: [11][116/196]	LR: 0.0004	Loss 1.9727 (1.9652)	Prec@1 61.328 (60.537)	
Epoch: [11][155/196]	LR: 0.0004	Loss 2.0312 (1.9680)	Prec@1 57.031 (60.429)	
Epoch: [11][194/196]	LR: 0.0004	Loss 2.0098 (1.9692)	Prec@1 59.375 (60.399)	
Total train loss: 1.9692

 * Prec@1 43.580 Prec@5 72.000 Loss 2.5020
Best acc: 43.580
--------------------------------------------------------------------------------
Epoch: [12][38/196]	LR: 0.0004	Loss 1.9189 (1.9562)	Prec@1 62.500 (60.787)	
Epoch: [12][77/196]	LR: 0.0004	Loss 1.9697 (1.9623)	Prec@1 62.109 (60.317)	
Epoch: [12][116/196]	LR: 0.0004	Loss 1.9268 (1.9582)	Prec@1 63.672 (60.463)	
Epoch: [12][155/196]	LR: 0.0004	Loss 2.0957 (1.9613)	Prec@1 57.031 (60.314)	
Epoch: [12][194/196]	LR: 0.0004	Loss 2.1113 (1.9636)	Prec@1 51.953 (60.361)	
Total train loss: 1.9641

 * Prec@1 43.190 Prec@5 72.020 Loss 2.4980
Best acc: 43.580
--------------------------------------------------------------------------------
Epoch: [13][38/196]	LR: 0.0004	Loss 2.0391 (1.9470)	Prec@1 58.594 (61.669)	
Epoch: [13][77/196]	LR: 0.0004	Loss 1.8857 (1.9519)	Prec@1 59.375 (61.208)	
Epoch: [13][116/196]	LR: 0.0004	Loss 1.9932 (1.9579)	Prec@1 56.641 (60.871)	
Epoch: [13][155/196]	LR: 0.0004	Loss 1.9023 (1.9562)	Prec@1 60.938 (60.905)	
Epoch: [13][194/196]	LR: 0.0004	Loss 2.0215 (1.9568)	Prec@1 62.891 (60.851)	
Total train loss: 1.9569

 * Prec@1 43.420 Prec@5 72.150 Loss 2.5000
Best acc: 43.580
--------------------------------------------------------------------------------
