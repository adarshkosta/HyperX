
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 68.690 Prec@5 90.190 Loss 1.1846
Pre-trained Prec@1 with 1 layers frozen: 68.68999481201172 	 Loss: 1.1845703125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.7671 (1.1594)	Prec@1 82.031 (67.117)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.0400 (1.0937)	Prec@1 69.531 (69.040)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.0645 (1.0572)	Prec@1 67.188 (70.005)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.8853 (1.0420)	Prec@1 76.562 (70.403)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.7729 (1.0221)	Prec@1 77.344 (70.931)	
Total train loss: 1.0221

 * Prec@1 68.740 Prec@5 90.740 Loss 1.1475
Best acc: 68.740
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.9019 (0.9064)	Prec@1 74.219 (73.998)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.9746 (0.9097)	Prec@1 71.875 (73.773)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.7617 (0.9067)	Prec@1 75.781 (73.892)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.8726 (0.9025)	Prec@1 73.438 (74.139)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.8423 (0.9019)	Prec@1 73.438 (74.175)	
Total train loss: 0.9018

 * Prec@1 68.860 Prec@5 90.480 Loss 1.1426
Best acc: 68.860
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.7993 (0.8480)	Prec@1 76.562 (76.042)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0059 (0.8499)	Prec@1 71.875 (75.691)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.8760 (0.8486)	Prec@1 75.781 (75.868)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.9268 (0.8514)	Prec@1 74.219 (75.861)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.0186 (0.8534)	Prec@1 67.188 (75.849)	
Total train loss: 0.8534

 * Prec@1 69.240 Prec@5 90.680 Loss 1.1348
Best acc: 69.240
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7949 (0.8288)	Prec@1 75.781 (76.562)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8696 (0.8328)	Prec@1 71.875 (76.287)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.6929 (0.8326)	Prec@1 81.250 (76.265)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.0244 (0.8319)	Prec@1 68.750 (76.275)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.1162 (0.8282)	Prec@1 68.750 (76.434)	
Total train loss: 0.8284

 * Prec@1 69.180 Prec@5 90.940 Loss 1.1270
Best acc: 69.240
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.7705 (0.7912)	Prec@1 77.344 (77.454)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.7524 (0.7975)	Prec@1 78.906 (77.264)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.7803 (0.7971)	Prec@1 76.562 (77.177)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.7900 (0.8000)	Prec@1 78.906 (77.211)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.8691 (0.7998)	Prec@1 75.781 (77.290)	
Total train loss: 0.7996

 * Prec@1 69.300 Prec@5 90.930 Loss 1.1270
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.7300 (0.7798)	Prec@1 81.250 (77.604)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.6562 (0.7807)	Prec@1 84.375 (77.739)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.7612 (0.7814)	Prec@1 78.125 (77.885)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.7710 (0.7840)	Prec@1 76.562 (77.947)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.7661 (0.7842)	Prec@1 78.906 (77.989)	
Total train loss: 0.7840

 * Prec@1 69.250 Prec@5 90.830 Loss 1.1328
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.7383 (0.7557)	Prec@1 75.781 (78.576)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.8340 (0.7570)	Prec@1 75.000 (78.656)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.7236 (0.7655)	Prec@1 75.781 (78.312)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.5869 (0.7656)	Prec@1 81.250 (78.370)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.7847 (0.7683)	Prec@1 79.688 (78.323)	
Total train loss: 0.7684

 * Prec@1 69.190 Prec@5 90.700 Loss 1.1328
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.7139 (0.7612)	Prec@1 81.250 (78.456)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.7607 (0.7576)	Prec@1 76.562 (78.370)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.6240 (0.7500)	Prec@1 84.375 (78.763)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.8428 (0.7515)	Prec@1 78.906 (78.748)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.6616 (0.7540)	Prec@1 82.812 (78.696)	
Total train loss: 0.7545

 * Prec@1 69.010 Prec@5 90.630 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.6294 (0.7244)	Prec@1 85.156 (80.098)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.6743 (0.7315)	Prec@1 80.469 (79.587)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.8457 (0.7352)	Prec@1 75.000 (79.374)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.7769 (0.7412)	Prec@1 80.469 (79.137)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.7490 (0.7401)	Prec@1 76.562 (79.169)	
Total train loss: 0.7401

 * Prec@1 69.180 Prec@5 90.620 Loss 1.1367
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.5933 (0.7246)	Prec@1 81.250 (80.008)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.8716 (0.7257)	Prec@1 76.562 (79.903)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.7832 (0.7265)	Prec@1 77.344 (79.788)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.6152 (0.7287)	Prec@1 78.906 (79.630)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.7979 (0.7255)	Prec@1 76.562 (79.764)	
Total train loss: 0.7258

 * Prec@1 68.930 Prec@5 90.480 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.6787 (0.7050)	Prec@1 78.906 (80.258)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.6484 (0.7048)	Prec@1 81.250 (80.534)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.6543 (0.7025)	Prec@1 79.688 (80.499)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.6069 (0.7022)	Prec@1 78.125 (80.471)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.6626 (0.7015)	Prec@1 84.375 (80.523)	
Total train loss: 0.7016

 * Prec@1 69.060 Prec@5 90.530 Loss 1.1406
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.6128 (0.7066)	Prec@1 86.719 (80.549)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.6040 (0.7069)	Prec@1 86.719 (80.529)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.6743 (0.7038)	Prec@1 78.125 (80.542)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.7344 (0.7039)	Prec@1 82.031 (80.684)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.7681 (0.7051)	Prec@1 82.031 (80.525)	
Total train loss: 0.7051

 * Prec@1 68.710 Prec@5 90.580 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.8984 (0.7102)	Prec@1 71.875 (79.778)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.7256 (0.7003)	Prec@1 82.812 (80.233)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.7900 (0.6993)	Prec@1 75.781 (80.372)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.7749 (0.7007)	Prec@1 78.906 (80.406)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.6401 (0.7010)	Prec@1 82.031 (80.535)	
Total train loss: 0.7010

 * Prec@1 69.170 Prec@5 90.510 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.6323 (0.6912)	Prec@1 84.375 (81.120)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.6675 (0.6980)	Prec@1 82.031 (80.859)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.5737 (0.6963)	Prec@1 84.375 (80.776)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.8481 (0.6964)	Prec@1 75.781 (80.674)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.5859 (0.6985)	Prec@1 80.469 (80.573)	
Total train loss: 0.6985

 * Prec@1 68.700 Prec@5 90.430 Loss 1.1426
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.6597 (0.7104)	Prec@1 80.469 (80.108)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.8032 (0.7013)	Prec@1 78.906 (80.379)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.6606 (0.7048)	Prec@1 82.031 (80.262)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.6709 (0.6984)	Prec@1 81.250 (80.541)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.9248 (0.6954)	Prec@1 69.531 (80.669)	
Total train loss: 0.6958

 * Prec@1 68.780 Prec@5 90.560 Loss 1.1445
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.5381 (0.6874)	Prec@1 86.719 (80.669)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.9751 (0.6924)	Prec@1 77.344 (80.699)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.8169 (0.6981)	Prec@1 75.781 (80.559)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.6499 (0.6972)	Prec@1 80.469 (80.594)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.8438 (0.6975)	Prec@1 78.125 (80.637)	
Total train loss: 0.6976

 * Prec@1 68.880 Prec@5 90.510 Loss 1.1406
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.5146 (0.6869)	Prec@1 88.281 (80.849)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.8169 (0.6904)	Prec@1 78.125 (80.724)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.6719 (0.6927)	Prec@1 80.469 (80.823)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.8013 (0.6939)	Prec@1 78.125 (80.819)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.8516 (0.6945)	Prec@1 72.656 (80.703)	
Total train loss: 0.6945

 * Prec@1 69.220 Prec@5 90.510 Loss 1.1406
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.6865 (0.6986)	Prec@1 82.812 (80.639)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.6982 (0.6889)	Prec@1 80.469 (80.744)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.6685 (0.6976)	Prec@1 79.688 (80.589)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.5229 (0.6936)	Prec@1 88.281 (80.684)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.6538 (0.6945)	Prec@1 84.375 (80.621)	
Total train loss: 0.6946

 * Prec@1 68.950 Prec@5 90.480 Loss 1.1396
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.6299 (0.7037)	Prec@1 85.938 (80.699)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.6753 (0.6979)	Prec@1 80.469 (80.724)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.8584 (0.6980)	Prec@1 73.438 (80.649)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.6836 (0.6954)	Prec@1 80.469 (80.739)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.7725 (0.6949)	Prec@1 78.125 (80.813)	
Total train loss: 0.6952

 * Prec@1 68.840 Prec@5 90.560 Loss 1.1426
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.5737 (0.6911)	Prec@1 85.938 (80.769)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.6523 (0.6894)	Prec@1 80.469 (80.769)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.7451 (0.6923)	Prec@1 78.906 (80.773)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.6655 (0.6919)	Prec@1 81.250 (80.739)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.6001 (0.6913)	Prec@1 85.938 (80.813)	
Total train loss: 0.6915

 * Prec@1 68.780 Prec@5 90.600 Loss 1.1396
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.5908 (0.6951)	Prec@1 82.031 (80.699)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.7949 (0.6938)	Prec@1 80.469 (80.960)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.7212 (0.6942)	Prec@1 77.344 (80.896)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.8027 (0.6928)	Prec@1 75.781 (80.912)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.7051 (0.6921)	Prec@1 81.250 (80.923)	
Total train loss: 0.6923

 * Prec@1 68.980 Prec@5 90.440 Loss 1.1436
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.6592 (0.6899)	Prec@1 78.906 (80.599)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.7881 (0.6985)	Prec@1 79.688 (80.549)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.7666 (0.7002)	Prec@1 77.344 (80.515)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.7656 (0.6973)	Prec@1 78.125 (80.639)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.6934 (0.6952)	Prec@1 78.906 (80.679)	
Total train loss: 0.6950

 * Prec@1 68.950 Prec@5 90.700 Loss 1.1436
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.6836 (0.6899)	Prec@1 82.031 (81.130)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.7803 (0.6964)	Prec@1 75.781 (80.764)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.7021 (0.6906)	Prec@1 80.469 (80.863)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.7095 (0.6898)	Prec@1 78.906 (80.990)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.7241 (0.6912)	Prec@1 77.344 (80.889)	
Total train loss: 0.6914

 * Prec@1 69.210 Prec@5 90.800 Loss 1.1377
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.5894 (0.6892)	Prec@1 82.812 (80.679)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.6743 (0.6948)	Prec@1 82.031 (80.524)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.6758 (0.6919)	Prec@1 81.250 (80.766)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.5854 (0.6948)	Prec@1 85.156 (80.689)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.5728 (0.6928)	Prec@1 85.156 (80.793)	
Total train loss: 0.6930

 * Prec@1 68.840 Prec@5 90.590 Loss 1.1475
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.5942 (0.6969)	Prec@1 82.812 (80.649)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.6523 (0.6959)	Prec@1 83.594 (80.764)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.7368 (0.6911)	Prec@1 76.562 (81.060)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.5977 (0.6902)	Prec@1 84.375 (81.035)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.7666 (0.6908)	Prec@1 77.344 (80.964)	
Total train loss: 0.6909

 * Prec@1 69.200 Prec@5 90.510 Loss 1.1406
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.7007 (0.6913)	Prec@1 85.156 (80.749)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.6875 (0.6811)	Prec@1 81.250 (81.365)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.7515 (0.6857)	Prec@1 79.688 (81.207)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.6528 (0.6872)	Prec@1 84.375 (81.055)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.5791 (0.6846)	Prec@1 81.250 (81.028)	
Total train loss: 0.6844

 * Prec@1 69.090 Prec@5 90.530 Loss 1.1396
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.5562 (0.6816)	Prec@1 88.281 (81.410)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.6245 (0.6826)	Prec@1 80.469 (81.310)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.6572 (0.6824)	Prec@1 81.250 (81.380)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.7686 (0.6827)	Prec@1 75.781 (81.275)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.7012 (0.6845)	Prec@1 78.906 (81.146)	
Total train loss: 0.6845

 * Prec@1 68.990 Prec@5 90.530 Loss 1.1436
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.6787 (0.6930)	Prec@1 81.250 (80.519)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.6470 (0.6875)	Prec@1 80.469 (80.769)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.5737 (0.6914)	Prec@1 82.812 (80.702)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.5024 (0.6897)	Prec@1 88.281 (80.867)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.7632 (0.6887)	Prec@1 75.781 (80.815)	
Total train loss: 0.6886

 * Prec@1 68.910 Prec@5 90.510 Loss 1.1465
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.8193 (0.6782)	Prec@1 71.094 (81.380)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.7202 (0.6835)	Prec@1 80.469 (81.190)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.6133 (0.6880)	Prec@1 85.156 (81.023)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.6050 (0.6861)	Prec@1 86.719 (80.947)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.6328 (0.6898)	Prec@1 84.375 (80.992)	
Total train loss: 0.6901

 * Prec@1 68.610 Prec@5 90.560 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.7178 (0.6821)	Prec@1 80.469 (81.310)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.6299 (0.6828)	Prec@1 83.594 (81.300)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.7363 (0.6848)	Prec@1 75.000 (81.263)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.6660 (0.6852)	Prec@1 82.812 (81.162)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.6758 (0.6849)	Prec@1 83.594 (81.146)	
Total train loss: 0.6850

 * Prec@1 68.720 Prec@5 90.530 Loss 1.1504
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.7778 (0.6938)	Prec@1 79.688 (80.919)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.6196 (0.6852)	Prec@1 81.250 (81.035)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.7139 (0.6817)	Prec@1 84.375 (81.250)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.7466 (0.6842)	Prec@1 80.469 (81.105)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.7046 (0.6857)	Prec@1 81.250 (81.122)	
Total train loss: 0.6859

 * Prec@1 69.100 Prec@5 90.570 Loss 1.1416
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.7563 (0.6861)	Prec@1 78.125 (80.669)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.5796 (0.6894)	Prec@1 84.375 (80.694)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.5806 (0.6840)	Prec@1 83.594 (80.973)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.6030 (0.6827)	Prec@1 84.375 (81.075)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.6494 (0.6820)	Prec@1 79.688 (81.142)	
Total train loss: 0.6820

 * Prec@1 68.940 Prec@5 90.680 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.7412 (0.6787)	Prec@1 78.125 (81.250)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.6553 (0.6814)	Prec@1 81.250 (81.135)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.7622 (0.6842)	Prec@1 79.688 (81.133)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.7065 (0.6829)	Prec@1 84.375 (81.180)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.6968 (0.6834)	Prec@1 77.344 (81.234)	
Total train loss: 0.6834

 * Prec@1 68.880 Prec@5 90.540 Loss 1.1504
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.7393 (0.6680)	Prec@1 77.344 (81.490)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.7754 (0.6840)	Prec@1 80.469 (81.045)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.6704 (0.6840)	Prec@1 82.031 (81.023)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.6221 (0.6861)	Prec@1 84.375 (80.940)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.5005 (0.6858)	Prec@1 92.188 (80.948)	
Total train loss: 0.6860

 * Prec@1 69.070 Prec@5 90.780 Loss 1.1426
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.7939 (0.6826)	Prec@1 78.906 (81.180)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.7368 (0.6794)	Prec@1 81.250 (81.185)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.6675 (0.6808)	Prec@1 79.688 (81.183)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.6597 (0.6816)	Prec@1 85.938 (81.125)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.7046 (0.6851)	Prec@1 79.688 (81.002)	
Total train loss: 0.6854

 * Prec@1 68.910 Prec@5 90.510 Loss 1.1445
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.5547 (0.6812)	Prec@1 85.938 (81.330)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.7368 (0.6755)	Prec@1 79.688 (81.500)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.6514 (0.6817)	Prec@1 83.594 (81.257)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.7651 (0.6844)	Prec@1 81.250 (81.092)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.6201 (0.6859)	Prec@1 82.031 (81.096)	
Total train loss: 0.6857

 * Prec@1 68.760 Prec@5 90.580 Loss 1.1465
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.6558 (0.6708)	Prec@1 84.375 (81.731)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.7358 (0.6810)	Prec@1 77.344 (81.440)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.6670 (0.6779)	Prec@1 78.906 (81.494)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.6431 (0.6795)	Prec@1 83.594 (81.335)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.6860 (0.6825)	Prec@1 81.250 (81.218)	
Total train loss: 0.6826

 * Prec@1 68.930 Prec@5 90.550 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.6182 (0.6868)	Prec@1 84.375 (80.719)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.6860 (0.6846)	Prec@1 80.469 (81.080)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.7305 (0.6819)	Prec@1 75.781 (81.123)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.6167 (0.6780)	Prec@1 85.156 (81.318)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.7134 (0.6817)	Prec@1 82.812 (81.286)	
Total train loss: 0.6818

 * Prec@1 68.610 Prec@5 90.590 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.6646 (0.6792)	Prec@1 85.938 (81.350)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.8188 (0.6836)	Prec@1 75.000 (81.135)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.9204 (0.6851)	Prec@1 72.656 (81.217)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.8320 (0.6825)	Prec@1 78.906 (81.215)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.6953 (0.6829)	Prec@1 81.250 (81.234)	
Total train loss: 0.6832

 * Prec@1 69.030 Prec@5 90.610 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.6187 (0.6790)	Prec@1 82.031 (81.510)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.6255 (0.6837)	Prec@1 82.812 (81.435)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.9395 (0.6806)	Prec@1 71.094 (81.437)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.6006 (0.6771)	Prec@1 82.812 (81.470)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.5444 (0.6791)	Prec@1 84.375 (81.360)	
Total train loss: 0.6790

 * Prec@1 68.940 Prec@5 90.470 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.6948 (0.6872)	Prec@1 82.031 (80.980)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.7144 (0.6869)	Prec@1 73.438 (80.940)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.6499 (0.6830)	Prec@1 80.469 (81.073)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.7510 (0.6827)	Prec@1 75.000 (81.095)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.8335 (0.6814)	Prec@1 76.562 (81.158)	
Total train loss: 0.6816

 * Prec@1 68.790 Prec@5 90.490 Loss 1.1455
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.6362 (0.6738)	Prec@1 81.250 (81.540)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.5190 (0.6761)	Prec@1 88.281 (81.676)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.7051 (0.6756)	Prec@1 78.906 (81.571)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.6196 (0.6780)	Prec@1 85.938 (81.503)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.7070 (0.6794)	Prec@1 76.562 (81.402)	
Total train loss: 0.6793

 * Prec@1 68.660 Prec@5 90.440 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.6011 (0.6882)	Prec@1 84.375 (81.010)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.7202 (0.6765)	Prec@1 78.906 (81.275)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.6211 (0.6694)	Prec@1 84.375 (81.517)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.7861 (0.6705)	Prec@1 77.344 (81.548)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.5776 (0.6744)	Prec@1 82.812 (81.446)	
Total train loss: 0.6747

 * Prec@1 68.990 Prec@5 90.720 Loss 1.1445
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.6494 (0.6813)	Prec@1 85.156 (80.980)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.5991 (0.6751)	Prec@1 82.812 (81.265)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.7739 (0.6810)	Prec@1 77.344 (81.113)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.6587 (0.6802)	Prec@1 78.906 (81.180)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.8105 (0.6790)	Prec@1 73.438 (81.138)	
Total train loss: 0.6791

 * Prec@1 69.010 Prec@5 90.630 Loss 1.1436
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.6592 (0.6745)	Prec@1 83.594 (81.350)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.7700 (0.6810)	Prec@1 79.688 (81.265)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.6924 (0.6786)	Prec@1 82.031 (81.263)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.5581 (0.6772)	Prec@1 87.500 (81.268)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.7480 (0.6774)	Prec@1 85.156 (81.278)	
Total train loss: 0.6777

 * Prec@1 69.040 Prec@5 90.440 Loss 1.1465
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.6226 (0.6804)	Prec@1 84.375 (81.040)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.6895 (0.6773)	Prec@1 84.375 (81.020)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.6338 (0.6755)	Prec@1 85.156 (81.260)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.7358 (0.6736)	Prec@1 78.906 (81.315)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.5845 (0.6746)	Prec@1 84.375 (81.356)	
Total train loss: 0.6747

 * Prec@1 68.940 Prec@5 90.580 Loss 1.1416
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.6235 (0.6623)	Prec@1 82.031 (81.961)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.5581 (0.6753)	Prec@1 88.281 (81.350)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.6201 (0.6793)	Prec@1 83.594 (81.243)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.5630 (0.6786)	Prec@1 84.375 (81.275)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.7354 (0.6772)	Prec@1 81.250 (81.246)	
Total train loss: 0.6775

 * Prec@1 68.830 Prec@5 90.520 Loss 1.1494
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.7417 (0.6707)	Prec@1 75.781 (81.571)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.7070 (0.6732)	Prec@1 78.125 (81.646)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.8159 (0.6754)	Prec@1 73.438 (81.527)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.6665 (0.6751)	Prec@1 81.250 (81.368)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.6450 (0.6749)	Prec@1 82.031 (81.436)	
Total train loss: 0.6748

 * Prec@1 68.890 Prec@5 90.510 Loss 1.1475
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.7275 (0.6800)	Prec@1 80.469 (80.819)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.6753 (0.6780)	Prec@1 79.688 (81.130)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.6694 (0.6765)	Prec@1 82.812 (81.287)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.7461 (0.6794)	Prec@1 78.125 (81.150)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.8105 (0.6796)	Prec@1 78.906 (81.162)	
Total train loss: 0.6797

 * Prec@1 68.650 Prec@5 90.580 Loss 1.1484
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.5371 (0.6665)	Prec@1 86.719 (81.771)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.5352 (0.6705)	Prec@1 85.938 (81.430)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.6074 (0.6718)	Prec@1 85.938 (81.657)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.7651 (0.6733)	Prec@1 82.031 (81.586)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.7290 (0.6735)	Prec@1 75.781 (81.538)	
Total train loss: 0.6736

 * Prec@1 68.950 Prec@5 90.530 Loss 1.1445
Best acc: 69.300
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 67.640 Prec@5 89.480 Loss 1.2285
Pre-trained Prec@1 with 3 layers frozen: 67.63999938964844 	 Loss: 1.228515625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5010 (0.5903)	Prec@1 89.062 (83.363)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5737 (0.5821)	Prec@1 85.938 (83.484)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5977 (0.5711)	Prec@1 86.719 (83.834)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.5693 (0.5751)	Prec@1 82.031 (83.709)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.5767 (0.5755)	Prec@1 82.031 (83.718)	
Total train loss: 0.5756

 * Prec@1 69.610 Prec@5 90.340 Loss 1.1699
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.4504 (0.5248)	Prec@1 85.156 (85.256)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.4824 (0.5283)	Prec@1 89.844 (85.051)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6421 (0.5334)	Prec@1 80.469 (84.983)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4534 (0.5402)	Prec@1 86.719 (84.806)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.5645 (0.5420)	Prec@1 85.938 (84.768)	
Total train loss: 0.5421

 * Prec@1 69.310 Prec@5 90.190 Loss 1.1777
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5205 (0.5206)	Prec@1 84.375 (85.657)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.6206 (0.5154)	Prec@1 85.156 (85.958)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.3560 (0.5189)	Prec@1 89.062 (85.761)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5024 (0.5234)	Prec@1 87.500 (85.544)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5527 (0.5248)	Prec@1 82.812 (85.487)	
Total train loss: 0.5251

 * Prec@1 68.960 Prec@5 90.230 Loss 1.1904
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4668 (0.4885)	Prec@1 89.844 (86.769)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6738 (0.5004)	Prec@1 81.250 (86.528)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5864 (0.5041)	Prec@1 85.156 (86.418)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.3716 (0.5026)	Prec@1 92.188 (86.403)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6431 (0.5055)	Prec@1 80.469 (86.324)	
Total train loss: 0.5056

 * Prec@1 69.270 Prec@5 90.220 Loss 1.1914
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4817 (0.4864)	Prec@1 92.188 (87.139)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4202 (0.4815)	Prec@1 89.844 (87.280)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4753 (0.4865)	Prec@1 88.281 (87.169)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5645 (0.4891)	Prec@1 84.375 (86.959)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.3975 (0.4919)	Prec@1 92.188 (86.827)	
Total train loss: 0.4918

 * Prec@1 69.010 Prec@5 90.150 Loss 1.1904
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4766 (0.4643)	Prec@1 85.938 (88.011)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4160 (0.4678)	Prec@1 89.844 (87.981)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5557 (0.4691)	Prec@1 82.031 (87.834)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.4053 (0.4695)	Prec@1 91.406 (87.760)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3950 (0.4724)	Prec@1 89.844 (87.676)	
Total train loss: 0.4724

 * Prec@1 69.010 Prec@5 89.960 Loss 1.2021
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4070 (0.4558)	Prec@1 89.062 (87.841)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.4446 (0.4567)	Prec@1 89.062 (88.096)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.4287 (0.4599)	Prec@1 90.625 (88.171)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4321 (0.4615)	Prec@1 89.844 (88.076)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.5591 (0.4607)	Prec@1 82.812 (88.081)	
Total train loss: 0.4607

 * Prec@1 68.800 Prec@5 90.080 Loss 1.2090
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2854 (0.4407)	Prec@1 95.312 (89.183)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.4578 (0.4442)	Prec@1 85.938 (88.717)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.4231 (0.4488)	Prec@1 89.844 (88.558)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.4590 (0.4496)	Prec@1 89.062 (88.569)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4221 (0.4500)	Prec@1 89.062 (88.532)	
Total train loss: 0.4500

 * Prec@1 68.660 Prec@5 90.020 Loss 1.2207
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.4614 (0.4314)	Prec@1 87.500 (89.253)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.3940 (0.4267)	Prec@1 91.406 (89.358)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.3879 (0.4351)	Prec@1 90.625 (88.946)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.4153 (0.4378)	Prec@1 88.281 (88.877)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.3638 (0.4403)	Prec@1 91.406 (88.822)	
Total train loss: 0.4403

 * Prec@1 68.660 Prec@5 89.810 Loss 1.2246
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3621 (0.4179)	Prec@1 92.969 (89.974)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.4170 (0.4194)	Prec@1 90.625 (89.694)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3518 (0.4192)	Prec@1 93.750 (89.747)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.3806 (0.4239)	Prec@1 88.281 (89.506)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.4368 (0.4269)	Prec@1 87.500 (89.339)	
Total train loss: 0.4271

 * Prec@1 68.400 Prec@5 89.730 Loss 1.2441
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4080 (0.3961)	Prec@1 90.625 (91.146)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3567 (0.4017)	Prec@1 92.969 (90.620)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.3220 (0.4046)	Prec@1 93.750 (90.445)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.3433 (0.4031)	Prec@1 93.750 (90.537)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.4355 (0.4031)	Prec@1 89.062 (90.521)	
Total train loss: 0.4032

 * Prec@1 68.530 Prec@5 89.800 Loss 1.2383
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.3821 (0.3929)	Prec@1 90.625 (90.465)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3713 (0.4007)	Prec@1 91.406 (90.485)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3899 (0.4029)	Prec@1 87.500 (90.375)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.4854 (0.4018)	Prec@1 86.719 (90.412)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.3840 (0.4009)	Prec@1 93.750 (90.531)	
Total train loss: 0.4009

 * Prec@1 68.550 Prec@5 89.690 Loss 1.2246
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3975 (0.3994)	Prec@1 92.188 (90.495)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3916 (0.3969)	Prec@1 91.406 (90.595)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.4814 (0.4000)	Prec@1 83.594 (90.435)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3623 (0.4011)	Prec@1 92.188 (90.360)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.5127 (0.4038)	Prec@1 86.719 (90.294)	
Total train loss: 0.4039

 * Prec@1 68.600 Prec@5 89.780 Loss 1.2324
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4480 (0.4009)	Prec@1 90.625 (90.395)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3489 (0.3998)	Prec@1 91.406 (90.440)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3970 (0.3993)	Prec@1 89.844 (90.455)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3596 (0.3982)	Prec@1 93.750 (90.517)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.3953 (0.3998)	Prec@1 90.625 (90.533)	
Total train loss: 0.4000

 * Prec@1 68.790 Prec@5 89.690 Loss 1.2314
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2891 (0.3845)	Prec@1 96.875 (91.116)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.4097 (0.3896)	Prec@1 88.281 (90.925)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.4390 (0.3954)	Prec@1 88.281 (90.595)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.4214 (0.3970)	Prec@1 89.844 (90.560)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.5146 (0.3987)	Prec@1 87.500 (90.507)	
Total train loss: 0.3986

 * Prec@1 68.270 Prec@5 89.640 Loss 1.2354
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.3755 (0.3958)	Prec@1 89.844 (90.795)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.4153 (0.3968)	Prec@1 88.281 (90.635)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.4812 (0.3981)	Prec@1 89.062 (90.628)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.3738 (0.3965)	Prec@1 89.844 (90.663)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.3792 (0.3976)	Prec@1 90.625 (90.649)	
Total train loss: 0.3977

 * Prec@1 68.310 Prec@5 89.720 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.3340 (0.4009)	Prec@1 93.750 (90.635)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.3582 (0.4066)	Prec@1 92.188 (90.400)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3730 (0.4026)	Prec@1 89.062 (90.478)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.4336 (0.4002)	Prec@1 87.500 (90.472)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.4631 (0.3990)	Prec@1 87.500 (90.533)	
Total train loss: 0.3991

 * Prec@1 68.370 Prec@5 89.740 Loss 1.2324
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.3960 (0.4087)	Prec@1 92.188 (89.994)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.4070 (0.4016)	Prec@1 88.281 (90.269)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.4509 (0.3996)	Prec@1 86.719 (90.224)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.4568 (0.4000)	Prec@1 84.375 (90.297)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.4189 (0.3999)	Prec@1 91.406 (90.319)	
Total train loss: 0.3998

 * Prec@1 68.250 Prec@5 89.580 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.4277 (0.4084)	Prec@1 92.188 (90.204)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.4590 (0.4037)	Prec@1 92.188 (90.495)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.4326 (0.4031)	Prec@1 88.281 (90.488)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.4043 (0.4012)	Prec@1 94.531 (90.542)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.3972 (0.3995)	Prec@1 93.750 (90.579)	
Total train loss: 0.3995

 * Prec@1 68.380 Prec@5 89.800 Loss 1.2354
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.3743 (0.3925)	Prec@1 92.188 (91.026)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.4097 (0.3920)	Prec@1 89.062 (90.946)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.3772 (0.3920)	Prec@1 92.969 (90.855)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.5015 (0.3931)	Prec@1 86.719 (90.805)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.3865 (0.3962)	Prec@1 92.188 (90.731)	
Total train loss: 0.3965

 * Prec@1 68.420 Prec@5 89.630 Loss 1.2354
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.3215 (0.4015)	Prec@1 96.875 (90.154)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.4285 (0.3943)	Prec@1 89.844 (90.495)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.4080 (0.3941)	Prec@1 91.406 (90.585)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.4565 (0.3935)	Prec@1 86.719 (90.595)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.3389 (0.3951)	Prec@1 92.969 (90.611)	
Total train loss: 0.3953

 * Prec@1 68.210 Prec@5 89.730 Loss 1.2373
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.3989 (0.3898)	Prec@1 89.844 (90.865)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.4004 (0.3904)	Prec@1 92.188 (90.905)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.3992 (0.3917)	Prec@1 89.062 (90.825)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.4946 (0.3949)	Prec@1 88.281 (90.643)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.3328 (0.3951)	Prec@1 92.188 (90.627)	
Total train loss: 0.3953

 * Prec@1 68.420 Prec@5 89.790 Loss 1.2344
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.4688 (0.3955)	Prec@1 88.281 (90.555)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.4265 (0.3995)	Prec@1 89.062 (90.580)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.4102 (0.3957)	Prec@1 89.844 (90.648)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.3401 (0.3957)	Prec@1 92.188 (90.678)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.2961 (0.3964)	Prec@1 94.531 (90.635)	
Total train loss: 0.3965

 * Prec@1 68.300 Prec@5 89.760 Loss 1.2354
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.4958 (0.3862)	Prec@1 88.281 (91.216)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.3992 (0.3863)	Prec@1 93.750 (91.261)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.2998 (0.3885)	Prec@1 92.188 (91.132)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.3652 (0.3914)	Prec@1 96.094 (90.976)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.3188 (0.3928)	Prec@1 95.312 (90.903)	
Total train loss: 0.3931

 * Prec@1 68.340 Prec@5 89.690 Loss 1.2383
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.4294 (0.3885)	Prec@1 87.500 (90.705)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.4338 (0.3882)	Prec@1 88.281 (90.915)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.3667 (0.3903)	Prec@1 92.188 (90.979)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.3733 (0.3929)	Prec@1 92.969 (90.855)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.3357 (0.3937)	Prec@1 91.406 (90.769)	
Total train loss: 0.3937

 * Prec@1 68.430 Prec@5 89.710 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.4619 (0.4010)	Prec@1 88.281 (90.675)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.3254 (0.4004)	Prec@1 93.750 (90.665)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.3193 (0.3974)	Prec@1 93.750 (90.718)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.3853 (0.3958)	Prec@1 89.844 (90.753)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.3660 (0.3943)	Prec@1 95.312 (90.791)	
Total train loss: 0.3943

 * Prec@1 68.550 Prec@5 89.680 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.3896 (0.3887)	Prec@1 92.188 (91.076)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.4272 (0.3937)	Prec@1 85.938 (90.905)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.3337 (0.3952)	Prec@1 92.188 (90.739)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.4866 (0.3953)	Prec@1 89.062 (90.750)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.4456 (0.3938)	Prec@1 89.062 (90.775)	
Total train loss: 0.3936

 * Prec@1 68.440 Prec@5 89.800 Loss 1.2314
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.4946 (0.4017)	Prec@1 89.062 (90.535)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.4158 (0.3974)	Prec@1 87.500 (90.815)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.4468 (0.3966)	Prec@1 89.062 (90.829)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.4470 (0.3937)	Prec@1 90.625 (90.933)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.4595 (0.3927)	Prec@1 87.500 (90.954)	
Total train loss: 0.3928

 * Prec@1 68.520 Prec@5 89.680 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.4075 (0.3940)	Prec@1 87.500 (90.695)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.3506 (0.3888)	Prec@1 91.406 (90.915)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.5332 (0.3892)	Prec@1 85.156 (90.909)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.3347 (0.3905)	Prec@1 94.531 (90.880)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.3289 (0.3910)	Prec@1 91.406 (90.861)	
Total train loss: 0.3910

 * Prec@1 68.190 Prec@5 89.760 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.3171 (0.3803)	Prec@1 93.750 (91.306)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.4409 (0.3898)	Prec@1 90.625 (90.910)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.4102 (0.3876)	Prec@1 92.188 (91.086)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.3284 (0.3876)	Prec@1 92.969 (91.011)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.4041 (0.3898)	Prec@1 88.281 (90.948)	
Total train loss: 0.3901

 * Prec@1 68.090 Prec@5 89.550 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.4294 (0.3935)	Prec@1 83.594 (90.885)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.3667 (0.3879)	Prec@1 91.406 (91.151)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.3679 (0.3913)	Prec@1 92.969 (90.979)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.3367 (0.3912)	Prec@1 90.625 (90.993)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.3962 (0.3915)	Prec@1 89.062 (90.970)	
Total train loss: 0.3916

 * Prec@1 68.150 Prec@5 89.780 Loss 1.2373
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.3843 (0.3948)	Prec@1 91.406 (90.825)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.3076 (0.3938)	Prec@1 94.531 (90.815)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.3992 (0.3946)	Prec@1 90.625 (90.775)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.4709 (0.3939)	Prec@1 89.062 (90.900)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.3049 (0.3924)	Prec@1 95.312 (90.889)	
Total train loss: 0.3924

 * Prec@1 68.360 Prec@5 89.720 Loss 1.2354
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.3848 (0.3966)	Prec@1 92.188 (90.725)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.3867 (0.3909)	Prec@1 90.625 (91.026)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.3586 (0.3904)	Prec@1 91.406 (90.889)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.4392 (0.3919)	Prec@1 89.062 (90.858)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.4287 (0.3910)	Prec@1 92.188 (90.857)	
Total train loss: 0.3913

 * Prec@1 68.390 Prec@5 89.330 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.3440 (0.3917)	Prec@1 90.625 (90.575)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.3682 (0.3945)	Prec@1 92.969 (90.650)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.3457 (0.3922)	Prec@1 92.188 (90.809)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.4238 (0.3900)	Prec@1 89.844 (90.890)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.4973 (0.3906)	Prec@1 88.281 (90.865)	
Total train loss: 0.3910

 * Prec@1 68.210 Prec@5 89.590 Loss 1.2402
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4202 (0.3887)	Prec@1 89.844 (91.086)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4446 (0.3854)	Prec@1 85.156 (91.216)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.4116 (0.3857)	Prec@1 90.625 (91.122)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.4846 (0.3865)	Prec@1 91.406 (91.126)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.4219 (0.3874)	Prec@1 88.281 (91.080)	
Total train loss: 0.3876

 * Prec@1 68.410 Prec@5 89.640 Loss 1.2344
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4287 (0.3803)	Prec@1 89.844 (91.096)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.4656 (0.3852)	Prec@1 89.062 (91.046)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.4075 (0.3878)	Prec@1 92.188 (90.986)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.2898 (0.3903)	Prec@1 96.094 (90.903)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.4404 (0.3877)	Prec@1 88.281 (91.000)	
Total train loss: 0.3878

 * Prec@1 68.150 Prec@5 89.540 Loss 1.2471
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.3965 (0.3933)	Prec@1 90.625 (91.036)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.3740 (0.3898)	Prec@1 92.188 (91.116)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.4944 (0.3875)	Prec@1 87.500 (91.216)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3989 (0.3887)	Prec@1 88.281 (91.158)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.3230 (0.3877)	Prec@1 92.969 (91.108)	
Total train loss: 0.3879

 * Prec@1 68.270 Prec@5 89.770 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.4812 (0.3928)	Prec@1 86.719 (90.895)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.4263 (0.3919)	Prec@1 87.500 (90.865)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.3262 (0.3892)	Prec@1 93.750 (90.952)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.3499 (0.3871)	Prec@1 94.531 (91.011)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.3767 (0.3878)	Prec@1 91.406 (90.984)	
Total train loss: 0.3881

 * Prec@1 68.450 Prec@5 89.770 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.3054 (0.3794)	Prec@1 95.312 (91.286)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.3477 (0.3853)	Prec@1 92.188 (91.126)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.3953 (0.3872)	Prec@1 88.281 (91.032)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.3188 (0.3872)	Prec@1 95.312 (90.963)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.3621 (0.3878)	Prec@1 94.531 (90.948)	
Total train loss: 0.3878

 * Prec@1 68.500 Prec@5 89.820 Loss 1.2383
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.3286 (0.3839)	Prec@1 94.531 (91.346)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.3403 (0.3828)	Prec@1 92.969 (91.196)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.4280 (0.3862)	Prec@1 89.844 (91.122)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.4277 (0.3866)	Prec@1 88.281 (91.043)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.3625 (0.3885)	Prec@1 93.750 (90.958)	
Total train loss: 0.3885

 * Prec@1 68.220 Prec@5 89.670 Loss 1.2480
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.3704 (0.3830)	Prec@1 88.281 (91.236)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.3625 (0.3829)	Prec@1 91.406 (91.121)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.4192 (0.3866)	Prec@1 92.969 (91.102)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.2971 (0.3857)	Prec@1 94.531 (91.146)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.4014 (0.3877)	Prec@1 87.500 (91.018)	
Total train loss: 0.3880

 * Prec@1 68.390 Prec@5 89.650 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.3599 (0.3810)	Prec@1 90.625 (91.296)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.4448 (0.3848)	Prec@1 89.062 (91.031)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.3594 (0.3832)	Prec@1 92.188 (91.169)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.4265 (0.3864)	Prec@1 89.844 (91.048)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4829 (0.3852)	Prec@1 90.625 (91.130)	
Total train loss: 0.3852

 * Prec@1 68.080 Prec@5 89.670 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.3196 (0.3850)	Prec@1 92.969 (91.196)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.3545 (0.3814)	Prec@1 93.750 (91.306)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.3745 (0.3835)	Prec@1 89.844 (91.166)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.3730 (0.3838)	Prec@1 92.188 (91.106)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.5581 (0.3861)	Prec@1 82.812 (91.074)	
Total train loss: 0.3861

 * Prec@1 68.270 Prec@5 89.490 Loss 1.2422
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.2847 (0.3916)	Prec@1 94.531 (90.935)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.4189 (0.3882)	Prec@1 90.625 (90.880)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.3538 (0.3841)	Prec@1 91.406 (91.049)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.3752 (0.3845)	Prec@1 92.188 (91.071)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.5234 (0.3852)	Prec@1 85.938 (91.090)	
Total train loss: 0.3854

 * Prec@1 68.390 Prec@5 89.670 Loss 1.2393
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.3215 (0.3954)	Prec@1 93.750 (90.815)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.3892 (0.3908)	Prec@1 88.281 (90.870)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.4150 (0.3894)	Prec@1 90.625 (91.022)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.4810 (0.3882)	Prec@1 85.156 (91.006)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.4351 (0.3869)	Prec@1 85.938 (91.082)	
Total train loss: 0.3870

 * Prec@1 68.340 Prec@5 89.750 Loss 1.2441
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.4397 (0.3913)	Prec@1 88.281 (90.645)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.4165 (0.3911)	Prec@1 88.281 (90.875)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.4226 (0.3855)	Prec@1 90.625 (91.099)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3989 (0.3832)	Prec@1 89.844 (91.208)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.4231 (0.3847)	Prec@1 90.625 (91.150)	
Total train loss: 0.3849

 * Prec@1 68.210 Prec@5 89.390 Loss 1.2451
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.3621 (0.3884)	Prec@1 95.312 (91.106)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.4338 (0.3871)	Prec@1 88.281 (91.026)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.3374 (0.3836)	Prec@1 92.969 (91.146)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.4375 (0.3838)	Prec@1 89.844 (91.148)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.2610 (0.3837)	Prec@1 96.094 (91.140)	
Total train loss: 0.3837

 * Prec@1 68.220 Prec@5 89.510 Loss 1.2471
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.3945 (0.3876)	Prec@1 88.281 (91.236)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.4019 (0.3829)	Prec@1 89.844 (91.306)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.3440 (0.3856)	Prec@1 96.094 (91.226)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.3218 (0.3852)	Prec@1 92.188 (91.201)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.4199 (0.3860)	Prec@1 92.969 (91.100)	
Total train loss: 0.3861

 * Prec@1 68.150 Prec@5 89.670 Loss 1.2432
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.3115 (0.3846)	Prec@1 92.969 (91.386)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.3816 (0.3871)	Prec@1 91.406 (91.176)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.3284 (0.3868)	Prec@1 92.188 (91.169)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.4106 (0.3855)	Prec@1 91.406 (91.193)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4380 (0.3851)	Prec@1 91.406 (91.254)	
Total train loss: 0.3852

 * Prec@1 68.340 Prec@5 89.490 Loss 1.2480
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.3281 (0.3778)	Prec@1 95.312 (91.456)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.3706 (0.3785)	Prec@1 88.281 (91.391)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.3733 (0.3784)	Prec@1 91.406 (91.416)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.5342 (0.3807)	Prec@1 85.938 (91.316)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.3630 (0.3820)	Prec@1 91.406 (91.270)	
Total train loss: 0.3823

 * Prec@1 68.150 Prec@5 89.530 Loss 1.2471
Best acc: 69.610
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 66.330 Prec@5 88.910 Loss 1.2773
Pre-trained Prec@1 with 5 layers frozen: 66.33000183105469 	 Loss: 1.27734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.6030 (0.6043)	Prec@1 85.156 (82.933)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.8862 (0.6006)	Prec@1 73.438 (82.993)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5771 (0.5961)	Prec@1 85.938 (83.096)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6982 (0.5945)	Prec@1 80.469 (83.110)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.6470 (0.5928)	Prec@1 81.250 (83.179)	
Total train loss: 0.5926

 * Prec@1 69.250 Prec@5 90.290 Loss 1.1816
Best acc: 69.250
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.6079 (0.5636)	Prec@1 82.031 (84.004)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5508 (0.5594)	Prec@1 84.375 (84.370)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5903 (0.5575)	Prec@1 82.812 (84.435)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4878 (0.5560)	Prec@1 85.156 (84.553)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.6182 (0.5562)	Prec@1 82.812 (84.483)	
Total train loss: 0.5558

 * Prec@1 69.490 Prec@5 90.520 Loss 1.1758
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.3740 (0.5290)	Prec@1 92.969 (85.597)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4551 (0.5340)	Prec@1 89.062 (85.181)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5898 (0.5393)	Prec@1 82.812 (84.966)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5776 (0.5348)	Prec@1 80.469 (85.026)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.4939 (0.5363)	Prec@1 85.156 (84.996)	
Total train loss: 0.5363

 * Prec@1 68.760 Prec@5 89.970 Loss 1.1934
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.5107 (0.5038)	Prec@1 86.719 (86.769)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.4116 (0.5108)	Prec@1 89.844 (86.378)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5103 (0.5134)	Prec@1 87.500 (86.288)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5068 (0.5165)	Prec@1 84.375 (86.038)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.4548 (0.5175)	Prec@1 87.500 (85.942)	
Total train loss: 0.5175

 * Prec@1 68.960 Prec@5 90.110 Loss 1.1992
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.5400 (0.4881)	Prec@1 85.156 (86.699)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4385 (0.4928)	Prec@1 89.062 (86.879)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4233 (0.4957)	Prec@1 88.281 (86.655)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.4763 (0.4982)	Prec@1 87.500 (86.704)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.4065 (0.5007)	Prec@1 89.844 (86.514)	
Total train loss: 0.5008

 * Prec@1 68.900 Prec@5 90.090 Loss 1.1992
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4543 (0.4778)	Prec@1 86.719 (87.570)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4802 (0.4759)	Prec@1 87.500 (87.510)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5322 (0.4763)	Prec@1 85.156 (87.543)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.5063 (0.4809)	Prec@1 87.500 (87.402)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.4846 (0.4829)	Prec@1 87.500 (87.274)	
Total train loss: 0.4830

 * Prec@1 68.760 Prec@5 89.910 Loss 1.2090
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4648 (0.4654)	Prec@1 85.938 (88.121)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.5107 (0.4648)	Prec@1 88.281 (88.061)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3950 (0.4674)	Prec@1 89.844 (87.814)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.6528 (0.4710)	Prec@1 76.562 (87.628)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.5337 (0.4728)	Prec@1 83.594 (87.542)	
Total train loss: 0.4730

 * Prec@1 68.670 Prec@5 89.680 Loss 1.2266
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4280 (0.4385)	Prec@1 89.062 (89.163)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3826 (0.4464)	Prec@1 91.406 (88.777)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.5293 (0.4509)	Prec@1 84.375 (88.635)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.4375 (0.4556)	Prec@1 92.188 (88.449)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4919 (0.4602)	Prec@1 89.062 (88.203)	
Total train loss: 0.4604

 * Prec@1 68.260 Prec@5 89.860 Loss 1.2227
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.3555 (0.4336)	Prec@1 91.406 (89.283)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.5864 (0.4402)	Prec@1 84.375 (88.917)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.4131 (0.4403)	Prec@1 89.844 (88.829)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.4233 (0.4439)	Prec@1 88.281 (88.672)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.4067 (0.4484)	Prec@1 89.844 (88.560)	
Total train loss: 0.4483

 * Prec@1 68.340 Prec@5 89.710 Loss 1.2285
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3862 (0.4245)	Prec@1 92.188 (89.653)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.4644 (0.4283)	Prec@1 89.062 (89.533)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3662 (0.4331)	Prec@1 89.844 (89.216)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.4971 (0.4350)	Prec@1 86.719 (89.155)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.5293 (0.4372)	Prec@1 85.156 (89.048)	
Total train loss: 0.4373

 * Prec@1 68.220 Prec@5 89.770 Loss 1.2354
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4321 (0.4174)	Prec@1 89.062 (90.084)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.4629 (0.4100)	Prec@1 89.062 (90.099)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.3159 (0.4121)	Prec@1 92.969 (90.128)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.3689 (0.4140)	Prec@1 92.969 (90.037)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3914 (0.4130)	Prec@1 92.188 (90.050)	
Total train loss: 0.4129

 * Prec@1 67.950 Prec@5 89.720 Loss 1.2373
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.4653 (0.4135)	Prec@1 85.938 (90.074)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4407 (0.4105)	Prec@1 88.281 (90.224)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3667 (0.4096)	Prec@1 90.625 (90.174)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.3120 (0.4107)	Prec@1 93.750 (90.089)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.3494 (0.4112)	Prec@1 92.188 (90.036)	
Total train loss: 0.4114

 * Prec@1 67.890 Prec@5 89.700 Loss 1.2393
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3611 (0.4084)	Prec@1 91.406 (90.124)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4849 (0.4051)	Prec@1 85.938 (90.209)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.4307 (0.4069)	Prec@1 91.406 (90.144)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.4656 (0.4065)	Prec@1 89.062 (90.167)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3774 (0.4087)	Prec@1 92.188 (90.096)	
Total train loss: 0.4088

 * Prec@1 68.120 Prec@5 89.700 Loss 1.2393
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4819 (0.4056)	Prec@1 85.156 (90.495)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3660 (0.4051)	Prec@1 91.406 (90.390)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4092 (0.4011)	Prec@1 92.188 (90.625)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.4480 (0.4038)	Prec@1 88.281 (90.512)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.3384 (0.4062)	Prec@1 92.969 (90.441)	
Total train loss: 0.4064

 * Prec@1 68.430 Prec@5 89.760 Loss 1.2334
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.4175 (0.4071)	Prec@1 94.531 (90.174)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3293 (0.4091)	Prec@1 93.750 (90.104)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.3843 (0.4060)	Prec@1 88.281 (90.198)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.3977 (0.4073)	Prec@1 89.062 (90.187)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3787 (0.4058)	Prec@1 92.188 (90.216)	
Total train loss: 0.4059

 * Prec@1 67.990 Prec@5 89.620 Loss 1.2422
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.3772 (0.4134)	Prec@1 94.531 (90.304)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.4214 (0.4026)	Prec@1 89.062 (90.725)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.3423 (0.4079)	Prec@1 90.625 (90.361)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.3904 (0.4068)	Prec@1 91.406 (90.355)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.4292 (0.4076)	Prec@1 89.844 (90.327)	
Total train loss: 0.4078

 * Prec@1 68.160 Prec@5 89.610 Loss 1.2383
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.3369 (0.4066)	Prec@1 93.750 (90.425)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.4006 (0.4057)	Prec@1 85.156 (90.355)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3982 (0.4054)	Prec@1 92.188 (90.371)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.3459 (0.4055)	Prec@1 92.969 (90.370)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.4556 (0.4043)	Prec@1 89.062 (90.353)	
Total train loss: 0.4043

 * Prec@1 67.940 Prec@5 89.590 Loss 1.2451
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.4597 (0.4085)	Prec@1 88.281 (90.785)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.3948 (0.4012)	Prec@1 91.406 (90.835)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.5601 (0.4047)	Prec@1 82.812 (90.542)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.3835 (0.4034)	Prec@1 90.625 (90.572)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.4897 (0.4045)	Prec@1 84.375 (90.529)	
Total train loss: 0.4045

 * Prec@1 68.060 Prec@5 89.620 Loss 1.2354
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.5542 (0.4052)	Prec@1 85.938 (90.475)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.3242 (0.4054)	Prec@1 92.969 (90.430)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.4717 (0.4069)	Prec@1 88.281 (90.348)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.4412 (0.4067)	Prec@1 87.500 (90.382)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.4060 (0.4047)	Prec@1 92.188 (90.393)	
Total train loss: 0.4047

 * Prec@1 68.130 Prec@5 89.750 Loss 1.2412
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.4290 (0.3952)	Prec@1 88.281 (90.865)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.4255 (0.3947)	Prec@1 91.406 (90.755)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.4446 (0.3971)	Prec@1 89.844 (90.712)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.3865 (0.4002)	Prec@1 91.406 (90.607)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.4363 (0.4038)	Prec@1 89.844 (90.497)	
Total train loss: 0.4041

 * Prec@1 68.090 Prec@5 89.570 Loss 1.2461
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.4382 (0.4125)	Prec@1 89.844 (90.124)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.3848 (0.4059)	Prec@1 92.188 (90.330)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.4382 (0.4033)	Prec@1 87.500 (90.401)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.5415 (0.4028)	Prec@1 85.938 (90.467)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.3252 (0.4026)	Prec@1 94.531 (90.461)	
Total train loss: 0.4027

 * Prec@1 68.140 Prec@5 89.690 Loss 1.2324
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.3784 (0.4022)	Prec@1 89.062 (90.545)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.3440 (0.4037)	Prec@1 92.969 (90.570)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.4246 (0.4004)	Prec@1 88.281 (90.675)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.4463 (0.4035)	Prec@1 91.406 (90.532)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.3789 (0.4029)	Prec@1 90.625 (90.533)	
Total train loss: 0.4029

 * Prec@1 68.420 Prec@5 89.610 Loss 1.2422
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.4893 (0.3963)	Prec@1 83.594 (90.595)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.4194 (0.4006)	Prec@1 86.719 (90.625)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.4553 (0.4016)	Prec@1 86.719 (90.592)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.3967 (0.3998)	Prec@1 89.062 (90.597)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.4011 (0.3994)	Prec@1 91.406 (90.625)	
Total train loss: 0.3998

 * Prec@1 67.930 Prec@5 89.560 Loss 1.2412
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.3909 (0.3987)	Prec@1 92.969 (90.425)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.4485 (0.3977)	Prec@1 89.062 (90.445)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.4419 (0.3994)	Prec@1 87.500 (90.385)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.4104 (0.4029)	Prec@1 89.062 (90.307)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.3462 (0.4031)	Prec@1 94.531 (90.282)	
Total train loss: 0.4034

 * Prec@1 68.320 Prec@5 89.580 Loss 1.2412
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.5249 (0.3933)	Prec@1 85.156 (90.575)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.3887 (0.3974)	Prec@1 90.625 (90.660)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.3845 (0.3984)	Prec@1 90.625 (90.625)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.5610 (0.4014)	Prec@1 86.719 (90.560)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.4331 (0.4006)	Prec@1 89.844 (90.557)	
Total train loss: 0.4006

 * Prec@1 68.070 Prec@5 89.580 Loss 1.2480
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.3708 (0.3969)	Prec@1 92.188 (90.665)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.4634 (0.3974)	Prec@1 88.281 (90.585)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.4163 (0.3976)	Prec@1 89.062 (90.588)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.4028 (0.3983)	Prec@1 90.625 (90.552)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.3716 (0.3978)	Prec@1 91.406 (90.629)	
Total train loss: 0.3981

 * Prec@1 68.170 Prec@5 89.590 Loss 1.2334
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.4021 (0.3930)	Prec@1 90.625 (90.966)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.4333 (0.4001)	Prec@1 89.062 (90.600)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.3564 (0.3989)	Prec@1 93.750 (90.558)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.3989 (0.3984)	Prec@1 90.625 (90.550)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.5000 (0.4000)	Prec@1 88.281 (90.537)	
Total train loss: 0.4004

 * Prec@1 67.960 Prec@5 89.570 Loss 1.2422
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.3745 (0.3924)	Prec@1 91.406 (90.966)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.3882 (0.3957)	Prec@1 89.062 (90.750)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.3579 (0.3978)	Prec@1 88.281 (90.668)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.3662 (0.3954)	Prec@1 89.844 (90.745)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.3374 (0.3966)	Prec@1 92.188 (90.685)	
Total train loss: 0.3965

 * Prec@1 68.190 Prec@5 89.440 Loss 1.2422
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.3772 (0.4074)	Prec@1 92.188 (90.385)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.4021 (0.4029)	Prec@1 89.844 (90.460)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.4260 (0.4030)	Prec@1 89.844 (90.408)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.3330 (0.4017)	Prec@1 89.062 (90.450)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.3953 (0.4001)	Prec@1 90.625 (90.501)	
Total train loss: 0.4004

 * Prec@1 68.010 Prec@5 89.490 Loss 1.2490
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.5073 (0.3957)	Prec@1 88.281 (90.946)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.3433 (0.3962)	Prec@1 92.188 (90.900)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.3533 (0.3916)	Prec@1 94.531 (90.969)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.3369 (0.3917)	Prec@1 92.188 (90.908)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.3564 (0.3944)	Prec@1 89.844 (90.781)	
Total train loss: 0.3946

 * Prec@1 67.930 Prec@5 89.470 Loss 1.2441
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.3804 (0.3952)	Prec@1 92.188 (90.765)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.4634 (0.3947)	Prec@1 87.500 (90.780)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.4666 (0.3962)	Prec@1 89.062 (90.835)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.4836 (0.3955)	Prec@1 88.281 (90.863)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.3740 (0.3968)	Prec@1 91.406 (90.725)	
Total train loss: 0.3969

 * Prec@1 68.020 Prec@5 89.530 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.3806 (0.3953)	Prec@1 92.969 (90.665)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.3677 (0.3934)	Prec@1 89.062 (90.820)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.4253 (0.3921)	Prec@1 90.625 (90.839)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.4424 (0.3921)	Prec@1 87.500 (90.798)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.4575 (0.3946)	Prec@1 87.500 (90.715)	
Total train loss: 0.3947

 * Prec@1 67.830 Prec@5 89.560 Loss 1.2480
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.4102 (0.3893)	Prec@1 90.625 (90.635)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.4670 (0.3932)	Prec@1 87.500 (90.690)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.3284 (0.3940)	Prec@1 92.969 (90.658)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.3579 (0.3937)	Prec@1 94.531 (90.705)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.3193 (0.3948)	Prec@1 95.312 (90.765)	
Total train loss: 0.3948

 * Prec@1 67.710 Prec@5 89.500 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.3735 (0.3910)	Prec@1 92.188 (90.645)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.3901 (0.3952)	Prec@1 90.625 (90.710)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.3977 (0.3943)	Prec@1 91.406 (90.712)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.4500 (0.3951)	Prec@1 90.625 (90.755)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.3350 (0.3947)	Prec@1 94.531 (90.793)	
Total train loss: 0.3948

 * Prec@1 68.130 Prec@5 89.490 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4231 (0.3965)	Prec@1 89.062 (90.635)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4971 (0.3963)	Prec@1 87.500 (90.675)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.4739 (0.3976)	Prec@1 88.281 (90.662)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.3984 (0.3997)	Prec@1 89.844 (90.590)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.3230 (0.3961)	Prec@1 92.188 (90.723)	
Total train loss: 0.3964

 * Prec@1 67.960 Prec@5 89.490 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.3423 (0.3917)	Prec@1 92.188 (90.996)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.3372 (0.3950)	Prec@1 92.188 (90.875)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.3621 (0.3963)	Prec@1 91.406 (90.859)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.4255 (0.3972)	Prec@1 89.844 (90.855)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.3877 (0.3955)	Prec@1 89.844 (90.885)	
Total train loss: 0.3956

 * Prec@1 67.810 Prec@5 89.530 Loss 1.2422
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.4761 (0.4001)	Prec@1 85.938 (90.665)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.3403 (0.3982)	Prec@1 92.188 (90.545)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.3276 (0.3966)	Prec@1 92.188 (90.588)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3760 (0.3977)	Prec@1 92.188 (90.580)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.3333 (0.3965)	Prec@1 93.750 (90.661)	
Total train loss: 0.3967

 * Prec@1 67.870 Prec@5 89.580 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.4106 (0.3894)	Prec@1 89.844 (91.016)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.3264 (0.3911)	Prec@1 93.750 (91.031)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.3289 (0.3937)	Prec@1 92.969 (90.966)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.3286 (0.3923)	Prec@1 92.969 (91.068)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.2881 (0.3920)	Prec@1 96.094 (91.064)	
Total train loss: 0.3920

 * Prec@1 68.070 Prec@5 89.540 Loss 1.2393
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.3845 (0.4032)	Prec@1 92.188 (90.485)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.3425 (0.3996)	Prec@1 92.969 (90.690)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.3879 (0.3972)	Prec@1 87.500 (90.755)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.4189 (0.3944)	Prec@1 89.844 (90.765)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.3789 (0.3930)	Prec@1 90.625 (90.785)	
Total train loss: 0.3930

 * Prec@1 67.890 Prec@5 89.530 Loss 1.2441
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.3567 (0.4015)	Prec@1 92.188 (90.545)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.3535 (0.3935)	Prec@1 89.844 (90.780)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.3923 (0.3908)	Prec@1 92.969 (90.905)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.5190 (0.3907)	Prec@1 85.938 (90.933)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.4355 (0.3884)	Prec@1 89.062 (91.060)	
Total train loss: 0.3885

 * Prec@1 68.030 Prec@5 89.570 Loss 1.2461
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.4053 (0.3934)	Prec@1 90.625 (91.236)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.3911 (0.3969)	Prec@1 91.406 (91.001)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.4153 (0.3972)	Prec@1 90.625 (90.919)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.3467 (0.3954)	Prec@1 91.406 (90.930)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.4446 (0.3954)	Prec@1 87.500 (90.948)	
Total train loss: 0.3954

 * Prec@1 67.840 Prec@5 89.420 Loss 1.2490
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.4446 (0.3936)	Prec@1 87.500 (90.645)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.4619 (0.3962)	Prec@1 89.062 (90.820)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.4468 (0.3936)	Prec@1 90.625 (90.932)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.3181 (0.3938)	Prec@1 93.750 (90.930)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4199 (0.3927)	Prec@1 87.500 (90.905)	
Total train loss: 0.3929

 * Prec@1 67.900 Prec@5 89.440 Loss 1.2520
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.3325 (0.3930)	Prec@1 93.750 (90.745)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.3157 (0.3978)	Prec@1 94.531 (90.600)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.3589 (0.3943)	Prec@1 91.406 (90.782)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.4290 (0.3933)	Prec@1 89.844 (90.768)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.4475 (0.3940)	Prec@1 87.500 (90.715)	
Total train loss: 0.3941

 * Prec@1 67.870 Prec@5 89.570 Loss 1.2500
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.3708 (0.3930)	Prec@1 90.625 (90.625)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.5293 (0.3945)	Prec@1 87.500 (90.790)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.4058 (0.3935)	Prec@1 91.406 (90.732)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.3359 (0.3933)	Prec@1 96.094 (90.765)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.3884 (0.3932)	Prec@1 89.062 (90.777)	
Total train loss: 0.3933

 * Prec@1 67.680 Prec@5 89.440 Loss 1.2559
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.4641 (0.4026)	Prec@1 86.719 (90.304)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4053 (0.3958)	Prec@1 89.062 (90.780)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.3450 (0.3942)	Prec@1 92.969 (90.879)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.3027 (0.3917)	Prec@1 94.531 (90.943)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.3857 (0.3940)	Prec@1 90.625 (90.871)	
Total train loss: 0.3944

 * Prec@1 68.130 Prec@5 89.480 Loss 1.2461
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.4949 (0.3997)	Prec@1 89.844 (90.725)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.3904 (0.3934)	Prec@1 90.625 (90.920)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.3701 (0.3895)	Prec@1 91.406 (91.082)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3518 (0.3901)	Prec@1 90.625 (90.983)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.5010 (0.3922)	Prec@1 86.719 (90.887)	
Total train loss: 0.3922

 * Prec@1 68.070 Prec@5 89.530 Loss 1.2461
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.3918 (0.3865)	Prec@1 89.062 (91.356)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.4768 (0.3888)	Prec@1 87.500 (91.251)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.3713 (0.3908)	Prec@1 92.188 (90.962)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.4836 (0.3896)	Prec@1 87.500 (90.983)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.3508 (0.3898)	Prec@1 94.531 (90.964)	
Total train loss: 0.3901

 * Prec@1 67.960 Prec@5 89.470 Loss 1.2471
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.4231 (0.3844)	Prec@1 91.406 (91.186)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.3691 (0.3880)	Prec@1 90.625 (90.971)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.3337 (0.3865)	Prec@1 92.969 (91.019)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.4404 (0.3890)	Prec@1 91.406 (90.941)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.4512 (0.3915)	Prec@1 88.281 (90.863)	
Total train loss: 0.3915

 * Prec@1 67.910 Prec@5 89.470 Loss 1.2510
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.3806 (0.3873)	Prec@1 91.406 (91.006)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.3467 (0.3903)	Prec@1 90.625 (90.810)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.2776 (0.3861)	Prec@1 96.094 (91.009)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.3247 (0.3867)	Prec@1 92.188 (91.033)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4187 (0.3885)	Prec@1 87.500 (91.038)	
Total train loss: 0.3887

 * Prec@1 67.920 Prec@5 89.430 Loss 1.2500
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.3342 (0.3919)	Prec@1 93.750 (90.976)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.3950 (0.3964)	Prec@1 90.625 (90.830)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.4023 (0.3918)	Prec@1 89.062 (91.009)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.2878 (0.3913)	Prec@1 96.094 (90.920)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.4111 (0.3927)	Prec@1 92.188 (90.903)	
Total train loss: 0.3927

 * Prec@1 68.030 Prec@5 89.460 Loss 1.2529
Best acc: 69.490
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 65.160 Prec@5 88.110 Loss 1.3447
Pre-trained Prec@1 with 7 layers frozen: 65.15999603271484 	 Loss: 1.3447265625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5918 (0.6373)	Prec@1 82.031 (81.921)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.7070 (0.6173)	Prec@1 80.469 (82.462)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5210 (0.6121)	Prec@1 85.156 (82.686)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.5791 (0.6106)	Prec@1 85.156 (82.715)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.6562 (0.6076)	Prec@1 78.906 (82.764)	
Total train loss: 0.6077

 * Prec@1 69.140 Prec@5 90.340 Loss 1.1865
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.5132 (0.5593)	Prec@1 85.156 (84.245)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5576 (0.5652)	Prec@1 84.375 (83.989)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5229 (0.5654)	Prec@1 83.594 (84.018)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6880 (0.5688)	Prec@1 82.031 (83.957)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.5347 (0.5700)	Prec@1 85.156 (83.972)	
Total train loss: 0.5699

 * Prec@1 68.960 Prec@5 90.360 Loss 1.1836
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5317 (0.5487)	Prec@1 86.719 (84.746)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4417 (0.5433)	Prec@1 88.281 (85.026)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5200 (0.5457)	Prec@1 86.719 (84.909)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.4792 (0.5444)	Prec@1 88.281 (84.961)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5342 (0.5447)	Prec@1 82.812 (84.904)	
Total train loss: 0.5450

 * Prec@1 68.890 Prec@5 90.370 Loss 1.1943
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4282 (0.5038)	Prec@1 87.500 (85.978)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.3767 (0.5134)	Prec@1 88.281 (85.912)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.4817 (0.5218)	Prec@1 85.938 (85.787)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6616 (0.5223)	Prec@1 83.594 (85.645)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6206 (0.5251)	Prec@1 78.906 (85.583)	
Total train loss: 0.5253

 * Prec@1 68.660 Prec@5 90.260 Loss 1.2002
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4702 (0.4894)	Prec@1 84.375 (87.009)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4543 (0.4976)	Prec@1 86.719 (86.669)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4473 (0.5042)	Prec@1 91.406 (86.452)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5854 (0.5086)	Prec@1 82.031 (86.301)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5366 (0.5094)	Prec@1 87.500 (86.210)	
Total train loss: 0.5097

 * Prec@1 68.610 Prec@5 89.970 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4307 (0.4767)	Prec@1 91.406 (87.400)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4866 (0.4822)	Prec@1 88.281 (87.169)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.4895 (0.4854)	Prec@1 85.938 (87.036)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.6108 (0.4903)	Prec@1 81.250 (86.854)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.4741 (0.4923)	Prec@1 89.844 (86.785)	
Total train loss: 0.4924

 * Prec@1 68.590 Prec@5 89.840 Loss 1.2139
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4919 (0.4690)	Prec@1 87.500 (87.800)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.4121 (0.4719)	Prec@1 89.844 (87.565)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.5054 (0.4754)	Prec@1 85.938 (87.470)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4924 (0.4783)	Prec@1 85.938 (87.360)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.4844 (0.4791)	Prec@1 85.938 (87.430)	
Total train loss: 0.4792

 * Prec@1 68.320 Prec@5 89.900 Loss 1.2227
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4333 (0.4702)	Prec@1 88.281 (87.951)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.5347 (0.4636)	Prec@1 83.594 (88.111)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.4226 (0.4640)	Prec@1 88.281 (88.071)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.3828 (0.4627)	Prec@1 87.500 (88.108)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4565 (0.4662)	Prec@1 88.281 (88.025)	
Total train loss: 0.4666

 * Prec@1 68.550 Prec@5 89.850 Loss 1.2236
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.3184 (0.4542)	Prec@1 92.188 (88.492)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.4307 (0.4449)	Prec@1 85.938 (88.767)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.5220 (0.4487)	Prec@1 84.375 (88.535)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.4551 (0.4511)	Prec@1 88.281 (88.552)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.4849 (0.4522)	Prec@1 88.281 (88.470)	
Total train loss: 0.4524

 * Prec@1 68.140 Prec@5 89.560 Loss 1.2393
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.5435 (0.4262)	Prec@1 82.812 (89.413)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.4421 (0.4334)	Prec@1 87.500 (89.153)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3691 (0.4390)	Prec@1 89.844 (88.892)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.5376 (0.4432)	Prec@1 87.500 (88.749)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3972 (0.4455)	Prec@1 90.625 (88.672)	
Total train loss: 0.4455

 * Prec@1 67.910 Prec@5 89.460 Loss 1.2441
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4231 (0.4169)	Prec@1 90.625 (90.014)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.4009 (0.4206)	Prec@1 89.844 (89.764)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.4092 (0.4179)	Prec@1 90.625 (89.874)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.3833 (0.4181)	Prec@1 87.500 (89.809)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.4778 (0.4205)	Prec@1 89.062 (89.790)	
Total train loss: 0.4207

 * Prec@1 68.190 Prec@5 89.440 Loss 1.2461
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.3533 (0.4290)	Prec@1 92.969 (89.503)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3706 (0.4245)	Prec@1 92.188 (89.759)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.4092 (0.4220)	Prec@1 89.844 (89.814)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.4199 (0.4231)	Prec@1 86.719 (89.726)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.4600 (0.4192)	Prec@1 88.281 (89.910)	
Total train loss: 0.4194

 * Prec@1 68.170 Prec@5 89.440 Loss 1.2461
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3806 (0.4123)	Prec@1 92.969 (90.124)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4773 (0.4162)	Prec@1 86.719 (90.144)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.3801 (0.4198)	Prec@1 89.844 (89.864)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3684 (0.4174)	Prec@1 91.406 (89.979)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3811 (0.4169)	Prec@1 91.406 (89.992)	
Total train loss: 0.4170

 * Prec@1 67.730 Prec@5 89.420 Loss 1.2432
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3882 (0.4198)	Prec@1 92.188 (89.794)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4355 (0.4128)	Prec@1 89.062 (90.054)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3386 (0.4162)	Prec@1 89.844 (90.034)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3794 (0.4140)	Prec@1 89.844 (90.082)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.4114 (0.4148)	Prec@1 89.062 (90.044)	
Total train loss: 0.4148

 * Prec@1 67.880 Prec@5 89.440 Loss 1.2432
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.3882 (0.4075)	Prec@1 88.281 (90.585)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3892 (0.4082)	Prec@1 89.844 (90.345)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.4375 (0.4102)	Prec@1 91.406 (90.224)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.4626 (0.4129)	Prec@1 88.281 (90.099)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.4451 (0.4143)	Prec@1 88.281 (90.092)	
Total train loss: 0.4145

 * Prec@1 67.980 Prec@5 89.440 Loss 1.2471
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.2961 (0.4141)	Prec@1 95.312 (90.134)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.3369 (0.4094)	Prec@1 92.969 (90.279)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.4297 (0.4108)	Prec@1 89.844 (90.238)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.3208 (0.4130)	Prec@1 93.750 (90.059)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.4507 (0.4132)	Prec@1 85.156 (90.036)	
Total train loss: 0.4135

 * Prec@1 68.110 Prec@5 89.570 Loss 1.2422
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.4404 (0.4048)	Prec@1 90.625 (90.234)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.3308 (0.4072)	Prec@1 92.188 (90.294)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3416 (0.4111)	Prec@1 95.312 (90.208)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.3489 (0.4099)	Prec@1 91.406 (90.152)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.3684 (0.4100)	Prec@1 92.188 (90.188)	
Total train loss: 0.4103

 * Prec@1 68.170 Prec@5 89.570 Loss 1.2432
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.4087 (0.4127)	Prec@1 91.406 (90.435)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.3372 (0.4131)	Prec@1 92.188 (90.269)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.3674 (0.4097)	Prec@1 93.750 (90.478)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.3809 (0.4102)	Prec@1 92.188 (90.385)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.3455 (0.4094)	Prec@1 92.969 (90.375)	
Total train loss: 0.4094

 * Prec@1 67.840 Prec@5 89.560 Loss 1.2471
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.3330 (0.4136)	Prec@1 94.531 (89.894)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.4866 (0.4106)	Prec@1 88.281 (90.129)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.4290 (0.4103)	Prec@1 89.844 (90.131)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.3074 (0.4102)	Prec@1 94.531 (90.152)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.4365 (0.4118)	Prec@1 91.406 (90.098)	
Total train loss: 0.4118

 * Prec@1 68.120 Prec@5 89.440 Loss 1.2461
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.3074 (0.4035)	Prec@1 93.750 (90.435)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.3462 (0.4045)	Prec@1 96.094 (90.410)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.5649 (0.4080)	Prec@1 83.594 (90.244)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.5430 (0.4101)	Prec@1 83.594 (90.204)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.3486 (0.4090)	Prec@1 91.406 (90.296)	
Total train loss: 0.4091

 * Prec@1 68.000 Prec@5 89.490 Loss 1.2441
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.4656 (0.4150)	Prec@1 87.500 (90.034)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.4607 (0.4133)	Prec@1 88.281 (90.274)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.3406 (0.4117)	Prec@1 92.188 (90.241)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.3748 (0.4130)	Prec@1 92.969 (90.154)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.4385 (0.4104)	Prec@1 85.938 (90.206)	
Total train loss: 0.4103

 * Prec@1 67.780 Prec@5 89.500 Loss 1.2441
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.4966 (0.4089)	Prec@1 87.500 (90.244)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.4336 (0.4051)	Prec@1 92.188 (90.400)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.5107 (0.4079)	Prec@1 89.062 (90.304)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.4448 (0.4077)	Prec@1 87.500 (90.227)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.4226 (0.4089)	Prec@1 89.062 (90.214)	
Total train loss: 0.4090

 * Prec@1 68.090 Prec@5 89.510 Loss 1.2432
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.3757 (0.4045)	Prec@1 91.406 (90.345)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.3508 (0.4049)	Prec@1 94.531 (90.289)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.3782 (0.4044)	Prec@1 91.406 (90.418)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.4236 (0.4081)	Prec@1 87.500 (90.264)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.3762 (0.4077)	Prec@1 93.750 (90.312)	
Total train loss: 0.4080

 * Prec@1 68.040 Prec@5 89.390 Loss 1.2520
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.4888 (0.4099)	Prec@1 87.500 (90.315)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.3606 (0.4149)	Prec@1 92.188 (90.094)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.3455 (0.4089)	Prec@1 90.625 (90.221)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.4285 (0.4111)	Prec@1 92.188 (90.197)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.3694 (0.4090)	Prec@1 93.750 (90.260)	
Total train loss: 0.4091

 * Prec@1 67.830 Prec@5 89.340 Loss 1.2559
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.5288 (0.4101)	Prec@1 85.156 (90.174)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.4370 (0.4054)	Prec@1 89.844 (90.495)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.3833 (0.4090)	Prec@1 93.750 (90.348)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.3403 (0.4067)	Prec@1 89.844 (90.400)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.3875 (0.4054)	Prec@1 90.625 (90.399)	
Total train loss: 0.4058

 * Prec@1 67.580 Prec@5 89.320 Loss 1.2549
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.4373 (0.4040)	Prec@1 90.625 (90.775)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.3286 (0.4047)	Prec@1 92.188 (90.470)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.3826 (0.4044)	Prec@1 92.188 (90.532)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.3542 (0.4056)	Prec@1 89.844 (90.380)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.3950 (0.4074)	Prec@1 88.281 (90.302)	
Total train loss: 0.4074

 * Prec@1 67.980 Prec@5 89.560 Loss 1.2529
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.3938 (0.4052)	Prec@1 90.625 (90.505)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.3857 (0.4085)	Prec@1 89.844 (90.194)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.5142 (0.4095)	Prec@1 82.031 (90.208)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.4011 (0.4074)	Prec@1 90.625 (90.264)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.3892 (0.4073)	Prec@1 89.844 (90.182)	
Total train loss: 0.4073

 * Prec@1 68.050 Prec@5 89.450 Loss 1.2500
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.4055 (0.4045)	Prec@1 89.062 (90.304)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.4758 (0.4073)	Prec@1 88.281 (90.184)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.3682 (0.4052)	Prec@1 92.969 (90.261)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.3621 (0.4070)	Prec@1 92.188 (90.264)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.4226 (0.4041)	Prec@1 91.406 (90.431)	
Total train loss: 0.4043

 * Prec@1 68.000 Prec@5 89.440 Loss 1.2529
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.3835 (0.4028)	Prec@1 89.844 (90.795)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.4954 (0.4007)	Prec@1 86.719 (90.750)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.4700 (0.4088)	Prec@1 86.719 (90.388)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.5405 (0.4075)	Prec@1 85.938 (90.430)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.3362 (0.4071)	Prec@1 93.750 (90.401)	
Total train loss: 0.4072

 * Prec@1 68.000 Prec@5 89.330 Loss 1.2510
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.3247 (0.4156)	Prec@1 93.750 (90.014)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.5103 (0.4085)	Prec@1 89.844 (90.370)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.3560 (0.4061)	Prec@1 92.969 (90.385)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.3069 (0.4077)	Prec@1 91.406 (90.292)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.3923 (0.4075)	Prec@1 92.188 (90.304)	
Total train loss: 0.4077

 * Prec@1 67.670 Prec@5 89.490 Loss 1.2559
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.4333 (0.3989)	Prec@1 86.719 (90.875)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.4316 (0.4052)	Prec@1 89.062 (90.375)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.3809 (0.4086)	Prec@1 91.406 (90.308)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.4368 (0.4065)	Prec@1 89.844 (90.410)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.3301 (0.4059)	Prec@1 94.531 (90.373)	
Total train loss: 0.4060

 * Prec@1 67.950 Prec@5 89.540 Loss 1.2480
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.3101 (0.3983)	Prec@1 96.094 (90.535)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.3716 (0.3987)	Prec@1 92.969 (90.615)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.3882 (0.4029)	Prec@1 89.844 (90.358)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.3291 (0.4046)	Prec@1 92.969 (90.375)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.3792 (0.4046)	Prec@1 89.844 (90.379)	
Total train loss: 0.4045

 * Prec@1 68.020 Prec@5 89.520 Loss 1.2422
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.4434 (0.3986)	Prec@1 90.625 (90.735)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.3040 (0.4002)	Prec@1 96.094 (90.780)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.4275 (0.4039)	Prec@1 89.844 (90.478)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.4111 (0.4026)	Prec@1 91.406 (90.537)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.4294 (0.4042)	Prec@1 88.281 (90.447)	
Total train loss: 0.4041

 * Prec@1 67.870 Prec@5 89.440 Loss 1.2549
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.4355 (0.4015)	Prec@1 86.719 (90.595)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.3813 (0.3989)	Prec@1 91.406 (90.560)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.4102 (0.4011)	Prec@1 89.062 (90.505)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.3269 (0.4018)	Prec@1 92.188 (90.532)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.3711 (0.4010)	Prec@1 92.188 (90.565)	
Total train loss: 0.4010

 * Prec@1 67.970 Prec@5 89.350 Loss 1.2490
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.2854 (0.3962)	Prec@1 92.969 (90.455)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4329 (0.3991)	Prec@1 87.500 (90.505)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.3110 (0.4002)	Prec@1 94.531 (90.498)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.4617 (0.4012)	Prec@1 90.625 (90.470)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.3601 (0.4005)	Prec@1 91.406 (90.551)	
Total train loss: 0.4006

 * Prec@1 67.960 Prec@5 89.430 Loss 1.2500
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4526 (0.4033)	Prec@1 87.500 (90.615)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.4482 (0.4051)	Prec@1 92.188 (90.605)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.4319 (0.4044)	Prec@1 88.281 (90.625)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.4102 (0.4034)	Prec@1 92.969 (90.643)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.3938 (0.4033)	Prec@1 89.062 (90.659)	
Total train loss: 0.4034

 * Prec@1 68.030 Prec@5 89.280 Loss 1.2500
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.3059 (0.3987)	Prec@1 92.969 (90.665)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.5801 (0.4038)	Prec@1 82.031 (90.490)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.3538 (0.3999)	Prec@1 91.406 (90.588)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3618 (0.3989)	Prec@1 94.531 (90.723)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.4287 (0.3988)	Prec@1 89.844 (90.657)	
Total train loss: 0.3990

 * Prec@1 67.780 Prec@5 89.400 Loss 1.2588
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.4658 (0.4126)	Prec@1 85.938 (90.385)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.4226 (0.4121)	Prec@1 87.500 (90.184)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.3425 (0.4084)	Prec@1 92.188 (90.351)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.4065 (0.4070)	Prec@1 88.281 (90.322)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.3389 (0.4054)	Prec@1 93.750 (90.459)	
Total train loss: 0.4056

 * Prec@1 68.060 Prec@5 89.480 Loss 1.2471
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.3186 (0.4096)	Prec@1 94.531 (90.435)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.3921 (0.4034)	Prec@1 91.406 (90.545)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.4885 (0.3975)	Prec@1 89.844 (90.722)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.3240 (0.3999)	Prec@1 90.625 (90.670)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.3779 (0.4003)	Prec@1 90.625 (90.633)	
Total train loss: 0.4004

 * Prec@1 67.870 Prec@5 89.270 Loss 1.2510
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.3184 (0.4023)	Prec@1 92.188 (90.365)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.3149 (0.3962)	Prec@1 93.750 (90.655)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.3975 (0.3994)	Prec@1 90.625 (90.575)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.4338 (0.4001)	Prec@1 91.406 (90.607)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.3015 (0.3998)	Prec@1 96.094 (90.669)	
Total train loss: 0.3999

 * Prec@1 67.690 Prec@5 89.370 Loss 1.2549
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.4236 (0.3994)	Prec@1 90.625 (90.635)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.3503 (0.3954)	Prec@1 93.750 (90.810)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.2942 (0.3980)	Prec@1 92.969 (90.658)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.4724 (0.4016)	Prec@1 85.938 (90.522)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.3455 (0.4004)	Prec@1 89.844 (90.565)	
Total train loss: 0.4007

 * Prec@1 67.600 Prec@5 89.440 Loss 1.2588
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.3325 (0.3886)	Prec@1 93.750 (91.046)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.4565 (0.4009)	Prec@1 89.844 (90.675)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.5151 (0.3992)	Prec@1 86.719 (90.692)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.3245 (0.3990)	Prec@1 94.531 (90.743)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4502 (0.4007)	Prec@1 86.719 (90.693)	
Total train loss: 0.4006

 * Prec@1 67.960 Prec@5 89.410 Loss 1.2578
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.3157 (0.4074)	Prec@1 95.312 (90.254)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.4185 (0.4086)	Prec@1 89.844 (90.355)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.2939 (0.4016)	Prec@1 92.188 (90.568)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.3547 (0.4015)	Prec@1 92.969 (90.535)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.4512 (0.4019)	Prec@1 88.281 (90.535)	
Total train loss: 0.4020

 * Prec@1 67.910 Prec@5 89.360 Loss 1.2520
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.5063 (0.4026)	Prec@1 85.938 (90.655)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.4080 (0.3968)	Prec@1 90.625 (90.830)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.4146 (0.4026)	Prec@1 90.625 (90.632)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.3167 (0.4013)	Prec@1 94.531 (90.683)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.3984 (0.4005)	Prec@1 92.969 (90.735)	
Total train loss: 0.4004

 * Prec@1 67.800 Prec@5 89.250 Loss 1.2559
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.4236 (0.4055)	Prec@1 91.406 (90.715)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4287 (0.4029)	Prec@1 86.719 (90.635)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.3835 (0.4002)	Prec@1 92.188 (90.665)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.4609 (0.3979)	Prec@1 88.281 (90.685)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.3586 (0.3984)	Prec@1 92.969 (90.655)	
Total train loss: 0.3985

 * Prec@1 67.890 Prec@5 89.270 Loss 1.2559
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.3713 (0.3954)	Prec@1 92.969 (90.795)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.3982 (0.4037)	Prec@1 91.406 (90.435)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.4126 (0.3987)	Prec@1 89.062 (90.662)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3250 (0.3990)	Prec@1 92.188 (90.673)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.3237 (0.3974)	Prec@1 92.969 (90.669)	
Total train loss: 0.3975

 * Prec@1 67.740 Prec@5 89.430 Loss 1.2588
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.4194 (0.3941)	Prec@1 89.844 (90.745)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.4753 (0.3968)	Prec@1 87.500 (90.860)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.4104 (0.3980)	Prec@1 89.062 (90.698)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.3564 (0.3992)	Prec@1 93.750 (90.668)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.3201 (0.3990)	Prec@1 96.094 (90.719)	
Total train loss: 0.3989

 * Prec@1 67.800 Prec@5 89.380 Loss 1.2471
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.3979 (0.3925)	Prec@1 89.844 (90.735)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.3933 (0.3914)	Prec@1 90.625 (90.900)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.4004 (0.3968)	Prec@1 89.844 (90.862)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.4004 (0.3968)	Prec@1 88.281 (90.865)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.4541 (0.3982)	Prec@1 89.062 (90.751)	
Total train loss: 0.3982

 * Prec@1 67.840 Prec@5 89.310 Loss 1.2539
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.3718 (0.4008)	Prec@1 90.625 (90.425)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.3857 (0.4029)	Prec@1 89.844 (90.490)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.3728 (0.4010)	Prec@1 91.406 (90.535)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.4443 (0.3982)	Prec@1 91.406 (90.607)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4453 (0.3996)	Prec@1 91.406 (90.583)	
Total train loss: 0.3997

 * Prec@1 67.750 Prec@5 89.540 Loss 1.2559
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.3350 (0.4007)	Prec@1 97.656 (90.715)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.3979 (0.3996)	Prec@1 92.969 (90.725)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.3398 (0.3982)	Prec@1 90.625 (90.698)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.5146 (0.4000)	Prec@1 84.375 (90.610)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.3435 (0.3997)	Prec@1 92.969 (90.629)	
Total train loss: 0.3997

 * Prec@1 67.570 Prec@5 89.260 Loss 1.2588
Best acc: 69.140
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 57.720 Prec@5 83.750 Loss 1.7061
Pre-trained Prec@1 with 9 layers frozen: 57.71999740600586 	 Loss: 1.7060546875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5679 (0.7172)	Prec@1 85.938 (79.307)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5522 (0.6929)	Prec@1 87.500 (79.978)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.8154 (0.6783)	Prec@1 78.906 (80.442)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6543 (0.6684)	Prec@1 80.469 (80.744)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.5371 (0.6665)	Prec@1 85.156 (80.839)	
Total train loss: 0.6663

 * Prec@1 68.520 Prec@5 90.120 Loss 1.1855
Best acc: 68.520
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.7041 (0.5800)	Prec@1 78.125 (83.864)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.7271 (0.5878)	Prec@1 80.469 (83.469)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.7129 (0.5916)	Prec@1 76.562 (83.413)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.5015 (0.5933)	Prec@1 85.156 (83.243)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.6860 (0.5980)	Prec@1 79.688 (83.051)	
Total train loss: 0.5980

 * Prec@1 68.370 Prec@5 89.990 Loss 1.1934
Best acc: 68.520
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5610 (0.5666)	Prec@1 88.281 (84.525)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4092 (0.5685)	Prec@1 88.281 (84.275)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5654 (0.5736)	Prec@1 82.031 (83.844)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.6304 (0.5763)	Prec@1 80.469 (83.799)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5337 (0.5740)	Prec@1 83.594 (83.920)	
Total train loss: 0.5740

 * Prec@1 68.530 Prec@5 89.930 Loss 1.1982
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4575 (0.5488)	Prec@1 86.719 (84.716)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.4336 (0.5450)	Prec@1 88.281 (84.991)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5693 (0.5476)	Prec@1 85.156 (84.859)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5781 (0.5489)	Prec@1 85.156 (84.793)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.5229 (0.5514)	Prec@1 84.375 (84.692)	
Total train loss: 0.5515

 * Prec@1 68.140 Prec@5 89.650 Loss 1.2100
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.5303 (0.5053)	Prec@1 87.500 (86.619)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.5918 (0.5177)	Prec@1 82.812 (86.118)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.5405 (0.5219)	Prec@1 85.156 (85.811)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5752 (0.5287)	Prec@1 85.156 (85.477)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.4629 (0.5328)	Prec@1 90.625 (85.339)	
Total train loss: 0.5327

 * Prec@1 68.220 Prec@5 89.960 Loss 1.2080
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4226 (0.5046)	Prec@1 89.844 (86.458)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4619 (0.5111)	Prec@1 88.281 (86.288)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.6152 (0.5115)	Prec@1 81.250 (86.185)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.4368 (0.5130)	Prec@1 88.281 (86.100)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.5791 (0.5178)	Prec@1 81.250 (85.925)	
Total train loss: 0.5180

 * Prec@1 68.120 Prec@5 89.700 Loss 1.2207
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4514 (0.4869)	Prec@1 87.500 (87.149)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.5596 (0.4933)	Prec@1 84.375 (86.954)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.5293 (0.4997)	Prec@1 84.375 (86.635)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.6172 (0.5020)	Prec@1 85.156 (86.543)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.5547 (0.5018)	Prec@1 85.156 (86.538)	
Total train loss: 0.5018

 * Prec@1 68.010 Prec@5 89.520 Loss 1.2236
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4387 (0.4739)	Prec@1 89.062 (87.921)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.5151 (0.4806)	Prec@1 85.938 (87.610)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.4602 (0.4869)	Prec@1 87.500 (87.333)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.4282 (0.4885)	Prec@1 92.969 (87.252)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4653 (0.4902)	Prec@1 82.812 (87.123)	
Total train loss: 0.4905

 * Prec@1 67.870 Prec@5 89.400 Loss 1.2422
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.4673 (0.4620)	Prec@1 88.281 (88.221)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.5288 (0.4680)	Prec@1 81.250 (88.086)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.5020 (0.4675)	Prec@1 87.500 (88.094)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.3643 (0.4705)	Prec@1 91.406 (87.926)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.5630 (0.4747)	Prec@1 82.031 (87.734)	
Total train loss: 0.4748

 * Prec@1 67.950 Prec@5 89.450 Loss 1.2402
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3882 (0.4503)	Prec@1 92.969 (88.261)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.5210 (0.4536)	Prec@1 85.938 (88.482)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.5029 (0.4571)	Prec@1 87.500 (88.328)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.4849 (0.4589)	Prec@1 89.844 (88.254)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.4280 (0.4627)	Prec@1 87.500 (88.125)	
Total train loss: 0.4628

 * Prec@1 67.980 Prec@5 89.370 Loss 1.2354
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4055 (0.4342)	Prec@1 90.625 (88.922)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.4668 (0.4328)	Prec@1 87.500 (89.253)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.3521 (0.4391)	Prec@1 92.969 (89.062)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4663 (0.4431)	Prec@1 90.625 (88.935)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.5371 (0.4416)	Prec@1 85.938 (89.016)	
Total train loss: 0.4417

 * Prec@1 68.080 Prec@5 89.230 Loss 1.2402
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.4814 (0.4330)	Prec@1 85.938 (89.503)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4866 (0.4342)	Prec@1 89.062 (89.553)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.4482 (0.4379)	Prec@1 87.500 (89.280)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.4985 (0.4397)	Prec@1 88.281 (89.188)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.4404 (0.4380)	Prec@1 88.281 (89.223)	
Total train loss: 0.4381

 * Prec@1 67.920 Prec@5 89.370 Loss 1.2383
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.5010 (0.4336)	Prec@1 87.500 (89.293)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4893 (0.4342)	Prec@1 85.938 (89.228)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.4824 (0.4351)	Prec@1 89.062 (89.243)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.4363 (0.4349)	Prec@1 91.406 (89.163)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.4551 (0.4344)	Prec@1 87.500 (89.207)	
Total train loss: 0.4343

 * Prec@1 67.920 Prec@5 89.360 Loss 1.2461
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4812 (0.4366)	Prec@1 86.719 (89.163)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4868 (0.4349)	Prec@1 84.375 (89.178)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.5142 (0.4380)	Prec@1 85.938 (89.032)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3250 (0.4354)	Prec@1 92.188 (89.145)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.3450 (0.4360)	Prec@1 90.625 (89.191)	
Total train loss: 0.4359

 * Prec@1 67.910 Prec@5 89.330 Loss 1.2461
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.4075 (0.4363)	Prec@1 89.062 (89.363)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3792 (0.4337)	Prec@1 89.062 (89.313)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.4143 (0.4338)	Prec@1 93.750 (89.330)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.4302 (0.4355)	Prec@1 92.188 (89.270)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.4026 (0.4340)	Prec@1 89.062 (89.355)	
Total train loss: 0.4340

 * Prec@1 67.680 Prec@5 89.310 Loss 1.2461
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.4587 (0.4294)	Prec@1 85.938 (89.563)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.5034 (0.4347)	Prec@1 83.594 (89.458)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.4600 (0.4343)	Prec@1 88.281 (89.413)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.4172 (0.4348)	Prec@1 92.969 (89.315)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.3682 (0.4344)	Prec@1 89.844 (89.277)	
Total train loss: 0.4344

 * Prec@1 67.660 Prec@5 89.350 Loss 1.2432
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.3943 (0.4379)	Prec@1 90.625 (89.443)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.5664 (0.4359)	Prec@1 85.156 (89.418)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.3918 (0.4313)	Prec@1 90.625 (89.557)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.3853 (0.4291)	Prec@1 90.625 (89.646)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.4490 (0.4285)	Prec@1 85.156 (89.613)	
Total train loss: 0.4287

 * Prec@1 67.650 Prec@5 89.310 Loss 1.2471
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.5259 (0.4309)	Prec@1 84.375 (89.453)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.4070 (0.4302)	Prec@1 92.188 (89.423)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.4597 (0.4264)	Prec@1 89.844 (89.633)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.3943 (0.4293)	Prec@1 92.188 (89.473)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.2573 (0.4297)	Prec@1 97.656 (89.395)	
Total train loss: 0.4299

 * Prec@1 67.760 Prec@5 89.100 Loss 1.2471
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.4346 (0.4213)	Prec@1 90.625 (89.754)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.5151 (0.4244)	Prec@1 88.281 (89.653)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.3235 (0.4271)	Prec@1 92.969 (89.580)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.2988 (0.4289)	Prec@1 94.531 (89.563)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.4692 (0.4290)	Prec@1 86.719 (89.497)	
Total train loss: 0.4293

 * Prec@1 67.770 Prec@5 89.350 Loss 1.2480
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.3960 (0.4246)	Prec@1 88.281 (89.673)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.4460 (0.4257)	Prec@1 89.844 (89.643)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.4392 (0.4273)	Prec@1 85.938 (89.673)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.4211 (0.4272)	Prec@1 90.625 (89.653)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.3950 (0.4302)	Prec@1 92.188 (89.539)	
Total train loss: 0.4301

 * Prec@1 67.670 Prec@5 89.250 Loss 1.2480
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.5254 (0.4317)	Prec@1 87.500 (89.493)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.4275 (0.4336)	Prec@1 90.625 (89.418)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.4150 (0.4320)	Prec@1 92.969 (89.473)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.4541 (0.4315)	Prec@1 89.062 (89.441)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.4304 (0.4294)	Prec@1 90.625 (89.513)	
Total train loss: 0.4298

 * Prec@1 67.910 Prec@5 89.250 Loss 1.2461
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.4080 (0.4385)	Prec@1 91.406 (89.183)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.4407 (0.4390)	Prec@1 88.281 (89.193)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.4304 (0.4362)	Prec@1 87.500 (89.223)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.3967 (0.4303)	Prec@1 89.844 (89.456)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.3914 (0.4303)	Prec@1 90.625 (89.435)	
Total train loss: 0.4301

 * Prec@1 67.900 Prec@5 89.210 Loss 1.2451
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.3481 (0.4176)	Prec@1 94.531 (90.294)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.3894 (0.4269)	Prec@1 88.281 (89.834)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.5259 (0.4237)	Prec@1 85.156 (89.777)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.3254 (0.4258)	Prec@1 92.969 (89.686)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.3853 (0.4262)	Prec@1 95.312 (89.663)	
Total train loss: 0.4262

 * Prec@1 67.710 Prec@5 89.270 Loss 1.2520
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.5083 (0.4251)	Prec@1 85.156 (89.724)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.4028 (0.4247)	Prec@1 93.750 (89.694)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.5044 (0.4291)	Prec@1 89.844 (89.640)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.3184 (0.4303)	Prec@1 92.969 (89.613)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.4292 (0.4292)	Prec@1 88.281 (89.609)	
Total train loss: 0.4293

 * Prec@1 67.510 Prec@5 89.240 Loss 1.2520
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.4580 (0.4444)	Prec@1 87.500 (89.153)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.4434 (0.4296)	Prec@1 90.625 (89.528)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.4734 (0.4280)	Prec@1 87.500 (89.577)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.4204 (0.4250)	Prec@1 91.406 (89.714)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.4478 (0.4262)	Prec@1 90.625 (89.720)	
Total train loss: 0.4262

 * Prec@1 67.560 Prec@5 89.180 Loss 1.2588
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.3730 (0.4253)	Prec@1 90.625 (89.573)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.3953 (0.4278)	Prec@1 92.188 (89.538)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.5845 (0.4297)	Prec@1 85.156 (89.507)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.4333 (0.4311)	Prec@1 88.281 (89.431)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.4646 (0.4314)	Prec@1 87.500 (89.423)	
Total train loss: 0.4314

 * Prec@1 67.840 Prec@5 89.270 Loss 1.2500
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.6089 (0.4383)	Prec@1 83.594 (89.042)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.3840 (0.4283)	Prec@1 93.750 (89.593)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.3477 (0.4284)	Prec@1 94.531 (89.583)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.5137 (0.4296)	Prec@1 86.719 (89.521)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.5493 (0.4296)	Prec@1 85.938 (89.575)	
Total train loss: 0.4297

 * Prec@1 67.670 Prec@5 89.120 Loss 1.2549
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.5044 (0.4254)	Prec@1 85.156 (89.834)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.4436 (0.4266)	Prec@1 90.625 (89.668)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.4089 (0.4241)	Prec@1 93.750 (89.847)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.5728 (0.4253)	Prec@1 85.156 (89.734)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.4480 (0.4272)	Prec@1 89.062 (89.637)	
Total train loss: 0.4271

 * Prec@1 67.760 Prec@5 89.320 Loss 1.2441
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.5063 (0.4200)	Prec@1 85.938 (90.445)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.3298 (0.4195)	Prec@1 93.750 (90.219)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.4570 (0.4221)	Prec@1 88.281 (90.111)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.4128 (0.4214)	Prec@1 86.719 (89.984)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.4819 (0.4237)	Prec@1 88.281 (89.904)	
Total train loss: 0.4239

 * Prec@1 67.470 Prec@5 89.180 Loss 1.2559
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.4661 (0.4345)	Prec@1 86.719 (89.193)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.5024 (0.4213)	Prec@1 87.500 (89.809)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.4817 (0.4234)	Prec@1 90.625 (89.740)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.4783 (0.4247)	Prec@1 88.281 (89.731)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.4238 (0.4242)	Prec@1 90.625 (89.728)	
Total train loss: 0.4244

 * Prec@1 67.690 Prec@5 89.270 Loss 1.2559
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.4836 (0.4246)	Prec@1 84.375 (89.754)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.3806 (0.4233)	Prec@1 92.969 (89.869)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.3975 (0.4251)	Prec@1 91.406 (89.780)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.6128 (0.4256)	Prec@1 85.156 (89.754)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.4016 (0.4253)	Prec@1 92.188 (89.836)	
Total train loss: 0.4255

 * Prec@1 67.390 Prec@5 89.240 Loss 1.2588
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.3745 (0.4360)	Prec@1 92.188 (89.403)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.4307 (0.4283)	Prec@1 90.625 (89.694)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.5103 (0.4260)	Prec@1 85.156 (89.720)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.5205 (0.4245)	Prec@1 86.719 (89.714)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.4922 (0.4249)	Prec@1 87.500 (89.750)	
Total train loss: 0.4251

 * Prec@1 67.730 Prec@5 89.230 Loss 1.2529
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.3997 (0.4217)	Prec@1 91.406 (89.583)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.4250 (0.4159)	Prec@1 87.500 (89.864)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.3406 (0.4214)	Prec@1 92.969 (89.757)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.4456 (0.4190)	Prec@1 90.625 (89.869)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.5098 (0.4248)	Prec@1 82.812 (89.673)	
Total train loss: 0.4249

 * Prec@1 67.740 Prec@5 89.200 Loss 1.2539
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.4856 (0.4278)	Prec@1 86.719 (89.433)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.5796 (0.4272)	Prec@1 79.688 (89.533)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.4077 (0.4259)	Prec@1 92.188 (89.663)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.4424 (0.4231)	Prec@1 90.625 (89.774)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.3921 (0.4225)	Prec@1 90.625 (89.832)	
Total train loss: 0.4227

 * Prec@1 67.780 Prec@5 89.280 Loss 1.2510
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4092 (0.4202)	Prec@1 92.188 (89.924)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4150 (0.4168)	Prec@1 89.062 (90.214)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.4346 (0.4201)	Prec@1 89.844 (90.047)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.4338 (0.4209)	Prec@1 89.844 (89.949)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.3650 (0.4239)	Prec@1 92.969 (89.914)	
Total train loss: 0.4238

 * Prec@1 67.710 Prec@5 89.160 Loss 1.2559
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4248 (0.4241)	Prec@1 89.844 (89.864)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.3557 (0.4173)	Prec@1 90.625 (90.139)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.5625 (0.4180)	Prec@1 85.156 (90.108)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.3938 (0.4239)	Prec@1 91.406 (89.896)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.4004 (0.4217)	Prec@1 89.844 (89.894)	
Total train loss: 0.4218

 * Prec@1 67.640 Prec@5 89.100 Loss 1.2510
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.3660 (0.4233)	Prec@1 89.062 (89.493)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.3042 (0.4200)	Prec@1 94.531 (89.583)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.3735 (0.4201)	Prec@1 91.406 (89.734)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3979 (0.4228)	Prec@1 91.406 (89.621)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.4214 (0.4223)	Prec@1 88.281 (89.653)	
Total train loss: 0.4225

 * Prec@1 67.660 Prec@5 89.250 Loss 1.2510
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.5337 (0.4200)	Prec@1 82.812 (90.154)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.4626 (0.4196)	Prec@1 86.719 (90.029)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.3687 (0.4192)	Prec@1 91.406 (90.051)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.3340 (0.4213)	Prec@1 90.625 (89.961)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.4358 (0.4197)	Prec@1 88.281 (89.990)	
Total train loss: 0.4199

 * Prec@1 67.690 Prec@5 89.190 Loss 1.2520
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.3931 (0.4183)	Prec@1 89.844 (90.084)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.4680 (0.4199)	Prec@1 88.281 (90.044)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.4146 (0.4245)	Prec@1 89.062 (89.827)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.3496 (0.4213)	Prec@1 91.406 (89.919)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.4612 (0.4206)	Prec@1 87.500 (89.936)	
Total train loss: 0.4209

 * Prec@1 67.600 Prec@5 89.230 Loss 1.2549
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.4019 (0.4237)	Prec@1 89.844 (89.914)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.4316 (0.4245)	Prec@1 89.844 (89.909)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.3774 (0.4189)	Prec@1 89.844 (89.991)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.3687 (0.4180)	Prec@1 89.062 (90.087)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.3789 (0.4208)	Prec@1 89.844 (89.886)	
Total train loss: 0.4206

 * Prec@1 67.520 Prec@5 89.040 Loss 1.2578
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.4558 (0.4210)	Prec@1 88.281 (90.064)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.4583 (0.4235)	Prec@1 90.625 (89.849)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.4395 (0.4204)	Prec@1 90.625 (89.984)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.4165 (0.4200)	Prec@1 87.500 (89.921)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.4583 (0.4202)	Prec@1 91.406 (89.926)	
Total train loss: 0.4202

 * Prec@1 67.670 Prec@5 89.250 Loss 1.2539
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.3823 (0.4065)	Prec@1 89.844 (90.054)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.4062 (0.4104)	Prec@1 90.625 (90.054)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.4060 (0.4125)	Prec@1 86.719 (90.091)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.3762 (0.4149)	Prec@1 90.625 (90.044)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4470 (0.4170)	Prec@1 86.719 (90.026)	
Total train loss: 0.4173

 * Prec@1 67.600 Prec@5 89.140 Loss 1.2549
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.3916 (0.4144)	Prec@1 87.500 (90.224)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.3784 (0.4178)	Prec@1 89.844 (89.964)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.3772 (0.4147)	Prec@1 92.969 (90.087)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.3242 (0.4171)	Prec@1 95.312 (89.966)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.4968 (0.4196)	Prec@1 86.719 (89.906)	
Total train loss: 0.4196

 * Prec@1 67.490 Prec@5 89.180 Loss 1.2578
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.3120 (0.4098)	Prec@1 92.188 (90.064)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.4236 (0.4131)	Prec@1 89.062 (90.109)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.4419 (0.4135)	Prec@1 90.625 (90.164)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.4609 (0.4148)	Prec@1 88.281 (90.044)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.3062 (0.4168)	Prec@1 96.094 (90.050)	
Total train loss: 0.4171

 * Prec@1 67.690 Prec@5 89.100 Loss 1.2607
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.4141 (0.4111)	Prec@1 89.844 (90.375)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4543 (0.4142)	Prec@1 91.406 (90.144)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.4038 (0.4157)	Prec@1 91.406 (90.011)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.3652 (0.4174)	Prec@1 91.406 (90.049)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.4297 (0.4164)	Prec@1 87.500 (90.020)	
Total train loss: 0.4166

 * Prec@1 67.440 Prec@5 89.070 Loss 1.2559
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.3716 (0.4031)	Prec@1 94.531 (90.455)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.4690 (0.4100)	Prec@1 89.062 (90.254)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.4958 (0.4120)	Prec@1 86.719 (90.164)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3809 (0.4158)	Prec@1 89.062 (90.072)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.3381 (0.4156)	Prec@1 92.188 (90.094)	
Total train loss: 0.4159

 * Prec@1 67.420 Prec@5 89.050 Loss 1.2598
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.4131 (0.4083)	Prec@1 90.625 (90.315)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.5503 (0.4148)	Prec@1 80.469 (90.144)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.4519 (0.4154)	Prec@1 90.625 (90.178)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.4641 (0.4178)	Prec@1 89.844 (90.049)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.3848 (0.4176)	Prec@1 92.188 (90.112)	
Total train loss: 0.4175

 * Prec@1 68.000 Prec@5 89.190 Loss 1.2539
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.3699 (0.4200)	Prec@1 92.188 (89.964)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.4707 (0.4194)	Prec@1 88.281 (90.174)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.3530 (0.4168)	Prec@1 90.625 (90.148)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.3464 (0.4157)	Prec@1 93.750 (90.192)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.4043 (0.4157)	Prec@1 91.406 (90.198)	
Total train loss: 0.4159

 * Prec@1 67.700 Prec@5 89.170 Loss 1.2578
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.3884 (0.4195)	Prec@1 93.750 (90.034)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.4370 (0.4162)	Prec@1 88.281 (90.039)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.5269 (0.4142)	Prec@1 88.281 (90.097)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.3472 (0.4154)	Prec@1 90.625 (89.991)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4216 (0.4152)	Prec@1 92.188 (90.038)	
Total train loss: 0.4154

 * Prec@1 67.810 Prec@5 89.200 Loss 1.2559
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.3816 (0.4092)	Prec@1 91.406 (90.074)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.5098 (0.4092)	Prec@1 89.844 (90.405)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.4380 (0.4134)	Prec@1 89.062 (90.294)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.3943 (0.4167)	Prec@1 92.188 (90.162)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.3433 (0.4172)	Prec@1 93.750 (90.160)	
Total train loss: 0.4173

 * Prec@1 67.650 Prec@5 89.140 Loss 1.2529
Best acc: 68.530
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 52.200 Prec@5 79.060 Loss 2.0078
Pre-trained Prec@1 with 11 layers frozen: 52.19999694824219 	 Loss: 2.0078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.8242 (0.7819)	Prec@1 74.219 (77.113)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5723 (0.7529)	Prec@1 85.938 (78.110)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.7344 (0.7290)	Prec@1 77.344 (78.736)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.7603 (0.7183)	Prec@1 82.812 (79.129)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.7124 (0.7097)	Prec@1 78.906 (79.441)	
Total train loss: 0.7099

 * Prec@1 68.460 Prec@5 89.940 Loss 1.2070
Best acc: 68.460
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.5591 (0.6329)	Prec@1 85.156 (81.681)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.7100 (0.6308)	Prec@1 77.344 (81.916)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6494 (0.6333)	Prec@1 82.812 (81.838)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6475 (0.6343)	Prec@1 79.688 (81.843)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.4077 (0.6330)	Prec@1 90.625 (81.883)	
Total train loss: 0.6331

 * Prec@1 68.130 Prec@5 89.810 Loss 1.2119
Best acc: 68.460
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.6836 (0.5939)	Prec@1 78.906 (83.223)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5630 (0.5911)	Prec@1 84.375 (83.148)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5757 (0.5994)	Prec@1 85.938 (82.873)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5093 (0.6016)	Prec@1 84.375 (82.767)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.6504 (0.6035)	Prec@1 80.469 (82.730)	
Total train loss: 0.6036

 * Prec@1 68.630 Prec@5 89.620 Loss 1.2090
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4839 (0.5612)	Prec@1 88.281 (84.495)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6089 (0.5747)	Prec@1 85.156 (83.879)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.6636 (0.5774)	Prec@1 83.594 (83.767)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6284 (0.5769)	Prec@1 80.469 (83.849)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.5859 (0.5787)	Prec@1 82.031 (83.710)	
Total train loss: 0.5790

 * Prec@1 68.160 Prec@5 89.740 Loss 1.2217
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4734 (0.5566)	Prec@1 87.500 (84.395)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.6704 (0.5576)	Prec@1 82.812 (84.290)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.5371 (0.5578)	Prec@1 87.500 (84.368)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.6309 (0.5621)	Prec@1 82.031 (84.317)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5269 (0.5621)	Prec@1 84.375 (84.303)	
Total train loss: 0.5622

 * Prec@1 68.010 Prec@5 89.600 Loss 1.2275
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.5537 (0.5454)	Prec@1 87.500 (85.106)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.6230 (0.5447)	Prec@1 79.688 (84.991)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.6284 (0.5431)	Prec@1 78.125 (85.013)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.5088 (0.5447)	Prec@1 85.938 (84.928)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.4749 (0.5479)	Prec@1 85.156 (84.890)	
Total train loss: 0.5482

 * Prec@1 67.880 Prec@5 89.560 Loss 1.2354
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.6157 (0.5239)	Prec@1 83.594 (85.978)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.6055 (0.5211)	Prec@1 85.938 (86.023)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.6943 (0.5243)	Prec@1 78.125 (85.921)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.5220 (0.5273)	Prec@1 85.156 (85.725)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.6289 (0.5289)	Prec@1 82.031 (85.637)	
Total train loss: 0.5289

 * Prec@1 68.090 Prec@5 89.360 Loss 1.2383
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4922 (0.5106)	Prec@1 85.938 (86.218)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.5205 (0.5114)	Prec@1 83.594 (86.188)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.5771 (0.5160)	Prec@1 85.938 (86.074)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.6436 (0.5163)	Prec@1 79.688 (86.085)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4683 (0.5171)	Prec@1 89.062 (86.058)	
Total train loss: 0.5174

 * Prec@1 68.040 Prec@5 89.290 Loss 1.2422
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.5010 (0.4988)	Prec@1 87.500 (86.739)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.4658 (0.4983)	Prec@1 89.844 (86.909)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.4785 (0.5034)	Prec@1 85.938 (86.595)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.6138 (0.5052)	Prec@1 82.031 (86.586)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.4668 (0.5084)	Prec@1 89.844 (86.440)	
Total train loss: 0.5082

 * Prec@1 67.860 Prec@5 89.240 Loss 1.2471
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.5034 (0.4803)	Prec@1 87.500 (87.710)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.5464 (0.4846)	Prec@1 83.594 (87.570)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.6270 (0.4898)	Prec@1 79.688 (87.323)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.5337 (0.4887)	Prec@1 83.594 (87.267)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.5552 (0.4925)	Prec@1 85.938 (87.135)	
Total train loss: 0.4927

 * Prec@1 67.530 Prec@5 88.980 Loss 1.2559
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.5562 (0.4739)	Prec@1 85.938 (87.790)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.4790 (0.4763)	Prec@1 92.969 (87.755)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.5327 (0.4736)	Prec@1 85.938 (87.867)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4807 (0.4726)	Prec@1 88.281 (87.928)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.4443 (0.4693)	Prec@1 85.156 (88.001)	
Total train loss: 0.4694

 * Prec@1 67.480 Prec@5 89.110 Loss 1.2510
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.4707 (0.4762)	Prec@1 89.062 (87.740)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4810 (0.4692)	Prec@1 82.812 (87.921)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3850 (0.4640)	Prec@1 89.844 (88.224)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.4150 (0.4653)	Prec@1 90.625 (88.161)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.5298 (0.4665)	Prec@1 85.938 (88.141)	
Total train loss: 0.4664

 * Prec@1 67.670 Prec@5 89.010 Loss 1.2559
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.4358 (0.4622)	Prec@1 90.625 (88.261)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4026 (0.4601)	Prec@1 89.062 (88.276)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.4072 (0.4614)	Prec@1 89.062 (88.325)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.4431 (0.4627)	Prec@1 89.062 (88.239)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3586 (0.4608)	Prec@1 89.062 (88.317)	
Total train loss: 0.4607

 * Prec@1 67.680 Prec@5 89.040 Loss 1.2529
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4365 (0.4591)	Prec@1 91.406 (88.562)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4414 (0.4626)	Prec@1 89.062 (88.271)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4995 (0.4650)	Prec@1 86.719 (88.184)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.4763 (0.4642)	Prec@1 85.156 (88.214)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.4822 (0.4634)	Prec@1 89.844 (88.263)	
Total train loss: 0.4635

 * Prec@1 67.360 Prec@5 89.080 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.4272 (0.4643)	Prec@1 89.062 (88.221)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.4170 (0.4610)	Prec@1 87.500 (88.517)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.5322 (0.4632)	Prec@1 85.156 (88.425)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.3906 (0.4627)	Prec@1 92.188 (88.341)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.5542 (0.4601)	Prec@1 85.938 (88.427)	
Total train loss: 0.4602

 * Prec@1 67.430 Prec@5 89.240 Loss 1.2510
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.4070 (0.4629)	Prec@1 92.969 (88.712)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.4607 (0.4567)	Prec@1 89.844 (88.757)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.3760 (0.4569)	Prec@1 96.094 (88.749)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.4675 (0.4610)	Prec@1 89.844 (88.569)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.5210 (0.4599)	Prec@1 89.844 (88.534)	
Total train loss: 0.4598

 * Prec@1 67.450 Prec@5 89.070 Loss 1.2588
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.4690 (0.4552)	Prec@1 89.062 (88.802)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.5664 (0.4567)	Prec@1 83.594 (88.597)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.4790 (0.4578)	Prec@1 85.938 (88.478)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.3850 (0.4570)	Prec@1 91.406 (88.472)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.4463 (0.4583)	Prec@1 89.844 (88.395)	
Total train loss: 0.4586

 * Prec@1 67.510 Prec@5 89.090 Loss 1.2578
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.4802 (0.4510)	Prec@1 85.156 (88.982)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.4756 (0.4606)	Prec@1 88.281 (88.522)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.3997 (0.4576)	Prec@1 90.625 (88.608)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.4592 (0.4547)	Prec@1 88.281 (88.619)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.6021 (0.4592)	Prec@1 81.250 (88.516)	
Total train loss: 0.4595

 * Prec@1 67.390 Prec@5 89.080 Loss 1.2646
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.3450 (0.4539)	Prec@1 92.969 (88.592)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.4670 (0.4562)	Prec@1 89.062 (88.517)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.3569 (0.4544)	Prec@1 91.406 (88.572)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.4604 (0.4553)	Prec@1 87.500 (88.487)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.4858 (0.4574)	Prec@1 85.938 (88.343)	
Total train loss: 0.4576

 * Prec@1 67.220 Prec@5 89.060 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.5332 (0.4484)	Prec@1 86.719 (88.732)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.4304 (0.4549)	Prec@1 87.500 (88.487)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.5127 (0.4516)	Prec@1 85.156 (88.682)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.4683 (0.4528)	Prec@1 86.719 (88.659)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.3945 (0.4525)	Prec@1 89.062 (88.600)	
Total train loss: 0.4526

 * Prec@1 67.490 Prec@5 89.050 Loss 1.2627
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.4270 (0.4455)	Prec@1 86.719 (88.982)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.4692 (0.4547)	Prec@1 89.844 (88.832)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.4419 (0.4550)	Prec@1 87.500 (88.829)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.4519 (0.4539)	Prec@1 87.500 (88.810)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.4561 (0.4557)	Prec@1 89.062 (88.726)	
Total train loss: 0.4560

 * Prec@1 67.410 Prec@5 88.890 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.5635 (0.4562)	Prec@1 86.719 (88.672)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.4363 (0.4554)	Prec@1 88.281 (88.672)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.4819 (0.4528)	Prec@1 89.844 (88.625)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.4734 (0.4533)	Prec@1 88.281 (88.632)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.3740 (0.4545)	Prec@1 91.406 (88.598)	
Total train loss: 0.4543

 * Prec@1 67.430 Prec@5 88.900 Loss 1.2598
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.5356 (0.4579)	Prec@1 85.156 (88.522)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.3853 (0.4597)	Prec@1 91.406 (88.467)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.4133 (0.4577)	Prec@1 89.062 (88.635)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.5498 (0.4583)	Prec@1 87.500 (88.552)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.5034 (0.4562)	Prec@1 87.500 (88.634)	
Total train loss: 0.4563

 * Prec@1 67.420 Prec@5 89.040 Loss 1.2627
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.4817 (0.4538)	Prec@1 87.500 (88.452)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.4817 (0.4534)	Prec@1 89.062 (88.617)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.4146 (0.4519)	Prec@1 90.625 (88.742)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.4155 (0.4535)	Prec@1 90.625 (88.684)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.4941 (0.4554)	Prec@1 89.062 (88.666)	
Total train loss: 0.4555

 * Prec@1 67.700 Prec@5 89.070 Loss 1.2666
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.4814 (0.4427)	Prec@1 85.938 (88.892)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.4224 (0.4504)	Prec@1 86.719 (88.672)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.3665 (0.4565)	Prec@1 92.969 (88.572)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.5034 (0.4543)	Prec@1 85.938 (88.599)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.5728 (0.4540)	Prec@1 83.594 (88.648)	
Total train loss: 0.4543

 * Prec@1 67.540 Prec@5 89.090 Loss 1.2627
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.3247 (0.4475)	Prec@1 93.750 (88.722)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.4099 (0.4506)	Prec@1 89.062 (88.722)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.5181 (0.4520)	Prec@1 84.375 (88.672)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.4961 (0.4514)	Prec@1 85.156 (88.652)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.4658 (0.4524)	Prec@1 89.062 (88.702)	
Total train loss: 0.4523

 * Prec@1 67.320 Prec@5 89.030 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.4199 (0.4596)	Prec@1 91.406 (88.482)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.3792 (0.4530)	Prec@1 95.312 (88.917)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.3955 (0.4561)	Prec@1 92.188 (88.689)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.4824 (0.4541)	Prec@1 87.500 (88.780)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.4580 (0.4538)	Prec@1 89.844 (88.736)	
Total train loss: 0.4542

 * Prec@1 67.290 Prec@5 89.050 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.4644 (0.4499)	Prec@1 88.281 (88.972)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.4148 (0.4521)	Prec@1 92.188 (88.812)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.4463 (0.4510)	Prec@1 88.281 (88.845)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.5776 (0.4505)	Prec@1 87.500 (88.822)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.3652 (0.4517)	Prec@1 91.406 (88.846)	
Total train loss: 0.4519

 * Prec@1 67.500 Prec@5 89.020 Loss 1.2646
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.4297 (0.4500)	Prec@1 85.938 (88.922)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.5269 (0.4528)	Prec@1 85.938 (88.777)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.4922 (0.4505)	Prec@1 85.938 (88.805)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.4146 (0.4487)	Prec@1 89.844 (88.910)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.3672 (0.4490)	Prec@1 92.969 (88.904)	
Total train loss: 0.4492

 * Prec@1 67.460 Prec@5 89.060 Loss 1.2607
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.4690 (0.4576)	Prec@1 86.719 (88.532)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.3420 (0.4520)	Prec@1 93.750 (88.892)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.4421 (0.4517)	Prec@1 89.062 (88.909)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.3992 (0.4504)	Prec@1 90.625 (88.902)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.4316 (0.4492)	Prec@1 89.844 (88.966)	
Total train loss: 0.4493

 * Prec@1 67.380 Prec@5 89.070 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.3606 (0.4437)	Prec@1 94.531 (89.193)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.4265 (0.4485)	Prec@1 89.844 (88.957)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.5806 (0.4497)	Prec@1 82.812 (88.949)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.4331 (0.4499)	Prec@1 89.844 (88.915)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.3906 (0.4525)	Prec@1 91.406 (88.824)	
Total train loss: 0.4527

 * Prec@1 67.310 Prec@5 88.920 Loss 1.2637
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.4404 (0.4463)	Prec@1 89.062 (88.882)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.4561 (0.4466)	Prec@1 89.844 (89.007)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.3931 (0.4508)	Prec@1 92.188 (88.839)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.4241 (0.4491)	Prec@1 87.500 (88.942)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.5210 (0.4491)	Prec@1 85.938 (88.880)	
Total train loss: 0.4491

 * Prec@1 67.270 Prec@5 89.080 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.4841 (0.4446)	Prec@1 86.719 (89.032)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.5410 (0.4438)	Prec@1 83.594 (88.932)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.6045 (0.4477)	Prec@1 80.469 (88.795)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.5625 (0.4496)	Prec@1 85.156 (88.729)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.5332 (0.4494)	Prec@1 89.062 (88.790)	
Total train loss: 0.4495

 * Prec@1 67.090 Prec@5 89.080 Loss 1.2734
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.4604 (0.4505)	Prec@1 85.156 (89.123)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.4756 (0.4492)	Prec@1 86.719 (88.857)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.4280 (0.4527)	Prec@1 89.844 (88.729)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.3755 (0.4488)	Prec@1 90.625 (88.872)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.3923 (0.4516)	Prec@1 89.062 (88.720)	
Total train loss: 0.4516

 * Prec@1 67.390 Prec@5 89.050 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4170 (0.4450)	Prec@1 89.844 (88.732)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4209 (0.4428)	Prec@1 89.062 (89.022)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.4941 (0.4476)	Prec@1 89.062 (88.886)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.4412 (0.4488)	Prec@1 91.406 (88.785)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.4885 (0.4474)	Prec@1 85.938 (88.852)	
Total train loss: 0.4477

 * Prec@1 67.620 Prec@5 88.810 Loss 1.2656
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4468 (0.4405)	Prec@1 86.719 (89.113)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.4712 (0.4430)	Prec@1 89.844 (89.108)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.4866 (0.4470)	Prec@1 82.812 (88.942)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.4517 (0.4482)	Prec@1 91.406 (88.880)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.3274 (0.4470)	Prec@1 94.531 (88.926)	
Total train loss: 0.4471

 * Prec@1 67.360 Prec@5 89.030 Loss 1.2676
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.4712 (0.4504)	Prec@1 89.844 (88.782)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.4045 (0.4461)	Prec@1 88.281 (88.987)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.5449 (0.4453)	Prec@1 83.594 (89.016)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.3835 (0.4491)	Prec@1 92.969 (88.962)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.4097 (0.4508)	Prec@1 91.406 (88.852)	
Total train loss: 0.4510

 * Prec@1 67.330 Prec@5 88.970 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.3726 (0.4337)	Prec@1 92.969 (89.764)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.4617 (0.4460)	Prec@1 87.500 (88.982)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.4365 (0.4455)	Prec@1 94.531 (88.909)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.3750 (0.4449)	Prec@1 89.844 (88.977)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.4446 (0.4467)	Prec@1 89.062 (88.892)	
Total train loss: 0.4468

 * Prec@1 67.540 Prec@5 88.800 Loss 1.2725
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.4316 (0.4538)	Prec@1 90.625 (88.602)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.3352 (0.4536)	Prec@1 92.969 (88.632)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.3806 (0.4486)	Prec@1 91.406 (88.849)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.4897 (0.4510)	Prec@1 86.719 (88.802)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.4119 (0.4497)	Prec@1 88.281 (88.844)	
Total train loss: 0.4500

 * Prec@1 67.300 Prec@5 89.010 Loss 1.2695
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.4634 (0.4446)	Prec@1 86.719 (88.962)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.4309 (0.4467)	Prec@1 88.281 (89.037)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.5488 (0.4457)	Prec@1 86.719 (88.956)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.4983 (0.4464)	Prec@1 86.719 (88.935)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.4075 (0.4468)	Prec@1 91.406 (88.918)	
Total train loss: 0.4466

 * Prec@1 67.340 Prec@5 88.970 Loss 1.2656
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.3650 (0.4536)	Prec@1 89.062 (89.163)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.4377 (0.4456)	Prec@1 90.625 (89.248)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.4324 (0.4460)	Prec@1 93.750 (89.136)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.3689 (0.4438)	Prec@1 90.625 (89.190)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.5356 (0.4446)	Prec@1 85.156 (89.117)	
Total train loss: 0.4447

 * Prec@1 67.280 Prec@5 88.940 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.5342 (0.4384)	Prec@1 86.719 (89.363)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.3118 (0.4428)	Prec@1 92.969 (89.088)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.3970 (0.4452)	Prec@1 92.188 (89.059)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.4639 (0.4457)	Prec@1 89.844 (88.917)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4729 (0.4457)	Prec@1 85.156 (88.910)	
Total train loss: 0.4461

 * Prec@1 67.660 Prec@5 88.940 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.4827 (0.4446)	Prec@1 89.062 (88.802)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.3003 (0.4494)	Prec@1 96.094 (88.862)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.4062 (0.4459)	Prec@1 92.188 (88.969)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.4380 (0.4446)	Prec@1 89.062 (88.970)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.6372 (0.4450)	Prec@1 82.812 (88.968)	
Total train loss: 0.4449

 * Prec@1 67.310 Prec@5 88.830 Loss 1.2744
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.4502 (0.4513)	Prec@1 89.062 (89.133)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.4875 (0.4465)	Prec@1 90.625 (89.148)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.4570 (0.4484)	Prec@1 84.375 (88.899)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.5967 (0.4461)	Prec@1 86.719 (88.927)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.3606 (0.4448)	Prec@1 92.188 (88.956)	
Total train loss: 0.4449

 * Prec@1 67.230 Prec@5 88.870 Loss 1.2666
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.4663 (0.4531)	Prec@1 89.062 (88.692)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4092 (0.4530)	Prec@1 91.406 (88.897)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.5151 (0.4500)	Prec@1 88.281 (88.972)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.4087 (0.4485)	Prec@1 89.062 (88.980)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.3943 (0.4466)	Prec@1 93.750 (89.085)	
Total train loss: 0.4465

 * Prec@1 67.270 Prec@5 88.860 Loss 1.2705
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.4971 (0.4455)	Prec@1 85.938 (88.872)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.3430 (0.4441)	Prec@1 93.750 (88.972)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.5859 (0.4429)	Prec@1 78.125 (88.976)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.3977 (0.4426)	Prec@1 90.625 (88.970)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.5195 (0.4422)	Prec@1 88.281 (89.042)	
Total train loss: 0.4426

 * Prec@1 67.330 Prec@5 88.860 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.3896 (0.4541)	Prec@1 89.844 (88.492)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.3979 (0.4497)	Prec@1 92.188 (88.652)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.4182 (0.4488)	Prec@1 89.062 (88.805)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.3577 (0.4470)	Prec@1 90.625 (88.917)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.5342 (0.4441)	Prec@1 85.938 (89.004)	
Total train loss: 0.4446

 * Prec@1 67.250 Prec@5 88.900 Loss 1.2695
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.4575 (0.4453)	Prec@1 91.406 (89.353)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.3916 (0.4403)	Prec@1 92.188 (89.573)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.5278 (0.4412)	Prec@1 90.625 (89.456)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.4438 (0.4427)	Prec@1 89.062 (89.310)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.3740 (0.4434)	Prec@1 89.844 (89.269)	
Total train loss: 0.4435

 * Prec@1 67.250 Prec@5 88.840 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.4136 (0.4371)	Prec@1 88.281 (89.443)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.4092 (0.4395)	Prec@1 89.062 (89.338)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.4893 (0.4430)	Prec@1 87.500 (89.203)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.5522 (0.4421)	Prec@1 85.156 (89.108)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4216 (0.4409)	Prec@1 87.500 (89.175)	
Total train loss: 0.4409

 * Prec@1 67.540 Prec@5 88.980 Loss 1.2646
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.3005 (0.4477)	Prec@1 94.531 (88.862)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.4414 (0.4509)	Prec@1 89.062 (88.827)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.4143 (0.4468)	Prec@1 89.844 (88.946)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.3931 (0.4445)	Prec@1 91.406 (89.073)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.4175 (0.4430)	Prec@1 89.062 (89.155)	
Total train loss: 0.4433

 * Prec@1 67.490 Prec@5 89.000 Loss 1.2686
Best acc: 68.630
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 39.440 Prec@5 67.140 Loss 2.8184
Pre-trained Prec@1 with 13 layers frozen: 39.439998626708984 	 Loss: 2.818359375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.9087 (0.9112)	Prec@1 71.875 (73.908)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.6362 (0.8613)	Prec@1 84.375 (75.055)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.6792 (0.8305)	Prec@1 77.344 (75.808)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.7173 (0.8116)	Prec@1 76.562 (76.417)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.8062 (0.7991)	Prec@1 75.781 (76.707)	
Total train loss: 0.7993

 * Prec@1 67.520 Prec@5 89.810 Loss 1.2363
Best acc: 67.520
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.6851 (0.6865)	Prec@1 79.688 (80.008)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.6318 (0.6946)	Prec@1 84.375 (79.808)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6152 (0.6881)	Prec@1 82.031 (80.025)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6689 (0.6918)	Prec@1 79.688 (79.870)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.7568 (0.6947)	Prec@1 80.469 (79.850)	
Total train loss: 0.6946

 * Prec@1 67.670 Prec@5 89.950 Loss 1.2227
Best acc: 67.670
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.7266 (0.6409)	Prec@1 76.562 (82.051)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5181 (0.6599)	Prec@1 88.281 (81.420)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.6343 (0.6574)	Prec@1 83.594 (81.390)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.6489 (0.6564)	Prec@1 81.250 (81.345)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.8252 (0.6573)	Prec@1 78.906 (81.178)	
Total train loss: 0.6572

 * Prec@1 67.840 Prec@5 89.650 Loss 1.2354
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7212 (0.6192)	Prec@1 78.906 (82.432)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8037 (0.6328)	Prec@1 77.344 (82.081)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.7603 (0.6324)	Prec@1 78.906 (82.075)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5391 (0.6307)	Prec@1 82.812 (82.076)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6250 (0.6318)	Prec@1 81.250 (81.997)	
Total train loss: 0.6321

 * Prec@1 67.730 Prec@5 89.620 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.6084 (0.6035)	Prec@1 82.812 (82.923)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.5405 (0.6046)	Prec@1 85.156 (82.597)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.6948 (0.6057)	Prec@1 78.125 (82.662)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5239 (0.6084)	Prec@1 87.500 (82.707)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.6553 (0.6115)	Prec@1 80.469 (82.614)	
Total train loss: 0.6114

 * Prec@1 66.970 Prec@5 89.690 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.5947 (0.5916)	Prec@1 80.469 (83.474)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.7979 (0.5932)	Prec@1 75.000 (83.554)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5220 (0.5897)	Prec@1 84.375 (83.664)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.6753 (0.5913)	Prec@1 80.469 (83.596)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.5137 (0.5944)	Prec@1 87.500 (83.433)	
Total train loss: 0.5946

 * Prec@1 67.620 Prec@5 89.350 Loss 1.2471
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4795 (0.5726)	Prec@1 85.156 (84.525)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.5688 (0.5696)	Prec@1 89.062 (84.505)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.5942 (0.5667)	Prec@1 81.250 (84.575)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4714 (0.5701)	Prec@1 87.500 (84.357)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.7144 (0.5752)	Prec@1 81.250 (84.165)	
Total train loss: 0.5755

 * Prec@1 67.290 Prec@5 89.350 Loss 1.2451
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.6382 (0.5675)	Prec@1 82.812 (84.295)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.5610 (0.5644)	Prec@1 83.594 (84.630)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.6196 (0.5630)	Prec@1 82.031 (84.462)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.6440 (0.5642)	Prec@1 78.125 (84.435)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.6470 (0.5661)	Prec@1 77.344 (84.331)	
Total train loss: 0.5662

 * Prec@1 67.000 Prec@5 89.360 Loss 1.2588
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.5479 (0.5266)	Prec@1 85.938 (86.068)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.6177 (0.5360)	Prec@1 81.250 (85.457)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.4968 (0.5423)	Prec@1 87.500 (85.220)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.6079 (0.5464)	Prec@1 80.469 (85.051)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.4678 (0.5513)	Prec@1 88.281 (84.914)	
Total train loss: 0.5513

 * Prec@1 67.140 Prec@5 89.370 Loss 1.2598
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.5151 (0.5290)	Prec@1 83.594 (85.657)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.5322 (0.5289)	Prec@1 85.156 (85.727)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.5254 (0.5296)	Prec@1 85.156 (85.901)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.4084 (0.5363)	Prec@1 88.281 (85.592)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3745 (0.5340)	Prec@1 92.969 (85.701)	
Total train loss: 0.5341

 * Prec@1 67.350 Prec@5 89.160 Loss 1.2607
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.4265 (0.5032)	Prec@1 87.500 (86.689)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.4878 (0.5080)	Prec@1 86.719 (86.594)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.6143 (0.5085)	Prec@1 83.594 (86.558)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4807 (0.5090)	Prec@1 89.062 (86.589)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.4895 (0.5076)	Prec@1 85.156 (86.683)	
Total train loss: 0.5075

 * Prec@1 67.270 Prec@5 89.120 Loss 1.2568
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.5117 (0.5025)	Prec@1 86.719 (87.099)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.5176 (0.4997)	Prec@1 88.281 (87.019)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.5942 (0.5028)	Prec@1 79.688 (86.912)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.5854 (0.5054)	Prec@1 79.688 (86.866)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.4192 (0.5067)	Prec@1 90.625 (86.813)	
Total train loss: 0.5071

 * Prec@1 67.010 Prec@5 89.170 Loss 1.2666
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.5044 (0.5053)	Prec@1 88.281 (86.949)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4651 (0.5022)	Prec@1 91.406 (87.089)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.4009 (0.5026)	Prec@1 90.625 (86.932)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.4297 (0.5042)	Prec@1 89.844 (86.882)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.5254 (0.5053)	Prec@1 86.719 (86.809)	
Total train loss: 0.5052

 * Prec@1 67.380 Prec@5 89.090 Loss 1.2598
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.5024 (0.5001)	Prec@1 86.719 (87.169)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4849 (0.5013)	Prec@1 87.500 (87.014)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4963 (0.4995)	Prec@1 87.500 (87.056)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.5703 (0.5014)	Prec@1 85.156 (86.984)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.5732 (0.5019)	Prec@1 81.250 (86.987)	
Total train loss: 0.5021

 * Prec@1 66.790 Prec@5 89.080 Loss 1.2715
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.4666 (0.5135)	Prec@1 87.500 (86.258)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.4604 (0.5065)	Prec@1 88.281 (86.548)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.4873 (0.5027)	Prec@1 85.156 (86.732)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.5195 (0.5030)	Prec@1 85.156 (86.774)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.5029 (0.5036)	Prec@1 89.844 (86.769)	
Total train loss: 0.5037

 * Prec@1 66.950 Prec@5 89.120 Loss 1.2646
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.5142 (0.4974)	Prec@1 87.500 (87.230)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.4153 (0.5004)	Prec@1 92.969 (86.944)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.5005 (0.4987)	Prec@1 82.812 (86.972)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.4795 (0.5011)	Prec@1 87.500 (86.917)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.5479 (0.5015)	Prec@1 88.281 (86.911)	
Total train loss: 0.5017

 * Prec@1 67.030 Prec@5 89.110 Loss 1.2646
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.3918 (0.4918)	Prec@1 91.406 (87.320)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.4868 (0.4963)	Prec@1 85.938 (87.169)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.5835 (0.4995)	Prec@1 85.938 (87.233)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.5166 (0.5002)	Prec@1 82.812 (87.149)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.4148 (0.5003)	Prec@1 89.062 (87.137)	
Total train loss: 0.5004

 * Prec@1 67.090 Prec@5 89.130 Loss 1.2627
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.5400 (0.4955)	Prec@1 86.719 (87.300)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.5264 (0.4982)	Prec@1 85.938 (87.109)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.4231 (0.4981)	Prec@1 90.625 (87.089)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.5356 (0.4988)	Prec@1 86.719 (87.074)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.5420 (0.5017)	Prec@1 85.938 (86.985)	
Total train loss: 0.5019

 * Prec@1 66.960 Prec@5 88.920 Loss 1.2705
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.5615 (0.5019)	Prec@1 81.250 (86.869)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.4958 (0.5041)	Prec@1 89.844 (86.799)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.5464 (0.5027)	Prec@1 84.375 (86.949)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.4941 (0.5037)	Prec@1 89.062 (86.904)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.6035 (0.5017)	Prec@1 81.250 (86.943)	
Total train loss: 0.5019

 * Prec@1 67.000 Prec@5 89.050 Loss 1.2715
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.6396 (0.5112)	Prec@1 82.031 (86.358)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.4070 (0.5042)	Prec@1 92.188 (86.719)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.4768 (0.5026)	Prec@1 89.062 (86.909)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.4822 (0.5023)	Prec@1 86.719 (86.987)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.4856 (0.5009)	Prec@1 86.719 (86.967)	
Total train loss: 0.5010

 * Prec@1 67.380 Prec@5 89.200 Loss 1.2617
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.5508 (0.4942)	Prec@1 86.719 (87.089)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.4468 (0.4949)	Prec@1 89.062 (87.139)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.5420 (0.4946)	Prec@1 85.938 (87.223)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.5132 (0.4947)	Prec@1 83.594 (87.192)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.5439 (0.4986)	Prec@1 83.594 (87.081)	
Total train loss: 0.4989

 * Prec@1 66.830 Prec@5 89.050 Loss 1.2734
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.4568 (0.4995)	Prec@1 89.844 (87.029)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.5190 (0.4976)	Prec@1 86.719 (87.159)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.4497 (0.4928)	Prec@1 90.625 (87.243)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.5894 (0.4948)	Prec@1 88.281 (87.212)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.4480 (0.4994)	Prec@1 88.281 (87.089)	
Total train loss: 0.4994

 * Prec@1 67.140 Prec@5 89.020 Loss 1.2715
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.5049 (0.5036)	Prec@1 84.375 (86.679)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.4575 (0.5073)	Prec@1 90.625 (86.719)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.5635 (0.4998)	Prec@1 82.812 (87.026)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.5078 (0.4995)	Prec@1 91.406 (86.987)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.4966 (0.4986)	Prec@1 85.156 (87.003)	
Total train loss: 0.4986

 * Prec@1 67.070 Prec@5 89.050 Loss 1.2695
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.4893 (0.4848)	Prec@1 88.281 (87.520)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.5757 (0.4955)	Prec@1 82.812 (87.174)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.4583 (0.4970)	Prec@1 88.281 (87.083)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.5596 (0.4995)	Prec@1 82.031 (87.077)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.4683 (0.4994)	Prec@1 89.062 (87.125)	
Total train loss: 0.4997

 * Prec@1 67.010 Prec@5 89.080 Loss 1.2695
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.6138 (0.5046)	Prec@1 82.812 (87.019)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.4719 (0.5022)	Prec@1 85.938 (87.089)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.4832 (0.4992)	Prec@1 87.500 (87.106)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.4534 (0.4970)	Prec@1 89.844 (87.039)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.4163 (0.4972)	Prec@1 90.625 (87.155)	
Total train loss: 0.4975

 * Prec@1 67.010 Prec@5 89.060 Loss 1.2744
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.4585 (0.4886)	Prec@1 91.406 (87.690)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.4124 (0.4924)	Prec@1 92.188 (87.395)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.5059 (0.4968)	Prec@1 85.156 (87.203)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.5366 (0.4966)	Prec@1 85.156 (87.079)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.5386 (0.4958)	Prec@1 85.156 (87.139)	
Total train loss: 0.4961

 * Prec@1 67.070 Prec@5 88.970 Loss 1.2715
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.4253 (0.4782)	Prec@1 86.719 (87.620)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.4819 (0.4898)	Prec@1 88.281 (87.345)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.4634 (0.4917)	Prec@1 87.500 (87.263)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.4644 (0.4942)	Prec@1 88.281 (87.222)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.4727 (0.4953)	Prec@1 88.281 (87.266)	
Total train loss: 0.4953

 * Prec@1 67.100 Prec@5 89.070 Loss 1.2715
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.5049 (0.4967)	Prec@1 86.719 (87.440)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.3926 (0.4922)	Prec@1 89.062 (87.455)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.5962 (0.4926)	Prec@1 81.250 (87.333)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.5938 (0.4938)	Prec@1 83.594 (87.325)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.4387 (0.4932)	Prec@1 91.406 (87.286)	
Total train loss: 0.4931

 * Prec@1 66.940 Prec@5 88.980 Loss 1.2676
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.6260 (0.5041)	Prec@1 83.594 (86.769)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.4453 (0.5020)	Prec@1 86.719 (87.079)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.5981 (0.4974)	Prec@1 85.156 (87.216)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.4465 (0.4949)	Prec@1 89.062 (87.297)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.5649 (0.4959)	Prec@1 87.500 (87.179)	
Total train loss: 0.4959

 * Prec@1 67.170 Prec@5 88.910 Loss 1.2686
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.4355 (0.4851)	Prec@1 88.281 (87.380)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.5693 (0.4858)	Prec@1 86.719 (87.515)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.4392 (0.4856)	Prec@1 89.062 (87.593)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.4844 (0.4897)	Prec@1 89.062 (87.530)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.5073 (0.4906)	Prec@1 86.719 (87.516)	
Total train loss: 0.4908

 * Prec@1 67.110 Prec@5 89.030 Loss 1.2725
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.6406 (0.4975)	Prec@1 80.469 (87.250)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.4771 (0.4963)	Prec@1 86.719 (87.210)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.4661 (0.4925)	Prec@1 85.938 (87.306)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.4377 (0.4921)	Prec@1 89.062 (87.280)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.6001 (0.4929)	Prec@1 84.375 (87.270)	
Total train loss: 0.4930

 * Prec@1 67.150 Prec@5 89.220 Loss 1.2686
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.4724 (0.4984)	Prec@1 85.938 (86.879)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.5581 (0.4948)	Prec@1 82.812 (87.039)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.4092 (0.4913)	Prec@1 90.625 (87.230)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.5850 (0.4916)	Prec@1 83.594 (87.200)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.4529 (0.4953)	Prec@1 85.938 (87.085)	
Total train loss: 0.4952

 * Prec@1 66.830 Prec@5 89.040 Loss 1.2793
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.3955 (0.4963)	Prec@1 93.750 (87.220)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.5454 (0.4947)	Prec@1 85.938 (87.285)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.4580 (0.4950)	Prec@1 87.500 (87.323)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.4238 (0.4950)	Prec@1 92.188 (87.320)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.4404 (0.4917)	Prec@1 86.719 (87.486)	
Total train loss: 0.4917

 * Prec@1 67.020 Prec@5 89.120 Loss 1.2734
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.5679 (0.5006)	Prec@1 84.375 (87.069)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.4783 (0.4958)	Prec@1 89.844 (87.240)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.4727 (0.4945)	Prec@1 88.281 (87.273)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.4956 (0.4947)	Prec@1 87.500 (87.322)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.5469 (0.4925)	Prec@1 89.062 (87.368)	
Total train loss: 0.4925

 * Prec@1 66.950 Prec@5 89.030 Loss 1.2725
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.4690 (0.5001)	Prec@1 89.062 (86.819)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.4607 (0.4953)	Prec@1 89.844 (87.215)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.4854 (0.4935)	Prec@1 88.281 (87.266)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.6226 (0.4908)	Prec@1 82.031 (87.382)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.4458 (0.4892)	Prec@1 87.500 (87.488)	
Total train loss: 0.4891

 * Prec@1 67.240 Prec@5 89.110 Loss 1.2734
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.4678 (0.4827)	Prec@1 86.719 (87.560)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.6104 (0.4900)	Prec@1 84.375 (87.375)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.4441 (0.4921)	Prec@1 89.062 (87.273)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.5342 (0.4960)	Prec@1 85.156 (87.227)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.5996 (0.4942)	Prec@1 86.719 (87.292)	
Total train loss: 0.4944

 * Prec@1 66.940 Prec@5 88.930 Loss 1.2754
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.5947 (0.4968)	Prec@1 82.812 (87.280)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.4575 (0.4952)	Prec@1 91.406 (87.415)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.4976 (0.4950)	Prec@1 89.062 (87.360)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.4785 (0.4903)	Prec@1 87.500 (87.533)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.4546 (0.4905)	Prec@1 88.281 (87.476)	
Total train loss: 0.4907

 * Prec@1 66.870 Prec@5 88.950 Loss 1.2793
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.5493 (0.5003)	Prec@1 85.938 (87.009)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.6626 (0.4974)	Prec@1 80.469 (87.079)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.4307 (0.4932)	Prec@1 90.625 (87.310)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.5757 (0.4937)	Prec@1 83.594 (87.220)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.5557 (0.4925)	Prec@1 83.594 (87.290)	
Total train loss: 0.4926

 * Prec@1 66.950 Prec@5 88.990 Loss 1.2773
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.4534 (0.4962)	Prec@1 86.719 (87.580)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.5757 (0.4933)	Prec@1 83.594 (87.500)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.4983 (0.4912)	Prec@1 88.281 (87.510)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.4419 (0.4870)	Prec@1 86.719 (87.610)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.5840 (0.4874)	Prec@1 82.031 (87.516)	
Total train loss: 0.4874

 * Prec@1 66.740 Prec@5 89.110 Loss 1.2832
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.4133 (0.4874)	Prec@1 92.188 (87.570)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.4211 (0.4836)	Prec@1 87.500 (87.745)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.4900 (0.4849)	Prec@1 87.500 (87.667)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.4944 (0.4840)	Prec@1 85.938 (87.575)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.5981 (0.4879)	Prec@1 85.156 (87.462)	
Total train loss: 0.4881

 * Prec@1 67.210 Prec@5 89.050 Loss 1.2695
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.4705 (0.5022)	Prec@1 86.719 (86.939)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.5498 (0.4986)	Prec@1 86.719 (87.450)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.4258 (0.4939)	Prec@1 90.625 (87.483)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.5122 (0.4894)	Prec@1 86.719 (87.573)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.4749 (0.4896)	Prec@1 88.281 (87.608)	
Total train loss: 0.4898

 * Prec@1 67.090 Prec@5 89.140 Loss 1.2686
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.5054 (0.4930)	Prec@1 89.844 (87.740)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.4988 (0.4903)	Prec@1 87.500 (87.545)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.5234 (0.4900)	Prec@1 89.844 (87.447)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.5020 (0.4878)	Prec@1 89.844 (87.560)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.4841 (0.4871)	Prec@1 88.281 (87.630)	
Total train loss: 0.4874

 * Prec@1 67.140 Prec@5 88.920 Loss 1.2695
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.4395 (0.4836)	Prec@1 89.844 (87.560)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.5171 (0.4902)	Prec@1 85.156 (87.475)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.4949 (0.4905)	Prec@1 84.375 (87.453)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.4351 (0.4851)	Prec@1 88.281 (87.728)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.6123 (0.4854)	Prec@1 82.812 (87.734)	
Total train loss: 0.4855

 * Prec@1 66.850 Prec@5 89.110 Loss 1.2686
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.5093 (0.4848)	Prec@1 88.281 (87.480)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.5728 (0.4889)	Prec@1 84.375 (87.340)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.3818 (0.4882)	Prec@1 90.625 (87.443)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.3376 (0.4884)	Prec@1 91.406 (87.515)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.5005 (0.4895)	Prec@1 86.719 (87.476)	
Total train loss: 0.4899

 * Prec@1 66.800 Prec@5 88.940 Loss 1.2812
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.3958 (0.4881)	Prec@1 88.281 (87.851)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.4128 (0.4886)	Prec@1 89.844 (87.625)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.4741 (0.4828)	Prec@1 89.844 (87.861)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.5566 (0.4844)	Prec@1 87.500 (87.798)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.4587 (0.4840)	Prec@1 87.500 (87.746)	
Total train loss: 0.4841

 * Prec@1 66.830 Prec@5 89.070 Loss 1.2803
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.5146 (0.4869)	Prec@1 86.719 (87.280)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.4373 (0.4836)	Prec@1 89.062 (87.510)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.4822 (0.4861)	Prec@1 87.500 (87.376)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.5298 (0.4853)	Prec@1 87.500 (87.477)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.5156 (0.4853)	Prec@1 82.812 (87.520)	
Total train loss: 0.4852

 * Prec@1 67.060 Prec@5 88.870 Loss 1.2812
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.4353 (0.4910)	Prec@1 92.188 (87.730)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.3318 (0.4939)	Prec@1 92.969 (87.370)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.5132 (0.4889)	Prec@1 89.062 (87.647)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.4849 (0.4871)	Prec@1 87.500 (87.545)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.4211 (0.4871)	Prec@1 91.406 (87.516)	
Total train loss: 0.4870

 * Prec@1 67.050 Prec@5 88.970 Loss 1.2822
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.5303 (0.4790)	Prec@1 86.719 (87.891)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.4639 (0.4759)	Prec@1 88.281 (87.901)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.5303 (0.4791)	Prec@1 85.156 (87.794)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.4133 (0.4847)	Prec@1 88.281 (87.545)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.3760 (0.4852)	Prec@1 91.406 (87.598)	
Total train loss: 0.4856

 * Prec@1 67.180 Prec@5 88.930 Loss 1.2783
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.5112 (0.4771)	Prec@1 88.281 (87.911)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.5278 (0.4816)	Prec@1 87.500 (87.881)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.4502 (0.4782)	Prec@1 89.062 (87.941)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.4143 (0.4804)	Prec@1 90.625 (87.836)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.4966 (0.4823)	Prec@1 87.500 (87.712)	
Total train loss: 0.4823

 * Prec@1 66.850 Prec@5 88.950 Loss 1.2764
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.5518 (0.4929)	Prec@1 88.281 (87.400)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.3699 (0.4859)	Prec@1 87.500 (87.575)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.5029 (0.4874)	Prec@1 88.281 (87.583)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.3604 (0.4852)	Prec@1 92.969 (87.563)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.5024 (0.4840)	Prec@1 85.938 (87.634)	
Total train loss: 0.4839

 * Prec@1 66.990 Prec@5 89.000 Loss 1.2793
Best acc: 67.840
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 34.890 Prec@5 63.170 Loss 3.1543
Pre-trained Prec@1 with 15 layers frozen: 34.88999938964844 	 Loss: 3.154296875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 1.2178 (1.4186)	Prec@1 66.406 (61.008)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.0605 (1.2883)	Prec@1 67.969 (64.037)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.9521 (1.2309)	Prec@1 74.219 (65.258)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.0850 (1.1930)	Prec@1 70.312 (66.078)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.0000 (1.1686)	Prec@1 71.875 (66.673)	
Total train loss: 1.1685

 * Prec@1 63.740 Prec@5 87.600 Loss 1.3799
Best acc: 63.740
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.8833 (0.9848)	Prec@1 70.312 (71.414)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.0049 (0.9692)	Prec@1 71.875 (71.780)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.0312 (0.9646)	Prec@1 68.750 (71.898)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.8701 (0.9583)	Prec@1 72.656 (72.196)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.7544 (0.9566)	Prec@1 77.344 (72.316)	
Total train loss: 0.9565

 * Prec@1 64.390 Prec@5 88.280 Loss 1.3398
Best acc: 64.390
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.8296 (0.8850)	Prec@1 70.312 (74.659)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0000 (0.8890)	Prec@1 69.531 (74.239)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.0234 (0.8835)	Prec@1 67.969 (74.319)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.1592 (0.8920)	Prec@1 64.844 (73.943)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.0156 (0.8933)	Prec@1 71.875 (73.946)	
Total train loss: 0.8932

 * Prec@1 65.130 Prec@5 88.540 Loss 1.3262
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7124 (0.8290)	Prec@1 78.906 (75.972)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.7754 (0.8488)	Prec@1 78.906 (75.366)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.8765 (0.8477)	Prec@1 73.438 (75.377)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6772 (0.8483)	Prec@1 82.031 (75.398)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.7432 (0.8493)	Prec@1 80.469 (75.331)	
Total train loss: 0.8493

 * Prec@1 65.290 Prec@5 88.690 Loss 1.3086
Best acc: 65.290
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.8306 (0.7955)	Prec@1 75.000 (77.484)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.0498 (0.8022)	Prec@1 74.219 (77.013)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.8228 (0.8083)	Prec@1 73.438 (76.679)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.7803 (0.8156)	Prec@1 78.125 (76.452)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.0020 (0.8184)	Prec@1 73.438 (76.354)	
Total train loss: 0.8185

 * Prec@1 65.300 Prec@5 88.520 Loss 1.3164
Best acc: 65.300
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.7852 (0.7796)	Prec@1 78.906 (77.214)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.8179 (0.7951)	Prec@1 77.344 (76.943)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.8174 (0.7962)	Prec@1 78.125 (76.980)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.7769 (0.7922)	Prec@1 75.000 (77.209)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.7397 (0.7945)	Prec@1 75.781 (77.065)	
Total train loss: 0.7945

 * Prec@1 65.130 Prec@5 88.580 Loss 1.3105
Best acc: 65.300
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.9653 (0.7518)	Prec@1 73.438 (78.395)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.7344 (0.7585)	Prec@1 78.906 (78.295)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.8999 (0.7655)	Prec@1 72.656 (78.011)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.7427 (0.7659)	Prec@1 79.688 (77.955)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.9570 (0.7727)	Prec@1 70.312 (77.746)	
Total train loss: 0.7728

 * Prec@1 65.710 Prec@5 88.650 Loss 1.3135
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.6562 (0.7260)	Prec@1 82.812 (79.387)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.7046 (0.7384)	Prec@1 78.906 (78.886)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.7378 (0.7462)	Prec@1 77.344 (78.669)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.6733 (0.7514)	Prec@1 82.031 (78.533)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.5762 (0.7544)	Prec@1 83.594 (78.458)	
Total train loss: 0.7548

 * Prec@1 65.670 Prec@5 88.410 Loss 1.2959
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.7954 (0.7320)	Prec@1 76.562 (79.407)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.8682 (0.7273)	Prec@1 72.656 (79.232)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.7344 (0.7285)	Prec@1 78.906 (79.320)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.6929 (0.7316)	Prec@1 78.125 (79.157)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.8389 (0.7342)	Prec@1 78.125 (78.970)	
Total train loss: 0.7341

 * Prec@1 65.270 Prec@5 88.330 Loss 1.3203
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.7622 (0.7178)	Prec@1 79.688 (79.657)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.6802 (0.7093)	Prec@1 81.250 (79.778)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.7734 (0.7145)	Prec@1 78.906 (79.671)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.7168 (0.7157)	Prec@1 82.812 (79.677)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.9048 (0.7196)	Prec@1 71.875 (79.627)	
Total train loss: 0.7194

 * Prec@1 65.370 Prec@5 88.520 Loss 1.3213
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.6885 (0.6791)	Prec@1 82.812 (81.070)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.8228 (0.6855)	Prec@1 71.875 (81.005)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.6724 (0.6881)	Prec@1 85.156 (80.823)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.7646 (0.6893)	Prec@1 78.125 (80.792)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.5898 (0.6892)	Prec@1 87.500 (80.749)	
Total train loss: 0.6892

 * Prec@1 65.600 Prec@5 88.500 Loss 1.3242
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.8218 (0.6871)	Prec@1 75.781 (80.990)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.6182 (0.6936)	Prec@1 86.719 (80.839)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.6743 (0.6968)	Prec@1 82.812 (80.596)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.7754 (0.6948)	Prec@1 73.438 (80.596)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.7559 (0.6903)	Prec@1 79.688 (80.663)	
Total train loss: 0.6904

 * Prec@1 65.370 Prec@5 88.510 Loss 1.3174
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.6421 (0.6804)	Prec@1 82.031 (80.990)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.6445 (0.6850)	Prec@1 78.906 (80.849)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.7935 (0.6872)	Prec@1 79.688 (80.873)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.6719 (0.6859)	Prec@1 78.125 (80.847)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.5688 (0.6826)	Prec@1 85.156 (80.925)	
Total train loss: 0.6824

 * Prec@1 65.420 Prec@5 88.560 Loss 1.3184
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.5337 (0.6796)	Prec@1 85.938 (81.400)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.6880 (0.6841)	Prec@1 82.812 (80.869)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.6348 (0.6866)	Prec@1 85.156 (80.853)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.5776 (0.6840)	Prec@1 83.594 (80.975)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.6636 (0.6804)	Prec@1 86.719 (80.966)	
Total train loss: 0.6808

 * Prec@1 65.360 Prec@5 88.510 Loss 1.3262
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.6821 (0.6947)	Prec@1 79.688 (80.539)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.6133 (0.6855)	Prec@1 81.250 (80.859)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.7866 (0.6892)	Prec@1 77.344 (80.756)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.6460 (0.6876)	Prec@1 81.250 (80.739)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.5195 (0.6843)	Prec@1 85.938 (80.863)	
Total train loss: 0.6842

 * Prec@1 65.550 Prec@5 88.450 Loss 1.3242
Best acc: 65.710
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.7119 (0.6665)	Prec@1 80.469 (81.370)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.5718 (0.6767)	Prec@1 85.938 (80.985)	
Epoch: [15][233/391]	LR: 0.0002	Loss 0.7256 (0.6777)	Prec@1 78.125 (81.090)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.5151 (0.6766)	Prec@1 88.281 (81.017)	
Epoch: [15][389/391]	LR: 0.0002	Loss 0.8345 (0.6782)	Prec@1 77.344 (81.076)	
Total train loss: 0.6788

 * Prec@1 65.820 Prec@5 88.390 Loss 1.3145
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 0.6724 (0.6633)	Prec@1 82.812 (81.310)	
Epoch: [16][155/391]	LR: 0.0002	Loss 0.5723 (0.6771)	Prec@1 82.031 (80.759)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.6865 (0.6751)	Prec@1 80.469 (80.929)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.6509 (0.6787)	Prec@1 82.031 (80.932)	
Epoch: [16][389/391]	LR: 0.0002	Loss 0.6836 (0.6810)	Prec@1 78.906 (80.865)	
Total train loss: 0.6813

 * Prec@1 65.590 Prec@5 88.390 Loss 1.3193
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.6372 (0.6747)	Prec@1 78.125 (81.040)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.6138 (0.6760)	Prec@1 83.594 (81.190)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.7021 (0.6773)	Prec@1 82.031 (81.113)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.7056 (0.6746)	Prec@1 82.812 (81.265)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.5786 (0.6760)	Prec@1 85.938 (81.192)	
Total train loss: 0.6761

 * Prec@1 65.470 Prec@5 88.320 Loss 1.3271
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.6562 (0.6760)	Prec@1 82.031 (81.170)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.6709 (0.6777)	Prec@1 82.812 (81.105)	
Epoch: [18][233/391]	LR: 0.0002	Loss 0.6284 (0.6802)	Prec@1 81.250 (81.036)	
Epoch: [18][311/391]	LR: 0.0002	Loss 0.5835 (0.6799)	Prec@1 85.156 (80.990)	
Epoch: [18][389/391]	LR: 0.0002	Loss 0.5659 (0.6788)	Prec@1 81.250 (81.056)	
Total train loss: 0.6788

 * Prec@1 65.430 Prec@5 88.540 Loss 1.3203
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 0.6890 (0.6709)	Prec@1 81.250 (81.300)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.5435 (0.6683)	Prec@1 86.719 (81.696)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.5884 (0.6745)	Prec@1 87.500 (81.347)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.7729 (0.6747)	Prec@1 78.906 (81.207)	
Epoch: [19][389/391]	LR: 0.0002	Loss 0.7148 (0.6789)	Prec@1 78.906 (81.132)	
Total train loss: 0.6789

 * Prec@1 65.330 Prec@5 88.470 Loss 1.3223
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 0.5098 (0.6727)	Prec@1 87.500 (81.300)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.5103 (0.6750)	Prec@1 85.156 (81.225)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.7100 (0.6775)	Prec@1 79.688 (81.060)	
Epoch: [20][311/391]	LR: 0.0002	Loss 0.6968 (0.6788)	Prec@1 82.031 (80.997)	
Epoch: [20][389/391]	LR: 0.0002	Loss 0.6841 (0.6763)	Prec@1 78.906 (81.138)	
Total train loss: 0.6765

 * Prec@1 65.630 Prec@5 88.500 Loss 1.3184
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.6226 (0.6694)	Prec@1 85.156 (81.470)	
Epoch: [21][155/391]	LR: 0.0002	Loss 0.6475 (0.6678)	Prec@1 83.594 (81.560)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.7407 (0.6714)	Prec@1 81.250 (81.353)	
Epoch: [21][311/391]	LR: 0.0002	Loss 0.6216 (0.6733)	Prec@1 81.250 (81.265)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.5181 (0.6742)	Prec@1 87.500 (81.254)	
Total train loss: 0.6743

 * Prec@1 65.560 Prec@5 88.440 Loss 1.3174
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.5278 (0.6633)	Prec@1 83.594 (81.560)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.5874 (0.6690)	Prec@1 83.594 (81.425)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.6523 (0.6692)	Prec@1 80.469 (81.327)	
Epoch: [22][311/391]	LR: 0.0002	Loss 0.5684 (0.6740)	Prec@1 85.938 (81.205)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.7236 (0.6761)	Prec@1 80.469 (81.222)	
Total train loss: 0.6762

 * Prec@1 65.240 Prec@5 88.340 Loss 1.3291
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.7441 (0.6821)	Prec@1 82.031 (80.749)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.6641 (0.6881)	Prec@1 82.812 (80.654)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.6377 (0.6821)	Prec@1 77.344 (80.712)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.6509 (0.6767)	Prec@1 82.812 (80.877)	
Epoch: [23][389/391]	LR: 0.0002	Loss 0.6069 (0.6735)	Prec@1 85.156 (81.152)	
Total train loss: 0.6739

 * Prec@1 65.140 Prec@5 88.190 Loss 1.3301
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.6118 (0.6827)	Prec@1 87.500 (80.559)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.6489 (0.6814)	Prec@1 80.469 (80.734)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.5977 (0.6743)	Prec@1 85.938 (81.100)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.6963 (0.6739)	Prec@1 78.125 (81.112)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.6523 (0.6734)	Prec@1 82.031 (81.258)	
Total train loss: 0.6733

 * Prec@1 65.570 Prec@5 88.360 Loss 1.3271
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 0.8506 (0.6706)	Prec@1 75.000 (81.540)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.5884 (0.6657)	Prec@1 85.938 (81.581)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.7051 (0.6684)	Prec@1 78.125 (81.540)	
Epoch: [25][311/391]	LR: 0.0002	Loss 0.7749 (0.6675)	Prec@1 75.781 (81.555)	
Epoch: [25][389/391]	LR: 0.0002	Loss 0.7290 (0.6706)	Prec@1 77.344 (81.444)	
Total train loss: 0.6707

 * Prec@1 65.310 Prec@5 88.440 Loss 1.3301
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 0.5864 (0.6574)	Prec@1 84.375 (81.801)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.6479 (0.6569)	Prec@1 80.469 (81.791)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.7236 (0.6656)	Prec@1 81.250 (81.537)	
Epoch: [26][311/391]	LR: 0.0002	Loss 0.5249 (0.6690)	Prec@1 83.594 (81.498)	
Epoch: [26][389/391]	LR: 0.0002	Loss 0.6396 (0.6701)	Prec@1 84.375 (81.396)	
Total train loss: 0.6701

 * Prec@1 65.340 Prec@5 88.400 Loss 1.3242
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.6182 (0.6611)	Prec@1 83.594 (81.370)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.5732 (0.6617)	Prec@1 87.500 (81.485)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.7109 (0.6637)	Prec@1 77.344 (81.427)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.6445 (0.6673)	Prec@1 81.250 (81.445)	
Epoch: [27][389/391]	LR: 0.0002	Loss 0.9717 (0.6677)	Prec@1 74.219 (81.318)	
Total train loss: 0.6676

 * Prec@1 65.460 Prec@5 88.300 Loss 1.3291
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.7158 (0.6826)	Prec@1 80.469 (80.759)	
Epoch: [28][155/391]	LR: 0.0002	Loss 0.6899 (0.6818)	Prec@1 78.906 (80.929)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.7134 (0.6746)	Prec@1 82.031 (81.290)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.6123 (0.6737)	Prec@1 82.812 (81.195)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.7290 (0.6704)	Prec@1 74.219 (81.320)	
Total train loss: 0.6703

 * Prec@1 65.450 Prec@5 88.400 Loss 1.3242
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.5889 (0.6624)	Prec@1 80.469 (81.240)	
Epoch: [29][155/391]	LR: 0.0002	Loss 0.7573 (0.6702)	Prec@1 79.688 (81.220)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.5044 (0.6659)	Prec@1 88.281 (81.454)	
Epoch: [29][311/391]	LR: 0.0002	Loss 0.5537 (0.6656)	Prec@1 85.156 (81.518)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.8008 (0.6669)	Prec@1 77.344 (81.474)	
Total train loss: 0.6669

 * Prec@1 65.420 Prec@5 88.250 Loss 1.3262
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 0.6758 (0.6729)	Prec@1 82.031 (81.160)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.6021 (0.6687)	Prec@1 82.812 (81.465)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.6753 (0.6680)	Prec@1 82.812 (81.444)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.6162 (0.6673)	Prec@1 85.156 (81.453)	
Epoch: [30][389/391]	LR: 0.0002	Loss 0.7344 (0.6672)	Prec@1 81.250 (81.360)	
Total train loss: 0.6671

 * Prec@1 65.590 Prec@5 88.480 Loss 1.3232
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.5356 (0.6616)	Prec@1 85.156 (81.801)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.6992 (0.6632)	Prec@1 78.906 (81.751)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.6387 (0.6619)	Prec@1 82.031 (81.791)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.7012 (0.6627)	Prec@1 79.688 (81.708)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.7051 (0.6683)	Prec@1 82.812 (81.508)	
Total train loss: 0.6684

 * Prec@1 65.460 Prec@5 88.490 Loss 1.3262
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 0.7520 (0.6611)	Prec@1 78.125 (81.981)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.7114 (0.6574)	Prec@1 78.906 (82.121)	
Epoch: [32][233/391]	LR: 0.0002	Loss 0.6548 (0.6612)	Prec@1 78.125 (81.894)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.5781 (0.6650)	Prec@1 83.594 (81.576)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.6255 (0.6659)	Prec@1 80.469 (81.581)	
Total train loss: 0.6660

 * Prec@1 65.330 Prec@5 88.190 Loss 1.3311
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.7354 (0.6690)	Prec@1 76.562 (81.731)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.7236 (0.6764)	Prec@1 82.812 (81.360)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.5830 (0.6737)	Prec@1 83.594 (81.313)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.7505 (0.6704)	Prec@1 81.250 (81.503)	
Epoch: [33][389/391]	LR: 0.0002	Loss 0.6289 (0.6651)	Prec@1 80.469 (81.593)	
Total train loss: 0.6651

 * Prec@1 65.300 Prec@5 88.380 Loss 1.3223
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 0.5137 (0.6574)	Prec@1 85.938 (81.861)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.6650 (0.6598)	Prec@1 84.375 (81.931)	
Epoch: [34][233/391]	LR: 0.0002	Loss 0.7476 (0.6605)	Prec@1 79.688 (81.761)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.6909 (0.6598)	Prec@1 76.562 (81.761)	
Epoch: [34][389/391]	LR: 0.0002	Loss 0.6460 (0.6622)	Prec@1 82.812 (81.695)	
Total train loss: 0.6624

 * Prec@1 65.290 Prec@5 88.170 Loss 1.3330
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.6372 (0.6796)	Prec@1 80.469 (80.829)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.6401 (0.6766)	Prec@1 85.156 (81.095)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.5215 (0.6688)	Prec@1 87.500 (81.197)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.5840 (0.6689)	Prec@1 85.156 (81.212)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.7324 (0.6657)	Prec@1 80.469 (81.362)	
Total train loss: 0.6660

 * Prec@1 65.480 Prec@5 88.280 Loss 1.3281
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.5464 (0.6622)	Prec@1 82.812 (81.901)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.5098 (0.6674)	Prec@1 86.719 (81.846)	
Epoch: [36][233/391]	LR: 0.0002	Loss 0.6489 (0.6562)	Prec@1 81.250 (82.091)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.7432 (0.6573)	Prec@1 81.250 (81.944)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.6387 (0.6603)	Prec@1 81.250 (81.737)	
Total train loss: 0.6602

 * Prec@1 65.230 Prec@5 88.340 Loss 1.3320
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.7476 (0.6589)	Prec@1 78.906 (81.280)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.6860 (0.6557)	Prec@1 82.031 (81.641)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.5771 (0.6620)	Prec@1 87.500 (81.440)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.5591 (0.6637)	Prec@1 88.281 (81.591)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.7437 (0.6609)	Prec@1 75.781 (81.653)	
Total train loss: 0.6611

 * Prec@1 65.450 Prec@5 88.260 Loss 1.3271
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.6934 (0.6563)	Prec@1 82.812 (81.581)	
Epoch: [38][155/391]	LR: 0.0002	Loss 0.6992 (0.6568)	Prec@1 75.781 (81.586)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.5532 (0.6555)	Prec@1 87.500 (81.701)	
Epoch: [38][311/391]	LR: 0.0002	Loss 0.5913 (0.6588)	Prec@1 82.812 (81.641)	
Epoch: [38][389/391]	LR: 0.0002	Loss 0.7017 (0.6614)	Prec@1 79.688 (81.603)	
Total train loss: 0.6616

 * Prec@1 65.400 Prec@5 88.360 Loss 1.3301
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.6489 (0.6664)	Prec@1 81.250 (81.571)	
Epoch: [39][155/391]	LR: 0.0002	Loss 0.7246 (0.6554)	Prec@1 78.906 (82.061)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.7759 (0.6575)	Prec@1 79.688 (81.908)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.6958 (0.6578)	Prec@1 78.125 (81.848)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.7354 (0.6579)	Prec@1 79.688 (81.865)	
Total train loss: 0.6577

 * Prec@1 65.380 Prec@5 88.380 Loss 1.3262
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 0.6074 (0.6535)	Prec@1 80.469 (81.991)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.6274 (0.6525)	Prec@1 83.594 (82.146)	
Epoch: [40][233/391]	LR: 0.0002	Loss 0.6343 (0.6618)	Prec@1 83.594 (81.771)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.7632 (0.6609)	Prec@1 80.469 (81.716)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.7422 (0.6591)	Prec@1 80.469 (81.723)	
Total train loss: 0.6596

 * Prec@1 65.390 Prec@5 88.270 Loss 1.3301
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.4739 (0.6690)	Prec@1 88.281 (81.360)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.6187 (0.6681)	Prec@1 83.594 (81.475)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.7490 (0.6633)	Prec@1 82.031 (81.671)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.6060 (0.6640)	Prec@1 83.594 (81.611)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.5249 (0.6629)	Prec@1 85.156 (81.565)	
Total train loss: 0.6628

 * Prec@1 65.430 Prec@5 88.370 Loss 1.3232
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.7412 (0.6676)	Prec@1 78.906 (81.340)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.6011 (0.6594)	Prec@1 84.375 (81.886)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.8096 (0.6599)	Prec@1 77.344 (81.737)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.6646 (0.6596)	Prec@1 81.250 (81.683)	
Epoch: [42][389/391]	LR: 0.0002	Loss 0.6494 (0.6612)	Prec@1 81.250 (81.737)	
Total train loss: 0.6611

 * Prec@1 65.330 Prec@5 88.530 Loss 1.3252
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.6631 (0.6554)	Prec@1 83.594 (82.222)	
Epoch: [43][155/391]	LR: 0.0002	Loss 0.5244 (0.6545)	Prec@1 89.062 (82.156)	
Epoch: [43][233/391]	LR: 0.0002	Loss 0.6802 (0.6534)	Prec@1 82.031 (82.138)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.6548 (0.6581)	Prec@1 85.156 (81.936)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.6899 (0.6572)	Prec@1 78.906 (81.923)	
Total train loss: 0.6571

 * Prec@1 65.130 Prec@5 88.260 Loss 1.3359
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.6816 (0.6647)	Prec@1 82.031 (81.721)	
Epoch: [44][155/391]	LR: 0.0002	Loss 0.7227 (0.6585)	Prec@1 81.250 (81.846)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.5015 (0.6595)	Prec@1 84.375 (81.801)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.6797 (0.6548)	Prec@1 80.469 (82.029)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.5312 (0.6570)	Prec@1 87.500 (81.861)	
Total train loss: 0.6572

 * Prec@1 65.410 Prec@5 88.280 Loss 1.3408
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.7500 (0.6580)	Prec@1 78.906 (82.041)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.5356 (0.6480)	Prec@1 84.375 (82.156)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.7212 (0.6502)	Prec@1 78.906 (82.055)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.6733 (0.6519)	Prec@1 82.031 (82.001)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.7939 (0.6554)	Prec@1 71.875 (81.841)	
Total train loss: 0.6559

 * Prec@1 65.360 Prec@5 87.960 Loss 1.3398
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.8896 (0.6470)	Prec@1 74.219 (81.821)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.5439 (0.6514)	Prec@1 86.719 (82.031)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.5381 (0.6495)	Prec@1 88.281 (82.055)	
Epoch: [46][311/391]	LR: 0.0002	Loss 0.7144 (0.6532)	Prec@1 75.000 (81.931)	
Epoch: [46][389/391]	LR: 0.0002	Loss 0.5630 (0.6572)	Prec@1 82.031 (81.791)	
Total train loss: 0.6572

 * Prec@1 65.290 Prec@5 88.190 Loss 1.3291
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 0.6230 (0.6778)	Prec@1 84.375 (80.909)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.6875 (0.6639)	Prec@1 80.469 (81.706)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.8447 (0.6614)	Prec@1 75.781 (81.771)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.5693 (0.6580)	Prec@1 82.812 (81.873)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.6631 (0.6584)	Prec@1 81.250 (81.811)	
Total train loss: 0.6587

 * Prec@1 65.470 Prec@5 88.270 Loss 1.3359
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 0.5566 (0.6438)	Prec@1 86.719 (82.372)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.7544 (0.6497)	Prec@1 75.781 (82.066)	
Epoch: [48][233/391]	LR: 0.0002	Loss 0.7192 (0.6535)	Prec@1 78.125 (82.001)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.5830 (0.6540)	Prec@1 85.156 (81.984)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.6812 (0.6543)	Prec@1 78.906 (82.023)	
Total train loss: 0.6542

 * Prec@1 65.270 Prec@5 88.220 Loss 1.3340
Best acc: 65.820
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 0.6626 (0.6502)	Prec@1 84.375 (82.472)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.6387 (0.6464)	Prec@1 81.250 (82.682)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.7207 (0.6503)	Prec@1 80.469 (82.452)	
Epoch: [49][311/391]	LR: 0.0002	Loss 0.7046 (0.6552)	Prec@1 81.250 (82.076)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.5591 (0.6554)	Prec@1 82.812 (82.061)	
Total train loss: 0.6557

 * Prec@1 65.410 Prec@5 88.260 Loss 1.3330
Best acc: 65.820
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 27.520 Prec@5 57.080 Loss 3.8301
Pre-trained Prec@1 with 17 layers frozen: 27.51999855041504 	 Loss: 3.830078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 1.4170 (1.7274)	Prec@1 61.719 (54.417)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.3203 (1.6011)	Prec@1 60.938 (56.791)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.3730 (1.5251)	Prec@1 58.594 (58.423)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.3291 (1.4773)	Prec@1 63.281 (59.398)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.2969 (1.4444)	Prec@1 63.281 (60.072)	
Total train loss: 1.4453

 * Prec@1 60.060 Prec@5 86.140 Loss 1.5088
Best acc: 60.060
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 1.3818 (1.2407)	Prec@1 60.938 (64.854)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.1562 (1.2395)	Prec@1 69.531 (65.124)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.3457 (1.2350)	Prec@1 56.250 (65.218)	
Epoch: [1][311/391]	LR: 0.001	Loss 1.1113 (1.2351)	Prec@1 64.844 (65.167)	
Epoch: [1][389/391]	LR: 0.001	Loss 1.1807 (1.2255)	Prec@1 68.750 (65.363)	
Total train loss: 1.2258

 * Prec@1 62.010 Prec@5 86.890 Loss 1.4297
Best acc: 62.010
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 1.2949 (1.1608)	Prec@1 64.062 (67.037)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0479 (1.1622)	Prec@1 67.188 (66.892)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.2305 (1.1617)	Prec@1 67.969 (66.887)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.1670 (1.1603)	Prec@1 67.969 (66.897)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.1816 (1.1570)	Prec@1 67.188 (67.003)	
Total train loss: 1.1566

 * Prec@1 62.390 Prec@5 87.260 Loss 1.4062
Best acc: 62.390
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 1.2686 (1.1422)	Prec@1 67.969 (67.218)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8477 (1.1232)	Prec@1 77.344 (67.934)	
Epoch: [3][233/391]	LR: 0.001	Loss 1.2285 (1.1275)	Prec@1 68.750 (67.755)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.0928 (1.1189)	Prec@1 68.750 (67.939)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.3691 (1.1157)	Prec@1 60.938 (68.069)	
Total train loss: 1.1159

 * Prec@1 62.790 Prec@5 87.510 Loss 1.3838
Best acc: 62.790
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 1.1387 (1.0874)	Prec@1 67.969 (68.790)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.2109 (1.0922)	Prec@1 64.844 (68.850)	
Epoch: [4][233/391]	LR: 0.001	Loss 1.0029 (1.0909)	Prec@1 74.219 (68.944)	
Epoch: [4][311/391]	LR: 0.001	Loss 1.1533 (1.0887)	Prec@1 66.406 (68.955)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.0820 (1.0875)	Prec@1 69.531 (68.840)	
Total train loss: 1.0874

 * Prec@1 63.140 Prec@5 87.530 Loss 1.3711
Best acc: 63.140
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.8999 (1.0671)	Prec@1 75.000 (69.311)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.9814 (1.0600)	Prec@1 72.656 (69.486)	
Epoch: [5][233/391]	LR: 0.001	Loss 1.0918 (1.0630)	Prec@1 68.750 (69.625)	
Epoch: [5][311/391]	LR: 0.001	Loss 1.0547 (1.0592)	Prec@1 71.875 (69.722)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.9556 (1.0599)	Prec@1 74.219 (69.856)	
Total train loss: 1.0597

 * Prec@1 63.230 Prec@5 87.600 Loss 1.3691
Best acc: 63.230
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.9678 (1.0475)	Prec@1 74.219 (70.262)	
Epoch: [6][155/391]	LR: 0.001	Loss 1.0400 (1.0352)	Prec@1 66.406 (70.348)	
Epoch: [6][233/391]	LR: 0.001	Loss 1.1152 (1.0390)	Prec@1 67.969 (70.246)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.9419 (1.0358)	Prec@1 67.969 (70.360)	
Epoch: [6][389/391]	LR: 0.001	Loss 1.0166 (1.0403)	Prec@1 71.875 (70.164)	
Total train loss: 1.0405

 * Prec@1 63.330 Prec@5 87.490 Loss 1.3711
Best acc: 63.330
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.9912 (1.0169)	Prec@1 74.219 (70.823)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.9678 (1.0255)	Prec@1 72.656 (70.448)	
Epoch: [7][233/391]	LR: 0.001	Loss 1.1709 (1.0212)	Prec@1 67.188 (70.396)	
Epoch: [7][311/391]	LR: 0.001	Loss 1.0869 (1.0263)	Prec@1 69.531 (70.513)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.8535 (1.0231)	Prec@1 78.125 (70.565)	
Total train loss: 1.0236

 * Prec@1 63.480 Prec@5 87.610 Loss 1.3584
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 1.0527 (1.0167)	Prec@1 71.875 (70.933)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.9824 (1.0057)	Prec@1 72.656 (71.419)	
Epoch: [8][233/391]	LR: 0.001	Loss 1.0605 (1.0060)	Prec@1 70.312 (71.498)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.9761 (1.0052)	Prec@1 74.219 (71.502)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.9907 (1.0043)	Prec@1 72.656 (71.482)	
Total train loss: 1.0049

 * Prec@1 63.680 Prec@5 87.640 Loss 1.3574
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.9307 (0.9657)	Prec@1 73.438 (72.586)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.9619 (0.9791)	Prec@1 68.750 (72.276)	
Epoch: [9][233/391]	LR: 0.001	Loss 1.0938 (0.9836)	Prec@1 67.188 (72.135)	
Epoch: [9][311/391]	LR: 0.001	Loss 1.1309 (0.9897)	Prec@1 66.406 (71.908)	
Epoch: [9][389/391]	LR: 0.001	Loss 1.0195 (0.9925)	Prec@1 69.531 (71.731)	
Total train loss: 0.9927

 * Prec@1 63.620 Prec@5 87.810 Loss 1.3623
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 1.0586 (0.9824)	Prec@1 69.531 (71.985)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.7764 (0.9818)	Prec@1 75.000 (71.910)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.9380 (0.9798)	Prec@1 67.969 (71.888)	
Epoch: [10][311/391]	LR: 0.0002	Loss 1.0703 (0.9726)	Prec@1 66.406 (72.115)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.7563 (0.9730)	Prec@1 76.562 (72.210)	
Total train loss: 0.9728

 * Prec@1 63.550 Prec@5 87.600 Loss 1.3613
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.9336 (0.9716)	Prec@1 75.000 (72.336)	
Epoch: [11][155/391]	LR: 0.0002	Loss 1.0537 (0.9711)	Prec@1 73.438 (72.286)	
Epoch: [11][233/391]	LR: 0.0002	Loss 1.0889 (0.9677)	Prec@1 69.531 (72.406)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.9219 (0.9654)	Prec@1 74.219 (72.468)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.9741 (0.9684)	Prec@1 73.438 (72.348)	
Total train loss: 0.9684

 * Prec@1 63.560 Prec@5 87.760 Loss 1.3594
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.8594 (0.9673)	Prec@1 74.219 (72.316)	
Epoch: [12][155/391]	LR: 0.0002	Loss 1.0254 (0.9579)	Prec@1 70.312 (72.611)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.8784 (0.9648)	Prec@1 77.344 (72.423)	
Epoch: [12][311/391]	LR: 0.0002	Loss 1.0879 (0.9635)	Prec@1 71.875 (72.433)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.7476 (0.9670)	Prec@1 80.469 (72.390)	
Total train loss: 0.9674

 * Prec@1 63.650 Prec@5 87.760 Loss 1.3594
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.8506 (0.9557)	Prec@1 74.219 (73.007)	
Epoch: [13][155/391]	LR: 0.0002	Loss 1.1592 (0.9640)	Prec@1 71.875 (72.596)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.8794 (0.9665)	Prec@1 75.781 (72.469)	
Epoch: [13][311/391]	LR: 0.0002	Loss 1.0742 (0.9621)	Prec@1 73.438 (72.504)	
Epoch: [13][389/391]	LR: 0.0002	Loss 1.0283 (0.9644)	Prec@1 75.000 (72.434)	
Total train loss: 0.9647

 * Prec@1 63.610 Prec@5 87.530 Loss 1.3643
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.9297 (0.9648)	Prec@1 71.875 (72.326)	
Epoch: [14][155/391]	LR: 0.0002	Loss 1.1416 (0.9648)	Prec@1 67.188 (72.516)	
Epoch: [14][233/391]	LR: 0.0002	Loss 1.1387 (0.9604)	Prec@1 67.188 (72.493)	
Epoch: [14][311/391]	LR: 0.0002	Loss 1.0420 (0.9624)	Prec@1 70.312 (72.599)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.9307 (0.9611)	Prec@1 75.000 (72.570)	
Total train loss: 0.9610

 * Prec@1 63.460 Prec@5 87.650 Loss 1.3613
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 0.9741 (0.9531)	Prec@1 71.875 (72.596)	
Epoch: [15][155/391]	LR: 0.0002	Loss 0.8447 (0.9526)	Prec@1 72.656 (72.781)	
Epoch: [15][233/391]	LR: 0.0002	Loss 1.0195 (0.9559)	Prec@1 67.969 (72.740)	
Epoch: [15][311/391]	LR: 0.0002	Loss 0.9429 (0.9547)	Prec@1 74.219 (72.756)	
Epoch: [15][389/391]	LR: 0.0002	Loss 1.0635 (0.9612)	Prec@1 70.312 (72.560)	
Total train loss: 0.9614

 * Prec@1 63.430 Prec@5 87.730 Loss 1.3643
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 1.0869 (0.9740)	Prec@1 67.188 (71.825)	
Epoch: [16][155/391]	LR: 0.0002	Loss 1.0029 (0.9708)	Prec@1 67.188 (71.875)	
Epoch: [16][233/391]	LR: 0.0002	Loss 0.9404 (0.9625)	Prec@1 67.969 (72.165)	
Epoch: [16][311/391]	LR: 0.0002	Loss 0.9229 (0.9589)	Prec@1 75.781 (72.408)	
Epoch: [16][389/391]	LR: 0.0002	Loss 1.0957 (0.9609)	Prec@1 63.281 (72.376)	
Total train loss: 0.9610

 * Prec@1 63.500 Prec@5 87.740 Loss 1.3594
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 0.9517 (0.9548)	Prec@1 75.781 (72.646)	
Epoch: [17][155/391]	LR: 0.0002	Loss 0.9385 (0.9667)	Prec@1 71.875 (72.501)	
Epoch: [17][233/391]	LR: 0.0002	Loss 0.9248 (0.9647)	Prec@1 73.438 (72.593)	
Epoch: [17][311/391]	LR: 0.0002	Loss 0.9497 (0.9603)	Prec@1 74.219 (72.639)	
Epoch: [17][389/391]	LR: 0.0002	Loss 0.8975 (0.9613)	Prec@1 71.875 (72.596)	
Total train loss: 0.9614

 * Prec@1 63.600 Prec@5 87.630 Loss 1.3652
Best acc: 63.680
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 0.9062 (0.9514)	Prec@1 75.781 (72.827)	
Epoch: [18][155/391]	LR: 0.0002	Loss 0.8735 (0.9595)	Prec@1 75.000 (72.601)	
Epoch: [18][233/391]	LR: 0.0002	Loss 1.2217 (0.9599)	Prec@1 62.500 (72.660)	
Epoch: [18][311/391]	LR: 0.0002	Loss 1.1865 (0.9606)	Prec@1 65.625 (72.624)	
Epoch: [18][389/391]	LR: 0.0002	Loss 1.0156 (0.9630)	Prec@1 72.656 (72.612)	
Total train loss: 0.9630

 * Prec@1 63.970 Prec@5 87.680 Loss 1.3584
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 1.0537 (0.9575)	Prec@1 68.750 (72.506)	
Epoch: [19][155/391]	LR: 0.0002	Loss 0.8037 (0.9598)	Prec@1 75.000 (72.566)	
Epoch: [19][233/391]	LR: 0.0002	Loss 0.8901 (0.9636)	Prec@1 75.000 (72.416)	
Epoch: [19][311/391]	LR: 0.0002	Loss 0.9717 (0.9603)	Prec@1 76.562 (72.401)	
Epoch: [19][389/391]	LR: 0.0002	Loss 1.1543 (0.9624)	Prec@1 69.531 (72.418)	
Total train loss: 0.9623

 * Prec@1 63.740 Prec@5 87.730 Loss 1.3613
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 1.1777 (0.9622)	Prec@1 60.938 (72.155)	
Epoch: [20][155/391]	LR: 0.0002	Loss 0.9263 (0.9679)	Prec@1 74.219 (72.256)	
Epoch: [20][233/391]	LR: 0.0002	Loss 0.9331 (0.9685)	Prec@1 72.656 (72.216)	
Epoch: [20][311/391]	LR: 0.0002	Loss 1.0371 (0.9624)	Prec@1 72.656 (72.493)	
Epoch: [20][389/391]	LR: 0.0002	Loss 1.0127 (0.9584)	Prec@1 69.531 (72.534)	
Total train loss: 0.9583

 * Prec@1 63.700 Prec@5 87.730 Loss 1.3604
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 0.9209 (0.9522)	Prec@1 70.312 (72.606)	
Epoch: [21][155/391]	LR: 0.0002	Loss 1.0410 (0.9576)	Prec@1 71.094 (72.401)	
Epoch: [21][233/391]	LR: 0.0002	Loss 0.9604 (0.9582)	Prec@1 67.969 (72.506)	
Epoch: [21][311/391]	LR: 0.0002	Loss 1.0830 (0.9577)	Prec@1 67.188 (72.584)	
Epoch: [21][389/391]	LR: 0.0002	Loss 0.9736 (0.9593)	Prec@1 74.219 (72.622)	
Total train loss: 0.9594

 * Prec@1 63.630 Prec@5 87.770 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 0.8647 (0.9459)	Prec@1 77.344 (73.417)	
Epoch: [22][155/391]	LR: 0.0002	Loss 0.8516 (0.9508)	Prec@1 77.344 (73.017)	
Epoch: [22][233/391]	LR: 0.0002	Loss 0.9341 (0.9503)	Prec@1 71.875 (72.863)	
Epoch: [22][311/391]	LR: 0.0002	Loss 1.0156 (0.9555)	Prec@1 67.969 (72.756)	
Epoch: [22][389/391]	LR: 0.0002	Loss 0.8311 (0.9565)	Prec@1 78.906 (72.810)	
Total train loss: 0.9563

 * Prec@1 63.650 Prec@5 87.640 Loss 1.3613
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 0.8315 (0.9447)	Prec@1 75.781 (72.456)	
Epoch: [23][155/391]	LR: 0.0002	Loss 0.9214 (0.9533)	Prec@1 71.094 (72.516)	
Epoch: [23][233/391]	LR: 0.0002	Loss 0.9517 (0.9490)	Prec@1 73.438 (72.640)	
Epoch: [23][311/391]	LR: 0.0002	Loss 0.8823 (0.9546)	Prec@1 71.094 (72.458)	
Epoch: [23][389/391]	LR: 0.0002	Loss 1.2236 (0.9558)	Prec@1 68.750 (72.520)	
Total train loss: 0.9558

 * Prec@1 63.530 Prec@5 87.840 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 0.9966 (0.9447)	Prec@1 72.656 (73.137)	
Epoch: [24][155/391]	LR: 0.0002	Loss 0.8521 (0.9439)	Prec@1 76.562 (73.212)	
Epoch: [24][233/391]	LR: 0.0002	Loss 0.9492 (0.9487)	Prec@1 73.438 (73.064)	
Epoch: [24][311/391]	LR: 0.0002	Loss 0.8486 (0.9531)	Prec@1 75.000 (72.874)	
Epoch: [24][389/391]	LR: 0.0002	Loss 0.8706 (0.9552)	Prec@1 81.250 (72.758)	
Total train loss: 0.9553

 * Prec@1 63.460 Prec@5 87.720 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 1.0117 (0.9557)	Prec@1 71.094 (72.406)	
Epoch: [25][155/391]	LR: 0.0002	Loss 0.8213 (0.9554)	Prec@1 75.000 (72.666)	
Epoch: [25][233/391]	LR: 0.0002	Loss 0.9375 (0.9570)	Prec@1 70.312 (72.513)	
Epoch: [25][311/391]	LR: 0.0002	Loss 1.1084 (0.9573)	Prec@1 65.625 (72.641)	
Epoch: [25][389/391]	LR: 0.0002	Loss 1.1250 (0.9552)	Prec@1 66.406 (72.678)	
Total train loss: 0.9551

 * Prec@1 63.540 Prec@5 87.670 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 1.0283 (0.9368)	Prec@1 67.969 (73.327)	
Epoch: [26][155/391]	LR: 0.0002	Loss 0.9023 (0.9506)	Prec@1 75.000 (73.152)	
Epoch: [26][233/391]	LR: 0.0002	Loss 0.9326 (0.9596)	Prec@1 72.656 (72.763)	
Epoch: [26][311/391]	LR: 0.0002	Loss 1.0137 (0.9573)	Prec@1 71.875 (72.771)	
Epoch: [26][389/391]	LR: 0.0002	Loss 1.1143 (0.9550)	Prec@1 67.969 (72.790)	
Total train loss: 0.9549

 * Prec@1 63.700 Prec@5 87.770 Loss 1.3613
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 0.8896 (0.9639)	Prec@1 75.000 (72.266)	
Epoch: [27][155/391]	LR: 0.0002	Loss 0.8472 (0.9599)	Prec@1 75.000 (72.211)	
Epoch: [27][233/391]	LR: 0.0002	Loss 0.8926 (0.9555)	Prec@1 72.656 (72.566)	
Epoch: [27][311/391]	LR: 0.0002	Loss 0.8960 (0.9558)	Prec@1 71.875 (72.644)	
Epoch: [27][389/391]	LR: 0.0002	Loss 1.0420 (0.9543)	Prec@1 69.531 (72.752)	
Total train loss: 0.9545

 * Prec@1 63.710 Prec@5 87.650 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 0.9556 (0.9495)	Prec@1 75.000 (72.766)	
Epoch: [28][155/391]	LR: 0.0002	Loss 1.0439 (0.9536)	Prec@1 69.531 (72.726)	
Epoch: [28][233/391]	LR: 0.0002	Loss 0.7515 (0.9561)	Prec@1 78.125 (72.673)	
Epoch: [28][311/391]	LR: 0.0002	Loss 0.9551 (0.9574)	Prec@1 71.094 (72.779)	
Epoch: [28][389/391]	LR: 0.0002	Loss 0.9678 (0.9540)	Prec@1 71.094 (72.909)	
Total train loss: 0.9542

 * Prec@1 63.700 Prec@5 87.770 Loss 1.3594
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 0.8677 (0.9376)	Prec@1 71.094 (73.808)	
Epoch: [29][155/391]	LR: 0.0002	Loss 1.0918 (0.9495)	Prec@1 70.312 (73.297)	
Epoch: [29][233/391]	LR: 0.0002	Loss 0.9707 (0.9569)	Prec@1 65.625 (72.813)	
Epoch: [29][311/391]	LR: 0.0002	Loss 1.1406 (0.9575)	Prec@1 68.750 (72.739)	
Epoch: [29][389/391]	LR: 0.0002	Loss 0.9238 (0.9551)	Prec@1 72.656 (72.740)	
Total train loss: 0.9550

 * Prec@1 63.640 Prec@5 87.840 Loss 1.3574
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 1.2158 (0.9653)	Prec@1 65.625 (72.436)	
Epoch: [30][155/391]	LR: 0.0002	Loss 0.9297 (0.9638)	Prec@1 72.656 (72.276)	
Epoch: [30][233/391]	LR: 0.0002	Loss 0.9800 (0.9508)	Prec@1 69.531 (72.780)	
Epoch: [30][311/391]	LR: 0.0002	Loss 0.9180 (0.9522)	Prec@1 72.656 (72.779)	
Epoch: [30][389/391]	LR: 0.0002	Loss 1.0244 (0.9529)	Prec@1 71.875 (72.656)	
Total train loss: 0.9532

 * Prec@1 63.680 Prec@5 87.720 Loss 1.3594
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 0.8457 (0.9447)	Prec@1 72.656 (73.357)	
Epoch: [31][155/391]	LR: 0.0002	Loss 0.8540 (0.9438)	Prec@1 74.219 (73.102)	
Epoch: [31][233/391]	LR: 0.0002	Loss 0.9951 (0.9520)	Prec@1 74.219 (72.790)	
Epoch: [31][311/391]	LR: 0.0002	Loss 0.8735 (0.9495)	Prec@1 75.781 (72.942)	
Epoch: [31][389/391]	LR: 0.0002	Loss 0.9961 (0.9535)	Prec@1 71.875 (72.796)	
Total train loss: 0.9535

 * Prec@1 63.560 Prec@5 87.590 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 1.0078 (0.9411)	Prec@1 70.312 (73.167)	
Epoch: [32][155/391]	LR: 0.0002	Loss 0.9736 (0.9405)	Prec@1 73.438 (73.097)	
Epoch: [32][233/391]	LR: 0.0002	Loss 1.1084 (0.9456)	Prec@1 66.406 (72.776)	
Epoch: [32][311/391]	LR: 0.0002	Loss 0.9038 (0.9492)	Prec@1 73.438 (72.669)	
Epoch: [32][389/391]	LR: 0.0002	Loss 0.9893 (0.9522)	Prec@1 78.906 (72.560)	
Total train loss: 0.9521

 * Prec@1 63.740 Prec@5 87.720 Loss 1.3613
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 0.8818 (0.9415)	Prec@1 74.219 (72.817)	
Epoch: [33][155/391]	LR: 0.0002	Loss 0.9761 (0.9578)	Prec@1 72.656 (72.586)	
Epoch: [33][233/391]	LR: 0.0002	Loss 0.8447 (0.9600)	Prec@1 72.656 (72.439)	
Epoch: [33][311/391]	LR: 0.0002	Loss 0.9829 (0.9562)	Prec@1 67.188 (72.574)	
Epoch: [33][389/391]	LR: 0.0002	Loss 1.0088 (0.9533)	Prec@1 67.969 (72.634)	
Total train loss: 0.9534

 * Prec@1 63.580 Prec@5 87.520 Loss 1.3643
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 1.1826 (0.9467)	Prec@1 63.281 (73.508)	
Epoch: [34][155/391]	LR: 0.0002	Loss 0.9146 (0.9426)	Prec@1 71.094 (73.468)	
Epoch: [34][233/391]	LR: 0.0002	Loss 1.2725 (0.9470)	Prec@1 62.500 (73.431)	
Epoch: [34][311/391]	LR: 0.0002	Loss 0.8296 (0.9466)	Prec@1 72.656 (73.310)	
Epoch: [34][389/391]	LR: 0.0002	Loss 1.0020 (0.9485)	Prec@1 72.656 (73.191)	
Total train loss: 0.9483

 * Prec@1 63.620 Prec@5 87.820 Loss 1.3652
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 0.9287 (0.9294)	Prec@1 77.344 (73.818)	
Epoch: [35][155/391]	LR: 0.0002	Loss 0.9097 (0.9474)	Prec@1 71.875 (73.047)	
Epoch: [35][233/391]	LR: 0.0002	Loss 0.8555 (0.9499)	Prec@1 74.219 (72.960)	
Epoch: [35][311/391]	LR: 0.0002	Loss 0.8853 (0.9501)	Prec@1 75.781 (72.994)	
Epoch: [35][389/391]	LR: 0.0002	Loss 0.8296 (0.9502)	Prec@1 75.000 (72.961)	
Total train loss: 0.9504

 * Prec@1 63.680 Prec@5 87.800 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 0.8271 (0.9425)	Prec@1 76.562 (73.247)	
Epoch: [36][155/391]	LR: 0.0002	Loss 0.8672 (0.9412)	Prec@1 77.344 (73.187)	
Epoch: [36][233/391]	LR: 0.0002	Loss 1.0479 (0.9432)	Prec@1 74.219 (73.187)	
Epoch: [36][311/391]	LR: 0.0002	Loss 0.9429 (0.9514)	Prec@1 73.438 (72.957)	
Epoch: [36][389/391]	LR: 0.0002	Loss 0.9673 (0.9480)	Prec@1 71.094 (73.081)	
Total train loss: 0.9480

 * Prec@1 63.390 Prec@5 87.550 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 0.8647 (0.9594)	Prec@1 77.344 (72.786)	
Epoch: [37][155/391]	LR: 0.0002	Loss 0.9429 (0.9554)	Prec@1 74.219 (72.726)	
Epoch: [37][233/391]	LR: 0.0002	Loss 0.9966 (0.9480)	Prec@1 69.531 (73.007)	
Epoch: [37][311/391]	LR: 0.0002	Loss 0.8428 (0.9492)	Prec@1 78.906 (72.992)	
Epoch: [37][389/391]	LR: 0.0002	Loss 0.8286 (0.9482)	Prec@1 75.000 (73.005)	
Total train loss: 0.9484

 * Prec@1 63.510 Prec@5 87.740 Loss 1.3652
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 0.9453 (0.9502)	Prec@1 75.000 (72.977)	
Epoch: [38][155/391]	LR: 0.0002	Loss 1.0273 (0.9444)	Prec@1 69.531 (73.152)	
Epoch: [38][233/391]	LR: 0.0002	Loss 0.8901 (0.9458)	Prec@1 75.000 (73.180)	
Epoch: [38][311/391]	LR: 0.0002	Loss 1.1162 (0.9490)	Prec@1 67.969 (72.974)	
Epoch: [38][389/391]	LR: 0.0002	Loss 1.1387 (0.9494)	Prec@1 72.656 (72.945)	
Total train loss: 0.9497

 * Prec@1 63.850 Prec@5 87.800 Loss 1.3652
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 0.9102 (0.9591)	Prec@1 73.438 (72.817)	
Epoch: [39][155/391]	LR: 0.0002	Loss 1.0029 (0.9538)	Prec@1 71.094 (72.877)	
Epoch: [39][233/391]	LR: 0.0002	Loss 0.9985 (0.9505)	Prec@1 75.000 (72.873)	
Epoch: [39][311/391]	LR: 0.0002	Loss 0.7231 (0.9441)	Prec@1 79.688 (73.119)	
Epoch: [39][389/391]	LR: 0.0002	Loss 0.8872 (0.9437)	Prec@1 75.781 (73.139)	
Total train loss: 0.9444

 * Prec@1 63.640 Prec@5 87.570 Loss 1.3604
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 1.0166 (0.9501)	Prec@1 66.406 (73.107)	
Epoch: [40][155/391]	LR: 0.0002	Loss 0.8979 (0.9419)	Prec@1 74.219 (73.207)	
Epoch: [40][233/391]	LR: 0.0002	Loss 1.0010 (0.9390)	Prec@1 67.969 (73.170)	
Epoch: [40][311/391]	LR: 0.0002	Loss 0.7515 (0.9385)	Prec@1 76.562 (73.207)	
Epoch: [40][389/391]	LR: 0.0002	Loss 0.9194 (0.9445)	Prec@1 71.094 (73.091)	
Total train loss: 0.9441

 * Prec@1 63.670 Prec@5 87.750 Loss 1.3584
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 0.8057 (0.9598)	Prec@1 73.438 (72.686)	
Epoch: [41][155/391]	LR: 0.0002	Loss 0.8472 (0.9525)	Prec@1 76.562 (72.877)	
Epoch: [41][233/391]	LR: 0.0002	Loss 0.9561 (0.9503)	Prec@1 71.875 (72.913)	
Epoch: [41][311/391]	LR: 0.0002	Loss 0.9385 (0.9486)	Prec@1 72.656 (72.932)	
Epoch: [41][389/391]	LR: 0.0002	Loss 0.9648 (0.9459)	Prec@1 73.438 (73.091)	
Total train loss: 0.9458

 * Prec@1 63.870 Prec@5 87.690 Loss 1.3604
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 0.8555 (0.9285)	Prec@1 75.000 (73.588)	
Epoch: [42][155/391]	LR: 0.0002	Loss 0.9810 (0.9481)	Prec@1 71.875 (73.157)	
Epoch: [42][233/391]	LR: 0.0002	Loss 0.9233 (0.9483)	Prec@1 75.781 (73.197)	
Epoch: [42][311/391]	LR: 0.0002	Loss 0.7896 (0.9510)	Prec@1 77.344 (73.112)	
Epoch: [42][389/391]	LR: 0.0002	Loss 1.0049 (0.9476)	Prec@1 74.219 (73.193)	
Total train loss: 0.9479

 * Prec@1 63.630 Prec@5 87.800 Loss 1.3613
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 0.8213 (0.9427)	Prec@1 78.125 (73.458)	
Epoch: [43][155/391]	LR: 0.0002	Loss 1.1426 (0.9518)	Prec@1 68.750 (72.997)	
Epoch: [43][233/391]	LR: 0.0002	Loss 1.1152 (0.9539)	Prec@1 67.188 (72.753)	
Epoch: [43][311/391]	LR: 0.0002	Loss 0.8682 (0.9499)	Prec@1 75.781 (73.002)	
Epoch: [43][389/391]	LR: 0.0002	Loss 0.9248 (0.9442)	Prec@1 75.781 (73.119)	
Total train loss: 0.9442

 * Prec@1 63.520 Prec@5 87.640 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 0.7974 (0.9295)	Prec@1 75.781 (73.778)	
Epoch: [44][155/391]	LR: 0.0002	Loss 1.0303 (0.9407)	Prec@1 67.969 (73.307)	
Epoch: [44][233/391]	LR: 0.0002	Loss 0.8320 (0.9424)	Prec@1 76.562 (73.291)	
Epoch: [44][311/391]	LR: 0.0002	Loss 0.9775 (0.9425)	Prec@1 74.219 (73.232)	
Epoch: [44][389/391]	LR: 0.0002	Loss 0.8203 (0.9446)	Prec@1 79.688 (73.149)	
Total train loss: 0.9448

 * Prec@1 63.610 Prec@5 87.700 Loss 1.3662
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 0.9282 (0.9335)	Prec@1 73.438 (73.608)	
Epoch: [45][155/391]	LR: 0.0002	Loss 0.9390 (0.9442)	Prec@1 71.875 (73.287)	
Epoch: [45][233/391]	LR: 0.0002	Loss 0.7964 (0.9408)	Prec@1 78.906 (73.448)	
Epoch: [45][311/391]	LR: 0.0002	Loss 0.9297 (0.9402)	Prec@1 70.312 (73.375)	
Epoch: [45][389/391]	LR: 0.0002	Loss 0.8516 (0.9431)	Prec@1 71.094 (73.295)	
Total train loss: 0.9432

 * Prec@1 63.450 Prec@5 87.540 Loss 1.3701
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 0.8779 (0.9350)	Prec@1 75.781 (73.427)	
Epoch: [46][155/391]	LR: 0.0002	Loss 0.9785 (0.9372)	Prec@1 71.094 (73.463)	
Epoch: [46][233/391]	LR: 0.0002	Loss 0.9785 (0.9364)	Prec@1 73.438 (73.481)	
Epoch: [46][311/391]	LR: 0.0002	Loss 1.0273 (0.9367)	Prec@1 71.875 (73.493)	
Epoch: [46][389/391]	LR: 0.0002	Loss 1.0684 (0.9428)	Prec@1 66.406 (73.211)	
Total train loss: 0.9431

 * Prec@1 63.790 Prec@5 87.820 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 1.2451 (0.9330)	Prec@1 62.500 (73.247)	
Epoch: [47][155/391]	LR: 0.0002	Loss 0.9238 (0.9456)	Prec@1 73.438 (72.756)	
Epoch: [47][233/391]	LR: 0.0002	Loss 0.8979 (0.9452)	Prec@1 75.781 (72.977)	
Epoch: [47][311/391]	LR: 0.0002	Loss 0.8945 (0.9433)	Prec@1 77.344 (73.057)	
Epoch: [47][389/391]	LR: 0.0002	Loss 0.9712 (0.9398)	Prec@1 72.656 (73.211)	
Total train loss: 0.9401

 * Prec@1 63.590 Prec@5 87.690 Loss 1.3604
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 1.0537 (0.9532)	Prec@1 68.750 (73.027)	
Epoch: [48][155/391]	LR: 0.0002	Loss 0.9790 (0.9421)	Prec@1 70.312 (73.257)	
Epoch: [48][233/391]	LR: 0.0002	Loss 1.0205 (0.9438)	Prec@1 71.875 (73.264)	
Epoch: [48][311/391]	LR: 0.0002	Loss 0.8931 (0.9449)	Prec@1 75.781 (73.212)	
Epoch: [48][389/391]	LR: 0.0002	Loss 0.9463 (0.9407)	Prec@1 76.562 (73.271)	
Total train loss: 0.9411

 * Prec@1 63.660 Prec@5 87.730 Loss 1.3633
Best acc: 63.970
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 1.0566 (0.9407)	Prec@1 67.188 (73.377)	
Epoch: [49][155/391]	LR: 0.0002	Loss 0.9463 (0.9413)	Prec@1 69.531 (73.292)	
Epoch: [49][233/391]	LR: 0.0002	Loss 0.8828 (0.9388)	Prec@1 73.438 (73.411)	
Epoch: [49][311/391]	LR: 0.0002	Loss 1.0547 (0.9434)	Prec@1 67.188 (73.192)	
Epoch: [49][389/391]	LR: 0.0002	Loss 0.8169 (0.9419)	Prec@1 76.562 (73.195)	
Total train loss: 0.9420

 * Prec@1 63.730 Prec@5 87.610 Loss 1.3623
Best acc: 63.970
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 23.800 Prec@5 53.360 Loss 4.3320
Pre-trained Prec@1 with 19 layers frozen: 23.799999237060547 	 Loss: 4.33203125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 2.1738 (2.0161)	Prec@1 42.969 (47.796)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.9492 (1.9646)	Prec@1 50.000 (48.873)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.9678 (1.9191)	Prec@1 54.688 (49.893)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.8262 (1.8914)	Prec@1 52.344 (50.503)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.6475 (1.8676)	Prec@1 58.594 (50.917)	
Total train loss: 1.8675

 * Prec@1 52.860 Prec@5 80.250 Loss 1.8545
Best acc: 52.860
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 1.7881 (1.7169)	Prec@1 54.688 (54.046)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.6396 (1.7092)	Prec@1 53.125 (54.147)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.6367 (1.7094)	Prec@1 57.031 (54.284)	
Epoch: [1][311/391]	LR: 0.001	Loss 1.6084 (1.7079)	Prec@1 58.594 (54.269)	
Epoch: [1][389/391]	LR: 0.001	Loss 1.7373 (1.6997)	Prec@1 57.812 (54.479)	
Total train loss: 1.7000

 * Prec@1 54.540 Prec@5 81.930 Loss 1.7607
Best acc: 54.540
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 1.8398 (1.6494)	Prec@1 51.562 (55.759)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.6426 (1.6473)	Prec@1 57.031 (55.414)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.6729 (1.6378)	Prec@1 55.469 (55.639)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.5127 (1.6379)	Prec@1 58.594 (55.724)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.8604 (1.6381)	Prec@1 51.562 (55.813)	
Total train loss: 1.6377

 * Prec@1 55.310 Prec@5 82.820 Loss 1.7061
Best acc: 55.310
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 1.5645 (1.5883)	Prec@1 58.594 (56.861)	
Epoch: [3][155/391]	LR: 0.001	Loss 1.4922 (1.5919)	Prec@1 55.469 (56.976)	
Epoch: [3][233/391]	LR: 0.001	Loss 1.5312 (1.5925)	Prec@1 57.031 (57.028)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.4082 (1.5941)	Prec@1 61.719 (56.916)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.7168 (1.5946)	Prec@1 55.469 (56.921)	
Total train loss: 1.5947

 * Prec@1 55.970 Prec@5 83.540 Loss 1.6709
Best acc: 55.970
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 1.7285 (1.5579)	Prec@1 56.250 (57.572)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.6338 (1.5729)	Prec@1 56.250 (57.021)	
Epoch: [4][233/391]	LR: 0.001	Loss 1.6279 (1.5685)	Prec@1 57.812 (57.315)	
Epoch: [4][311/391]	LR: 0.001	Loss 1.4580 (1.5718)	Prec@1 57.031 (57.161)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.5498 (1.5693)	Prec@1 57.812 (57.344)	
Total train loss: 1.5692

 * Prec@1 56.840 Prec@5 83.810 Loss 1.6387
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 1.5947 (1.5549)	Prec@1 60.156 (58.193)	
Epoch: [5][155/391]	LR: 0.001	Loss 1.5957 (1.5474)	Prec@1 60.938 (58.318)	
Epoch: [5][233/391]	LR: 0.001	Loss 1.3047 (1.5563)	Prec@1 71.875 (58.140)	
Epoch: [5][311/391]	LR: 0.001	Loss 1.5078 (1.5519)	Prec@1 65.625 (58.286)	
Epoch: [5][389/391]	LR: 0.001	Loss 1.4697 (1.5484)	Prec@1 62.500 (58.205)	
Total train loss: 1.5485

 * Prec@1 56.780 Prec@5 83.780 Loss 1.6328
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 1.5977 (1.5419)	Prec@1 56.250 (58.383)	
Epoch: [6][155/391]	LR: 0.001	Loss 1.2949 (1.5427)	Prec@1 65.625 (58.278)	
Epoch: [6][233/391]	LR: 0.001	Loss 1.4902 (1.5383)	Prec@1 55.469 (58.500)	
Epoch: [6][311/391]	LR: 0.001	Loss 1.4053 (1.5390)	Prec@1 58.594 (58.333)	
Epoch: [6][389/391]	LR: 0.001	Loss 1.6973 (1.5346)	Prec@1 55.469 (58.371)	
Total train loss: 1.5347

 * Prec@1 57.100 Prec@5 84.270 Loss 1.6191
Best acc: 57.100
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 1.5771 (1.5393)	Prec@1 57.812 (58.464)	
Epoch: [7][155/391]	LR: 0.001	Loss 1.4834 (1.5289)	Prec@1 60.156 (58.689)	
Epoch: [7][233/391]	LR: 0.001	Loss 1.3984 (1.5331)	Prec@1 65.625 (58.514)	
Epoch: [7][311/391]	LR: 0.001	Loss 1.4512 (1.5241)	Prec@1 58.594 (58.674)	
Epoch: [7][389/391]	LR: 0.001	Loss 1.5615 (1.5200)	Prec@1 62.500 (58.866)	
Total train loss: 1.5200

 * Prec@1 57.450 Prec@5 84.390 Loss 1.6123
Best acc: 57.450
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 1.6270 (1.5361)	Prec@1 57.031 (58.484)	
Epoch: [8][155/391]	LR: 0.001	Loss 1.7246 (1.5111)	Prec@1 54.688 (59.215)	
Epoch: [8][233/391]	LR: 0.001	Loss 1.4287 (1.5127)	Prec@1 62.500 (58.771)	
Epoch: [8][311/391]	LR: 0.001	Loss 1.4180 (1.5160)	Prec@1 55.469 (58.729)	
Epoch: [8][389/391]	LR: 0.001	Loss 1.3721 (1.5125)	Prec@1 61.719 (58.788)	
Total train loss: 1.5131

 * Prec@1 57.540 Prec@5 84.580 Loss 1.5957
Best acc: 57.540
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 1.4922 (1.5075)	Prec@1 60.938 (58.854)	
Epoch: [9][155/391]	LR: 0.001	Loss 1.2812 (1.5173)	Prec@1 68.750 (58.624)	
Epoch: [9][233/391]	LR: 0.001	Loss 1.5020 (1.5116)	Prec@1 60.156 (58.807)	
Epoch: [9][311/391]	LR: 0.001	Loss 1.4990 (1.5078)	Prec@1 52.344 (58.914)	
Epoch: [9][389/391]	LR: 0.001	Loss 1.5254 (1.5048)	Prec@1 57.812 (59.065)	
Total train loss: 1.5050

 * Prec@1 57.840 Prec@5 84.700 Loss 1.5840
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 1.3555 (1.4716)	Prec@1 62.500 (60.317)	
Epoch: [10][155/391]	LR: 0.0002	Loss 1.4893 (1.4786)	Prec@1 57.812 (59.981)	
Epoch: [10][233/391]	LR: 0.0002	Loss 1.4717 (1.4922)	Prec@1 60.938 (59.642)	
Epoch: [10][311/391]	LR: 0.0002	Loss 1.5312 (1.4947)	Prec@1 58.594 (59.450)	
Epoch: [10][389/391]	LR: 0.0002	Loss 1.5381 (1.4977)	Prec@1 58.594 (59.439)	
Total train loss: 1.4980

 * Prec@1 57.710 Prec@5 84.610 Loss 1.5918
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 1.3887 (1.5077)	Prec@1 60.156 (59.515)	
Epoch: [11][155/391]	LR: 0.0002	Loss 1.5576 (1.5037)	Prec@1 59.375 (59.385)	
Epoch: [11][233/391]	LR: 0.0002	Loss 1.4629 (1.5048)	Prec@1 65.625 (59.231)	
Epoch: [11][311/391]	LR: 0.0002	Loss 1.4346 (1.4982)	Prec@1 56.250 (59.388)	
Epoch: [11][389/391]	LR: 0.0002	Loss 1.6260 (1.4941)	Prec@1 59.375 (59.507)	
Total train loss: 1.4941

 * Prec@1 57.640 Prec@5 84.610 Loss 1.5908
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 1.4736 (1.4970)	Prec@1 59.375 (58.994)	
Epoch: [12][155/391]	LR: 0.0002	Loss 1.3740 (1.4836)	Prec@1 64.844 (59.585)	
Epoch: [12][233/391]	LR: 0.0002	Loss 1.3740 (1.4852)	Prec@1 64.062 (59.652)	
Epoch: [12][311/391]	LR: 0.0002	Loss 1.4824 (1.4896)	Prec@1 60.156 (59.533)	
Epoch: [12][389/391]	LR: 0.0002	Loss 1.3936 (1.4950)	Prec@1 60.938 (59.309)	
Total train loss: 1.4949

 * Prec@1 57.800 Prec@5 84.800 Loss 1.5879
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 1.5127 (1.5152)	Prec@1 55.469 (58.964)	
Epoch: [13][155/391]	LR: 0.0002	Loss 1.4102 (1.5007)	Prec@1 60.938 (59.390)	
Epoch: [13][233/391]	LR: 0.0002	Loss 1.6025 (1.4997)	Prec@1 59.375 (59.282)	
Epoch: [13][311/391]	LR: 0.0002	Loss 1.5684 (1.4983)	Prec@1 60.156 (59.277)	
Epoch: [13][389/391]	LR: 0.0002	Loss 1.5693 (1.4965)	Prec@1 59.375 (59.307)	
Total train loss: 1.4964

 * Prec@1 57.790 Prec@5 84.740 Loss 1.5830
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 1.4824 (1.4960)	Prec@1 59.375 (58.954)	
Epoch: [14][155/391]	LR: 0.0002	Loss 1.4570 (1.4997)	Prec@1 54.688 (58.904)	
Epoch: [14][233/391]	LR: 0.0002	Loss 1.4346 (1.4947)	Prec@1 61.719 (59.292)	
Epoch: [14][311/391]	LR: 0.0002	Loss 1.3242 (1.4972)	Prec@1 66.406 (59.290)	
Epoch: [14][389/391]	LR: 0.0002	Loss 1.3994 (1.4936)	Prec@1 66.406 (59.421)	
Total train loss: 1.4942

 * Prec@1 57.570 Prec@5 84.870 Loss 1.5918
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0002	Loss 1.4551 (1.4851)	Prec@1 58.594 (59.315)	
Epoch: [15][155/391]	LR: 0.0002	Loss 1.4707 (1.4905)	Prec@1 58.594 (59.405)	
Epoch: [15][233/391]	LR: 0.0002	Loss 1.4160 (1.4921)	Prec@1 61.719 (59.478)	
Epoch: [15][311/391]	LR: 0.0002	Loss 1.5303 (1.4906)	Prec@1 53.906 (59.455)	
Epoch: [15][389/391]	LR: 0.0002	Loss 1.5996 (1.4968)	Prec@1 54.688 (59.347)	
Total train loss: 1.4968

 * Prec@1 57.710 Prec@5 84.710 Loss 1.5898
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0002	Loss 1.4619 (1.5232)	Prec@1 56.250 (58.243)	
Epoch: [16][155/391]	LR: 0.0002	Loss 1.5400 (1.5056)	Prec@1 57.812 (59.014)	
Epoch: [16][233/391]	LR: 0.0002	Loss 1.4805 (1.5026)	Prec@1 64.844 (58.924)	
Epoch: [16][311/391]	LR: 0.0002	Loss 1.6924 (1.5001)	Prec@1 59.375 (59.052)	
Epoch: [16][389/391]	LR: 0.0002	Loss 1.5947 (1.4962)	Prec@1 57.031 (59.321)	
Total train loss: 1.4963

 * Prec@1 57.450 Prec@5 84.560 Loss 1.5859
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0002	Loss 1.5557 (1.4737)	Prec@1 56.250 (60.056)	
Epoch: [17][155/391]	LR: 0.0002	Loss 1.4248 (1.4860)	Prec@1 60.938 (59.420)	
Epoch: [17][233/391]	LR: 0.0002	Loss 1.6016 (1.4944)	Prec@1 55.469 (59.318)	
Epoch: [17][311/391]	LR: 0.0002	Loss 1.4033 (1.4926)	Prec@1 60.938 (59.413)	
Epoch: [17][389/391]	LR: 0.0002	Loss 1.1162 (1.4935)	Prec@1 63.281 (59.319)	
Total train loss: 1.4936

 * Prec@1 57.650 Prec@5 84.910 Loss 1.5830
Best acc: 57.840
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0002	Loss 1.5801 (1.4935)	Prec@1 56.250 (59.746)	
Epoch: [18][155/391]	LR: 0.0002	Loss 1.6641 (1.4912)	Prec@1 57.812 (59.555)	
Epoch: [18][233/391]	LR: 0.0002	Loss 1.6143 (1.4955)	Prec@1 58.594 (59.365)	
Epoch: [18][311/391]	LR: 0.0002	Loss 1.5664 (1.4971)	Prec@1 60.156 (59.388)	
Epoch: [18][389/391]	LR: 0.0002	Loss 1.2559 (1.4957)	Prec@1 64.844 (59.413)	
Total train loss: 1.4956

 * Prec@1 57.980 Prec@5 84.740 Loss 1.5811
Best acc: 57.980
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0002	Loss 1.2451 (1.4938)	Prec@1 64.844 (59.796)	
Epoch: [19][155/391]	LR: 0.0002	Loss 1.2197 (1.4984)	Prec@1 69.531 (59.335)	
Epoch: [19][233/391]	LR: 0.0002	Loss 1.4209 (1.5075)	Prec@1 64.844 (59.098)	
Epoch: [19][311/391]	LR: 0.0002	Loss 1.4609 (1.5007)	Prec@1 62.500 (59.225)	
Epoch: [19][389/391]	LR: 0.0002	Loss 1.7969 (1.4958)	Prec@1 50.000 (59.401)	
Total train loss: 1.4955

 * Prec@1 58.040 Prec@5 84.770 Loss 1.5811
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0002	Loss 1.4961 (1.4795)	Prec@1 56.250 (59.495)	
Epoch: [20][155/391]	LR: 0.0002	Loss 1.3740 (1.4886)	Prec@1 62.500 (59.155)	
Epoch: [20][233/391]	LR: 0.0002	Loss 1.5879 (1.4931)	Prec@1 56.250 (59.235)	
Epoch: [20][311/391]	LR: 0.0002	Loss 1.5635 (1.4959)	Prec@1 57.031 (59.122)	
Epoch: [20][389/391]	LR: 0.0002	Loss 1.4180 (1.4933)	Prec@1 59.375 (59.225)	
Total train loss: 1.4935

 * Prec@1 57.750 Prec@5 84.930 Loss 1.5781
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0002	Loss 1.4688 (1.4866)	Prec@1 57.812 (59.565)	
Epoch: [21][155/391]	LR: 0.0002	Loss 1.3115 (1.4913)	Prec@1 63.281 (59.330)	
Epoch: [21][233/391]	LR: 0.0002	Loss 1.3486 (1.4829)	Prec@1 64.062 (59.412)	
Epoch: [21][311/391]	LR: 0.0002	Loss 1.4883 (1.4896)	Prec@1 57.031 (59.317)	
Epoch: [21][389/391]	LR: 0.0002	Loss 1.4541 (1.4907)	Prec@1 57.031 (59.379)	
Total train loss: 1.4906

 * Prec@1 57.890 Prec@5 84.740 Loss 1.5879
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0002	Loss 1.5469 (1.5075)	Prec@1 55.469 (58.714)	
Epoch: [22][155/391]	LR: 0.0002	Loss 1.5068 (1.5004)	Prec@1 60.938 (59.065)	
Epoch: [22][233/391]	LR: 0.0002	Loss 1.3389 (1.4964)	Prec@1 64.844 (59.195)	
Epoch: [22][311/391]	LR: 0.0002	Loss 1.3438 (1.4952)	Prec@1 62.500 (59.380)	
Epoch: [22][389/391]	LR: 0.0002	Loss 1.4863 (1.4949)	Prec@1 57.812 (59.415)	
Total train loss: 1.4949

 * Prec@1 57.990 Prec@5 84.870 Loss 1.5781
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0002	Loss 1.3496 (1.5136)	Prec@1 63.281 (59.275)	
Epoch: [23][155/391]	LR: 0.0002	Loss 1.3174 (1.5097)	Prec@1 62.500 (59.009)	
Epoch: [23][233/391]	LR: 0.0002	Loss 1.5791 (1.5049)	Prec@1 57.812 (59.412)	
Epoch: [23][311/391]	LR: 0.0002	Loss 1.1807 (1.4944)	Prec@1 67.188 (59.578)	
Epoch: [23][389/391]	LR: 0.0002	Loss 1.6143 (1.4931)	Prec@1 53.125 (59.535)	
Total train loss: 1.4931

 * Prec@1 57.950 Prec@5 84.750 Loss 1.5869
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0002	Loss 1.5156 (1.4842)	Prec@1 58.594 (59.605)	
Epoch: [24][155/391]	LR: 0.0002	Loss 1.3262 (1.4821)	Prec@1 64.844 (59.781)	
Epoch: [24][233/391]	LR: 0.0002	Loss 1.8867 (1.4841)	Prec@1 51.562 (59.929)	
Epoch: [24][311/391]	LR: 0.0002	Loss 1.6533 (1.4870)	Prec@1 53.125 (59.763)	
Epoch: [24][389/391]	LR: 0.0002	Loss 1.4492 (1.4901)	Prec@1 65.625 (59.589)	
Total train loss: 1.4903

 * Prec@1 57.820 Prec@5 84.710 Loss 1.5869
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0002	Loss 1.3682 (1.4869)	Prec@1 60.156 (58.884)	
Epoch: [25][155/391]	LR: 0.0002	Loss 1.3252 (1.4937)	Prec@1 63.281 (58.944)	
Epoch: [25][233/391]	LR: 0.0002	Loss 1.4922 (1.4844)	Prec@1 57.031 (59.428)	
Epoch: [25][311/391]	LR: 0.0002	Loss 1.5010 (1.4875)	Prec@1 55.469 (59.398)	
Epoch: [25][389/391]	LR: 0.0002	Loss 1.5225 (1.4946)	Prec@1 57.031 (59.187)	
Total train loss: 1.4946

 * Prec@1 57.650 Prec@5 84.740 Loss 1.5830
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0002	Loss 1.4385 (1.5029)	Prec@1 57.031 (59.175)	
Epoch: [26][155/391]	LR: 0.0002	Loss 1.4971 (1.4817)	Prec@1 62.500 (59.851)	
Epoch: [26][233/391]	LR: 0.0002	Loss 1.2588 (1.4839)	Prec@1 65.625 (59.622)	
Epoch: [26][311/391]	LR: 0.0002	Loss 1.5508 (1.4850)	Prec@1 57.031 (59.673)	
Epoch: [26][389/391]	LR: 0.0002	Loss 1.5488 (1.4897)	Prec@1 57.812 (59.535)	
Total train loss: 1.4900

 * Prec@1 57.870 Prec@5 84.880 Loss 1.5820
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0002	Loss 1.3711 (1.5058)	Prec@1 57.031 (58.714)	
Epoch: [27][155/391]	LR: 0.0002	Loss 1.5088 (1.4959)	Prec@1 58.594 (59.205)	
Epoch: [27][233/391]	LR: 0.0002	Loss 1.3584 (1.4929)	Prec@1 67.969 (59.375)	
Epoch: [27][311/391]	LR: 0.0002	Loss 1.6523 (1.4922)	Prec@1 56.250 (59.362)	
Epoch: [27][389/391]	LR: 0.0002	Loss 1.5859 (1.4928)	Prec@1 56.250 (59.373)	
Total train loss: 1.4928

 * Prec@1 57.660 Prec@5 84.920 Loss 1.5801
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0002	Loss 1.1328 (1.5008)	Prec@1 67.969 (59.075)	
Epoch: [28][155/391]	LR: 0.0002	Loss 1.6631 (1.4943)	Prec@1 53.906 (59.345)	
Epoch: [28][233/391]	LR: 0.0002	Loss 1.5078 (1.4924)	Prec@1 56.250 (59.435)	
Epoch: [28][311/391]	LR: 0.0002	Loss 1.2100 (1.4965)	Prec@1 67.969 (59.365)	
Epoch: [28][389/391]	LR: 0.0002	Loss 1.5947 (1.4952)	Prec@1 57.031 (59.295)	
Total train loss: 1.4950

 * Prec@1 57.690 Prec@5 84.820 Loss 1.5811
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0002	Loss 1.5771 (1.4735)	Prec@1 56.250 (59.856)	
Epoch: [29][155/391]	LR: 0.0002	Loss 1.6162 (1.4930)	Prec@1 52.344 (59.415)	
Epoch: [29][233/391]	LR: 0.0002	Loss 1.2676 (1.4925)	Prec@1 62.500 (59.442)	
Epoch: [29][311/391]	LR: 0.0002	Loss 1.6006 (1.4953)	Prec@1 55.469 (59.352)	
Epoch: [29][389/391]	LR: 0.0002	Loss 1.3711 (1.4923)	Prec@1 65.625 (59.485)	
Total train loss: 1.4922

 * Prec@1 57.690 Prec@5 84.750 Loss 1.5898
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [30][77/391]	LR: 0.0002	Loss 1.6895 (1.4924)	Prec@1 48.438 (59.635)	
Epoch: [30][155/391]	LR: 0.0002	Loss 1.5000 (1.4990)	Prec@1 58.594 (59.440)	
Epoch: [30][233/391]	LR: 0.0002	Loss 1.4785 (1.4935)	Prec@1 60.156 (59.572)	
Epoch: [30][311/391]	LR: 0.0002	Loss 1.5508 (1.4928)	Prec@1 58.594 (59.525)	
Epoch: [30][389/391]	LR: 0.0002	Loss 1.3789 (1.4909)	Prec@1 65.625 (59.503)	
Total train loss: 1.4910

 * Prec@1 57.850 Prec@5 84.860 Loss 1.5801
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [31][77/391]	LR: 0.0002	Loss 1.5566 (1.4760)	Prec@1 57.812 (60.026)	
Epoch: [31][155/391]	LR: 0.0002	Loss 1.4805 (1.4820)	Prec@1 62.500 (59.836)	
Epoch: [31][233/391]	LR: 0.0002	Loss 1.1602 (1.4875)	Prec@1 68.750 (59.605)	
Epoch: [31][311/391]	LR: 0.0002	Loss 1.4600 (1.4921)	Prec@1 62.500 (59.480)	
Epoch: [31][389/391]	LR: 0.0002	Loss 1.5146 (1.4912)	Prec@1 56.250 (59.467)	
Total train loss: 1.4914

 * Prec@1 57.860 Prec@5 84.860 Loss 1.5791
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [32][77/391]	LR: 0.0002	Loss 1.4844 (1.4761)	Prec@1 58.594 (59.816)	
Epoch: [32][155/391]	LR: 0.0002	Loss 1.5029 (1.4748)	Prec@1 57.812 (59.670)	
Epoch: [32][233/391]	LR: 0.0002	Loss 1.4258 (1.4769)	Prec@1 57.812 (59.736)	
Epoch: [32][311/391]	LR: 0.0002	Loss 1.3896 (1.4890)	Prec@1 63.281 (59.503)	
Epoch: [32][389/391]	LR: 0.0002	Loss 1.8359 (1.4900)	Prec@1 48.438 (59.459)	
Total train loss: 1.4900

 * Prec@1 57.950 Prec@5 84.710 Loss 1.5791
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [33][77/391]	LR: 0.0002	Loss 1.3701 (1.4670)	Prec@1 64.062 (60.296)	
Epoch: [33][155/391]	LR: 0.0002	Loss 1.6641 (1.4712)	Prec@1 59.375 (60.226)	
Epoch: [33][233/391]	LR: 0.0002	Loss 1.5811 (1.4839)	Prec@1 55.469 (59.822)	
Epoch: [33][311/391]	LR: 0.0002	Loss 1.5547 (1.4872)	Prec@1 60.938 (59.643)	
Epoch: [33][389/391]	LR: 0.0002	Loss 1.5967 (1.4928)	Prec@1 54.688 (59.377)	
Total train loss: 1.4928

 * Prec@1 57.770 Prec@5 84.990 Loss 1.5840
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [34][77/391]	LR: 0.0002	Loss 1.5000 (1.4964)	Prec@1 57.031 (59.635)	
Epoch: [34][155/391]	LR: 0.0002	Loss 1.3857 (1.4901)	Prec@1 62.500 (59.685)	
Epoch: [34][233/391]	LR: 0.0002	Loss 1.5381 (1.4943)	Prec@1 60.938 (59.482)	
Epoch: [34][311/391]	LR: 0.0002	Loss 1.6201 (1.4868)	Prec@1 57.031 (59.635)	
Epoch: [34][389/391]	LR: 0.0002	Loss 1.4219 (1.4920)	Prec@1 60.938 (59.437)	
Total train loss: 1.4918

 * Prec@1 57.830 Prec@5 84.760 Loss 1.5869
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [35][77/391]	LR: 0.0002	Loss 1.4580 (1.4781)	Prec@1 60.156 (59.255)	
Epoch: [35][155/391]	LR: 0.0002	Loss 1.3428 (1.4784)	Prec@1 62.500 (59.400)	
Epoch: [35][233/391]	LR: 0.0002	Loss 1.3818 (1.4885)	Prec@1 63.281 (59.171)	
Epoch: [35][311/391]	LR: 0.0002	Loss 1.5469 (1.4914)	Prec@1 62.500 (59.122)	
Epoch: [35][389/391]	LR: 0.0002	Loss 1.6055 (1.4891)	Prec@1 60.938 (59.297)	
Total train loss: 1.4893

 * Prec@1 57.820 Prec@5 84.980 Loss 1.5840
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [36][77/391]	LR: 0.0002	Loss 1.4775 (1.4883)	Prec@1 57.031 (58.824)	
Epoch: [36][155/391]	LR: 0.0002	Loss 1.1240 (1.4798)	Prec@1 67.969 (59.430)	
Epoch: [36][233/391]	LR: 0.0002	Loss 1.6406 (1.4835)	Prec@1 60.156 (59.388)	
Epoch: [36][311/391]	LR: 0.0002	Loss 1.4678 (1.4871)	Prec@1 58.594 (59.282)	
Epoch: [36][389/391]	LR: 0.0002	Loss 1.1963 (1.4880)	Prec@1 66.406 (59.331)	
Total train loss: 1.4882

 * Prec@1 57.850 Prec@5 84.680 Loss 1.5830
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [37][77/391]	LR: 0.0002	Loss 1.5918 (1.4820)	Prec@1 57.031 (59.635)	
Epoch: [37][155/391]	LR: 0.0002	Loss 1.4531 (1.4901)	Prec@1 61.719 (59.811)	
Epoch: [37][233/391]	LR: 0.0002	Loss 1.6074 (1.4910)	Prec@1 53.125 (59.532)	
Epoch: [37][311/391]	LR: 0.0002	Loss 1.4844 (1.4893)	Prec@1 57.031 (59.570)	
Epoch: [37][389/391]	LR: 0.0002	Loss 1.6406 (1.4907)	Prec@1 53.906 (59.485)	
Total train loss: 1.4909

 * Prec@1 57.660 Prec@5 84.900 Loss 1.5762
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [38][77/391]	LR: 0.0002	Loss 1.6250 (1.4963)	Prec@1 56.250 (59.946)	
Epoch: [38][155/391]	LR: 0.0002	Loss 1.5762 (1.4936)	Prec@1 58.594 (59.921)	
Epoch: [38][233/391]	LR: 0.0002	Loss 1.5322 (1.4931)	Prec@1 55.469 (59.802)	
Epoch: [38][311/391]	LR: 0.0002	Loss 1.4502 (1.4927)	Prec@1 60.938 (59.613)	
Epoch: [38][389/391]	LR: 0.0002	Loss 1.3438 (1.4913)	Prec@1 65.625 (59.591)	
Total train loss: 1.4911

 * Prec@1 57.740 Prec@5 84.730 Loss 1.5830
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [39][77/391]	LR: 0.0002	Loss 1.6104 (1.5150)	Prec@1 53.125 (58.744)	
Epoch: [39][155/391]	LR: 0.0002	Loss 1.3555 (1.5006)	Prec@1 62.500 (59.170)	
Epoch: [39][233/391]	LR: 0.0002	Loss 1.5020 (1.4989)	Prec@1 56.250 (59.141)	
Epoch: [39][311/391]	LR: 0.0002	Loss 1.4746 (1.4916)	Prec@1 62.500 (59.352)	
Epoch: [39][389/391]	LR: 0.0002	Loss 1.3965 (1.4896)	Prec@1 60.156 (59.371)	
Total train loss: 1.4897

 * Prec@1 57.910 Prec@5 84.800 Loss 1.5781
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [40][77/391]	LR: 0.0002	Loss 1.6875 (1.5010)	Prec@1 51.562 (59.125)	
Epoch: [40][155/391]	LR: 0.0002	Loss 1.4150 (1.4919)	Prec@1 60.156 (59.500)	
Epoch: [40][233/391]	LR: 0.0002	Loss 1.5195 (1.4959)	Prec@1 58.594 (59.325)	
Epoch: [40][311/391]	LR: 0.0002	Loss 1.4785 (1.4930)	Prec@1 58.594 (59.495)	
Epoch: [40][389/391]	LR: 0.0002	Loss 1.4131 (1.4888)	Prec@1 67.188 (59.495)	
Total train loss: 1.4887

 * Prec@1 57.760 Prec@5 84.920 Loss 1.5820
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [41][77/391]	LR: 0.0002	Loss 1.3379 (1.4881)	Prec@1 66.406 (59.245)	
Epoch: [41][155/391]	LR: 0.0002	Loss 1.5371 (1.4886)	Prec@1 59.375 (59.310)	
Epoch: [41][233/391]	LR: 0.0002	Loss 1.3604 (1.4845)	Prec@1 67.969 (59.445)	
Epoch: [41][311/391]	LR: 0.0002	Loss 1.5137 (1.4850)	Prec@1 55.469 (59.453)	
Epoch: [41][389/391]	LR: 0.0002	Loss 1.4434 (1.4887)	Prec@1 64.062 (59.385)	
Total train loss: 1.4886

 * Prec@1 57.890 Prec@5 84.830 Loss 1.5811
Best acc: 58.040
--------------------------------------------------------------------------------
Epoch: [42][77/391]	LR: 0.0002	Loss 1.3594 (1.4801)	Prec@1 67.969 (59.395)	
Epoch: [42][155/391]	LR: 0.0002	Loss 1.3857 (1.4813)	Prec@1 64.844 (59.395)	
Epoch: [42][233/391]	LR: 0.0002	Loss 1.3828 (1.4884)	Prec@1 66.406 (59.268)	
Epoch: [42][311/391]	LR: 0.0002	Loss 1.6465 (1.4934)	Prec@1 53.906 (59.245)	
Epoch: [42][389/391]	LR: 0.0002	Loss 1.5283 (1.4899)	Prec@1 58.594 (59.377)	
Total train loss: 1.4895

 * Prec@1 58.090 Prec@5 84.830 Loss 1.5801
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [43][77/391]	LR: 0.0002	Loss 1.5146 (1.4983)	Prec@1 60.938 (58.984)	
Epoch: [43][155/391]	LR: 0.0002	Loss 1.5254 (1.4902)	Prec@1 54.688 (59.455)	
Epoch: [43][233/391]	LR: 0.0002	Loss 1.5811 (1.4945)	Prec@1 59.375 (59.428)	
Epoch: [43][311/391]	LR: 0.0002	Loss 1.6094 (1.4975)	Prec@1 51.562 (59.398)	
Epoch: [43][389/391]	LR: 0.0002	Loss 1.4648 (1.4932)	Prec@1 61.719 (59.411)	
Total train loss: 1.4933

 * Prec@1 57.690 Prec@5 84.780 Loss 1.5781
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [44][77/391]	LR: 0.0002	Loss 1.5195 (1.4885)	Prec@1 59.375 (59.065)	
Epoch: [44][155/391]	LR: 0.0002	Loss 1.5908 (1.4785)	Prec@1 52.344 (59.415)	
Epoch: [44][233/391]	LR: 0.0002	Loss 1.5410 (1.4872)	Prec@1 60.156 (59.445)	
Epoch: [44][311/391]	LR: 0.0002	Loss 1.6689 (1.4895)	Prec@1 55.469 (59.510)	
Epoch: [44][389/391]	LR: 0.0002	Loss 1.5312 (1.4904)	Prec@1 59.375 (59.475)	
Total train loss: 1.4906

 * Prec@1 57.800 Prec@5 84.910 Loss 1.5811
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [45][77/391]	LR: 0.0002	Loss 1.7432 (1.4874)	Prec@1 54.688 (59.515)	
Epoch: [45][155/391]	LR: 0.0002	Loss 1.2930 (1.4827)	Prec@1 64.844 (59.665)	
Epoch: [45][233/391]	LR: 0.0002	Loss 1.3701 (1.4808)	Prec@1 56.250 (59.485)	
Epoch: [45][311/391]	LR: 0.0002	Loss 1.5508 (1.4865)	Prec@1 62.500 (59.398)	
Epoch: [45][389/391]	LR: 0.0002	Loss 1.5205 (1.4889)	Prec@1 55.469 (59.353)	
Total train loss: 1.4887

 * Prec@1 57.790 Prec@5 84.800 Loss 1.5791
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [46][77/391]	LR: 0.0002	Loss 1.4854 (1.4866)	Prec@1 63.281 (59.896)	
Epoch: [46][155/391]	LR: 0.0002	Loss 1.7100 (1.4844)	Prec@1 52.344 (59.821)	
Epoch: [46][233/391]	LR: 0.0002	Loss 1.3643 (1.4855)	Prec@1 63.281 (59.702)	
Epoch: [46][311/391]	LR: 0.0002	Loss 1.5938 (1.4825)	Prec@1 61.719 (59.600)	
Epoch: [46][389/391]	LR: 0.0002	Loss 1.6104 (1.4859)	Prec@1 57.812 (59.479)	
Total train loss: 1.4864

 * Prec@1 57.650 Prec@5 84.710 Loss 1.5879
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [47][77/391]	LR: 0.0002	Loss 1.4941 (1.5135)	Prec@1 59.375 (58.884)	
Epoch: [47][155/391]	LR: 0.0002	Loss 1.4336 (1.4991)	Prec@1 61.719 (59.185)	
Epoch: [47][233/391]	LR: 0.0002	Loss 1.2676 (1.4914)	Prec@1 67.188 (59.292)	
Epoch: [47][311/391]	LR: 0.0002	Loss 1.3408 (1.4905)	Prec@1 63.281 (59.390)	
Epoch: [47][389/391]	LR: 0.0002	Loss 1.5137 (1.4900)	Prec@1 57.812 (59.357)	
Total train loss: 1.4894

 * Prec@1 58.000 Prec@5 84.870 Loss 1.5791
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [48][77/391]	LR: 0.0002	Loss 1.7529 (1.4688)	Prec@1 56.250 (60.256)	
Epoch: [48][155/391]	LR: 0.0002	Loss 1.4912 (1.4788)	Prec@1 59.375 (59.756)	
Epoch: [48][233/391]	LR: 0.0002	Loss 1.3994 (1.4783)	Prec@1 62.500 (59.759)	
Epoch: [48][311/391]	LR: 0.0002	Loss 1.6250 (1.4843)	Prec@1 57.812 (59.645)	
Epoch: [48][389/391]	LR: 0.0002	Loss 1.5684 (1.4879)	Prec@1 57.031 (59.505)	
Total train loss: 1.4883

 * Prec@1 57.610 Prec@5 84.800 Loss 1.5889
Best acc: 58.090
--------------------------------------------------------------------------------
Epoch: [49][77/391]	LR: 0.0002	Loss 1.4111 (1.5105)	Prec@1 61.719 (58.343)	
Epoch: [49][155/391]	LR: 0.0002	Loss 1.6162 (1.4967)	Prec@1 52.344 (59.070)	
Epoch: [49][233/391]	LR: 0.0002	Loss 1.3525 (1.4943)	Prec@1 62.500 (59.148)	
Epoch: [49][311/391]	LR: 0.0002	Loss 1.4219 (1.4910)	Prec@1 67.188 (59.443)	
Epoch: [49][389/391]	LR: 0.0002	Loss 1.6055 (1.4884)	Prec@1 55.469 (59.525)	
Total train loss: 1.4887

 * Prec@1 57.800 Prec@5 84.770 Loss 1.5830
Best acc: 58.090
--------------------------------------------------------------------------------
