
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 0.850 Prec@5 4.420 Loss 4.6250
Pre-trained Prec@1 with 9 layers frozen: 0.8499999642372131 	 Loss: 4.625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 4.0078 (4.6265)	Prec@1 10.938 (4.277)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.2051 (4.1075)	Prec@1 29.688 (11.909)	
Epoch: [0][116/196]	LR: 0.002	Loss 2.9258 (3.7630)	Prec@1 35.547 (18.219)	
Epoch: [0][155/196]	LR: 0.002	Loss 2.7012 (3.5094)	Prec@1 40.234 (23.490)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.3613 (3.3132)	Prec@1 48.047 (27.584)	
Total train loss: 3.3120

 * Prec@1 48.190 Prec@5 79.160 Loss 2.3652
Best acc: 48.190
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.2754 (2.2923)	Prec@1 51.953 (50.942)	
Epoch: [1][77/196]	LR: 0.002	Loss 2.0371 (2.2427)	Prec@1 57.812 (51.763)	
Epoch: [1][116/196]	LR: 0.002	Loss 2.0742 (2.1933)	Prec@1 51.953 (52.674)	
Epoch: [1][155/196]	LR: 0.002	Loss 2.0234 (2.1516)	Prec@1 51.953 (53.466)	
Epoch: [1][194/196]	LR: 0.002	Loss 1.9346 (2.1096)	Prec@1 61.719 (54.317)	
Total train loss: 2.1095

 * Prec@1 57.020 Prec@5 85.790 Loss 1.9395
Best acc: 57.020
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 1.8301 (1.8376)	Prec@1 59.766 (60.727)	
Epoch: [2][77/196]	LR: 0.002	Loss 1.8242 (1.8187)	Prec@1 60.938 (61.178)	
Epoch: [2][116/196]	LR: 0.002	Loss 1.7373 (1.8027)	Prec@1 61.719 (61.365)	
Epoch: [2][155/196]	LR: 0.002	Loss 1.7734 (1.7909)	Prec@1 62.891 (61.486)	
Epoch: [2][194/196]	LR: 0.002	Loss 1.5811 (1.7727)	Prec@1 67.188 (61.693)	
Total train loss: 1.7730

 * Prec@1 61.010 Prec@5 88.100 Loss 1.7393
Best acc: 61.010
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.5449 (1.5993)	Prec@1 67.578 (66.426)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.6631 (1.5934)	Prec@1 64.844 (66.371)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.5029 (1.5880)	Prec@1 68.359 (66.309)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.6260 (1.5786)	Prec@1 61.719 (66.136)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.4980 (1.5696)	Prec@1 67.578 (66.240)	
Total train loss: 1.5699

 * Prec@1 63.180 Prec@5 89.570 Loss 1.6084
Best acc: 63.180
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.3438 (1.4461)	Prec@1 73.047 (69.010)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.2734 (1.4364)	Prec@1 75.000 (69.401)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.4023 (1.4325)	Prec@1 67.188 (69.374)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.4014 (1.4285)	Prec@1 69.922 (69.328)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.4736 (1.4258)	Prec@1 67.188 (69.287)	
Total train loss: 1.4261

 * Prec@1 64.890 Prec@5 90.490 Loss 1.5186
Best acc: 64.890
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.2432 (1.3391)	Prec@1 71.484 (71.625)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.3779 (1.3198)	Prec@1 67.969 (72.251)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.4062 (1.3214)	Prec@1 69.141 (72.185)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.3076 (1.3142)	Prec@1 70.703 (72.228)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.3467 (1.3136)	Prec@1 72.266 (72.119)	
Total train loss: 1.3140

 * Prec@1 66.140 Prec@5 91.070 Loss 1.4658
Best acc: 66.140
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.2607 (1.2239)	Prec@1 70.312 (74.379)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.2432 (1.2135)	Prec@1 74.219 (74.955)	
Epoch: [6][116/196]	LR: 0.002	Loss 1.2871 (1.2176)	Prec@1 73.828 (74.710)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.2695 (1.2194)	Prec@1 76.953 (74.639)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.2578 (1.2186)	Prec@1 75.391 (74.541)	
Total train loss: 1.2188

 * Prec@1 66.760 Prec@5 91.320 Loss 1.4102
Best acc: 66.760
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 1.2256 (1.1283)	Prec@1 74.609 (77.574)	
Epoch: [7][77/196]	LR: 0.002	Loss 1.0723 (1.1321)	Prec@1 78.125 (77.254)	
Epoch: [7][116/196]	LR: 0.002	Loss 1.1191 (1.1312)	Prec@1 77.344 (76.946)	
Epoch: [7][155/196]	LR: 0.002	Loss 1.2549 (1.1312)	Prec@1 75.000 (76.893)	
Epoch: [7][194/196]	LR: 0.002	Loss 1.0664 (1.1343)	Prec@1 79.297 (76.783)	
Total train loss: 1.1345

 * Prec@1 67.460 Prec@5 91.590 Loss 1.3838
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 1.1299 (1.0669)	Prec@1 72.266 (79.447)	
Epoch: [8][77/196]	LR: 0.002	Loss 1.0469 (1.0585)	Prec@1 80.078 (79.407)	
Epoch: [8][116/196]	LR: 0.002	Loss 1.0273 (1.0608)	Prec@1 80.078 (78.956)	
Epoch: [8][155/196]	LR: 0.002	Loss 1.0684 (1.0604)	Prec@1 77.734 (78.861)	
Epoch: [8][194/196]	LR: 0.002	Loss 1.1396 (1.0599)	Prec@1 76.562 (78.736)	
Total train loss: 1.0605

 * Prec@1 68.280 Prec@5 91.730 Loss 1.3555
Best acc: 68.280
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 0.9829 (0.9959)	Prec@1 80.078 (80.789)	
Epoch: [9][77/196]	LR: 0.002	Loss 0.9941 (0.9931)	Prec@1 80.078 (80.834)	
Epoch: [9][116/196]	LR: 0.002	Loss 0.9658 (0.9949)	Prec@1 83.594 (80.763)	
Epoch: [9][155/196]	LR: 0.002	Loss 0.9912 (0.9966)	Prec@1 78.516 (80.614)	
Epoch: [9][194/196]	LR: 0.002	Loss 0.9951 (0.9942)	Prec@1 79.688 (80.567)	
Total train loss: 0.9948

 * Prec@1 68.530 Prec@5 92.120 Loss 1.3340
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 0.9380 (0.9277)	Prec@1 83.594 (82.141)	
Epoch: [10][77/196]	LR: 0.0004	Loss 0.8794 (0.9186)	Prec@1 87.500 (82.762)	
Epoch: [10][116/196]	LR: 0.0004	Loss 0.9146 (0.9190)	Prec@1 82.031 (82.712)	
Epoch: [10][155/196]	LR: 0.0004	Loss 0.8794 (0.9201)	Prec@1 82.422 (82.700)	
Epoch: [10][194/196]	LR: 0.0004	Loss 0.9854 (0.9222)	Prec@1 81.641 (82.680)	
Total train loss: 0.9224

 * Prec@1 68.510 Prec@5 91.980 Loss 1.3379
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [11][38/196]	LR: 0.0004	Loss 0.8477 (0.9101)	Prec@1 82.031 (83.363)	
Epoch: [11][77/196]	LR: 0.0004	Loss 0.9868 (0.9177)	Prec@1 80.469 (83.013)	
Epoch: [11][116/196]	LR: 0.0004	Loss 0.9297 (0.9214)	Prec@1 82.031 (82.843)	
Epoch: [11][155/196]	LR: 0.0004	Loss 0.9512 (0.9238)	Prec@1 83.203 (82.755)	
Epoch: [11][194/196]	LR: 0.0004	Loss 0.9473 (0.9215)	Prec@1 80.078 (82.837)	
Total train loss: 0.9217

 * Prec@1 68.590 Prec@5 91.960 Loss 1.3340
Best acc: 68.590
--------------------------------------------------------------------------------
Epoch: [12][38/196]	LR: 0.0004	Loss 0.8652 (0.9258)	Prec@1 83.984 (82.752)	
Epoch: [12][77/196]	LR: 0.0004	Loss 0.9067 (0.9128)	Prec@1 81.641 (83.073)	
Epoch: [12][116/196]	LR: 0.0004	Loss 1.0107 (0.9174)	Prec@1 80.469 (82.799)	
Epoch: [12][155/196]	LR: 0.0004	Loss 0.8833 (0.9141)	Prec@1 82.031 (82.933)	
Epoch: [12][194/196]	LR: 0.0004	Loss 0.9409 (0.9144)	Prec@1 82.812 (82.925)	
Total train loss: 0.9148

 * Prec@1 68.420 Prec@5 92.070 Loss 1.3320
Best acc: 68.590
--------------------------------------------------------------------------------
Epoch: [13][38/196]	LR: 0.0004	Loss 0.8281 (0.9098)	Prec@1 85.547 (83.143)	
Epoch: [13][77/196]	LR: 0.0004	Loss 0.9102 (0.9085)	Prec@1 82.422 (83.273)	
Epoch: [13][116/196]	LR: 0.0004	Loss 0.8726 (0.9061)	Prec@1 83.984 (83.146)	
Epoch: [13][155/196]	LR: 0.0004	Loss 0.9038 (0.9075)	Prec@1 83.594 (83.148)	
Epoch: [13][194/196]	LR: 0.0004	Loss 0.9272 (0.9085)	Prec@1 80.859 (83.017)	
Total train loss: 0.9089

 * Prec@1 68.620 Prec@5 92.070 Loss 1.3281
Best acc: 68.620
--------------------------------------------------------------------------------
