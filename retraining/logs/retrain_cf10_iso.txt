
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 91.190 Prec@5 99.480 Loss 0.3542
Pre-trained Prec@1 with 1 layers frozen: 91.18999481201172 	 Loss: 0.354248046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.1108 (0.1703)	Prec@1 97.656 (94.401)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0868 (0.1443)	Prec@1 95.312 (95.187)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.0765 (0.1296)	Prec@1 99.219 (95.700)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.1134 (0.1201)	Prec@1 94.531 (96.016)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.0844 (0.1141)	Prec@1 97.656 (96.236)	
Total train loss: 0.1141

 * Prec@1 91.510 Prec@5 99.560 Loss 0.3005
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0380 (0.0801)	Prec@1 99.219 (97.446)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0704 (0.0801)	Prec@1 97.656 (97.371)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.1486 (0.0798)	Prec@1 95.312 (97.352)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0709 (0.0789)	Prec@1 96.875 (97.403)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.1046 (0.0785)	Prec@1 96.094 (97.444)	
Total train loss: 0.0785

 * Prec@1 91.440 Prec@5 99.620 Loss 0.3044
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0285 (0.0691)	Prec@1 100.000 (97.736)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0679 (0.0687)	Prec@1 98.438 (97.811)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0739 (0.0686)	Prec@1 97.656 (97.857)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0697 (0.0688)	Prec@1 97.656 (97.844)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0301 (0.0683)	Prec@1 100.000 (97.877)	
Total train loss: 0.0683

 * Prec@1 91.480 Prec@5 99.600 Loss 0.3018
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0346 (0.0639)	Prec@1 100.000 (98.117)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0300 (0.0620)	Prec@1 99.219 (98.172)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.1006 (0.0619)	Prec@1 96.875 (98.210)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0660 (0.0621)	Prec@1 98.438 (98.207)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0979 (0.0621)	Prec@1 96.875 (98.157)	
Total train loss: 0.0622

 * Prec@1 91.380 Prec@5 99.570 Loss 0.3032
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0576 (0.0646)	Prec@1 98.438 (98.057)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0605 (0.0594)	Prec@1 97.656 (98.227)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0442 (0.0599)	Prec@1 99.219 (98.217)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0716 (0.0592)	Prec@1 97.656 (98.222)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0385 (0.0585)	Prec@1 100.000 (98.277)	
Total train loss: 0.0585

 * Prec@1 91.480 Prec@5 99.540 Loss 0.3091
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0648 (0.0531)	Prec@1 97.656 (98.548)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0600 (0.0512)	Prec@1 96.875 (98.553)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0750 (0.0525)	Prec@1 97.656 (98.511)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0264 (0.0538)	Prec@1 99.219 (98.435)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0392 (0.0532)	Prec@1 99.219 (98.468)	
Total train loss: 0.0532

 * Prec@1 91.370 Prec@5 99.560 Loss 0.3091
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0795 (0.0526)	Prec@1 97.656 (98.568)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0836 (0.0532)	Prec@1 97.656 (98.553)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0725 (0.0532)	Prec@1 96.875 (98.534)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0211 (0.0530)	Prec@1 100.000 (98.550)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0429 (0.0520)	Prec@1 99.219 (98.582)	
Total train loss: 0.0521

 * Prec@1 91.470 Prec@5 99.550 Loss 0.3040
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0373 (0.0538)	Prec@1 100.000 (98.397)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0408 (0.0527)	Prec@1 99.219 (98.473)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0466 (0.0525)	Prec@1 100.000 (98.541)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0306 (0.0524)	Prec@1 100.000 (98.543)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0631 (0.0516)	Prec@1 96.875 (98.564)	
Total train loss: 0.0517

 * Prec@1 91.380 Prec@5 99.560 Loss 0.3052
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0375 (0.0499)	Prec@1 98.438 (98.688)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0717 (0.0514)	Prec@1 95.312 (98.573)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0512 (0.0513)	Prec@1 98.438 (98.571)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0326 (0.0506)	Prec@1 99.219 (98.600)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.1011 (0.0515)	Prec@1 96.094 (98.548)	
Total train loss: 0.0515

 * Prec@1 91.390 Prec@5 99.570 Loss 0.3083
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0642 (0.0526)	Prec@1 97.656 (98.347)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0295 (0.0513)	Prec@1 99.219 (98.508)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0723 (0.0516)	Prec@1 98.438 (98.541)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0218 (0.0514)	Prec@1 100.000 (98.518)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0448 (0.0513)	Prec@1 99.219 (98.520)	
Total train loss: 0.0514

 * Prec@1 91.470 Prec@5 99.570 Loss 0.3032
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0817 (0.0507)	Prec@1 98.438 (98.528)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0517 (0.0492)	Prec@1 98.438 (98.693)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0370 (0.0496)	Prec@1 100.000 (98.695)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0463 (0.0505)	Prec@1 97.656 (98.628)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0521 (0.0515)	Prec@1 99.219 (98.614)	
Total train loss: 0.0515

 * Prec@1 91.420 Prec@5 99.580 Loss 0.3049
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0632 (0.0491)	Prec@1 98.438 (98.778)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0528 (0.0512)	Prec@1 99.219 (98.598)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0876 (0.0515)	Prec@1 96.875 (98.588)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0244 (0.0505)	Prec@1 99.219 (98.595)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0392 (0.0505)	Prec@1 100.000 (98.612)	
Total train loss: 0.0505

 * Prec@1 91.450 Prec@5 99.540 Loss 0.3059
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0703 (0.0508)	Prec@1 98.438 (98.658)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0580 (0.0513)	Prec@1 97.656 (98.608)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0632 (0.0519)	Prec@1 97.656 (98.598)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0817 (0.0508)	Prec@1 96.875 (98.590)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0380 (0.0511)	Prec@1 100.000 (98.588)	
Total train loss: 0.0511

 * Prec@1 91.420 Prec@5 99.560 Loss 0.3066
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0859 (0.0521)	Prec@1 97.656 (98.578)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0917 (0.0510)	Prec@1 96.875 (98.573)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0694 (0.0519)	Prec@1 97.656 (98.514)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0808 (0.0515)	Prec@1 96.094 (98.560)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0790 (0.0516)	Prec@1 97.656 (98.544)	
Total train loss: 0.0516

 * Prec@1 91.340 Prec@5 99.580 Loss 0.3062
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0444 (0.0524)	Prec@1 99.219 (98.548)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0250 (0.0507)	Prec@1 100.000 (98.603)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0353 (0.0497)	Prec@1 100.000 (98.678)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0476 (0.0501)	Prec@1 97.656 (98.668)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0740 (0.0500)	Prec@1 97.656 (98.670)	
Total train loss: 0.0501

 * Prec@1 91.330 Prec@5 99.580 Loss 0.3071
Best acc: 91.510
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0779 (0.0460)	Prec@1 97.656 (99.008)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0447 (0.0483)	Prec@1 99.219 (98.833)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0435 (0.0488)	Prec@1 98.438 (98.801)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0870 (0.0490)	Prec@1 96.094 (98.758)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0553 (0.0493)	Prec@1 98.438 (98.732)	
Total train loss: 0.0496

 * Prec@1 91.590 Prec@5 99.580 Loss 0.3013
Best acc: 91.590
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0270 (0.0530)	Prec@1 100.000 (98.548)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0516 (0.0531)	Prec@1 97.656 (98.533)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0690 (0.0526)	Prec@1 96.875 (98.524)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0502 (0.0515)	Prec@1 97.656 (98.583)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0377 (0.0515)	Prec@1 99.219 (98.582)	
Total train loss: 0.0514

 * Prec@1 91.440 Prec@5 99.560 Loss 0.3064
Best acc: 91.590
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0881 (0.0528)	Prec@1 96.094 (98.558)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0253 (0.0530)	Prec@1 100.000 (98.528)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0476 (0.0533)	Prec@1 99.219 (98.524)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0587 (0.0524)	Prec@1 97.656 (98.593)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0238 (0.0517)	Prec@1 100.000 (98.616)	
Total train loss: 0.0517

 * Prec@1 91.430 Prec@5 99.530 Loss 0.3064
Best acc: 91.590
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0695 (0.0483)	Prec@1 97.656 (98.748)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0524 (0.0487)	Prec@1 99.219 (98.748)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0245 (0.0505)	Prec@1 100.000 (98.614)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0414 (0.0506)	Prec@1 99.219 (98.623)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0293 (0.0508)	Prec@1 100.000 (98.620)	
Total train loss: 0.0508

 * Prec@1 91.390 Prec@5 99.570 Loss 0.3042
Best acc: 91.590
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0231 (0.0507)	Prec@1 100.000 (98.728)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0301 (0.0521)	Prec@1 99.219 (98.548)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0226 (0.0502)	Prec@1 100.000 (98.628)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0593 (0.0509)	Prec@1 99.219 (98.613)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0826 (0.0510)	Prec@1 96.875 (98.608)	
Total train loss: 0.0511

 * Prec@1 91.400 Prec@5 99.570 Loss 0.3071
Best acc: 91.590
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 90.780 Prec@5 99.470 Loss 0.3716
Pre-trained Prec@1 with 3 layers frozen: 90.77999877929688 	 Loss: 0.37158203125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.0348 (0.0445)	Prec@1 98.438 (98.698)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0479 (0.0465)	Prec@1 98.438 (98.588)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.0872 (0.0473)	Prec@1 96.875 (98.544)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.0161 (0.0467)	Prec@1 100.000 (98.550)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.0207 (0.0458)	Prec@1 100.000 (98.588)	
Total train loss: 0.0458

 * Prec@1 91.360 Prec@5 99.450 Loss 0.3398
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0303 (0.0342)	Prec@1 100.000 (99.099)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0307 (0.0341)	Prec@1 99.219 (99.074)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0335 (0.0348)	Prec@1 99.219 (99.072)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0392 (0.0349)	Prec@1 100.000 (99.058)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0248 (0.0350)	Prec@1 99.219 (99.040)	
Total train loss: 0.0351

 * Prec@1 91.340 Prec@5 99.430 Loss 0.3420
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0182 (0.0275)	Prec@1 100.000 (99.369)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0244 (0.0280)	Prec@1 100.000 (99.389)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0681 (0.0289)	Prec@1 97.656 (99.332)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0200 (0.0296)	Prec@1 100.000 (99.269)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0235 (0.0294)	Prec@1 99.219 (99.279)	
Total train loss: 0.0295

 * Prec@1 91.300 Prec@5 99.460 Loss 0.3511
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0167 (0.0261)	Prec@1 99.219 (99.399)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0158 (0.0265)	Prec@1 100.000 (99.409)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0219 (0.0269)	Prec@1 99.219 (99.376)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0199 (0.0268)	Prec@1 100.000 (99.384)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0158 (0.0269)	Prec@1 100.000 (99.389)	
Total train loss: 0.0269

 * Prec@1 91.290 Prec@5 99.430 Loss 0.3525
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0152 (0.0250)	Prec@1 100.000 (99.559)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0166 (0.0241)	Prec@1 100.000 (99.569)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0131 (0.0248)	Prec@1 100.000 (99.523)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0201 (0.0244)	Prec@1 100.000 (99.539)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0112 (0.0246)	Prec@1 100.000 (99.543)	
Total train loss: 0.0246

 * Prec@1 91.220 Prec@5 99.450 Loss 0.3555
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0143 (0.0211)	Prec@1 100.000 (99.700)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0190 (0.0215)	Prec@1 100.000 (99.684)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0358 (0.0218)	Prec@1 99.219 (99.659)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0217 (0.0218)	Prec@1 100.000 (99.664)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0158 (0.0223)	Prec@1 100.000 (99.639)	
Total train loss: 0.0223

 * Prec@1 91.290 Prec@5 99.470 Loss 0.3511
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0220 (0.0203)	Prec@1 100.000 (99.760)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0160 (0.0203)	Prec@1 100.000 (99.720)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0255 (0.0215)	Prec@1 99.219 (99.653)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0151 (0.0215)	Prec@1 100.000 (99.659)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0255 (0.0216)	Prec@1 100.000 (99.651)	
Total train loss: 0.0216

 * Prec@1 91.180 Prec@5 99.410 Loss 0.3542
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0212 (0.0196)	Prec@1 100.000 (99.840)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0208 (0.0216)	Prec@1 100.000 (99.664)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0412 (0.0217)	Prec@1 98.438 (99.633)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0291 (0.0218)	Prec@1 100.000 (99.634)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0562 (0.0222)	Prec@1 96.875 (99.617)	
Total train loss: 0.0222

 * Prec@1 91.160 Prec@5 99.460 Loss 0.3521
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0202 (0.0197)	Prec@1 100.000 (99.710)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0295 (0.0211)	Prec@1 99.219 (99.654)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0173 (0.0215)	Prec@1 100.000 (99.653)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0184 (0.0220)	Prec@1 100.000 (99.609)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0115 (0.0222)	Prec@1 100.000 (99.601)	
Total train loss: 0.0222

 * Prec@1 91.340 Prec@5 99.450 Loss 0.3542
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0144 (0.0208)	Prec@1 100.000 (99.750)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0179 (0.0210)	Prec@1 100.000 (99.679)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0663 (0.0214)	Prec@1 97.656 (99.676)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0381 (0.0212)	Prec@1 99.219 (99.687)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0125 (0.0215)	Prec@1 100.000 (99.665)	
Total train loss: 0.0214

 * Prec@1 91.280 Prec@5 99.420 Loss 0.3464
Best acc: 91.360
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0496 (0.0247)	Prec@1 98.438 (99.499)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0342 (0.0230)	Prec@1 99.219 (99.574)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0213 (0.0221)	Prec@1 100.000 (99.596)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0301 (0.0224)	Prec@1 100.000 (99.594)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0266 (0.0222)	Prec@1 99.219 (99.587)	
Total train loss: 0.0222

 * Prec@1 91.390 Prec@5 99.460 Loss 0.3501
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0305 (0.0220)	Prec@1 99.219 (99.679)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0245 (0.0220)	Prec@1 100.000 (99.674)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0123 (0.0214)	Prec@1 100.000 (99.663)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0285 (0.0217)	Prec@1 99.219 (99.649)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0147 (0.0218)	Prec@1 100.000 (99.639)	
Total train loss: 0.0219

 * Prec@1 91.280 Prec@5 99.460 Loss 0.3525
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0390 (0.0221)	Prec@1 98.438 (99.669)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0341 (0.0219)	Prec@1 99.219 (99.649)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0117 (0.0223)	Prec@1 100.000 (99.646)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0172 (0.0220)	Prec@1 100.000 (99.672)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0235 (0.0218)	Prec@1 100.000 (99.671)	
Total train loss: 0.0218

 * Prec@1 91.340 Prec@5 99.420 Loss 0.3521
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0247 (0.0230)	Prec@1 99.219 (99.569)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0189 (0.0216)	Prec@1 100.000 (99.649)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0442 (0.0215)	Prec@1 98.438 (99.666)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0251 (0.0214)	Prec@1 100.000 (99.662)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0221 (0.0216)	Prec@1 100.000 (99.639)	
Total train loss: 0.0217

 * Prec@1 91.240 Prec@5 99.480 Loss 0.3506
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0424 (0.0245)	Prec@1 98.438 (99.519)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0223 (0.0229)	Prec@1 100.000 (99.609)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0265 (0.0223)	Prec@1 99.219 (99.649)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0164 (0.0221)	Prec@1 100.000 (99.642)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0187 (0.0220)	Prec@1 100.000 (99.651)	
Total train loss: 0.0219

 * Prec@1 91.210 Prec@5 99.480 Loss 0.3569
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0154 (0.0222)	Prec@1 100.000 (99.619)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0246 (0.0220)	Prec@1 100.000 (99.614)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0171 (0.0229)	Prec@1 100.000 (99.603)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0127 (0.0219)	Prec@1 100.000 (99.632)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0320 (0.0215)	Prec@1 99.219 (99.655)	
Total train loss: 0.0215

 * Prec@1 91.270 Prec@5 99.460 Loss 0.3496
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0544 (0.0217)	Prec@1 98.438 (99.639)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0399 (0.0219)	Prec@1 98.438 (99.624)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0254 (0.0221)	Prec@1 99.219 (99.613)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0200 (0.0220)	Prec@1 100.000 (99.632)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0119 (0.0221)	Prec@1 100.000 (99.645)	
Total train loss: 0.0221

 * Prec@1 91.210 Prec@5 99.520 Loss 0.3562
Best acc: 91.390
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0104 (0.0212)	Prec@1 100.000 (99.700)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0178 (0.0211)	Prec@1 100.000 (99.720)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0139 (0.0209)	Prec@1 100.000 (99.703)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0139 (0.0209)	Prec@1 100.000 (99.690)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0128 (0.0210)	Prec@1 100.000 (99.679)	
Total train loss: 0.0210

 * Prec@1 91.430 Prec@5 99.460 Loss 0.3560
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0093 (0.0220)	Prec@1 100.000 (99.710)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0344 (0.0218)	Prec@1 99.219 (99.695)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0329 (0.0211)	Prec@1 98.438 (99.703)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0155 (0.0214)	Prec@1 99.219 (99.677)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0627 (0.0215)	Prec@1 96.875 (99.681)	
Total train loss: 0.0215

 * Prec@1 91.200 Prec@5 99.440 Loss 0.3535
Best acc: 91.430
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0328 (0.0221)	Prec@1 100.000 (99.649)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0264 (0.0212)	Prec@1 99.219 (99.649)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0166 (0.0213)	Prec@1 100.000 (99.646)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0132 (0.0213)	Prec@1 100.000 (99.667)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0281 (0.0219)	Prec@1 99.219 (99.639)	
Total train loss: 0.0219

 * Prec@1 91.420 Prec@5 99.480 Loss 0.3516
Best acc: 91.430
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 90.130 Prec@5 99.400 Loss 0.3926
Pre-trained Prec@1 with 5 layers frozen: 90.12999725341797 	 Loss: 0.392578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.0243 (0.0517)	Prec@1 99.219 (98.197)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0263 (0.0509)	Prec@1 99.219 (98.277)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.0577 (0.0487)	Prec@1 97.656 (98.334)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.0562 (0.0479)	Prec@1 99.219 (98.397)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.0342 (0.0487)	Prec@1 98.438 (98.363)	
Total train loss: 0.0487

 * Prec@1 91.140 Prec@5 99.440 Loss 0.3484
Best acc: 91.140
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0451 (0.0345)	Prec@1 98.438 (99.079)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0386 (0.0349)	Prec@1 99.219 (99.058)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0184 (0.0352)	Prec@1 100.000 (99.015)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0528 (0.0353)	Prec@1 98.438 (99.033)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0206 (0.0357)	Prec@1 99.219 (99.012)	
Total train loss: 0.0358

 * Prec@1 91.260 Prec@5 99.440 Loss 0.3491
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0254 (0.0282)	Prec@1 100.000 (99.339)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0113 (0.0303)	Prec@1 100.000 (99.254)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0157 (0.0306)	Prec@1 100.000 (99.279)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.1013 (0.0315)	Prec@1 96.094 (99.244)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0511 (0.0319)	Prec@1 98.438 (99.209)	
Total train loss: 0.0319

 * Prec@1 91.260 Prec@5 99.470 Loss 0.3503
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0350 (0.0285)	Prec@1 99.219 (99.399)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0240 (0.0286)	Prec@1 100.000 (99.394)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0272 (0.0287)	Prec@1 99.219 (99.399)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0169 (0.0285)	Prec@1 100.000 (99.399)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0337 (0.0281)	Prec@1 98.438 (99.395)	
Total train loss: 0.0281

 * Prec@1 91.170 Prec@5 99.410 Loss 0.3555
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0312 (0.0248)	Prec@1 99.219 (99.569)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0465 (0.0253)	Prec@1 98.438 (99.489)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0181 (0.0245)	Prec@1 100.000 (99.526)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0515 (0.0257)	Prec@1 97.656 (99.472)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0240 (0.0259)	Prec@1 100.000 (99.459)	
Total train loss: 0.0259

 * Prec@1 91.160 Prec@5 99.400 Loss 0.3586
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0569 (0.0250)	Prec@1 97.656 (99.559)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0308 (0.0239)	Prec@1 100.000 (99.584)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0160 (0.0234)	Prec@1 100.000 (99.613)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0175 (0.0232)	Prec@1 100.000 (99.609)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0090 (0.0237)	Prec@1 100.000 (99.583)	
Total train loss: 0.0237

 * Prec@1 91.150 Prec@5 99.340 Loss 0.3567
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0336 (0.0226)	Prec@1 99.219 (99.619)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0305 (0.0230)	Prec@1 100.000 (99.604)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0183 (0.0234)	Prec@1 100.000 (99.583)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0100 (0.0229)	Prec@1 100.000 (99.622)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0155 (0.0230)	Prec@1 100.000 (99.615)	
Total train loss: 0.0230

 * Prec@1 91.010 Prec@5 99.350 Loss 0.3582
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0184 (0.0239)	Prec@1 100.000 (99.589)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0194 (0.0238)	Prec@1 100.000 (99.609)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0187 (0.0234)	Prec@1 100.000 (99.623)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0427 (0.0229)	Prec@1 97.656 (99.624)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0228 (0.0228)	Prec@1 100.000 (99.629)	
Total train loss: 0.0228

 * Prec@1 91.090 Prec@5 99.370 Loss 0.3591
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0175 (0.0213)	Prec@1 99.219 (99.609)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0138 (0.0231)	Prec@1 100.000 (99.489)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0328 (0.0229)	Prec@1 98.438 (99.553)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0150 (0.0228)	Prec@1 100.000 (99.582)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0142 (0.0223)	Prec@1 100.000 (99.599)	
Total train loss: 0.0223

 * Prec@1 91.180 Prec@5 99.410 Loss 0.3625
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0171 (0.0211)	Prec@1 100.000 (99.669)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0090 (0.0226)	Prec@1 100.000 (99.649)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0175 (0.0224)	Prec@1 100.000 (99.666)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0189 (0.0222)	Prec@1 100.000 (99.664)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0364 (0.0223)	Prec@1 98.438 (99.643)	
Total train loss: 0.0223

 * Prec@1 91.220 Prec@5 99.400 Loss 0.3567
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0177 (0.0248)	Prec@1 100.000 (99.479)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0235 (0.0242)	Prec@1 99.219 (99.499)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0183 (0.0235)	Prec@1 100.000 (99.536)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0430 (0.0235)	Prec@1 99.219 (99.547)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0172 (0.0233)	Prec@1 100.000 (99.587)	
Total train loss: 0.0233

 * Prec@1 91.120 Prec@5 99.380 Loss 0.3572
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0135 (0.0220)	Prec@1 100.000 (99.589)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0209 (0.0223)	Prec@1 100.000 (99.624)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0220 (0.0225)	Prec@1 99.219 (99.609)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0325 (0.0223)	Prec@1 98.438 (99.634)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0236 (0.0223)	Prec@1 99.219 (99.629)	
Total train loss: 0.0223

 * Prec@1 91.050 Prec@5 99.390 Loss 0.3560
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0241 (0.0240)	Prec@1 99.219 (99.549)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0465 (0.0231)	Prec@1 99.219 (99.609)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0266 (0.0239)	Prec@1 99.219 (99.539)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0251 (0.0237)	Prec@1 98.438 (99.542)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0177 (0.0232)	Prec@1 99.219 (99.571)	
Total train loss: 0.0232

 * Prec@1 91.070 Prec@5 99.390 Loss 0.3604
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0208 (0.0243)	Prec@1 100.000 (99.479)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0307 (0.0245)	Prec@1 100.000 (99.539)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0270 (0.0238)	Prec@1 100.000 (99.569)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0143 (0.0234)	Prec@1 100.000 (99.592)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0210 (0.0237)	Prec@1 100.000 (99.569)	
Total train loss: 0.0238

 * Prec@1 91.200 Prec@5 99.400 Loss 0.3604
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0166 (0.0229)	Prec@1 100.000 (99.649)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0205 (0.0229)	Prec@1 99.219 (99.624)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0064 (0.0227)	Prec@1 100.000 (99.619)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0547 (0.0232)	Prec@1 99.219 (99.609)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0118 (0.0229)	Prec@1 100.000 (99.617)	
Total train loss: 0.0229

 * Prec@1 91.150 Prec@5 99.360 Loss 0.3594
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0161 (0.0207)	Prec@1 100.000 (99.639)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0300 (0.0220)	Prec@1 99.219 (99.644)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0118 (0.0226)	Prec@1 100.000 (99.626)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0198 (0.0226)	Prec@1 100.000 (99.644)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0236 (0.0227)	Prec@1 99.219 (99.637)	
Total train loss: 0.0227

 * Prec@1 91.080 Prec@5 99.360 Loss 0.3586
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0221 (0.0227)	Prec@1 100.000 (99.669)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0279 (0.0225)	Prec@1 99.219 (99.674)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0210 (0.0223)	Prec@1 99.219 (99.663)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0235 (0.0223)	Prec@1 99.219 (99.644)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0101 (0.0226)	Prec@1 100.000 (99.635)	
Total train loss: 0.0226

 * Prec@1 91.000 Prec@5 99.390 Loss 0.3582
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0278 (0.0248)	Prec@1 99.219 (99.589)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0140 (0.0230)	Prec@1 100.000 (99.609)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0106 (0.0227)	Prec@1 100.000 (99.619)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0196 (0.0222)	Prec@1 100.000 (99.644)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0078 (0.0225)	Prec@1 100.000 (99.633)	
Total train loss: 0.0225

 * Prec@1 91.110 Prec@5 99.400 Loss 0.3584
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0171 (0.0227)	Prec@1 100.000 (99.629)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0132 (0.0225)	Prec@1 100.000 (99.690)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0442 (0.0226)	Prec@1 99.219 (99.676)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0141 (0.0224)	Prec@1 100.000 (99.682)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0205 (0.0222)	Prec@1 99.219 (99.659)	
Total train loss: 0.0222

 * Prec@1 91.000 Prec@5 99.390 Loss 0.3623
Best acc: 91.260
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0158 (0.0239)	Prec@1 100.000 (99.579)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0202 (0.0228)	Prec@1 100.000 (99.584)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0223 (0.0230)	Prec@1 99.219 (99.599)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0137 (0.0225)	Prec@1 100.000 (99.612)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0108 (0.0227)	Prec@1 100.000 (99.593)	
Total train loss: 0.0227

 * Prec@1 91.100 Prec@5 99.390 Loss 0.3547
Best acc: 91.260
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 89.480 Prec@5 99.410 Loss 0.4285
Pre-trained Prec@1 with 7 layers frozen: 89.47999572753906 	 Loss: 0.428466796875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.0959 (0.0637)	Prec@1 96.094 (97.867)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0412 (0.0608)	Prec@1 99.219 (98.057)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.0163 (0.0588)	Prec@1 100.000 (98.134)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.0244 (0.0563)	Prec@1 100.000 (98.240)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.0849 (0.0548)	Prec@1 96.094 (98.281)	
Total train loss: 0.0548

 * Prec@1 91.230 Prec@5 99.430 Loss 0.3525
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0256 (0.0397)	Prec@1 99.219 (98.918)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0629 (0.0393)	Prec@1 98.438 (98.918)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0380 (0.0401)	Prec@1 97.656 (98.858)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0327 (0.0401)	Prec@1 99.219 (98.851)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0797 (0.0403)	Prec@1 98.438 (98.820)	
Total train loss: 0.0403

 * Prec@1 91.220 Prec@5 99.420 Loss 0.3547
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0513 (0.0349)	Prec@1 99.219 (99.129)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0420 (0.0341)	Prec@1 98.438 (99.144)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0127 (0.0349)	Prec@1 100.000 (99.105)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0311 (0.0354)	Prec@1 97.656 (99.053)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0519 (0.0354)	Prec@1 97.656 (99.048)	
Total train loss: 0.0354

 * Prec@1 90.910 Prec@5 99.340 Loss 0.3601
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0221 (0.0294)	Prec@1 99.219 (99.329)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0223 (0.0297)	Prec@1 100.000 (99.319)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0593 (0.0296)	Prec@1 98.438 (99.326)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0261 (0.0302)	Prec@1 100.000 (99.264)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0530 (0.0302)	Prec@1 99.219 (99.275)	
Total train loss: 0.0302

 * Prec@1 91.170 Prec@5 99.380 Loss 0.3569
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0177 (0.0255)	Prec@1 100.000 (99.479)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0158 (0.0247)	Prec@1 100.000 (99.569)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0273 (0.0255)	Prec@1 99.219 (99.516)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0213 (0.0258)	Prec@1 99.219 (99.502)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0188 (0.0264)	Prec@1 100.000 (99.487)	
Total train loss: 0.0265

 * Prec@1 90.970 Prec@5 99.330 Loss 0.3633
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0126 (0.0245)	Prec@1 100.000 (99.569)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0073 (0.0244)	Prec@1 100.000 (99.509)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0202 (0.0247)	Prec@1 99.219 (99.506)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0281 (0.0247)	Prec@1 99.219 (99.504)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0246 (0.0244)	Prec@1 99.219 (99.519)	
Total train loss: 0.0244

 * Prec@1 91.140 Prec@5 99.340 Loss 0.3616
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0208 (0.0280)	Prec@1 100.000 (99.399)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0220 (0.0260)	Prec@1 100.000 (99.484)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0285 (0.0256)	Prec@1 99.219 (99.503)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0244 (0.0245)	Prec@1 100.000 (99.577)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0267 (0.0241)	Prec@1 100.000 (99.585)	
Total train loss: 0.0241

 * Prec@1 91.210 Prec@5 99.320 Loss 0.3582
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0268 (0.0273)	Prec@1 98.438 (99.429)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0260 (0.0257)	Prec@1 100.000 (99.519)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0276 (0.0251)	Prec@1 100.000 (99.553)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0302 (0.0248)	Prec@1 100.000 (99.564)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0320 (0.0248)	Prec@1 98.438 (99.555)	
Total train loss: 0.0248

 * Prec@1 90.960 Prec@5 99.360 Loss 0.3660
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0446 (0.0263)	Prec@1 99.219 (99.509)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0274 (0.0244)	Prec@1 99.219 (99.574)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0167 (0.0250)	Prec@1 100.000 (99.543)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0229 (0.0250)	Prec@1 100.000 (99.559)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0333 (0.0246)	Prec@1 98.438 (99.569)	
Total train loss: 0.0246

 * Prec@1 91.080 Prec@5 99.350 Loss 0.3674
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0180 (0.0233)	Prec@1 100.000 (99.639)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0335 (0.0250)	Prec@1 100.000 (99.549)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0333 (0.0249)	Prec@1 99.219 (99.546)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0312 (0.0254)	Prec@1 99.219 (99.532)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0197 (0.0250)	Prec@1 99.219 (99.545)	
Total train loss: 0.0250

 * Prec@1 91.010 Prec@5 99.350 Loss 0.3665
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0171 (0.0230)	Prec@1 100.000 (99.639)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0116 (0.0227)	Prec@1 100.000 (99.634)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0270 (0.0234)	Prec@1 100.000 (99.626)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0165 (0.0236)	Prec@1 100.000 (99.607)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0200 (0.0241)	Prec@1 100.000 (99.591)	
Total train loss: 0.0242

 * Prec@1 91.060 Prec@5 99.370 Loss 0.3604
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0224 (0.0243)	Prec@1 100.000 (99.559)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0179 (0.0252)	Prec@1 100.000 (99.469)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0211 (0.0248)	Prec@1 99.219 (99.529)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0290 (0.0245)	Prec@1 98.438 (99.549)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0450 (0.0242)	Prec@1 98.438 (99.569)	
Total train loss: 0.0243

 * Prec@1 91.130 Prec@5 99.260 Loss 0.3657
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0234 (0.0206)	Prec@1 100.000 (99.750)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0153 (0.0231)	Prec@1 100.000 (99.654)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0393 (0.0233)	Prec@1 99.219 (99.633)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0569 (0.0234)	Prec@1 97.656 (99.624)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0290 (0.0234)	Prec@1 99.219 (99.643)	
Total train loss: 0.0235

 * Prec@1 91.090 Prec@5 99.330 Loss 0.3635
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0148 (0.0252)	Prec@1 100.000 (99.539)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0357 (0.0252)	Prec@1 98.438 (99.534)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0509 (0.0246)	Prec@1 98.438 (99.576)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0111 (0.0248)	Prec@1 100.000 (99.567)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0293 (0.0243)	Prec@1 99.219 (99.577)	
Total train loss: 0.0243

 * Prec@1 91.140 Prec@5 99.340 Loss 0.3611
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0157 (0.0254)	Prec@1 100.000 (99.489)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0177 (0.0245)	Prec@1 100.000 (99.529)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0488 (0.0247)	Prec@1 96.875 (99.516)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0242 (0.0246)	Prec@1 100.000 (99.542)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0194 (0.0244)	Prec@1 99.219 (99.559)	
Total train loss: 0.0243

 * Prec@1 91.090 Prec@5 99.330 Loss 0.3638
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0239 (0.0260)	Prec@1 99.219 (99.529)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0703 (0.0254)	Prec@1 96.875 (99.534)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0141 (0.0241)	Prec@1 100.000 (99.573)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0305 (0.0240)	Prec@1 99.219 (99.577)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0255 (0.0239)	Prec@1 99.219 (99.569)	
Total train loss: 0.0239

 * Prec@1 91.110 Prec@5 99.340 Loss 0.3616
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0494 (0.0227)	Prec@1 99.219 (99.639)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0110 (0.0224)	Prec@1 100.000 (99.690)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0169 (0.0229)	Prec@1 100.000 (99.609)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0280 (0.0231)	Prec@1 99.219 (99.612)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0056 (0.0233)	Prec@1 100.000 (99.597)	
Total train loss: 0.0233

 * Prec@1 91.120 Prec@5 99.330 Loss 0.3604
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0155 (0.0237)	Prec@1 100.000 (99.619)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0251 (0.0244)	Prec@1 99.219 (99.559)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0155 (0.0242)	Prec@1 99.219 (99.553)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0258 (0.0234)	Prec@1 100.000 (99.597)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0142 (0.0236)	Prec@1 100.000 (99.579)	
Total train loss: 0.0236

 * Prec@1 90.990 Prec@5 99.340 Loss 0.3650
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0158 (0.0255)	Prec@1 100.000 (99.489)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0140 (0.0249)	Prec@1 100.000 (99.554)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0165 (0.0248)	Prec@1 100.000 (99.553)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0158 (0.0249)	Prec@1 100.000 (99.544)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0655 (0.0246)	Prec@1 98.438 (99.553)	
Total train loss: 0.0246

 * Prec@1 91.120 Prec@5 99.380 Loss 0.3599
Best acc: 91.230
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0356 (0.0238)	Prec@1 99.219 (99.609)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0292 (0.0234)	Prec@1 100.000 (99.569)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0196 (0.0239)	Prec@1 100.000 (99.586)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0131 (0.0239)	Prec@1 100.000 (99.567)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0134 (0.0242)	Prec@1 100.000 (99.557)	
Total train loss: 0.0243

 * Prec@1 90.960 Prec@5 99.360 Loss 0.3669
Best acc: 91.230
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 86.060 Prec@5 98.980 Loss 0.5830
Pre-trained Prec@1 with 9 layers frozen: 86.05999755859375 	 Loss: 0.5830078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.1082 (0.0969)	Prec@1 96.875 (96.585)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0493 (0.0905)	Prec@1 97.656 (96.770)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.1298 (0.0846)	Prec@1 94.531 (96.972)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.0367 (0.0813)	Prec@1 99.219 (97.105)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.1421 (0.0795)	Prec@1 96.094 (97.177)	
Total train loss: 0.0794

 * Prec@1 91.020 Prec@5 99.410 Loss 0.3523
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0513 (0.0503)	Prec@1 98.438 (98.468)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0593 (0.0506)	Prec@1 97.656 (98.448)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0740 (0.0521)	Prec@1 96.875 (98.387)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0669 (0.0520)	Prec@1 98.438 (98.375)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0478 (0.0517)	Prec@1 96.875 (98.369)	
Total train loss: 0.0517

 * Prec@1 91.020 Prec@5 99.430 Loss 0.3594
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0349 (0.0415)	Prec@1 98.438 (98.858)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0294 (0.0410)	Prec@1 99.219 (98.873)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0334 (0.0409)	Prec@1 98.438 (98.888)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0421 (0.0417)	Prec@1 98.438 (98.826)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0580 (0.0426)	Prec@1 96.875 (98.798)	
Total train loss: 0.0426

 * Prec@1 90.850 Prec@5 99.310 Loss 0.3638
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0160 (0.0372)	Prec@1 100.000 (99.028)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0209 (0.0363)	Prec@1 100.000 (99.099)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0341 (0.0361)	Prec@1 99.219 (99.122)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0213 (0.0368)	Prec@1 100.000 (99.079)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0403 (0.0363)	Prec@1 98.438 (99.089)	
Total train loss: 0.0363

 * Prec@1 90.940 Prec@5 99.300 Loss 0.3679
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0211 (0.0319)	Prec@1 99.219 (99.309)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0241 (0.0323)	Prec@1 100.000 (99.259)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0274 (0.0329)	Prec@1 100.000 (99.265)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0539 (0.0326)	Prec@1 97.656 (99.276)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0651 (0.0330)	Prec@1 96.875 (99.249)	
Total train loss: 0.0330

 * Prec@1 91.060 Prec@5 99.280 Loss 0.3677
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0221 (0.0289)	Prec@1 100.000 (99.359)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0275 (0.0286)	Prec@1 99.219 (99.409)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0190 (0.0286)	Prec@1 100.000 (99.409)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0351 (0.0286)	Prec@1 99.219 (99.444)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0196 (0.0286)	Prec@1 100.000 (99.427)	
Total train loss: 0.0286

 * Prec@1 90.820 Prec@5 99.300 Loss 0.3684
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0296 (0.0304)	Prec@1 99.219 (99.399)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0103 (0.0291)	Prec@1 100.000 (99.434)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0318 (0.0285)	Prec@1 99.219 (99.429)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0349 (0.0289)	Prec@1 99.219 (99.424)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0432 (0.0291)	Prec@1 99.219 (99.425)	
Total train loss: 0.0291

 * Prec@1 91.000 Prec@5 99.290 Loss 0.3716
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0113 (0.0310)	Prec@1 100.000 (99.349)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0435 (0.0287)	Prec@1 97.656 (99.469)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0447 (0.0281)	Prec@1 97.656 (99.499)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0375 (0.0279)	Prec@1 99.219 (99.492)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0217 (0.0279)	Prec@1 100.000 (99.487)	
Total train loss: 0.0280

 * Prec@1 90.910 Prec@5 99.290 Loss 0.3721
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0212 (0.0307)	Prec@1 100.000 (99.349)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0267 (0.0284)	Prec@1 100.000 (99.474)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0166 (0.0281)	Prec@1 100.000 (99.472)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0296 (0.0287)	Prec@1 99.219 (99.454)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0341 (0.0288)	Prec@1 99.219 (99.429)	
Total train loss: 0.0288

 * Prec@1 90.920 Prec@5 99.250 Loss 0.3674
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0247 (0.0297)	Prec@1 99.219 (99.409)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0240 (0.0296)	Prec@1 99.219 (99.429)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0224 (0.0291)	Prec@1 100.000 (99.429)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0213 (0.0288)	Prec@1 99.219 (99.447)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0197 (0.0289)	Prec@1 99.219 (99.441)	
Total train loss: 0.0289

 * Prec@1 90.880 Prec@5 99.310 Loss 0.3708
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0188 (0.0299)	Prec@1 99.219 (99.409)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0303 (0.0285)	Prec@1 99.219 (99.444)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0228 (0.0273)	Prec@1 99.219 (99.496)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0209 (0.0279)	Prec@1 99.219 (99.454)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0199 (0.0280)	Prec@1 100.000 (99.453)	
Total train loss: 0.0280

 * Prec@1 90.970 Prec@5 99.290 Loss 0.3716
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0301 (0.0267)	Prec@1 99.219 (99.549)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0154 (0.0280)	Prec@1 100.000 (99.464)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0330 (0.0287)	Prec@1 100.000 (99.432)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0203 (0.0291)	Prec@1 100.000 (99.402)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0240 (0.0289)	Prec@1 100.000 (99.409)	
Total train loss: 0.0289

 * Prec@1 90.830 Prec@5 99.320 Loss 0.3743
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0398 (0.0277)	Prec@1 100.000 (99.579)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0248 (0.0288)	Prec@1 100.000 (99.439)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0402 (0.0283)	Prec@1 98.438 (99.472)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0191 (0.0282)	Prec@1 100.000 (99.479)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0273 (0.0284)	Prec@1 100.000 (99.455)	
Total train loss: 0.0285

 * Prec@1 90.910 Prec@5 99.230 Loss 0.3721
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0259 (0.0280)	Prec@1 100.000 (99.519)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0208 (0.0291)	Prec@1 100.000 (99.424)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0389 (0.0280)	Prec@1 98.438 (99.476)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0204 (0.0279)	Prec@1 99.219 (99.499)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0518 (0.0280)	Prec@1 96.875 (99.489)	
Total train loss: 0.0280

 * Prec@1 90.840 Prec@5 99.230 Loss 0.3716
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0248 (0.0284)	Prec@1 100.000 (99.479)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0174 (0.0283)	Prec@1 100.000 (99.464)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0287 (0.0282)	Prec@1 100.000 (99.499)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0339 (0.0287)	Prec@1 99.219 (99.464)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0241 (0.0286)	Prec@1 99.219 (99.443)	
Total train loss: 0.0286

 * Prec@1 90.780 Prec@5 99.250 Loss 0.3711
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0171 (0.0271)	Prec@1 100.000 (99.509)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0224 (0.0275)	Prec@1 100.000 (99.519)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0145 (0.0277)	Prec@1 100.000 (99.519)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0300 (0.0276)	Prec@1 99.219 (99.507)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0170 (0.0274)	Prec@1 100.000 (99.511)	
Total train loss: 0.0274

 * Prec@1 90.830 Prec@5 99.330 Loss 0.3687
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0336 (0.0286)	Prec@1 99.219 (99.449)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0166 (0.0285)	Prec@1 100.000 (99.459)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0506 (0.0286)	Prec@1 99.219 (99.476)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0408 (0.0287)	Prec@1 97.656 (99.457)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0356 (0.0279)	Prec@1 100.000 (99.479)	
Total train loss: 0.0279

 * Prec@1 90.700 Prec@5 99.320 Loss 0.3687
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0243 (0.0288)	Prec@1 99.219 (99.419)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0189 (0.0281)	Prec@1 100.000 (99.449)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0116 (0.0282)	Prec@1 100.000 (99.483)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0155 (0.0281)	Prec@1 100.000 (99.507)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0260 (0.0282)	Prec@1 99.219 (99.497)	
Total train loss: 0.0282

 * Prec@1 91.030 Prec@5 99.310 Loss 0.3718
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0338 (0.0290)	Prec@1 100.000 (99.499)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0220 (0.0289)	Prec@1 100.000 (99.474)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0149 (0.0285)	Prec@1 100.000 (99.486)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0222 (0.0283)	Prec@1 99.219 (99.484)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0285 (0.0285)	Prec@1 100.000 (99.471)	
Total train loss: 0.0286

 * Prec@1 90.880 Prec@5 99.320 Loss 0.3723
Best acc: 91.060
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0128 (0.0286)	Prec@1 100.000 (99.469)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0269 (0.0286)	Prec@1 100.000 (99.484)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0199 (0.0283)	Prec@1 100.000 (99.466)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0156 (0.0279)	Prec@1 100.000 (99.479)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0164 (0.0282)	Prec@1 100.000 (99.465)	
Total train loss: 0.0283

 * Prec@1 90.920 Prec@5 99.330 Loss 0.3716
Best acc: 91.060
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 82.250 Prec@5 98.550 Loss 0.7559
Pre-trained Prec@1 with 11 layers frozen: 82.25 	 Loss: 0.755859375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.0949 (0.1096)	Prec@1 96.875 (96.114)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.0867 (0.0999)	Prec@1 96.875 (96.414)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.0954 (0.0967)	Prec@1 96.875 (96.575)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.0327 (0.0941)	Prec@1 100.000 (96.685)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.0542 (0.0900)	Prec@1 96.875 (96.839)	
Total train loss: 0.0899

 * Prec@1 90.890 Prec@5 99.470 Loss 0.3542
Best acc: 90.890
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.1019 (0.0587)	Prec@1 95.312 (98.047)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0671 (0.0592)	Prec@1 97.656 (98.037)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0911 (0.0610)	Prec@1 96.875 (97.973)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0778 (0.0614)	Prec@1 97.656 (97.957)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0547 (0.0611)	Prec@1 98.438 (97.969)	
Total train loss: 0.0611

 * Prec@1 90.910 Prec@5 99.480 Loss 0.3574
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0493 (0.0487)	Prec@1 98.438 (98.498)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0660 (0.0482)	Prec@1 98.438 (98.533)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.0587 (0.0490)	Prec@1 97.656 (98.454)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0414 (0.0489)	Prec@1 98.438 (98.485)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0338 (0.0492)	Prec@1 99.219 (98.464)	
Total train loss: 0.0492

 * Prec@1 90.730 Prec@5 99.540 Loss 0.3623
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0419 (0.0421)	Prec@1 98.438 (98.788)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0514 (0.0421)	Prec@1 98.438 (98.808)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0322 (0.0430)	Prec@1 99.219 (98.805)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0332 (0.0437)	Prec@1 100.000 (98.791)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0370 (0.0431)	Prec@1 100.000 (98.824)	
Total train loss: 0.0432

 * Prec@1 90.700 Prec@5 99.420 Loss 0.3682
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0312 (0.0378)	Prec@1 100.000 (99.119)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0952 (0.0379)	Prec@1 96.875 (99.079)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0492 (0.0382)	Prec@1 99.219 (99.055)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0610 (0.0381)	Prec@1 98.438 (99.066)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0338 (0.0382)	Prec@1 99.219 (99.040)	
Total train loss: 0.0382

 * Prec@1 90.870 Prec@5 99.380 Loss 0.3684
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0179 (0.0371)	Prec@1 100.000 (99.139)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0307 (0.0354)	Prec@1 98.438 (99.179)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0148 (0.0356)	Prec@1 100.000 (99.182)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0369 (0.0352)	Prec@1 98.438 (99.176)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0215 (0.0352)	Prec@1 100.000 (99.181)	
Total train loss: 0.0351

 * Prec@1 90.910 Prec@5 99.380 Loss 0.3638
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0319 (0.0326)	Prec@1 99.219 (99.329)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0138 (0.0340)	Prec@1 100.000 (99.259)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0586 (0.0339)	Prec@1 98.438 (99.269)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0246 (0.0333)	Prec@1 100.000 (99.289)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0371 (0.0333)	Prec@1 98.438 (99.279)	
Total train loss: 0.0333

 * Prec@1 90.680 Prec@5 99.410 Loss 0.3687
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0272 (0.0312)	Prec@1 100.000 (99.409)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0306 (0.0318)	Prec@1 99.219 (99.404)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0194 (0.0319)	Prec@1 100.000 (99.382)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0184 (0.0320)	Prec@1 100.000 (99.399)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0786 (0.0314)	Prec@1 97.656 (99.399)	
Total train loss: 0.0315

 * Prec@1 90.840 Prec@5 99.360 Loss 0.3699
Best acc: 90.910
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0283 (0.0344)	Prec@1 99.219 (99.219)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0609 (0.0342)	Prec@1 98.438 (99.244)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0219 (0.0330)	Prec@1 99.219 (99.306)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0136 (0.0325)	Prec@1 100.000 (99.314)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0305 (0.0325)	Prec@1 99.219 (99.319)	
Total train loss: 0.0325

 * Prec@1 90.950 Prec@5 99.370 Loss 0.3716
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0250 (0.0330)	Prec@1 100.000 (99.349)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0173 (0.0314)	Prec@1 100.000 (99.389)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0299 (0.0327)	Prec@1 99.219 (99.346)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0280 (0.0323)	Prec@1 99.219 (99.359)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0503 (0.0326)	Prec@1 97.656 (99.351)	
Total train loss: 0.0326

 * Prec@1 90.930 Prec@5 99.420 Loss 0.3691
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0517 (0.0334)	Prec@1 99.219 (99.329)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0261 (0.0329)	Prec@1 100.000 (99.379)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0345 (0.0330)	Prec@1 99.219 (99.379)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0336 (0.0322)	Prec@1 100.000 (99.397)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0171 (0.0320)	Prec@1 100.000 (99.389)	
Total train loss: 0.0321

 * Prec@1 90.830 Prec@5 99.390 Loss 0.3716
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0253 (0.0355)	Prec@1 99.219 (99.119)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0195 (0.0342)	Prec@1 100.000 (99.209)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0217 (0.0338)	Prec@1 100.000 (99.245)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0336 (0.0330)	Prec@1 100.000 (99.274)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0153 (0.0325)	Prec@1 100.000 (99.307)	
Total train loss: 0.0325

 * Prec@1 90.790 Prec@5 99.360 Loss 0.3682
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0248 (0.0318)	Prec@1 100.000 (99.339)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0561 (0.0327)	Prec@1 98.438 (99.359)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0693 (0.0334)	Prec@1 97.656 (99.302)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0286 (0.0325)	Prec@1 100.000 (99.334)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0222 (0.0320)	Prec@1 100.000 (99.355)	
Total train loss: 0.0320

 * Prec@1 90.720 Prec@5 99.370 Loss 0.3652
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0301 (0.0345)	Prec@1 100.000 (99.239)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0217 (0.0336)	Prec@1 100.000 (99.319)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0221 (0.0330)	Prec@1 100.000 (99.316)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0190 (0.0328)	Prec@1 100.000 (99.349)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0290 (0.0326)	Prec@1 99.219 (99.349)	
Total train loss: 0.0326

 * Prec@1 90.800 Prec@5 99.400 Loss 0.3682
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0466 (0.0323)	Prec@1 98.438 (99.419)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0167 (0.0316)	Prec@1 100.000 (99.399)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0326 (0.0315)	Prec@1 99.219 (99.409)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0368 (0.0317)	Prec@1 99.219 (99.392)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0282 (0.0317)	Prec@1 99.219 (99.405)	
Total train loss: 0.0316

 * Prec@1 90.910 Prec@5 99.420 Loss 0.3691
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0284 (0.0305)	Prec@1 99.219 (99.349)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0319 (0.0316)	Prec@1 99.219 (99.314)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0355 (0.0310)	Prec@1 99.219 (99.319)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0404 (0.0313)	Prec@1 98.438 (99.301)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0244 (0.0316)	Prec@1 99.219 (99.307)	
Total train loss: 0.0316

 * Prec@1 90.790 Prec@5 99.370 Loss 0.3660
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0245 (0.0313)	Prec@1 100.000 (99.409)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0635 (0.0324)	Prec@1 97.656 (99.314)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0378 (0.0321)	Prec@1 97.656 (99.346)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0262 (0.0329)	Prec@1 100.000 (99.301)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0225 (0.0322)	Prec@1 100.000 (99.323)	
Total train loss: 0.0323

 * Prec@1 90.890 Prec@5 99.420 Loss 0.3713
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0345 (0.0317)	Prec@1 99.219 (99.269)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0242 (0.0317)	Prec@1 100.000 (99.319)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0368 (0.0326)	Prec@1 100.000 (99.279)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0229 (0.0323)	Prec@1 100.000 (99.289)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0277 (0.0326)	Prec@1 99.219 (99.281)	
Total train loss: 0.0326

 * Prec@1 90.850 Prec@5 99.390 Loss 0.3716
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0350 (0.0326)	Prec@1 100.000 (99.379)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0364 (0.0326)	Prec@1 99.219 (99.349)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0184 (0.0321)	Prec@1 100.000 (99.339)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0397 (0.0323)	Prec@1 99.219 (99.351)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0153 (0.0324)	Prec@1 100.000 (99.345)	
Total train loss: 0.0324

 * Prec@1 90.900 Prec@5 99.400 Loss 0.3660
Best acc: 90.950
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0224 (0.0324)	Prec@1 100.000 (99.319)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0203 (0.0329)	Prec@1 100.000 (99.309)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0284 (0.0324)	Prec@1 99.219 (99.329)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0349 (0.0326)	Prec@1 99.219 (99.311)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0330 (0.0325)	Prec@1 100.000 (99.331)	
Total train loss: 0.0325

 * Prec@1 90.870 Prec@5 99.400 Loss 0.3689
Best acc: 90.950
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 78.730 Prec@5 97.980 Loss 0.9321
Pre-trained Prec@1 with 13 layers frozen: 78.72999572753906 	 Loss: 0.93212890625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.1588 (0.1422)	Prec@1 92.969 (95.182)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.1375 (0.1337)	Prec@1 94.531 (95.358)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.2175 (0.1260)	Prec@1 96.094 (95.690)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.1357 (0.1201)	Prec@1 95.312 (95.848)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.1360 (0.1176)	Prec@1 96.875 (95.952)	
Total train loss: 0.1177

 * Prec@1 90.580 Prec@5 99.380 Loss 0.3562
Best acc: 90.580
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.0981 (0.0846)	Prec@1 95.312 (97.055)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.0848 (0.0852)	Prec@1 95.312 (97.075)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.0710 (0.0806)	Prec@1 97.656 (97.266)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.0346 (0.0801)	Prec@1 98.438 (97.286)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.0550 (0.0783)	Prec@1 99.219 (97.362)	
Total train loss: 0.0782

 * Prec@1 90.870 Prec@5 99.410 Loss 0.3557
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.0451 (0.0580)	Prec@1 99.219 (98.327)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.0899 (0.0613)	Prec@1 97.656 (98.117)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.1124 (0.0616)	Prec@1 96.094 (98.110)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0934 (0.0626)	Prec@1 96.875 (98.044)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0239 (0.0631)	Prec@1 99.219 (98.009)	
Total train loss: 0.0631

 * Prec@1 90.780 Prec@5 99.390 Loss 0.3550
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.0591 (0.0506)	Prec@1 97.656 (98.558)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0460 (0.0544)	Prec@1 98.438 (98.327)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0775 (0.0540)	Prec@1 98.438 (98.361)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0398 (0.0538)	Prec@1 99.219 (98.367)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.0565 (0.0543)	Prec@1 97.656 (98.349)	
Total train loss: 0.0543

 * Prec@1 90.630 Prec@5 99.420 Loss 0.3669
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0597 (0.0451)	Prec@1 98.438 (98.788)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0338 (0.0453)	Prec@1 99.219 (98.818)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.0715 (0.0463)	Prec@1 96.875 (98.775)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0550 (0.0467)	Prec@1 97.656 (98.756)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0439 (0.0473)	Prec@1 98.438 (98.728)	
Total train loss: 0.0473

 * Prec@1 90.740 Prec@5 99.430 Loss 0.3660
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.0343 (0.0428)	Prec@1 99.219 (98.928)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0386 (0.0430)	Prec@1 98.438 (98.928)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0232 (0.0420)	Prec@1 100.000 (98.915)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0190 (0.0418)	Prec@1 100.000 (98.926)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0454 (0.0416)	Prec@1 100.000 (98.956)	
Total train loss: 0.0416

 * Prec@1 90.620 Prec@5 99.420 Loss 0.3635
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0291 (0.0418)	Prec@1 100.000 (99.048)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.0400 (0.0415)	Prec@1 99.219 (99.008)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0482 (0.0413)	Prec@1 99.219 (99.035)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0598 (0.0419)	Prec@1 96.875 (98.971)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0344 (0.0415)	Prec@1 100.000 (98.986)	
Total train loss: 0.0416

 * Prec@1 90.650 Prec@5 99.400 Loss 0.3660
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0251 (0.0416)	Prec@1 99.219 (98.998)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.0310 (0.0417)	Prec@1 99.219 (99.013)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0497 (0.0409)	Prec@1 99.219 (99.025)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0479 (0.0412)	Prec@1 100.000 (99.028)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0499 (0.0406)	Prec@1 98.438 (99.040)	
Total train loss: 0.0406

 * Prec@1 90.590 Prec@5 99.440 Loss 0.3660
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0533 (0.0407)	Prec@1 98.438 (98.928)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0427 (0.0404)	Prec@1 98.438 (98.948)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0334 (0.0401)	Prec@1 100.000 (98.988)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0387 (0.0400)	Prec@1 98.438 (98.986)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0413 (0.0409)	Prec@1 99.219 (98.938)	
Total train loss: 0.0409

 * Prec@1 90.720 Prec@5 99.400 Loss 0.3669
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0473 (0.0386)	Prec@1 99.219 (99.069)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0239 (0.0394)	Prec@1 100.000 (99.069)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0202 (0.0412)	Prec@1 100.000 (99.005)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0302 (0.0405)	Prec@1 99.219 (99.018)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0219 (0.0404)	Prec@1 99.219 (99.006)	
Total train loss: 0.0404

 * Prec@1 90.750 Prec@5 99.390 Loss 0.3647
Best acc: 90.870
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0285 (0.0394)	Prec@1 100.000 (99.038)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0208 (0.0400)	Prec@1 100.000 (99.053)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0602 (0.0396)	Prec@1 98.438 (99.042)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0340 (0.0403)	Prec@1 99.219 (98.991)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0659 (0.0403)	Prec@1 96.875 (99.016)	
Total train loss: 0.0403

 * Prec@1 91.020 Prec@5 99.350 Loss 0.3640
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0275 (0.0379)	Prec@1 100.000 (99.139)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0328 (0.0383)	Prec@1 100.000 (99.084)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.0248 (0.0399)	Prec@1 100.000 (99.002)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0622 (0.0396)	Prec@1 97.656 (99.016)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0263 (0.0399)	Prec@1 99.219 (98.996)	
Total train loss: 0.0399

 * Prec@1 90.710 Prec@5 99.350 Loss 0.3667
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.0356 (0.0423)	Prec@1 99.219 (98.878)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0257 (0.0415)	Prec@1 100.000 (98.903)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0271 (0.0399)	Prec@1 99.219 (98.995)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0405 (0.0399)	Prec@1 99.219 (98.996)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.0188 (0.0408)	Prec@1 100.000 (98.952)	
Total train loss: 0.0409

 * Prec@1 90.740 Prec@5 99.380 Loss 0.3640
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0786 (0.0445)	Prec@1 96.875 (98.848)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0269 (0.0414)	Prec@1 100.000 (98.958)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0559 (0.0411)	Prec@1 97.656 (98.988)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0436 (0.0403)	Prec@1 98.438 (99.023)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0472 (0.0410)	Prec@1 97.656 (98.994)	
Total train loss: 0.0410

 * Prec@1 90.670 Prec@5 99.380 Loss 0.3687
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0379 (0.0406)	Prec@1 98.438 (99.018)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0316 (0.0419)	Prec@1 98.438 (98.908)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0307 (0.0411)	Prec@1 100.000 (98.948)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0446 (0.0415)	Prec@1 99.219 (98.953)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0130 (0.0414)	Prec@1 100.000 (98.972)	
Total train loss: 0.0414

 * Prec@1 90.800 Prec@5 99.400 Loss 0.3667
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0570 (0.0401)	Prec@1 98.438 (99.018)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0253 (0.0396)	Prec@1 99.219 (99.074)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0387 (0.0399)	Prec@1 99.219 (99.058)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0392 (0.0404)	Prec@1 98.438 (99.041)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0587 (0.0401)	Prec@1 99.219 (99.058)	
Total train loss: 0.0400

 * Prec@1 90.560 Prec@5 99.400 Loss 0.3713
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0290 (0.0418)	Prec@1 100.000 (98.838)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0314 (0.0412)	Prec@1 100.000 (98.948)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0670 (0.0408)	Prec@1 96.094 (98.938)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0459 (0.0406)	Prec@1 99.219 (98.971)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0246 (0.0407)	Prec@1 99.219 (98.980)	
Total train loss: 0.0407

 * Prec@1 90.620 Prec@5 99.400 Loss 0.3662
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0287 (0.0423)	Prec@1 100.000 (98.928)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0302 (0.0407)	Prec@1 99.219 (99.013)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0511 (0.0408)	Prec@1 98.438 (99.018)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0529 (0.0400)	Prec@1 98.438 (99.084)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0356 (0.0402)	Prec@1 99.219 (99.075)	
Total train loss: 0.0401

 * Prec@1 90.720 Prec@5 99.390 Loss 0.3657
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0240 (0.0388)	Prec@1 100.000 (99.089)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0750 (0.0400)	Prec@1 98.438 (99.013)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0301 (0.0404)	Prec@1 100.000 (98.982)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0275 (0.0407)	Prec@1 100.000 (98.988)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0501 (0.0404)	Prec@1 97.656 (99.014)	
Total train loss: 0.0404

 * Prec@1 90.850 Prec@5 99.450 Loss 0.3669
Best acc: 91.020
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0334 (0.0392)	Prec@1 99.219 (99.099)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0399 (0.0396)	Prec@1 99.219 (99.069)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0351 (0.0410)	Prec@1 98.438 (99.008)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0486 (0.0408)	Prec@1 99.219 (99.033)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0286 (0.0407)	Prec@1 100.000 (99.030)	
Total train loss: 0.0408

 * Prec@1 90.660 Prec@5 99.380 Loss 0.3669
Best acc: 91.020
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 75.210 Prec@5 97.830 Loss 1.0303
Pre-trained Prec@1 with 15 layers frozen: 75.20999908447266 	 Loss: 1.0302734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.2037 (0.2882)	Prec@1 92.188 (90.825)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.2546 (0.2567)	Prec@1 90.625 (91.732)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.2771 (0.2377)	Prec@1 91.406 (92.244)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.1281 (0.2282)	Prec@1 96.094 (92.433)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.1631 (0.2187)	Prec@1 95.312 (92.732)	
Total train loss: 0.2186

 * Prec@1 89.070 Prec@5 99.560 Loss 0.4092
Best acc: 89.070
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.2057 (0.1401)	Prec@1 95.312 (95.323)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.1030 (0.1416)	Prec@1 96.094 (95.107)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.1121 (0.1434)	Prec@1 94.531 (95.075)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.1157 (0.1418)	Prec@1 96.094 (95.130)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.1378 (0.1411)	Prec@1 95.312 (95.148)	
Total train loss: 0.1413

 * Prec@1 89.120 Prec@5 99.440 Loss 0.3948
Best acc: 89.120
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.1050 (0.1199)	Prec@1 96.094 (95.843)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.1096 (0.1171)	Prec@1 94.531 (95.964)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.1059 (0.1164)	Prec@1 96.094 (95.957)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.0493 (0.1177)	Prec@1 98.438 (95.984)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.0882 (0.1170)	Prec@1 97.656 (96.042)	
Total train loss: 0.1171

 * Prec@1 89.380 Prec@5 99.430 Loss 0.3914
Best acc: 89.380
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.1228 (0.0992)	Prec@1 96.094 (96.745)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.0943 (0.1009)	Prec@1 96.875 (96.680)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.1627 (0.1002)	Prec@1 96.875 (96.668)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.0891 (0.1021)	Prec@1 97.656 (96.557)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.1036 (0.1008)	Prec@1 96.875 (96.615)	
Total train loss: 0.1008

 * Prec@1 89.320 Prec@5 99.380 Loss 0.3972
Best acc: 89.380
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.0840 (0.0857)	Prec@1 96.875 (97.356)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.0648 (0.0884)	Prec@1 98.438 (97.110)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.1013 (0.0882)	Prec@1 96.094 (97.159)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.0629 (0.0880)	Prec@1 98.438 (97.183)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.0767 (0.0878)	Prec@1 96.875 (97.181)	
Total train loss: 0.0878

 * Prec@1 89.580 Prec@5 99.370 Loss 0.3936
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.1085 (0.0799)	Prec@1 96.094 (97.526)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.0486 (0.0783)	Prec@1 98.438 (97.631)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.0709 (0.0770)	Prec@1 98.438 (97.650)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.0643 (0.0775)	Prec@1 96.875 (97.641)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.0455 (0.0763)	Prec@1 100.000 (97.718)	
Total train loss: 0.0763

 * Prec@1 89.530 Prec@5 99.340 Loss 0.3987
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.0620 (0.0728)	Prec@1 98.438 (97.837)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.1098 (0.0712)	Prec@1 94.531 (97.877)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.0646 (0.0742)	Prec@1 97.656 (97.770)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.0814 (0.0743)	Prec@1 97.656 (97.741)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.0685 (0.0742)	Prec@1 97.656 (97.762)	
Total train loss: 0.0742

 * Prec@1 89.540 Prec@5 99.320 Loss 0.3992
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.0699 (0.0742)	Prec@1 97.656 (97.746)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.1267 (0.0731)	Prec@1 96.094 (97.766)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0830 (0.0724)	Prec@1 96.094 (97.823)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.0464 (0.0728)	Prec@1 99.219 (97.779)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.0523 (0.0731)	Prec@1 97.656 (97.770)	
Total train loss: 0.0731

 * Prec@1 89.530 Prec@5 99.410 Loss 0.3958
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1469 (0.0770)	Prec@1 95.312 (97.686)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0829 (0.0748)	Prec@1 97.656 (97.776)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.0574 (0.0739)	Prec@1 98.438 (97.766)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.0977 (0.0731)	Prec@1 97.656 (97.814)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.0980 (0.0730)	Prec@1 96.875 (97.819)	
Total train loss: 0.0730

 * Prec@1 89.450 Prec@5 99.350 Loss 0.4048
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0940 (0.0710)	Prec@1 98.438 (98.017)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.0663 (0.0710)	Prec@1 98.438 (97.967)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.0622 (0.0718)	Prec@1 98.438 (97.893)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0519 (0.0711)	Prec@1 98.438 (97.914)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.0529 (0.0716)	Prec@1 98.438 (97.887)	
Total train loss: 0.0716

 * Prec@1 89.550 Prec@5 99.360 Loss 0.3962
Best acc: 89.580
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.0561 (0.0720)	Prec@1 98.438 (97.726)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0772 (0.0704)	Prec@1 97.656 (97.872)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.0573 (0.0690)	Prec@1 96.875 (97.980)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.1144 (0.0703)	Prec@1 96.094 (97.947)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.0646 (0.0706)	Prec@1 98.438 (97.913)	
Total train loss: 0.0706

 * Prec@1 89.610 Prec@5 99.360 Loss 0.3975
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.1057 (0.0703)	Prec@1 96.094 (97.877)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.0600 (0.0710)	Prec@1 97.656 (97.832)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.1116 (0.0714)	Prec@1 94.531 (97.843)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.0293 (0.0716)	Prec@1 100.000 (97.837)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0477 (0.0717)	Prec@1 99.219 (97.865)	
Total train loss: 0.0719

 * Prec@1 89.370 Prec@5 99.390 Loss 0.4026
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.1106 (0.0702)	Prec@1 94.531 (97.847)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0994 (0.0709)	Prec@1 96.875 (97.872)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.0723 (0.0695)	Prec@1 97.656 (97.940)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.0559 (0.0703)	Prec@1 100.000 (97.929)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.1116 (0.0704)	Prec@1 96.094 (97.941)	
Total train loss: 0.0704

 * Prec@1 89.570 Prec@5 99.380 Loss 0.4021
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.0815 (0.0697)	Prec@1 96.875 (97.857)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.0391 (0.0694)	Prec@1 99.219 (97.907)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.0867 (0.0694)	Prec@1 97.656 (97.920)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.0591 (0.0695)	Prec@1 98.438 (97.902)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.0355 (0.0691)	Prec@1 99.219 (97.955)	
Total train loss: 0.0691

 * Prec@1 89.600 Prec@5 99.380 Loss 0.3989
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.0856 (0.0740)	Prec@1 98.438 (97.756)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.0521 (0.0731)	Prec@1 99.219 (97.741)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0819 (0.0718)	Prec@1 96.875 (97.843)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.0799 (0.0715)	Prec@1 97.656 (97.844)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.0546 (0.0715)	Prec@1 98.438 (97.841)	
Total train loss: 0.0715

 * Prec@1 89.360 Prec@5 99.340 Loss 0.4016
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.0540 (0.0717)	Prec@1 98.438 (97.806)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.0460 (0.0704)	Prec@1 99.219 (97.882)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.0489 (0.0699)	Prec@1 98.438 (97.933)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0361 (0.0713)	Prec@1 99.219 (97.879)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.0749 (0.0711)	Prec@1 98.438 (97.889)	
Total train loss: 0.0711

 * Prec@1 89.580 Prec@5 99.380 Loss 0.3972
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.1054 (0.0696)	Prec@1 96.094 (98.057)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.0682 (0.0709)	Prec@1 98.438 (97.992)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.0925 (0.0707)	Prec@1 94.531 (97.993)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0676 (0.0710)	Prec@1 96.875 (97.994)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.0401 (0.0708)	Prec@1 99.219 (98.005)	
Total train loss: 0.0708

 * Prec@1 89.380 Prec@5 99.380 Loss 0.3989
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.0660 (0.0707)	Prec@1 99.219 (97.967)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.0625 (0.0686)	Prec@1 98.438 (98.032)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.0484 (0.0692)	Prec@1 99.219 (98.017)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.0455 (0.0704)	Prec@1 98.438 (97.992)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.0626 (0.0705)	Prec@1 98.438 (97.969)	
Total train loss: 0.0705

 * Prec@1 89.540 Prec@5 99.390 Loss 0.3992
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.0771 (0.0756)	Prec@1 97.656 (97.837)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.0568 (0.0735)	Prec@1 97.656 (97.882)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.0388 (0.0709)	Prec@1 99.219 (97.947)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.0823 (0.0709)	Prec@1 98.438 (97.939)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.0436 (0.0704)	Prec@1 99.219 (97.951)	
Total train loss: 0.0704

 * Prec@1 89.470 Prec@5 99.390 Loss 0.3970
Best acc: 89.610
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.0598 (0.0737)	Prec@1 98.438 (97.736)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.0845 (0.0711)	Prec@1 99.219 (97.892)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.0728 (0.0708)	Prec@1 97.656 (97.910)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.0506 (0.0712)	Prec@1 99.219 (97.902)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.0659 (0.0719)	Prec@1 98.438 (97.885)	
Total train loss: 0.0719

 * Prec@1 89.540 Prec@5 99.370 Loss 0.3977
Best acc: 89.610
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 74.150 Prec@5 97.770 Loss 1.0439
Pre-trained Prec@1 with 17 layers frozen: 74.1500015258789 	 Loss: 1.0439453125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.3076 (0.3824)	Prec@1 90.625 (88.171)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.2183 (0.3477)	Prec@1 92.969 (88.992)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.2263 (0.3220)	Prec@1 91.406 (89.597)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.2856 (0.3101)	Prec@1 92.969 (89.966)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.2490 (0.2987)	Prec@1 89.844 (90.284)	
Total train loss: 0.2984

 * Prec@1 87.730 Prec@5 99.410 Loss 0.4502
Best acc: 87.730
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.2844 (0.2142)	Prec@1 89.844 (92.648)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.1438 (0.2130)	Prec@1 94.531 (92.513)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.2352 (0.2176)	Prec@1 92.969 (92.411)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.2632 (0.2184)	Prec@1 90.625 (92.420)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.1674 (0.2209)	Prec@1 94.531 (92.320)	
Total train loss: 0.2209

 * Prec@1 88.060 Prec@5 99.460 Loss 0.4258
Best acc: 88.060
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.2295 (0.1926)	Prec@1 92.188 (93.089)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.1570 (0.1906)	Prec@1 96.094 (93.214)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.2324 (0.1964)	Prec@1 92.969 (93.106)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.3154 (0.1955)	Prec@1 90.625 (93.157)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.2003 (0.1949)	Prec@1 91.406 (93.181)	
Total train loss: 0.1949

 * Prec@1 88.080 Prec@5 99.440 Loss 0.4241
Best acc: 88.080
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.2262 (0.1766)	Prec@1 94.531 (93.970)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.1965 (0.1698)	Prec@1 92.188 (94.106)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.0981 (0.1741)	Prec@1 96.094 (93.997)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.1667 (0.1785)	Prec@1 95.312 (93.855)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.1522 (0.1798)	Prec@1 95.312 (93.774)	
Total train loss: 0.1797

 * Prec@1 88.570 Prec@5 99.460 Loss 0.4089
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.1420 (0.1651)	Prec@1 96.094 (94.331)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.1874 (0.1689)	Prec@1 92.188 (94.181)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.1345 (0.1668)	Prec@1 95.312 (94.187)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.1976 (0.1655)	Prec@1 92.188 (94.253)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.1749 (0.1678)	Prec@1 94.531 (94.199)	
Total train loss: 0.1678

 * Prec@1 88.270 Prec@5 99.420 Loss 0.4209
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.1334 (0.1569)	Prec@1 95.312 (94.531)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.1780 (0.1565)	Prec@1 94.531 (94.601)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.2169 (0.1538)	Prec@1 92.969 (94.705)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.1055 (0.1543)	Prec@1 96.875 (94.734)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.1102 (0.1524)	Prec@1 95.312 (94.772)	
Total train loss: 0.1524

 * Prec@1 88.430 Prec@5 99.390 Loss 0.4202
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.1056 (0.1454)	Prec@1 95.312 (95.152)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.2285 (0.1504)	Prec@1 93.750 (94.947)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.2054 (0.1511)	Prec@1 92.188 (94.842)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.1779 (0.1514)	Prec@1 92.188 (94.829)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.1674 (0.1514)	Prec@1 93.750 (94.864)	
Total train loss: 0.1516

 * Prec@1 88.550 Prec@5 99.410 Loss 0.4141
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2026 (0.1519)	Prec@1 94.531 (94.822)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.1508 (0.1525)	Prec@1 94.531 (94.717)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.0621 (0.1501)	Prec@1 97.656 (94.802)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.1118 (0.1502)	Prec@1 96.094 (94.839)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2549 (0.1494)	Prec@1 91.406 (94.830)	
Total train loss: 0.1495

 * Prec@1 88.480 Prec@5 99.400 Loss 0.4114
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.0826 (0.1553)	Prec@1 97.656 (94.852)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.0800 (0.1492)	Prec@1 97.656 (94.922)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.1702 (0.1495)	Prec@1 94.531 (94.915)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.1438 (0.1494)	Prec@1 93.750 (94.864)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.1622 (0.1492)	Prec@1 92.188 (94.872)	
Total train loss: 0.1492

 * Prec@1 88.530 Prec@5 99.400 Loss 0.4189
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.0986 (0.1451)	Prec@1 97.656 (95.192)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.1227 (0.1463)	Prec@1 95.312 (95.077)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.1113 (0.1466)	Prec@1 95.312 (95.126)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.0899 (0.1459)	Prec@1 97.656 (95.137)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.1092 (0.1477)	Prec@1 97.656 (95.060)	
Total train loss: 0.1480

 * Prec@1 88.330 Prec@5 99.380 Loss 0.4153
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.1826 (0.1490)	Prec@1 94.531 (94.952)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.0909 (0.1503)	Prec@1 97.656 (94.977)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2058 (0.1472)	Prec@1 93.750 (95.082)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.0948 (0.1465)	Prec@1 97.656 (95.117)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.1772 (0.1470)	Prec@1 94.531 (95.046)	
Total train loss: 0.1471

 * Prec@1 88.270 Prec@5 99.410 Loss 0.4214
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.0872 (0.1495)	Prec@1 98.438 (94.962)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.1203 (0.1473)	Prec@1 96.875 (94.997)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.1248 (0.1492)	Prec@1 96.875 (94.932)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.1819 (0.1473)	Prec@1 93.750 (95.022)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.0846 (0.1461)	Prec@1 96.875 (95.072)	
Total train loss: 0.1461

 * Prec@1 88.540 Prec@5 99.410 Loss 0.4153
Best acc: 88.570
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.1768 (0.1447)	Prec@1 94.531 (95.302)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.0998 (0.1438)	Prec@1 96.875 (95.147)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.1995 (0.1454)	Prec@1 92.969 (95.042)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.1257 (0.1450)	Prec@1 95.312 (95.037)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.1654 (0.1468)	Prec@1 93.750 (95.018)	
Total train loss: 0.1468

 * Prec@1 88.630 Prec@5 99.400 Loss 0.4158
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.1620 (0.1496)	Prec@1 92.969 (94.792)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.1409 (0.1489)	Prec@1 94.531 (94.862)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.2058 (0.1473)	Prec@1 92.188 (95.012)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.1244 (0.1468)	Prec@1 96.875 (95.020)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.1328 (0.1460)	Prec@1 95.312 (95.044)	
Total train loss: 0.1459

 * Prec@1 88.390 Prec@5 99.360 Loss 0.4197
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.1375 (0.1456)	Prec@1 96.094 (94.912)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.1998 (0.1470)	Prec@1 91.406 (94.927)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.0932 (0.1445)	Prec@1 96.875 (95.052)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.1296 (0.1454)	Prec@1 94.531 (95.067)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.1891 (0.1459)	Prec@1 92.969 (95.036)	
Total train loss: 0.1461

 * Prec@1 88.410 Prec@5 99.370 Loss 0.4224
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.1891 (0.1436)	Prec@1 94.531 (95.262)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.1517 (0.1474)	Prec@1 92.969 (95.042)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.1224 (0.1477)	Prec@1 96.875 (95.005)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.0730 (0.1463)	Prec@1 98.438 (95.030)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.2292 (0.1474)	Prec@1 90.625 (95.006)	
Total train loss: 0.1475

 * Prec@1 88.330 Prec@5 99.370 Loss 0.4221
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.0939 (0.1448)	Prec@1 97.656 (95.152)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.1858 (0.1442)	Prec@1 91.406 (95.102)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2303 (0.1462)	Prec@1 91.406 (94.989)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.0973 (0.1453)	Prec@1 98.438 (95.002)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.1403 (0.1453)	Prec@1 93.750 (95.032)	
Total train loss: 0.1452

 * Prec@1 88.520 Prec@5 99.390 Loss 0.4136
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.1343 (0.1515)	Prec@1 94.531 (94.702)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.1707 (0.1488)	Prec@1 90.625 (94.842)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.1953 (0.1484)	Prec@1 94.531 (94.919)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.1151 (0.1492)	Prec@1 96.875 (94.882)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.1499 (0.1488)	Prec@1 94.531 (94.890)	
Total train loss: 0.1487

 * Prec@1 88.340 Prec@5 99.380 Loss 0.4204
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.1265 (0.1451)	Prec@1 96.094 (95.002)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.1163 (0.1419)	Prec@1 96.094 (95.187)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.1470 (0.1443)	Prec@1 92.188 (95.052)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.1050 (0.1460)	Prec@1 96.875 (94.942)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.1449 (0.1464)	Prec@1 95.312 (94.978)	
Total train loss: 0.1463

 * Prec@1 88.580 Prec@5 99.370 Loss 0.4192
Best acc: 88.630
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.1360 (0.1460)	Prec@1 95.312 (94.972)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.1757 (0.1470)	Prec@1 96.094 (94.877)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.1108 (0.1456)	Prec@1 96.094 (95.029)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.1215 (0.1475)	Prec@1 96.875 (94.962)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.2295 (0.1467)	Prec@1 92.969 (95.038)	
Total train loss: 0.1466

 * Prec@1 88.510 Prec@5 99.390 Loss 0.4148
Best acc: 88.630
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
 * Prec@1 73.620 Prec@5 97.920 Loss 1.0078
Pre-trained Prec@1 with 19 layers frozen: 73.6199951171875 	 Loss: 1.0078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.5205 (0.5076)	Prec@1 82.031 (84.525)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.3503 (0.4718)	Prec@1 87.500 (85.332)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.3665 (0.4511)	Prec@1 92.188 (85.751)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.5186 (0.4430)	Prec@1 84.375 (86.028)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.5698 (0.4327)	Prec@1 84.375 (86.256)	
Total train loss: 0.4326

 * Prec@1 85.370 Prec@5 99.090 Loss 0.5425
Best acc: 85.370
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.2678 (0.3711)	Prec@1 91.406 (88.141)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.4358 (0.3672)	Prec@1 83.594 (88.026)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.3872 (0.3736)	Prec@1 85.938 (87.704)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.3420 (0.3718)	Prec@1 86.719 (87.735)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.3035 (0.3695)	Prec@1 89.844 (87.758)	
Total train loss: 0.3694

 * Prec@1 86.050 Prec@5 99.100 Loss 0.4978
Best acc: 86.050
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.4495 (0.3587)	Prec@1 83.594 (87.991)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.3730 (0.3630)	Prec@1 87.500 (87.735)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.2576 (0.3598)	Prec@1 91.406 (87.851)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.2903 (0.3568)	Prec@1 89.844 (87.916)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.4172 (0.3544)	Prec@1 84.375 (88.037)	
Total train loss: 0.3545

 * Prec@1 86.080 Prec@5 99.160 Loss 0.4812
Best acc: 86.080
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.2756 (0.3381)	Prec@1 89.062 (88.421)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.4990 (0.3448)	Prec@1 81.250 (88.296)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.2756 (0.3461)	Prec@1 89.062 (88.258)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.3398 (0.3432)	Prec@1 88.281 (88.319)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.2915 (0.3424)	Prec@1 89.844 (88.365)	
Total train loss: 0.3425

 * Prec@1 86.390 Prec@5 99.170 Loss 0.4668
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.3274 (0.3375)	Prec@1 88.281 (88.421)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.2517 (0.3469)	Prec@1 90.625 (88.121)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.3672 (0.3381)	Prec@1 87.500 (88.442)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.3130 (0.3347)	Prec@1 87.500 (88.622)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.3311 (0.3369)	Prec@1 89.062 (88.508)	
Total train loss: 0.3369

 * Prec@1 86.300 Prec@5 99.270 Loss 0.4580
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.3120 (0.3468)	Prec@1 89.062 (88.091)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3237 (0.3391)	Prec@1 89.062 (88.276)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.3535 (0.3358)	Prec@1 85.938 (88.411)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.3203 (0.3334)	Prec@1 89.062 (88.431)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.4189 (0.3310)	Prec@1 86.719 (88.542)	
Total train loss: 0.3311

 * Prec@1 86.390 Prec@5 99.220 Loss 0.4641
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4478 (0.3401)	Prec@1 82.812 (88.502)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3462 (0.3318)	Prec@1 85.938 (88.622)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3342 (0.3341)	Prec@1 89.844 (88.488)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.3062 (0.3308)	Prec@1 91.406 (88.634)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.3040 (0.3314)	Prec@1 88.281 (88.596)	
Total train loss: 0.3315

 * Prec@1 86.290 Prec@5 99.220 Loss 0.4656
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4077 (0.3387)	Prec@1 84.375 (88.081)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.2832 (0.3392)	Prec@1 90.625 (88.296)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.2983 (0.3330)	Prec@1 89.844 (88.495)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.3811 (0.3334)	Prec@1 86.719 (88.494)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2164 (0.3327)	Prec@1 95.312 (88.540)	
Total train loss: 0.3331

 * Prec@1 86.270 Prec@5 99.210 Loss 0.4612
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.1709 (0.3274)	Prec@1 96.094 (88.642)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.2472 (0.3354)	Prec@1 90.625 (88.652)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.2167 (0.3326)	Prec@1 92.969 (88.762)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.3035 (0.3341)	Prec@1 86.719 (88.669)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2162 (0.3339)	Prec@1 91.406 (88.626)	
Total train loss: 0.3341

 * Prec@1 86.120 Prec@5 99.160 Loss 0.4685
Best acc: 86.390
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3735 (0.3372)	Prec@1 87.500 (88.161)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3145 (0.3392)	Prec@1 87.500 (88.286)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.2524 (0.3347)	Prec@1 91.406 (88.472)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.2222 (0.3313)	Prec@1 90.625 (88.634)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3296 (0.3318)	Prec@1 89.844 (88.656)	
Total train loss: 0.3320

 * Prec@1 86.400 Prec@5 99.160 Loss 0.4592
Best acc: 86.400
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.3179 (0.3409)	Prec@1 90.625 (88.542)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.2893 (0.3343)	Prec@1 88.281 (88.677)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.3452 (0.3335)	Prec@1 89.844 (88.655)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4038 (0.3305)	Prec@1 85.938 (88.785)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3066 (0.3322)	Prec@1 88.281 (88.666)	
Total train loss: 0.3323

 * Prec@1 86.260 Prec@5 99.190 Loss 0.4636
Best acc: 86.400
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.2220 (0.3285)	Prec@1 90.625 (88.712)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3997 (0.3402)	Prec@1 89.062 (88.251)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3237 (0.3361)	Prec@1 88.281 (88.452)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.3022 (0.3313)	Prec@1 92.188 (88.627)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.3296 (0.3325)	Prec@1 88.281 (88.598)	
Total train loss: 0.3325

 * Prec@1 86.330 Prec@5 99.190 Loss 0.4634
Best acc: 86.400
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2715 (0.3279)	Prec@1 89.062 (88.872)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3477 (0.3291)	Prec@1 89.062 (88.742)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2688 (0.3321)	Prec@1 89.844 (88.582)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.2473 (0.3323)	Prec@1 89.062 (88.544)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3745 (0.3322)	Prec@1 90.625 (88.584)	
Total train loss: 0.3320

 * Prec@1 86.410 Prec@5 99.200 Loss 0.4612
Best acc: 86.410
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3970 (0.3382)	Prec@1 83.594 (88.331)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3630 (0.3343)	Prec@1 90.625 (88.592)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3564 (0.3363)	Prec@1 89.062 (88.502)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.2852 (0.3378)	Prec@1 91.406 (88.489)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.4099 (0.3342)	Prec@1 85.938 (88.600)	
Total train loss: 0.3344

 * Prec@1 86.440 Prec@5 99.200 Loss 0.4622
Best acc: 86.440
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.3064 (0.3378)	Prec@1 89.844 (88.351)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.3206 (0.3322)	Prec@1 89.062 (88.637)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.2354 (0.3316)	Prec@1 91.406 (88.492)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.3569 (0.3285)	Prec@1 83.594 (88.579)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.2277 (0.3321)	Prec@1 90.625 (88.502)	
Total train loss: 0.3322

 * Prec@1 86.300 Prec@5 99.200 Loss 0.4617
Best acc: 86.440
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2220 (0.3283)	Prec@1 92.969 (88.712)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.2468 (0.3341)	Prec@1 91.406 (88.577)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.3779 (0.3298)	Prec@1 85.156 (88.712)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.3076 (0.3315)	Prec@1 88.281 (88.682)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.2859 (0.3305)	Prec@1 88.281 (88.708)	
Total train loss: 0.3306

 * Prec@1 86.310 Prec@5 99.110 Loss 0.4612
Best acc: 86.440
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.3557 (0.3311)	Prec@1 85.156 (88.632)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.4214 (0.3296)	Prec@1 85.156 (88.647)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2915 (0.3293)	Prec@1 92.188 (88.709)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2236 (0.3299)	Prec@1 92.969 (88.652)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.3943 (0.3323)	Prec@1 85.938 (88.602)	
Total train loss: 0.3323

 * Prec@1 86.450 Prec@5 99.190 Loss 0.4600
Best acc: 86.450
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.3367 (0.3346)	Prec@1 89.844 (88.592)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.3323 (0.3339)	Prec@1 91.406 (88.562)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.4023 (0.3342)	Prec@1 84.375 (88.612)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.1509 (0.3304)	Prec@1 95.312 (88.782)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.3579 (0.3326)	Prec@1 86.719 (88.610)	
Total train loss: 0.3325

 * Prec@1 86.270 Prec@5 99.200 Loss 0.4617
Best acc: 86.450
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.3071 (0.3228)	Prec@1 92.188 (89.083)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3916 (0.3311)	Prec@1 86.719 (88.652)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.3396 (0.3320)	Prec@1 89.062 (88.612)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.4041 (0.3334)	Prec@1 84.375 (88.577)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.3418 (0.3333)	Prec@1 90.625 (88.576)	
Total train loss: 0.3330

 * Prec@1 86.380 Prec@5 99.170 Loss 0.4604
Best acc: 86.450
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.3518 (0.3334)	Prec@1 89.062 (88.682)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.3596 (0.3373)	Prec@1 85.938 (88.627)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.2588 (0.3350)	Prec@1 89.844 (88.582)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.2825 (0.3340)	Prec@1 89.844 (88.572)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.4626 (0.3321)	Prec@1 88.281 (88.556)	
Total train loss: 0.3323

 * Prec@1 86.360 Prec@5 99.170 Loss 0.4609
Best acc: 86.450
--------------------------------------------------------------------------------
