
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 49.990 Loss inf
Pre-trained Prec@1 with 5 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.8950 (1.2141)	Prec@1 67.578 (59.896)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.7202 (1.0031)	Prec@1 73.047 (66.341)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.5967 (0.9016)	Prec@1 79.688 (69.571)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.6812 (0.8377)	Prec@1 76.172 (71.792)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.6641 (0.7967)	Prec@1 74.219 (73.065)	
Total train loss: 0.7965

Train time: 152.8585181236267
 * Prec@1 68.890 Prec@5 97.400 Loss 0.8799
Best acc: 68.890
--------------------------------------------------------------------------------
Test time: 160.3188397884369

Epoch: [1][38/196]	LR: 0.1	Loss 0.4739 (0.5463)	Prec@1 81.641 (81.220)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.5952 (0.5592)	Prec@1 80.859 (80.744)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.4290 (0.5583)	Prec@1 85.156 (80.783)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.5273 (0.5534)	Prec@1 82.031 (80.972)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.5259 (0.5507)	Prec@1 82.812 (81.094)	
Total train loss: 0.5507

Train time: 38.44321012496948
 * Prec@1 59.320 Prec@5 93.660 Loss 1.1953
Best acc: 68.890
--------------------------------------------------------------------------------
Test time: 50.58533072471619

Epoch: [2][38/196]	LR: 0.1	Loss 0.4407 (0.5056)	Prec@1 85.156 (82.752)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.5107 (0.4874)	Prec@1 83.203 (83.248)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.4678 (0.4814)	Prec@1 84.766 (83.484)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.3994 (0.4799)	Prec@1 85.156 (83.464)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.4338 (0.4747)	Prec@1 83.984 (83.632)	
Total train loss: 0.4750

Train time: 47.25603532791138
 * Prec@1 71.720 Prec@5 97.460 Loss 0.8232
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 53.38947677612305

Epoch: [3][38/196]	LR: 0.1	Loss 0.4285 (0.4221)	Prec@1 88.281 (85.657)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.4592 (0.4299)	Prec@1 82.422 (85.357)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.4680 (0.4321)	Prec@1 86.328 (85.263)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4509 (0.4330)	Prec@1 82.812 (85.199)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.4021 (0.4393)	Prec@1 86.719 (85.006)	
Total train loss: 0.4393

Train time: 39.748151540756226
 * Prec@1 70.380 Prec@5 97.230 Loss 0.8433
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 46.383702993392944

Epoch: [4][38/196]	LR: 0.1	Loss 0.5034 (0.4038)	Prec@1 83.984 (86.128)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.4600 (0.3998)	Prec@1 83.594 (86.258)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.5083 (0.4007)	Prec@1 80.859 (86.288)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4570 (0.3992)	Prec@1 82.812 (86.321)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.3809 (0.4008)	Prec@1 88.281 (86.330)	
Total train loss: 0.4007

Train time: 42.395530223846436
 * Prec@1 64.830 Prec@5 95.990 Loss 1.0469
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 52.66867685317993

Epoch: [5][38/196]	LR: 0.1	Loss 0.3684 (0.3576)	Prec@1 87.500 (87.750)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.3613 (0.3461)	Prec@1 86.328 (88.111)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.3708 (0.3431)	Prec@1 88.281 (88.218)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.3774 (0.3444)	Prec@1 88.281 (88.101)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.3225 (0.3428)	Prec@1 90.234 (88.145)	
Total train loss: 0.3430

Train time: 23.8724205493927
 * Prec@1 53.020 Prec@5 91.730 Loss 1.4004
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 27.007133960723877

Epoch: [6][38/196]	LR: 0.1	Loss 0.3066 (0.3061)	Prec@1 87.891 (89.153)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2494 (0.3052)	Prec@1 91.016 (89.313)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.3394 (0.3107)	Prec@1 87.500 (89.163)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.3115 (0.3143)	Prec@1 88.672 (89.047)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.3005 (0.3205)	Prec@1 91.016 (88.866)	
Total train loss: 0.3209

Train time: 18.456640005111694
 * Prec@1 69.350 Prec@5 96.310 Loss 1.0557
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 21.904428958892822

Epoch: [7][38/196]	LR: 0.1	Loss 0.3242 (0.3035)	Prec@1 87.891 (89.663)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.3850 (0.3136)	Prec@1 84.375 (89.348)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2661 (0.3096)	Prec@1 90.234 (89.386)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.3525 (0.3118)	Prec@1 87.109 (89.305)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3525 (0.3160)	Prec@1 85.547 (89.137)	
Total train loss: 0.3160

Train time: 17.168857097625732
 * Prec@1 45.910 Prec@5 89.640 Loss 2.1699
Best acc: 71.720
--------------------------------------------------------------------------------
Test time: 21.370904684066772

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2937 (0.2757)	Prec@1 90.234 (90.805)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2803 (0.2635)	Prec@1 91.406 (91.316)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2249 (0.2630)	Prec@1 92.188 (91.343)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2593 (0.2631)	Prec@1 90.234 (91.254)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2362 (0.2603)	Prec@1 92.578 (91.344)	
Total train loss: 0.2603

Train time: 18.45321750640869
 * Prec@1 80.250 Prec@5 98.820 Loss 0.6074
Best acc: 80.250
--------------------------------------------------------------------------------
Test time: 21.856866598129272

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2686 (0.2483)	Prec@1 91.016 (91.737)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2332 (0.2465)	Prec@1 92.188 (91.977)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2429 (0.2443)	Prec@1 92.969 (92.024)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2981 (0.2445)	Prec@1 89.844 (91.987)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2169 (0.2440)	Prec@1 92.969 (92.019)	
Total train loss: 0.2441

Train time: 18.047048568725586
 * Prec@1 80.500 Prec@5 98.790 Loss 0.6030
Best acc: 80.500
--------------------------------------------------------------------------------
Test time: 21.509528398513794

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2346 (0.2469)	Prec@1 91.797 (92.057)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3020 (0.2471)	Prec@1 90.234 (91.987)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1650 (0.2422)	Prec@1 96.094 (92.124)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2277 (0.2429)	Prec@1 91.016 (92.092)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2316 (0.2405)	Prec@1 92.578 (92.181)	
Total train loss: 0.2404

Train time: 16.804256200790405
 * Prec@1 80.690 Prec@5 98.830 Loss 0.6006
Best acc: 80.690
--------------------------------------------------------------------------------
Test time: 20.96457076072693

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2064 (0.2360)	Prec@1 93.750 (92.548)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.2054 (0.2338)	Prec@1 94.531 (92.568)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2507 (0.2360)	Prec@1 91.406 (92.441)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2937 (0.2363)	Prec@1 89.453 (92.355)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2018 (0.2367)	Prec@1 94.141 (92.330)	
Total train loss: 0.2368

Train time: 17.71017813682556
 * Prec@1 80.150 Prec@5 98.830 Loss 0.6226
Best acc: 80.690
--------------------------------------------------------------------------------
Test time: 21.027541160583496

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.3127 (0.2306)	Prec@1 89.453 (92.408)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2345 (0.2311)	Prec@1 89.453 (92.388)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2290 (0.2314)	Prec@1 92.969 (92.394)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3152 (0.2341)	Prec@1 88.672 (92.265)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2423 (0.2358)	Prec@1 91.016 (92.204)	
Total train loss: 0.2359

Train time: 17.44907307624817
 * Prec@1 80.740 Prec@5 98.840 Loss 0.6079
Best acc: 80.740
--------------------------------------------------------------------------------
Test time: 20.59948706626892

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2194 (0.2286)	Prec@1 93.359 (92.578)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2233 (0.2305)	Prec@1 93.359 (92.548)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2812 (0.2322)	Prec@1 90.625 (92.374)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.2131 (0.2348)	Prec@1 92.969 (92.270)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2378 (0.2349)	Prec@1 92.969 (92.264)	
Total train loss: 0.2352

Train time: 16.87305736541748
 * Prec@1 80.920 Prec@5 98.890 Loss 0.5947
Best acc: 80.920
--------------------------------------------------------------------------------
Test time: 21.304592847824097

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1472 (0.2382)	Prec@1 95.703 (92.318)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2219 (0.2368)	Prec@1 93.359 (92.167)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2656 (0.2369)	Prec@1 91.406 (92.198)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2778 (0.2338)	Prec@1 89.844 (92.295)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2314 (0.2349)	Prec@1 91.406 (92.272)	
Total train loss: 0.2349

Train time: 16.762808799743652
 * Prec@1 80.380 Prec@5 98.910 Loss 0.6201
Best acc: 80.920
--------------------------------------------------------------------------------
Test time: 20.076998710632324

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2605 (0.2368)	Prec@1 91.797 (92.328)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2385 (0.2345)	Prec@1 92.578 (92.448)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2211 (0.2355)	Prec@1 91.406 (92.321)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2283 (0.2364)	Prec@1 91.406 (92.165)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.3103 (0.2363)	Prec@1 89.453 (92.196)	
Total train loss: 0.2365

Train time: 17.73873734474182
 * Prec@1 80.100 Prec@5 98.860 Loss 0.6172
Best acc: 80.920
--------------------------------------------------------------------------------
Test time: 20.94324827194214

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1656 (0.2330)	Prec@1 94.141 (92.368)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.2264 (0.2314)	Prec@1 92.969 (92.433)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2529 (0.2338)	Prec@1 89.453 (92.294)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2185 (0.2352)	Prec@1 92.969 (92.225)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2227 (0.2338)	Prec@1 91.797 (92.288)	
Total train loss: 0.2337

Train time: 17.177849531173706
 * Prec@1 80.320 Prec@5 98.850 Loss 0.6162
Best acc: 80.920
--------------------------------------------------------------------------------
Test time: 21.54947590827942

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2461 (0.2351)	Prec@1 92.578 (92.348)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2939 (0.2301)	Prec@1 89.844 (92.453)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1902 (0.2345)	Prec@1 94.141 (92.234)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2495 (0.2333)	Prec@1 92.188 (92.295)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2458 (0.2325)	Prec@1 92.188 (92.290)	
Total train loss: 0.2327

Train time: 17.107728719711304
 * Prec@1 80.490 Prec@5 98.870 Loss 0.6177
Best acc: 80.920
--------------------------------------------------------------------------------
Test time: 20.43199872970581

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2139 (0.2296)	Prec@1 93.750 (92.508)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.2328 (0.2326)	Prec@1 92.969 (92.478)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2179 (0.2292)	Prec@1 92.188 (92.511)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1875 (0.2326)	Prec@1 95.312 (92.435)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.2622 (0.2339)	Prec@1 91.406 (92.338)	
Total train loss: 0.2337

Train time: 18.105584621429443
 * Prec@1 81.160 Prec@5 98.910 Loss 0.5908
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 21.315710067749023

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2151 (0.2284)	Prec@1 92.969 (92.548)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2281 (0.2269)	Prec@1 93.750 (92.563)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1575 (0.2283)	Prec@1 95.312 (92.561)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2629 (0.2310)	Prec@1 90.234 (92.443)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.2205 (0.2315)	Prec@1 91.406 (92.396)	
Total train loss: 0.2314

Train time: 18.384238958358765
 * Prec@1 80.360 Prec@5 98.860 Loss 0.6245
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 22.968410968780518

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2610 (0.2358)	Prec@1 92.188 (92.298)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.2374 (0.2318)	Prec@1 91.406 (92.198)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2389 (0.2331)	Prec@1 92.578 (92.211)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2534 (0.2318)	Prec@1 90.625 (92.343)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.2255 (0.2324)	Prec@1 93.359 (92.336)	
Total train loss: 0.2326

Train time: 17.27219843864441
 * Prec@1 79.960 Prec@5 98.880 Loss 0.6255
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 20.66650652885437

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2507 (0.2335)	Prec@1 91.797 (92.328)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2695 (0.2351)	Prec@1 92.188 (92.398)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2434 (0.2365)	Prec@1 92.188 (92.208)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2302 (0.2342)	Prec@1 91.797 (92.305)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1990 (0.2321)	Prec@1 93.359 (92.422)	
Total train loss: 0.2319

Train time: 17.441446542739868
 * Prec@1 80.080 Prec@5 98.830 Loss 0.6211
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 20.70124077796936

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1973 (0.2290)	Prec@1 94.531 (92.598)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1880 (0.2309)	Prec@1 94.141 (92.433)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2478 (0.2331)	Prec@1 90.625 (92.271)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1993 (0.2337)	Prec@1 93.359 (92.230)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2023 (0.2325)	Prec@1 93.359 (92.282)	
Total train loss: 0.2326

Train time: 17.949707984924316
 * Prec@1 80.460 Prec@5 98.820 Loss 0.6162
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 22.010207653045654

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.2654 (0.2312)	Prec@1 90.625 (92.198)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.3052 (0.2318)	Prec@1 89.844 (92.358)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2108 (0.2317)	Prec@1 93.359 (92.338)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1631 (0.2309)	Prec@1 96.484 (92.368)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2788 (0.2315)	Prec@1 92.969 (92.394)	
Total train loss: 0.2315

Train time: 17.655681371688843
 * Prec@1 80.680 Prec@5 98.890 Loss 0.6074
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 20.75897479057312

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1940 (0.2349)	Prec@1 94.141 (92.398)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2815 (0.2386)	Prec@1 90.234 (92.182)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2078 (0.2350)	Prec@1 93.359 (92.304)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.3135 (0.2315)	Prec@1 88.672 (92.415)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.2473 (0.2325)	Prec@1 93.359 (92.398)	
Total train loss: 0.2325

Train time: 16.094800233840942
 * Prec@1 80.410 Prec@5 98.900 Loss 0.6182
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.177240133285522

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2181 (0.2352)	Prec@1 93.750 (92.388)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2379 (0.2288)	Prec@1 91.406 (92.578)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2532 (0.2315)	Prec@1 90.625 (92.478)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2035 (0.2328)	Prec@1 93.359 (92.398)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1755 (0.2318)	Prec@1 93.359 (92.350)	
Total train loss: 0.2318

Train time: 16.10852313041687
 * Prec@1 80.500 Prec@5 98.860 Loss 0.6152
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.397515058517456

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1689 (0.2270)	Prec@1 94.531 (92.568)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1932 (0.2289)	Prec@1 93.359 (92.403)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1990 (0.2309)	Prec@1 92.188 (92.401)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.2527 (0.2314)	Prec@1 92.969 (92.368)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.2396 (0.2315)	Prec@1 91.016 (92.394)	
Total train loss: 0.2317

Train time: 16.402621507644653
 * Prec@1 80.890 Prec@5 98.900 Loss 0.6079
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 20.227344512939453

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1790 (0.2275)	Prec@1 94.531 (92.528)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.2793 (0.2316)	Prec@1 89.062 (92.318)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2656 (0.2308)	Prec@1 90.234 (92.321)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2195 (0.2320)	Prec@1 94.141 (92.340)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2008 (0.2335)	Prec@1 93.750 (92.282)	
Total train loss: 0.2336

Train time: 16.43712019920349
 * Prec@1 80.300 Prec@5 98.860 Loss 0.6221
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.581244230270386

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2317 (0.2300)	Prec@1 91.797 (92.548)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2273 (0.2287)	Prec@1 92.188 (92.568)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2900 (0.2309)	Prec@1 91.016 (92.485)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.3308 (0.2315)	Prec@1 88.281 (92.415)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.2089 (0.2318)	Prec@1 92.578 (92.394)	
Total train loss: 0.2319

Train time: 16.066896438598633
 * Prec@1 80.370 Prec@5 98.850 Loss 0.6162
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 18.916999101638794

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2388 (0.2279)	Prec@1 92.578 (92.298)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2910 (0.2318)	Prec@1 88.672 (92.258)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2457 (0.2324)	Prec@1 92.188 (92.208)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2578 (0.2318)	Prec@1 91.406 (92.333)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.2162 (0.2326)	Prec@1 93.359 (92.340)	
Total train loss: 0.2326

Train time: 15.833940744400024
 * Prec@1 80.190 Prec@5 98.810 Loss 0.6250
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.230759143829346

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.2109 (0.2400)	Prec@1 93.750 (92.268)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.2307 (0.2342)	Prec@1 92.188 (92.368)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.2456 (0.2337)	Prec@1 90.625 (92.398)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.2031 (0.2336)	Prec@1 92.969 (92.333)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2216 (0.2331)	Prec@1 92.578 (92.330)	
Total train loss: 0.2330

Train time: 15.97306513786316
 * Prec@1 80.280 Prec@5 98.870 Loss 0.6226
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.690133810043335

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.2479 (0.2298)	Prec@1 92.188 (92.448)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.2588 (0.2322)	Prec@1 90.234 (92.403)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1776 (0.2314)	Prec@1 93.750 (92.415)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.2130 (0.2317)	Prec@1 91.797 (92.418)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.2600 (0.2322)	Prec@1 92.188 (92.398)	
Total train loss: 0.2322

Train time: 16.346409797668457
 * Prec@1 79.910 Prec@5 98.750 Loss 0.6284
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.34329628944397

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.2430 (0.2380)	Prec@1 90.625 (92.047)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1957 (0.2352)	Prec@1 94.531 (92.193)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.2008 (0.2342)	Prec@1 93.750 (92.268)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.2074 (0.2334)	Prec@1 93.359 (92.320)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.2490 (0.2325)	Prec@1 89.453 (92.292)	
Total train loss: 0.2325

Train time: 16.16917848587036
 * Prec@1 80.390 Prec@5 98.850 Loss 0.6216
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.126068830490112

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.2198 (0.2341)	Prec@1 92.188 (92.338)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.2220 (0.2356)	Prec@1 92.578 (92.152)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.2590 (0.2335)	Prec@1 90.625 (92.248)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.2191 (0.2331)	Prec@1 92.578 (92.253)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.2805 (0.2326)	Prec@1 89.453 (92.292)	
Total train loss: 0.2325

Train time: 16.092451333999634
 * Prec@1 79.970 Prec@5 98.830 Loss 0.6255
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.37699317932129

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.3184 (0.2393)	Prec@1 89.844 (91.947)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.2585 (0.2327)	Prec@1 91.406 (92.283)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.2488 (0.2305)	Prec@1 92.969 (92.388)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.2445 (0.2314)	Prec@1 92.188 (92.400)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.2407 (0.2320)	Prec@1 92.578 (92.392)	
Total train loss: 0.2323

Train time: 15.584627866744995
 * Prec@1 80.250 Prec@5 98.810 Loss 0.6245
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.179386854171753

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.2406 (0.2272)	Prec@1 92.969 (92.798)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.2471 (0.2324)	Prec@1 91.797 (92.203)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.2432 (0.2330)	Prec@1 92.188 (92.314)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1986 (0.2307)	Prec@1 94.141 (92.458)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.2727 (0.2321)	Prec@1 90.234 (92.434)	
Total train loss: 0.2322

Train time: 16.650959968566895
 * Prec@1 80.600 Prec@5 98.870 Loss 0.6128
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 19.65647268295288

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.2308 (0.2385)	Prec@1 93.750 (91.977)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.2285 (0.2288)	Prec@1 93.359 (92.378)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.2825 (0.2329)	Prec@1 91.797 (92.204)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1879 (0.2320)	Prec@1 93.750 (92.283)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1646 (0.2318)	Prec@1 94.922 (92.382)	
Total train loss: 0.2318

Train time: 16.024060249328613
 * Prec@1 80.820 Prec@5 98.850 Loss 0.6064
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 18.91725778579712

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.2357 (0.2251)	Prec@1 92.188 (92.618)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.1637 (0.2303)	Prec@1 96.094 (92.448)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.2742 (0.2346)	Prec@1 89.844 (92.244)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.2455 (0.2331)	Prec@1 90.625 (92.345)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.2515 (0.2335)	Prec@1 92.188 (92.352)	
Total train loss: 0.2336

Train time: 15.455706357955933
 * Prec@1 80.460 Prec@5 98.850 Loss 0.6147
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 18.61628532409668

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2458 (0.2328)	Prec@1 90.234 (92.107)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.2460 (0.2328)	Prec@1 91.406 (92.167)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.2272 (0.2316)	Prec@1 92.188 (92.254)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.2878 (0.2321)	Prec@1 88.672 (92.338)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.1582 (0.2320)	Prec@1 94.922 (92.328)	
Total train loss: 0.2321

Train time: 16.42234468460083
 * Prec@1 80.730 Prec@5 98.820 Loss 0.6094
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 20.131706953048706

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.2491 (0.2306)	Prec@1 93.359 (92.428)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1871 (0.2309)	Prec@1 93.750 (92.403)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.2031 (0.2316)	Prec@1 92.578 (92.421)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1956 (0.2328)	Prec@1 92.578 (92.340)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.2072 (0.2327)	Prec@1 92.188 (92.346)	
Total train loss: 0.2329

Train time: 15.750946521759033
 * Prec@1 80.640 Prec@5 98.850 Loss 0.6152
Best acc: 81.160
--------------------------------------------------------------------------------
Test time: 18.610567569732666

