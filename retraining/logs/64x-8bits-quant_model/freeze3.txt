
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 49.990 Loss inf
Pre-trained Prec@1 with 3 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.8110 (1.1203)	Prec@1 72.656 (62.039)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.7163 (0.9590)	Prec@1 73.438 (67.122)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.6855 (0.8666)	Prec@1 72.266 (70.166)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.6763 (0.8034)	Prec@1 76.953 (72.478)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.4736 (0.7627)	Prec@1 83.984 (73.866)	
Total train loss: 0.7625

Train time: 84.12479877471924
 * Prec@1 70.420 Prec@5 97.410 Loss 0.8521
Best acc: 70.420
--------------------------------------------------------------------------------
Test time: 87.97853446006775

Epoch: [1][38/196]	LR: 0.1	Loss 0.4258 (0.5078)	Prec@1 85.156 (82.752)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.5522 (0.5161)	Prec@1 82.812 (82.482)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.5195 (0.5108)	Prec@1 85.156 (82.746)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.4856 (0.5045)	Prec@1 82.812 (82.973)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.4695 (0.5037)	Prec@1 83.594 (82.919)	
Total train loss: 0.5036

Train time: 17.818403482437134
 * Prec@1 63.350 Prec@5 95.350 Loss 1.4316
Best acc: 70.420
--------------------------------------------------------------------------------
Test time: 21.11052966117859

Epoch: [2][38/196]	LR: 0.1	Loss 0.4778 (0.4077)	Prec@1 82.812 (85.948)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.4702 (0.4057)	Prec@1 85.156 (86.278)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.3806 (0.4163)	Prec@1 87.109 (85.994)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.4539 (0.4165)	Prec@1 84.375 (85.965)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.3774 (0.4204)	Prec@1 86.719 (85.857)	
Total train loss: 0.4202

Train time: 17.449545860290527
 * Prec@1 72.280 Prec@5 97.440 Loss 1.1064
Best acc: 72.280
--------------------------------------------------------------------------------
Test time: 20.392685651779175

Epoch: [3][38/196]	LR: 0.1	Loss 0.3892 (0.3449)	Prec@1 87.109 (88.411)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.4109 (0.3495)	Prec@1 84.375 (88.241)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.2854 (0.3579)	Prec@1 90.625 (87.817)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.3835 (0.3624)	Prec@1 87.891 (87.620)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.3643 (0.3654)	Prec@1 87.109 (87.508)	
Total train loss: 0.3655

Train time: 16.13043451309204
 * Prec@1 76.260 Prec@5 98.060 Loss 0.7754
Best acc: 76.260
--------------------------------------------------------------------------------
Test time: 19.783328533172607

Epoch: [4][38/196]	LR: 0.1	Loss 0.3071 (0.2906)	Prec@1 90.625 (90.134)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.3716 (0.3005)	Prec@1 87.109 (89.508)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.3015 (0.3066)	Prec@1 88.281 (89.269)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.3381 (0.3202)	Prec@1 87.109 (88.845)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.3257 (0.3270)	Prec@1 86.719 (88.676)	
Total train loss: 0.3268

Train time: 16.564915895462036
 * Prec@1 77.160 Prec@5 98.800 Loss 0.6763
Best acc: 77.160
--------------------------------------------------------------------------------
Test time: 19.498685598373413

Epoch: [5][38/196]	LR: 0.1	Loss 0.3733 (0.2849)	Prec@1 86.328 (90.304)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.2651 (0.2895)	Prec@1 92.188 (90.284)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2869 (0.2943)	Prec@1 89.453 (90.011)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.2452 (0.2954)	Prec@1 89.453 (89.924)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.3403 (0.3031)	Prec@1 87.891 (89.683)	
Total train loss: 0.3033

Train time: 16.38155770301819
 * Prec@1 68.490 Prec@5 95.760 Loss 1.2988
Best acc: 77.160
--------------------------------------------------------------------------------
Test time: 19.73482847213745

Epoch: [6][38/196]	LR: 0.1	Loss 0.3120 (0.2606)	Prec@1 87.109 (91.056)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2512 (0.2750)	Prec@1 92.578 (90.545)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.2952 (0.2778)	Prec@1 89.844 (90.408)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.2747 (0.2783)	Prec@1 89.453 (90.355)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2435 (0.2800)	Prec@1 92.188 (90.312)	
Total train loss: 0.2801

Train time: 16.019273281097412
 * Prec@1 80.010 Prec@5 98.950 Loss 0.6152
Best acc: 80.010
--------------------------------------------------------------------------------
Test time: 19.078810453414917

Epoch: [7][38/196]	LR: 0.1	Loss 0.1898 (0.2445)	Prec@1 92.969 (91.607)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.2161 (0.2434)	Prec@1 93.359 (91.672)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.3264 (0.2485)	Prec@1 88.281 (91.563)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.2354 (0.2472)	Prec@1 92.578 (91.546)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3169 (0.2496)	Prec@1 89.062 (91.440)	
Total train loss: 0.2497

Train time: 17.319438934326172
 * Prec@1 76.470 Prec@5 98.510 Loss 0.7119
Best acc: 80.010
--------------------------------------------------------------------------------
Test time: 21.20955228805542

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2039 (0.1940)	Prec@1 93.359 (93.289)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2118 (0.1911)	Prec@1 92.969 (93.575)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1115 (0.1888)	Prec@1 96.875 (93.687)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2073 (0.1868)	Prec@1 92.969 (93.745)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1903 (0.1852)	Prec@1 93.359 (93.780)	
Total train loss: 0.1852

Train time: 17.20081067085266
 * Prec@1 84.010 Prec@5 99.350 Loss 0.4983
Best acc: 84.010
--------------------------------------------------------------------------------
Test time: 20.279837131500244

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1443 (0.1706)	Prec@1 95.312 (94.371)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1843 (0.1675)	Prec@1 93.750 (94.516)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1261 (0.1679)	Prec@1 95.703 (94.525)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1552 (0.1676)	Prec@1 94.531 (94.536)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.1830 (0.1671)	Prec@1 92.578 (94.499)	
Total train loss: 0.1672

Train time: 16.583118200302124
 * Prec@1 84.300 Prec@5 99.320 Loss 0.5024
Best acc: 84.300
--------------------------------------------------------------------------------
Test time: 19.95043134689331

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1968 (0.1536)	Prec@1 94.141 (95.012)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1577 (0.1575)	Prec@1 94.531 (94.962)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1395 (0.1626)	Prec@1 96.875 (94.758)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1338 (0.1619)	Prec@1 96.094 (94.817)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.1802 (0.1623)	Prec@1 93.750 (94.792)	
Total train loss: 0.1627

Train time: 16.812986135482788
 * Prec@1 84.350 Prec@5 99.330 Loss 0.5000
Best acc: 84.350
--------------------------------------------------------------------------------
Test time: 20.262999296188354

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1851 (0.1616)	Prec@1 94.141 (94.782)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1794 (0.1575)	Prec@1 94.922 (94.902)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1512 (0.1583)	Prec@1 95.312 (94.848)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1807 (0.1588)	Prec@1 95.703 (94.787)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1107 (0.1599)	Prec@1 96.484 (94.792)	
Total train loss: 0.1600

Train time: 16.46866488456726
 * Prec@1 84.120 Prec@5 99.310 Loss 0.5010
Best acc: 84.350
--------------------------------------------------------------------------------
Test time: 19.993895292282104

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1031 (0.1563)	Prec@1 97.266 (95.072)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2037 (0.1593)	Prec@1 93.750 (94.882)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1975 (0.1616)	Prec@1 92.969 (94.808)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2181 (0.1617)	Prec@1 92.188 (94.754)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1853 (0.1599)	Prec@1 93.750 (94.824)	
Total train loss: 0.1598

Train time: 16.478442430496216
 * Prec@1 84.380 Prec@5 99.320 Loss 0.5015
Best acc: 84.380
--------------------------------------------------------------------------------
Test time: 19.732269287109375

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1559 (0.1562)	Prec@1 94.922 (95.092)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1215 (0.1551)	Prec@1 96.484 (95.032)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1617 (0.1573)	Prec@1 96.484 (94.952)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1437 (0.1576)	Prec@1 95.312 (94.909)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1678 (0.1599)	Prec@1 93.750 (94.840)	
Total train loss: 0.1601

Train time: 16.65645718574524
 * Prec@1 83.980 Prec@5 99.290 Loss 0.5142
Best acc: 84.380
--------------------------------------------------------------------------------
Test time: 20.298256635665894

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1602 (0.1643)	Prec@1 94.922 (94.702)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1603 (0.1618)	Prec@1 93.750 (94.872)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1788 (0.1596)	Prec@1 93.359 (94.862)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1007 (0.1584)	Prec@1 96.875 (94.869)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1647 (0.1593)	Prec@1 93.359 (94.818)	
Total train loss: 0.1595

Train time: 16.869489908218384
 * Prec@1 84.460 Prec@5 99.380 Loss 0.4939
Best acc: 84.460
--------------------------------------------------------------------------------
Test time: 20.287503242492676

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0960 (0.1532)	Prec@1 97.656 (95.232)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1888 (0.1592)	Prec@1 94.531 (94.862)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1532 (0.1581)	Prec@1 94.141 (94.865)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1654 (0.1602)	Prec@1 94.141 (94.792)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1212 (0.1600)	Prec@1 96.875 (94.780)	
Total train loss: 0.1600

Train time: 16.462342739105225
 * Prec@1 84.240 Prec@5 99.350 Loss 0.5015
Best acc: 84.460
--------------------------------------------------------------------------------
Test time: 19.956359148025513

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.2212 (0.1596)	Prec@1 91.406 (94.882)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.2070 (0.1586)	Prec@1 92.188 (94.877)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1722 (0.1589)	Prec@1 94.531 (94.875)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2113 (0.1592)	Prec@1 93.750 (94.884)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1450 (0.1580)	Prec@1 95.703 (94.954)	
Total train loss: 0.1582

Train time: 16.77593994140625
 * Prec@1 84.480 Prec@5 99.390 Loss 0.4917
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.767650842666626

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1600 (0.1663)	Prec@1 92.969 (94.541)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1282 (0.1614)	Prec@1 95.312 (94.747)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1396 (0.1566)	Prec@1 96.484 (94.899)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1594 (0.1579)	Prec@1 94.141 (94.839)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2015 (0.1583)	Prec@1 93.359 (94.866)	
Total train loss: 0.1583

Train time: 16.88143229484558
 * Prec@1 84.430 Prec@5 99.330 Loss 0.5005
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.951720476150513

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2216 (0.1659)	Prec@1 91.016 (94.521)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1488 (0.1611)	Prec@1 95.703 (94.882)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1869 (0.1617)	Prec@1 92.969 (94.842)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1328 (0.1596)	Prec@1 96.094 (94.914)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1428 (0.1591)	Prec@1 94.531 (94.876)	
Total train loss: 0.1592

Train time: 16.64504361152649
 * Prec@1 84.150 Prec@5 99.340 Loss 0.5103
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.663917064666748

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1342 (0.1516)	Prec@1 96.875 (95.202)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2017 (0.1578)	Prec@1 93.359 (94.997)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1929 (0.1565)	Prec@1 93.359 (94.992)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1848 (0.1562)	Prec@1 94.141 (94.959)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1476 (0.1572)	Prec@1 96.094 (94.952)	
Total train loss: 0.1572

Train time: 16.377914905548096
 * Prec@1 84.230 Prec@5 99.360 Loss 0.5015
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.937665939331055

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1572 (0.1661)	Prec@1 94.531 (94.712)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1461 (0.1624)	Prec@1 95.312 (94.767)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2108 (0.1591)	Prec@1 93.359 (94.979)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1893 (0.1578)	Prec@1 94.922 (94.967)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1357 (0.1575)	Prec@1 96.484 (94.954)	
Total train loss: 0.1575

Train time: 15.85003399848938
 * Prec@1 84.240 Prec@5 99.350 Loss 0.5044
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 18.987802267074585

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1443 (0.1563)	Prec@1 94.531 (95.022)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1470 (0.1539)	Prec@1 96.484 (95.052)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1566 (0.1568)	Prec@1 94.922 (94.915)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1410 (0.1591)	Prec@1 94.531 (94.817)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1659 (0.1581)	Prec@1 93.750 (94.864)	
Total train loss: 0.1581

Train time: 16.796311855316162
 * Prec@1 84.270 Prec@5 99.370 Loss 0.5059
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.715219020843506

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1366 (0.1508)	Prec@1 96.484 (94.992)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2113 (0.1559)	Prec@1 92.969 (94.992)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1556 (0.1570)	Prec@1 93.750 (94.962)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1500 (0.1588)	Prec@1 94.141 (94.907)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1915 (0.1582)	Prec@1 93.359 (94.936)	
Total train loss: 0.1582

Train time: 16.79041624069214
 * Prec@1 84.290 Prec@5 99.370 Loss 0.5029
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.86478567123413

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1456 (0.1553)	Prec@1 93.359 (94.962)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1587 (0.1535)	Prec@1 94.531 (95.097)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1577 (0.1566)	Prec@1 94.531 (94.915)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1221 (0.1574)	Prec@1 97.266 (94.952)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1329 (0.1571)	Prec@1 95.312 (94.966)	
Total train loss: 0.1573

Train time: 16.973846673965454
 * Prec@1 84.200 Prec@5 99.350 Loss 0.5063
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.64224076271057

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1616 (0.1509)	Prec@1 95.312 (95.102)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1204 (0.1529)	Prec@1 96.484 (95.122)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1842 (0.1547)	Prec@1 94.141 (95.065)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1737 (0.1562)	Prec@1 94.531 (95.025)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1722 (0.1562)	Prec@1 96.484 (95.054)	
Total train loss: 0.1563

Train time: 16.278321504592896
 * Prec@1 84.180 Prec@5 99.350 Loss 0.5093
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.74167251586914

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1512 (0.1606)	Prec@1 94.141 (94.762)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1469 (0.1617)	Prec@1 96.094 (94.867)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1410 (0.1594)	Prec@1 95.703 (94.909)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1836 (0.1583)	Prec@1 93.750 (94.929)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1478 (0.1579)	Prec@1 94.531 (94.972)	
Total train loss: 0.1581

Train time: 16.868048429489136
 * Prec@1 84.040 Prec@5 99.350 Loss 0.5112
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.19034242630005

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1729 (0.1551)	Prec@1 94.531 (95.242)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1628 (0.1562)	Prec@1 93.359 (94.992)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1115 (0.1561)	Prec@1 96.484 (94.962)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1558 (0.1562)	Prec@1 94.922 (94.894)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1433 (0.1568)	Prec@1 94.531 (94.850)	
Total train loss: 0.1569

Train time: 16.60685110092163
 * Prec@1 84.390 Prec@5 99.380 Loss 0.5020
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.65170168876648

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1831 (0.1657)	Prec@1 94.531 (94.541)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1371 (0.1621)	Prec@1 96.094 (94.631)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1428 (0.1597)	Prec@1 95.703 (94.735)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1488 (0.1586)	Prec@1 95.312 (94.802)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2003 (0.1584)	Prec@1 92.969 (94.782)	
Total train loss: 0.1585

Train time: 16.801201820373535
 * Prec@1 84.230 Prec@5 99.340 Loss 0.5015
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.55693554878235

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1412 (0.1566)	Prec@1 96.094 (94.942)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1534 (0.1595)	Prec@1 96.094 (94.792)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1398 (0.1586)	Prec@1 95.703 (94.882)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2269 (0.1602)	Prec@1 91.016 (94.844)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1683 (0.1580)	Prec@1 94.922 (94.948)	
Total train loss: 0.1581

Train time: 16.895312547683716
 * Prec@1 84.400 Prec@5 99.360 Loss 0.4951
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.86160683631897

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1788 (0.1619)	Prec@1 94.531 (94.521)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1343 (0.1594)	Prec@1 96.094 (94.727)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2013 (0.1592)	Prec@1 92.969 (94.812)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2351 (0.1600)	Prec@1 92.188 (94.792)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1543 (0.1591)	Prec@1 94.922 (94.856)	
Total train loss: 0.1591

Train time: 16.2003915309906
 * Prec@1 84.460 Prec@5 99.320 Loss 0.4924
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.499271869659424

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.1613 (0.1593)	Prec@1 94.922 (94.822)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.1437 (0.1615)	Prec@1 94.531 (94.807)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.0908 (0.1591)	Prec@1 97.266 (94.892)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.1605 (0.1577)	Prec@1 94.922 (94.892)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.1583 (0.1584)	Prec@1 94.141 (94.872)	
Total train loss: 0.1583

Train time: 16.39557194709778
 * Prec@1 84.300 Prec@5 99.360 Loss 0.5010
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.494762182235718

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.1649 (0.1656)	Prec@1 93.750 (94.371)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.1854 (0.1588)	Prec@1 94.141 (94.772)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1904 (0.1573)	Prec@1 93.359 (94.808)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.1799 (0.1574)	Prec@1 93.750 (94.892)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1943 (0.1574)	Prec@1 92.969 (94.928)	
Total train loss: 0.1573

Train time: 16.89815354347229
 * Prec@1 84.230 Prec@5 99.330 Loss 0.5044
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.889657020568848

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1530 (0.1597)	Prec@1 94.141 (94.782)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1335 (0.1608)	Prec@1 96.484 (94.897)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1514 (0.1599)	Prec@1 96.094 (94.865)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1204 (0.1586)	Prec@1 96.484 (94.922)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1174 (0.1580)	Prec@1 98.047 (94.944)	
Total train loss: 0.1580

Train time: 16.53467297554016
 * Prec@1 84.390 Prec@5 99.350 Loss 0.4993
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.602766513824463

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.1622 (0.1543)	Prec@1 95.312 (95.032)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.1537 (0.1548)	Prec@1 94.531 (95.037)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1698 (0.1559)	Prec@1 94.141 (94.892)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.1736 (0.1581)	Prec@1 94.141 (94.882)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1599 (0.1578)	Prec@1 95.312 (94.926)	
Total train loss: 0.1578

Train time: 16.89630961418152
 * Prec@1 84.130 Prec@5 99.360 Loss 0.5098
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.535836935043335

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.1829 (0.1558)	Prec@1 93.750 (94.772)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.2529 (0.1577)	Prec@1 89.062 (94.767)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.1486 (0.1584)	Prec@1 94.922 (94.788)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.1035 (0.1581)	Prec@1 96.875 (94.864)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1877 (0.1575)	Prec@1 94.141 (94.886)	
Total train loss: 0.1575

Train time: 17.14566993713379
 * Prec@1 84.130 Prec@5 99.370 Loss 0.5034
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.664772033691406

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.1146 (0.1591)	Prec@1 95.703 (95.082)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.1160 (0.1590)	Prec@1 95.312 (94.962)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.1256 (0.1584)	Prec@1 96.094 (94.945)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1832 (0.1575)	Prec@1 93.359 (94.934)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1428 (0.1573)	Prec@1 96.094 (94.926)	
Total train loss: 0.1574

Train time: 16.645828247070312
 * Prec@1 84.120 Prec@5 99.330 Loss 0.5029
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.163411617279053

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1805 (0.1643)	Prec@1 95.312 (94.621)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1581 (0.1617)	Prec@1 95.312 (94.727)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.2306 (0.1585)	Prec@1 93.359 (94.845)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1179 (0.1584)	Prec@1 96.484 (94.837)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1599 (0.1574)	Prec@1 95.703 (94.948)	
Total train loss: 0.1574

Train time: 16.479845762252808
 * Prec@1 84.160 Prec@5 99.360 Loss 0.5054
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 19.521794080734253

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1473 (0.1540)	Prec@1 94.531 (95.092)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.1765 (0.1543)	Prec@1 93.750 (95.087)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1759 (0.1590)	Prec@1 95.312 (94.895)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1318 (0.1571)	Prec@1 96.875 (94.962)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.1781 (0.1570)	Prec@1 92.969 (94.948)	
Total train loss: 0.1571

Train time: 16.98676896095276
 * Prec@1 84.310 Prec@5 99.350 Loss 0.5010
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 21.03756356239319

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2039 (0.1547)	Prec@1 93.359 (94.932)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.2013 (0.1557)	Prec@1 92.578 (94.932)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.1783 (0.1558)	Prec@1 94.531 (94.919)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1718 (0.1590)	Prec@1 94.922 (94.852)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.1288 (0.1577)	Prec@1 96.094 (94.862)	
Total train loss: 0.1577

Train time: 17.345486640930176
 * Prec@1 84.190 Prec@5 99.330 Loss 0.5054
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.375208377838135

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.1608 (0.1612)	Prec@1 95.703 (94.692)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1506 (0.1591)	Prec@1 94.922 (94.837)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.1705 (0.1586)	Prec@1 95.312 (94.818)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1818 (0.1596)	Prec@1 93.750 (94.812)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1550 (0.1587)	Prec@1 94.531 (94.830)	
Total train loss: 0.1587

Train time: 16.62047004699707
 * Prec@1 84.430 Prec@5 99.360 Loss 0.4956
Best acc: 84.480
--------------------------------------------------------------------------------
Test time: 20.05133891105652

