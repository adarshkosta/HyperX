
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 53.140 Loss inf
Pre-trained Prec@1 with 9 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.1465 (1.7136)	Prec@1 58.594 (42.228)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.0938 (1.4088)	Prec@1 64.062 (51.277)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.8281 (1.2564)	Prec@1 70.703 (56.330)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.8652 (1.1588)	Prec@1 70.703 (59.650)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.7148 (1.0859)	Prec@1 74.219 (62.226)	
Total train loss: 1.0851

Train time: 208.84782457351685
 * Prec@1 31.990 Prec@5 84.390 Loss 2.1621
Best acc: 31.990
--------------------------------------------------------------------------------
Test time: 215.1074833869934

Epoch: [1][38/196]	LR: 0.1	Loss 0.5947 (0.7431)	Prec@1 79.297 (73.878)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.6167 (0.7342)	Prec@1 77.734 (74.324)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.6816 (0.7210)	Prec@1 77.344 (74.937)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.6284 (0.7075)	Prec@1 78.906 (75.446)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.6807 (0.6967)	Prec@1 76.953 (75.843)	
Total train loss: 0.6970

Train time: 20.97625231742859
 * Prec@1 40.650 Prec@5 90.440 Loss 1.7344
Best acc: 40.650
--------------------------------------------------------------------------------
Test time: 25.59348750114441

Epoch: [2][38/196]	LR: 0.1	Loss 0.6909 (0.6009)	Prec@1 78.516 (79.077)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.7334 (0.5993)	Prec@1 76.172 (79.277)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.4905 (0.6012)	Prec@1 81.250 (79.237)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.6152 (0.6019)	Prec@1 77.344 (79.264)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.5068 (0.5997)	Prec@1 81.641 (79.289)	
Total train loss: 0.5997

Train time: 18.730841398239136
 * Prec@1 49.430 Prec@5 92.770 Loss 1.5205
Best acc: 49.430
--------------------------------------------------------------------------------
Test time: 23.23032307624817

Epoch: [3][38/196]	LR: 0.1	Loss 0.5171 (0.5324)	Prec@1 81.641 (81.641)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.6665 (0.5345)	Prec@1 76.172 (81.490)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.5474 (0.5394)	Prec@1 82.031 (81.384)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4756 (0.5367)	Prec@1 81.641 (81.435)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.5308 (0.5399)	Prec@1 79.297 (81.330)	
Total train loss: 0.5401

Train time: 20.49285316467285
 * Prec@1 54.170 Prec@5 94.380 Loss 1.4180
Best acc: 54.170
--------------------------------------------------------------------------------
Test time: 25.154520988464355

Epoch: [4][38/196]	LR: 0.1	Loss 0.5498 (0.4911)	Prec@1 82.422 (82.853)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.4495 (0.4946)	Prec@1 85.156 (82.993)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.4365 (0.4942)	Prec@1 83.984 (82.966)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4604 (0.5009)	Prec@1 83.984 (82.640)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.3555 (0.4985)	Prec@1 89.453 (82.798)	
Total train loss: 0.4988

Train time: 20.517112731933594
 * Prec@1 48.040 Prec@5 92.260 Loss 1.6562
Best acc: 54.170
--------------------------------------------------------------------------------
Test time: 24.459054231643677

Epoch: [5][38/196]	LR: 0.1	Loss 0.4893 (0.4657)	Prec@1 84.375 (84.014)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.5073 (0.4677)	Prec@1 81.641 (84.095)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.4871 (0.4703)	Prec@1 84.766 (83.951)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.3577 (0.4732)	Prec@1 87.891 (83.862)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.5005 (0.4728)	Prec@1 82.422 (83.806)	
Total train loss: 0.4729

Train time: 22.208597421646118
 * Prec@1 59.920 Prec@5 94.170 Loss 1.2939
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.082782745361328

Epoch: [6][38/196]	LR: 0.1	Loss 0.4177 (0.4475)	Prec@1 84.766 (84.525)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5273 (0.4462)	Prec@1 82.422 (84.680)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.4893 (0.4460)	Prec@1 82.422 (84.645)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.3420 (0.4471)	Prec@1 89.062 (84.698)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.5254 (0.4490)	Prec@1 83.203 (84.593)	
Total train loss: 0.4490

Train time: 21.166443586349487
 * Prec@1 47.360 Prec@5 91.380 Loss 1.5908
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.344780206680298

Epoch: [7][38/196]	LR: 0.1	Loss 0.3967 (0.4302)	Prec@1 85.547 (85.276)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4385 (0.4315)	Prec@1 83.203 (85.322)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.4172 (0.4314)	Prec@1 86.719 (85.240)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.4419 (0.4326)	Prec@1 85.156 (85.144)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3721 (0.4331)	Prec@1 85.547 (85.060)	
Total train loss: 0.4330

Train time: 21.84239912033081
 * Prec@1 40.800 Prec@5 87.060 Loss 2.0371
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.110697031021118

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3899 (0.3970)	Prec@1 87.500 (86.679)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.4050 (0.3915)	Prec@1 84.766 (86.599)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3594 (0.3864)	Prec@1 88.281 (86.765)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3931 (0.3863)	Prec@1 83.984 (86.696)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4656 (0.3846)	Prec@1 87.109 (86.833)	
Total train loss: 0.3845

Train time: 20.00566029548645
 * Prec@1 54.930 Prec@5 92.520 Loss 1.4229
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.23324942588806

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.3594 (0.3732)	Prec@1 87.891 (87.480)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.4109 (0.3779)	Prec@1 85.938 (87.119)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.4011 (0.3784)	Prec@1 86.328 (87.123)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3376 (0.3790)	Prec@1 88.672 (86.997)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.3840 (0.3800)	Prec@1 85.156 (86.927)	
Total train loss: 0.3799

Train time: 21.6104474067688
 * Prec@1 54.700 Prec@5 92.590 Loss 1.4492
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.167990922927856

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.3796 (0.3819)	Prec@1 83.984 (86.989)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3264 (0.3764)	Prec@1 87.109 (87.210)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.3999 (0.3780)	Prec@1 85.938 (87.246)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.3762 (0.3797)	Prec@1 85.938 (87.064)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.3599 (0.3827)	Prec@1 85.938 (86.909)	
Total train loss: 0.3826

Train time: 23.48626732826233
 * Prec@1 54.370 Prec@5 92.760 Loss 1.4512
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 27.860146045684814

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.3411 (0.3650)	Prec@1 89.844 (87.650)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.4258 (0.3738)	Prec@1 83.594 (87.210)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.4150 (0.3791)	Prec@1 84.766 (87.093)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.4536 (0.3833)	Prec@1 84.766 (86.897)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.4219 (0.3853)	Prec@1 87.500 (86.801)	
Total train loss: 0.3852

Train time: 21.755802869796753
 * Prec@1 57.800 Prec@5 93.490 Loss 1.3223
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.51524567604065

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.4133 (0.4008)	Prec@1 85.938 (86.348)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5059 (0.3917)	Prec@1 85.156 (86.744)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.3630 (0.3884)	Prec@1 85.938 (86.745)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3345 (0.3920)	Prec@1 88.672 (86.548)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.4521 (0.3917)	Prec@1 83.594 (86.587)	
Total train loss: 0.3916

Train time: 22.752022981643677
 * Prec@1 53.550 Prec@5 91.990 Loss 1.4883
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.977341175079346

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.4739 (0.4005)	Prec@1 82.422 (86.428)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5283 (0.3969)	Prec@1 82.422 (86.533)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.5698 (0.3999)	Prec@1 82.812 (86.298)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.4253 (0.3986)	Prec@1 85.156 (86.371)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.3320 (0.3990)	Prec@1 87.891 (86.304)	
Total train loss: 0.3993

Train time: 19.902359008789062
 * Prec@1 54.060 Prec@5 92.270 Loss 1.4424
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 23.895551204681396

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.3420 (0.4007)	Prec@1 86.719 (86.008)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.3286 (0.4017)	Prec@1 89.062 (86.148)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.3762 (0.4063)	Prec@1 84.766 (85.921)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3921 (0.4112)	Prec@1 85.938 (85.702)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.4939 (0.4113)	Prec@1 82.422 (85.677)	
Total train loss: 0.4112

Train time: 20.083861827850342
 * Prec@1 50.530 Prec@5 91.080 Loss 1.6025
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.25683569908142

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.4060 (0.4112)	Prec@1 85.938 (85.487)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.4209 (0.4168)	Prec@1 84.766 (85.186)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.4297 (0.4205)	Prec@1 85.938 (85.193)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.3789 (0.4233)	Prec@1 88.281 (85.171)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.4324 (0.4221)	Prec@1 85.938 (85.220)	
Total train loss: 0.4220

Train time: 20.016929149627686
 * Prec@1 54.130 Prec@5 91.760 Loss 1.4932
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 23.979614734649658

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.4485 (0.4184)	Prec@1 82.812 (85.286)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.4624 (0.4185)	Prec@1 83.594 (85.271)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.4255 (0.4220)	Prec@1 84.375 (85.076)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.3979 (0.4211)	Prec@1 86.719 (85.201)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.3479 (0.4217)	Prec@1 88.281 (85.280)	
Total train loss: 0.4217

Train time: 22.502501726150513
 * Prec@1 52.700 Prec@5 91.930 Loss 1.5410
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.485039472579956

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.4309 (0.4051)	Prec@1 83.203 (85.968)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.3586 (0.4087)	Prec@1 87.500 (85.812)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.3579 (0.4174)	Prec@1 87.891 (85.463)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.4192 (0.4185)	Prec@1 85.938 (85.434)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.3523 (0.4190)	Prec@1 85.547 (85.381)	
Total train loss: 0.4192

Train time: 21.540099620819092
 * Prec@1 53.250 Prec@5 92.140 Loss 1.5039
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.751330852508545

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.4065 (0.4192)	Prec@1 85.938 (85.467)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.4170 (0.4185)	Prec@1 84.766 (85.372)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.4121 (0.4186)	Prec@1 85.547 (85.353)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5015 (0.4223)	Prec@1 82.422 (85.271)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.3909 (0.4206)	Prec@1 86.719 (85.333)	
Total train loss: 0.4205

Train time: 22.35353946685791
 * Prec@1 52.590 Prec@5 91.630 Loss 1.5420
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.835591554641724

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.4060 (0.4196)	Prec@1 85.156 (85.677)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.4131 (0.4218)	Prec@1 87.109 (85.472)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.4189 (0.4185)	Prec@1 84.375 (85.450)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.3423 (0.4207)	Prec@1 89.062 (85.397)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.3237 (0.4203)	Prec@1 88.672 (85.306)	
Total train loss: 0.4202

Train time: 20.294797897338867
 * Prec@1 52.690 Prec@5 92.000 Loss 1.5381
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.12749457359314

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.3962 (0.4133)	Prec@1 86.719 (85.847)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.3550 (0.4190)	Prec@1 87.109 (85.482)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.4229 (0.4226)	Prec@1 84.375 (85.270)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.4304 (0.4236)	Prec@1 87.500 (85.296)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.4817 (0.4206)	Prec@1 83.594 (85.375)	
Total train loss: 0.4205

Train time: 21.70924997329712
 * Prec@1 52.580 Prec@5 91.870 Loss 1.5469
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.210664987564087

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.4246 (0.4203)	Prec@1 84.766 (85.517)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.4612 (0.4195)	Prec@1 82.031 (85.301)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.3723 (0.4162)	Prec@1 86.719 (85.483)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.4402 (0.4160)	Prec@1 84.375 (85.547)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.4141 (0.4184)	Prec@1 87.109 (85.459)	
Total train loss: 0.4184

Train time: 21.674091577529907
 * Prec@1 52.600 Prec@5 91.540 Loss 1.5439
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.898298740386963

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.3848 (0.4189)	Prec@1 88.281 (85.447)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.3494 (0.4196)	Prec@1 87.500 (85.537)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5576 (0.4223)	Prec@1 79.688 (85.233)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.3762 (0.4212)	Prec@1 85.156 (85.244)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.4993 (0.4210)	Prec@1 84.375 (85.317)	
Total train loss: 0.4211

Train time: 20.501972913742065
 * Prec@1 52.660 Prec@5 91.600 Loss 1.5410
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.898779153823853

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.3521 (0.4135)	Prec@1 87.500 (85.296)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.4094 (0.4162)	Prec@1 86.719 (85.337)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.4172 (0.4178)	Prec@1 85.938 (85.447)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.3101 (0.4193)	Prec@1 89.844 (85.402)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.4514 (0.4201)	Prec@1 81.641 (85.361)	
Total train loss: 0.4199

Train time: 20.345669507980347
 * Prec@1 52.280 Prec@5 91.720 Loss 1.5576
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.315174102783203

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.3577 (0.4258)	Prec@1 86.328 (85.276)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.4634 (0.4172)	Prec@1 82.031 (85.622)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.4629 (0.4146)	Prec@1 83.594 (85.587)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.4419 (0.4191)	Prec@1 84.766 (85.384)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.4612 (0.4206)	Prec@1 82.031 (85.393)	
Total train loss: 0.4207

Train time: 22.079699993133545
 * Prec@1 52.600 Prec@5 91.820 Loss 1.5469
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 27.041545152664185

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.4604 (0.4110)	Prec@1 84.375 (85.667)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.4065 (0.4150)	Prec@1 84.375 (85.427)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.4905 (0.4199)	Prec@1 81.641 (85.230)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.3918 (0.4181)	Prec@1 84.375 (85.314)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.4258 (0.4174)	Prec@1 85.938 (85.383)	
Total train loss: 0.4177

Train time: 21.732300281524658
 * Prec@1 52.710 Prec@5 91.270 Loss 1.5479
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.761164903640747

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.4375 (0.4280)	Prec@1 84.375 (85.286)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.4253 (0.4234)	Prec@1 84.766 (85.432)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.4595 (0.4222)	Prec@1 85.547 (85.340)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.3909 (0.4185)	Prec@1 85.547 (85.454)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.4209 (0.4199)	Prec@1 83.984 (85.393)	
Total train loss: 0.4203

Train time: 21.709024667739868
 * Prec@1 52.100 Prec@5 91.670 Loss 1.5615
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.94423508644104

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.4109 (0.4227)	Prec@1 85.156 (85.206)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.4014 (0.4189)	Prec@1 85.156 (85.236)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.4717 (0.4216)	Prec@1 83.203 (85.260)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.4656 (0.4241)	Prec@1 84.375 (85.221)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.3657 (0.4174)	Prec@1 88.672 (85.429)	
Total train loss: 0.4178

Train time: 20.11918592453003
 * Prec@1 52.810 Prec@5 91.880 Loss 1.5283
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.25816321372986

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.4670 (0.4262)	Prec@1 83.984 (85.196)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.4150 (0.4187)	Prec@1 87.500 (85.392)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.4043 (0.4204)	Prec@1 86.719 (85.393)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.4834 (0.4169)	Prec@1 83.984 (85.524)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.5205 (0.4192)	Prec@1 82.031 (85.483)	
Total train loss: 0.4193

Train time: 20.367128610610962
 * Prec@1 52.780 Prec@5 91.710 Loss 1.5332
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.50574827194214

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.3843 (0.4225)	Prec@1 86.719 (85.106)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.4656 (0.4241)	Prec@1 83.984 (85.211)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.4187 (0.4212)	Prec@1 85.547 (85.256)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.3540 (0.4197)	Prec@1 90.234 (85.322)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.3489 (0.4188)	Prec@1 88.672 (85.403)	
Total train loss: 0.4191

Train time: 19.92805790901184
 * Prec@1 52.290 Prec@5 91.740 Loss 1.5537
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.729754209518433

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.3779 (0.4170)	Prec@1 87.891 (85.337)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.4492 (0.4122)	Prec@1 85.547 (85.572)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.4084 (0.4173)	Prec@1 86.719 (85.564)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.4707 (0.4187)	Prec@1 82.812 (85.484)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.4021 (0.4193)	Prec@1 87.891 (85.477)	
Total train loss: 0.4197

Train time: 21.767147064208984
 * Prec@1 52.990 Prec@5 91.410 Loss 1.5381
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.97830891609192

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.4072 (0.4268)	Prec@1 87.109 (84.976)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.4485 (0.4201)	Prec@1 83.984 (85.281)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.3994 (0.4184)	Prec@1 85.156 (85.423)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.4148 (0.4185)	Prec@1 85.547 (85.364)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.3914 (0.4183)	Prec@1 84.766 (85.413)	
Total train loss: 0.4185

Train time: 22.49903130531311
 * Prec@1 52.630 Prec@5 91.910 Loss 1.5420
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 27.3463773727417

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.4373 (0.4262)	Prec@1 87.109 (85.327)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.4509 (0.4206)	Prec@1 85.938 (85.211)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.4043 (0.4212)	Prec@1 87.500 (85.280)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.3816 (0.4192)	Prec@1 85.156 (85.407)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.4592 (0.4177)	Prec@1 81.250 (85.501)	
Total train loss: 0.4179

Train time: 22.446442127227783
 * Prec@1 52.880 Prec@5 91.780 Loss 1.5371
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 27.09145998954773

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.4346 (0.4227)	Prec@1 83.984 (85.317)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.4688 (0.4160)	Prec@1 83.984 (85.697)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.4219 (0.4163)	Prec@1 86.719 (85.574)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.4788 (0.4169)	Prec@1 83.984 (85.482)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.3750 (0.4177)	Prec@1 86.719 (85.467)	
Total train loss: 0.4176

Train time: 21.119971752166748
 * Prec@1 52.650 Prec@5 91.510 Loss 1.5469
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.039008378982544

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.4336 (0.4229)	Prec@1 82.422 (85.266)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.4253 (0.4210)	Prec@1 85.156 (85.367)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.3687 (0.4221)	Prec@1 86.719 (85.323)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.3398 (0.4204)	Prec@1 87.891 (85.402)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.4436 (0.4178)	Prec@1 85.547 (85.497)	
Total train loss: 0.4180

Train time: 19.70934772491455
 * Prec@1 52.510 Prec@5 91.810 Loss 1.5479
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.159388065338135

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.3972 (0.4203)	Prec@1 85.938 (85.527)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.4419 (0.4243)	Prec@1 82.422 (85.367)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.3892 (0.4245)	Prec@1 87.500 (85.286)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.4189 (0.4209)	Prec@1 85.547 (85.409)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.3630 (0.4207)	Prec@1 87.109 (85.387)	
Total train loss: 0.4205

Train time: 22.315077781677246
 * Prec@1 53.410 Prec@5 91.850 Loss 1.4971
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 26.392322540283203

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.3774 (0.4212)	Prec@1 86.719 (85.056)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.4167 (0.4145)	Prec@1 86.328 (85.317)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.4565 (0.4132)	Prec@1 85.547 (85.440)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.4021 (0.4138)	Prec@1 85.938 (85.439)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.5659 (0.4167)	Prec@1 78.516 (85.308)	
Total train loss: 0.4168

Train time: 20.358379364013672
 * Prec@1 52.010 Prec@5 91.270 Loss 1.5693
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.667313814163208

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.4563 (0.4284)	Prec@1 82.812 (85.337)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.5117 (0.4252)	Prec@1 82.422 (85.362)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.4771 (0.4222)	Prec@1 81.250 (85.427)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.4312 (0.4207)	Prec@1 83.594 (85.444)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.3811 (0.4197)	Prec@1 88.281 (85.429)	
Total train loss: 0.4196

Train time: 19.87050747871399
 * Prec@1 53.080 Prec@5 91.760 Loss 1.5273
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 23.991132259368896

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.3721 (0.4238)	Prec@1 87.891 (85.387)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.3708 (0.4202)	Prec@1 85.156 (85.557)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.3586 (0.4177)	Prec@1 87.109 (85.580)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.4541 (0.4197)	Prec@1 83.984 (85.512)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.3650 (0.4197)	Prec@1 86.328 (85.539)	
Total train loss: 0.4197

Train time: 21.105082035064697
 * Prec@1 52.980 Prec@5 91.690 Loss 1.5225
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 25.343326091766357

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.3672 (0.4155)	Prec@1 89.062 (85.196)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.4580 (0.4139)	Prec@1 84.375 (85.457)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.3740 (0.4164)	Prec@1 87.109 (85.473)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.4595 (0.4190)	Prec@1 83.984 (85.404)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.4641 (0.4199)	Prec@1 82.031 (85.369)	
Total train loss: 0.4197

Train time: 20.267252683639526
 * Prec@1 52.900 Prec@5 91.710 Loss 1.5244
Best acc: 59.920
--------------------------------------------------------------------------------
Test time: 24.122052907943726

