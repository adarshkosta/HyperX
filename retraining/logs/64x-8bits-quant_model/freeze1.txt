
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv2): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 50.140 Loss inf
Pre-trained Prec@1 with 1 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6406 (1.8752)	Prec@1 36.719 (31.360)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3906 (1.6917)	Prec@1 49.219 (37.210)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.3428 (1.5886)	Prec@1 45.703 (41.303)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1553 (1.5063)	Prec@1 57.422 (44.566)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.1055 (1.4400)	Prec@1 62.500 (47.185)	
Total train loss: 1.4393

Train time: 57.72355270385742
 * Prec@1 49.960 Prec@5 93.280 Loss 1.4492
Best acc: 49.960
--------------------------------------------------------------------------------
Test time: 61.662105321884155

Epoch: [1][38/196]	LR: 0.1	Loss 1.0010 (1.1079)	Prec@1 64.844 (59.776)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.9624 (1.0804)	Prec@1 64.844 (60.948)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1289 (1.0633)	Prec@1 58.203 (61.732)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0566 (1.0386)	Prec@1 62.109 (62.685)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.9248 (1.0179)	Prec@1 64.844 (63.478)	
Total train loss: 1.0177

Train time: 19.00847029685974
 * Prec@1 48.690 Prec@5 93.400 Loss 1.5127
Best acc: 49.960
--------------------------------------------------------------------------------
Test time: 21.984392881393433

Epoch: [2][38/196]	LR: 0.1	Loss 0.9800 (0.8978)	Prec@1 65.625 (68.259)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.8140 (0.8927)	Prec@1 72.266 (68.615)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9136 (0.8812)	Prec@1 66.016 (68.984)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.7964 (0.8796)	Prec@1 73.828 (68.940)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.8047 (0.8717)	Prec@1 73.047 (69.361)	
Total train loss: 0.8714

Train time: 17.174952507019043
 * Prec@1 57.520 Prec@5 95.220 Loss 1.3965
Best acc: 57.520
--------------------------------------------------------------------------------
Test time: 20.93057155609131

Epoch: [3][38/196]	LR: 0.1	Loss 0.6787 (0.7913)	Prec@1 78.125 (72.897)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8345 (0.7941)	Prec@1 69.531 (72.601)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.7075 (0.7941)	Prec@1 76.172 (72.389)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.8203 (0.7928)	Prec@1 71.094 (72.368)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.7681 (0.7882)	Prec@1 72.656 (72.504)	
Total train loss: 0.7883

Train time: 17.21424627304077
 * Prec@1 43.050 Prec@5 90.410 Loss 1.7676
Best acc: 57.520
--------------------------------------------------------------------------------
Test time: 20.211646795272827

Epoch: [4][38/196]	LR: 0.1	Loss 0.7251 (0.7378)	Prec@1 75.781 (74.369)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7847 (0.7421)	Prec@1 73.828 (74.229)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8765 (0.7422)	Prec@1 71.094 (74.122)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8350 (0.7400)	Prec@1 72.656 (74.299)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.9487 (0.7398)	Prec@1 64.844 (74.409)	
Total train loss: 0.7399

Train time: 17.16901922225952
 * Prec@1 20.390 Prec@5 72.730 Loss 3.5293
Best acc: 57.520
--------------------------------------------------------------------------------
Test time: 21.514430284500122

Epoch: [5][38/196]	LR: 0.1	Loss 0.7559 (0.7120)	Prec@1 75.000 (75.150)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6938 (0.7127)	Prec@1 75.781 (75.145)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7920 (0.7202)	Prec@1 69.922 (74.940)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.6143 (0.7137)	Prec@1 79.297 (75.140)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.6880 (0.7137)	Prec@1 76.172 (75.142)	
Total train loss: 0.7134

Train time: 17.68379855155945
 * Prec@1 58.460 Prec@5 95.380 Loss 1.3945
Best acc: 58.460
--------------------------------------------------------------------------------
Test time: 20.919570207595825

Epoch: [6][38/196]	LR: 0.1	Loss 0.6597 (0.6747)	Prec@1 75.781 (76.853)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.7344 (0.6797)	Prec@1 73.438 (76.462)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.7539 (0.6872)	Prec@1 71.094 (76.008)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6118 (0.6829)	Prec@1 77.344 (76.212)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7007 (0.6818)	Prec@1 76.562 (76.298)	
Total train loss: 0.6818

Train time: 17.02494192123413
 * Prec@1 48.010 Prec@5 86.970 Loss 2.1113
Best acc: 58.460
--------------------------------------------------------------------------------
Test time: 21.428967237472534

Epoch: [7][38/196]	LR: 0.1	Loss 0.5811 (0.6418)	Prec@1 76.172 (77.774)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.6924 (0.6581)	Prec@1 76.953 (77.138)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6816 (0.6630)	Prec@1 76.172 (76.793)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.5952 (0.6625)	Prec@1 76.562 (76.815)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6436 (0.6619)	Prec@1 78.125 (76.941)	
Total train loss: 0.6620

Train time: 17.608033657073975
 * Prec@1 61.890 Prec@5 95.660 Loss 1.3545
Best acc: 61.890
--------------------------------------------------------------------------------
Test time: 20.75905132293701

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.6216 (0.6337)	Prec@1 76.172 (77.504)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.6919 (0.6135)	Prec@1 72.656 (78.501)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.6577 (0.6015)	Prec@1 78.125 (79.087)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.6826 (0.6010)	Prec@1 75.000 (79.247)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.5493 (0.5977)	Prec@1 78.125 (79.393)	
Total train loss: 0.5979

Train time: 16.842454433441162
 * Prec@1 76.360 Prec@5 98.810 Loss 0.6738
Best acc: 76.360
--------------------------------------------------------------------------------
Test time: 20.90835213661194

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.6104 (0.5831)	Prec@1 77.344 (79.698)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.5962 (0.5794)	Prec@1 81.250 (79.743)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.4988 (0.5810)	Prec@1 82.031 (79.698)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.7310 (0.5841)	Prec@1 73.047 (79.775)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.5737 (0.5846)	Prec@1 80.078 (79.708)	
Total train loss: 0.5848

Train time: 17.113889932632446
 * Prec@1 75.990 Prec@5 98.720 Loss 0.6978
Best acc: 76.360
--------------------------------------------------------------------------------
Test time: 20.12933611869812

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.5205 (0.5784)	Prec@1 82.422 (80.078)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.5830 (0.5894)	Prec@1 79.688 (79.753)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.5474 (0.5875)	Prec@1 82.031 (79.708)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.5508 (0.5853)	Prec@1 82.031 (79.768)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5728 (0.5840)	Prec@1 81.641 (79.750)	
Total train loss: 0.5841

Train time: 17.143130779266357
 * Prec@1 76.100 Prec@5 98.730 Loss 0.6934
Best acc: 76.360
--------------------------------------------------------------------------------
Test time: 21.031723499298096

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.6230 (0.5980)	Prec@1 78.906 (79.097)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.6577 (0.5842)	Prec@1 78.516 (79.622)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.6074 (0.5827)	Prec@1 76.953 (79.681)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.5698 (0.5777)	Prec@1 80.078 (79.985)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.5752 (0.5777)	Prec@1 77.734 (79.970)	
Total train loss: 0.5778

Train time: 17.272595405578613
 * Prec@1 77.000 Prec@5 98.820 Loss 0.6675
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.418376922607422

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.5840 (0.5797)	Prec@1 80.859 (80.058)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5752 (0.5735)	Prec@1 79.297 (80.063)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5771 (0.5714)	Prec@1 80.078 (80.122)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.5464 (0.5731)	Prec@1 80.859 (79.998)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5562 (0.5739)	Prec@1 80.469 (80.078)	
Total train loss: 0.5743

Train time: 17.614968061447144
 * Prec@1 76.100 Prec@5 98.650 Loss 0.6851
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.560044765472412

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.5024 (0.5639)	Prec@1 85.547 (80.258)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5674 (0.5654)	Prec@1 79.688 (80.519)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.6348 (0.5675)	Prec@1 77.344 (80.445)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.5386 (0.5685)	Prec@1 82.031 (80.434)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.5869 (0.5712)	Prec@1 80.078 (80.260)	
Total train loss: 0.5715

Train time: 17.140713214874268
 * Prec@1 75.080 Prec@5 98.710 Loss 0.7114
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.286797285079956

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.6372 (0.5713)	Prec@1 76.562 (80.058)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.6230 (0.5650)	Prec@1 77.734 (80.604)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.6421 (0.5715)	Prec@1 75.781 (80.272)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.5723 (0.5743)	Prec@1 81.641 (80.178)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.6401 (0.5736)	Prec@1 78.125 (80.188)	
Total train loss: 0.5734

Train time: 17.7411470413208
 * Prec@1 76.590 Prec@5 98.710 Loss 0.6797
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.704126834869385

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.6191 (0.5537)	Prec@1 78.125 (81.020)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.6333 (0.5665)	Prec@1 79.297 (80.559)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.5283 (0.5688)	Prec@1 82.812 (80.499)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.5586 (0.5728)	Prec@1 81.641 (80.231)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.5957 (0.5736)	Prec@1 76.953 (80.132)	
Total train loss: 0.5738

Train time: 16.78246569633484
 * Prec@1 75.820 Prec@5 98.560 Loss 0.6943
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 19.837462902069092

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.6455 (0.5667)	Prec@1 76.562 (80.559)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.5874 (0.5705)	Prec@1 79.688 (80.148)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.5615 (0.5666)	Prec@1 81.250 (80.432)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.5908 (0.5674)	Prec@1 78.125 (80.374)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.5713 (0.5707)	Prec@1 81.250 (80.200)	
Total train loss: 0.5709

Train time: 17.046913146972656
 * Prec@1 76.790 Prec@5 98.790 Loss 0.6670
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.066867351531982

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.6372 (0.5674)	Prec@1 74.609 (80.128)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.5957 (0.5657)	Prec@1 80.469 (80.404)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.4785 (0.5674)	Prec@1 84.375 (80.202)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.6079 (0.5678)	Prec@1 79.688 (80.211)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.5430 (0.5694)	Prec@1 80.078 (80.236)	
Total train loss: 0.5695

Train time: 17.604523181915283
 * Prec@1 76.500 Prec@5 98.730 Loss 0.6738
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.595338344573975

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.5127 (0.5713)	Prec@1 81.250 (80.399)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.5181 (0.5724)	Prec@1 79.688 (80.263)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.6157 (0.5733)	Prec@1 78.125 (80.278)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5269 (0.5700)	Prec@1 82.422 (80.339)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.5762 (0.5689)	Prec@1 81.250 (80.268)	
Total train loss: 0.5689

Train time: 17.051795482635498
 * Prec@1 76.690 Prec@5 98.770 Loss 0.6699
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.01975917816162

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.6162 (0.5552)	Prec@1 78.125 (80.549)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.5894 (0.5609)	Prec@1 81.250 (80.624)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.6382 (0.5669)	Prec@1 74.609 (80.278)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.6445 (0.5666)	Prec@1 77.734 (80.273)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.5391 (0.5689)	Prec@1 80.859 (80.194)	
Total train loss: 0.5691

Train time: 17.109150409698486
 * Prec@1 76.330 Prec@5 98.720 Loss 0.6777
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 19.972888708114624

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.5435 (0.5786)	Prec@1 79.688 (80.098)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.5054 (0.5732)	Prec@1 83.594 (80.088)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.5845 (0.5711)	Prec@1 76.953 (80.148)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.5908 (0.5711)	Prec@1 78.516 (80.133)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.5586 (0.5690)	Prec@1 82.812 (80.224)	
Total train loss: 0.5691

Train time: 16.921065092086792
 * Prec@1 76.840 Prec@5 98.780 Loss 0.6646
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.143512964248657

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.4932 (0.5792)	Prec@1 81.641 (79.988)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.6523 (0.5735)	Prec@1 76.562 (80.043)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.5391 (0.5682)	Prec@1 83.594 (80.262)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.5791 (0.5702)	Prec@1 80.859 (80.206)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.5732 (0.5669)	Prec@1 80.078 (80.349)	
Total train loss: 0.5671

Train time: 17.263843059539795
 * Prec@1 76.790 Prec@5 98.760 Loss 0.6660
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.258644819259644

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.6792 (0.5727)	Prec@1 73.828 (79.878)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.6348 (0.5653)	Prec@1 77.734 (80.324)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.6216 (0.5671)	Prec@1 81.250 (80.409)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.6201 (0.5679)	Prec@1 78.906 (80.359)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.5459 (0.5685)	Prec@1 79.297 (80.208)	
Total train loss: 0.5686

Train time: 17.41946506500244
 * Prec@1 76.460 Prec@5 98.770 Loss 0.6714
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.61206316947937

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.5356 (0.5649)	Prec@1 80.469 (80.519)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.6299 (0.5765)	Prec@1 76.953 (79.988)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.5903 (0.5745)	Prec@1 77.344 (79.958)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.6245 (0.5708)	Prec@1 78.125 (79.993)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.5933 (0.5679)	Prec@1 78.516 (80.176)	
Total train loss: 0.5678

Train time: 17.088565587997437
 * Prec@1 76.800 Prec@5 98.770 Loss 0.6675
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.2132625579834

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.6265 (0.5611)	Prec@1 79.688 (80.629)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.5947 (0.5616)	Prec@1 79.297 (80.394)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.5522 (0.5631)	Prec@1 81.641 (80.342)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.4893 (0.5661)	Prec@1 82.812 (80.248)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.5918 (0.5668)	Prec@1 78.516 (80.294)	
Total train loss: 0.5670

Train time: 16.94869828224182
 * Prec@1 76.600 Prec@5 98.780 Loss 0.6729
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.044550895690918

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.6621 (0.5815)	Prec@1 76.953 (79.848)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.5894 (0.5709)	Prec@1 80.078 (80.138)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.6357 (0.5669)	Prec@1 76.953 (80.235)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.5991 (0.5638)	Prec@1 77.344 (80.341)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5640 (0.5670)	Prec@1 81.641 (80.214)	
Total train loss: 0.5671

Train time: 16.908200979232788
 * Prec@1 76.630 Prec@5 98.800 Loss 0.6650
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.06421446800232

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.5352 (0.5524)	Prec@1 82.812 (81.070)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.4795 (0.5603)	Prec@1 83.984 (80.599)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.5527 (0.5660)	Prec@1 80.469 (80.502)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.5176 (0.5657)	Prec@1 81.250 (80.399)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.4832 (0.5661)	Prec@1 83.984 (80.343)	
Total train loss: 0.5662

Train time: 17.536212921142578
 * Prec@1 76.540 Prec@5 98.790 Loss 0.6714
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.843920946121216

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.4539 (0.5669)	Prec@1 84.375 (80.459)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.5161 (0.5617)	Prec@1 81.250 (80.384)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.6416 (0.5657)	Prec@1 79.297 (80.292)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.6685 (0.5655)	Prec@1 78.906 (80.291)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.4851 (0.5669)	Prec@1 83.984 (80.256)	
Total train loss: 0.5673

Train time: 17.44613027572632
 * Prec@1 76.920 Prec@5 98.820 Loss 0.6655
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.525503158569336

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.6499 (0.5605)	Prec@1 77.344 (80.609)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.5698 (0.5673)	Prec@1 81.250 (80.168)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.4971 (0.5616)	Prec@1 83.984 (80.556)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.5728 (0.5686)	Prec@1 80.078 (80.321)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.6211 (0.5688)	Prec@1 79.297 (80.347)	
Total train loss: 0.5686

Train time: 17.249242782592773
 * Prec@1 76.560 Prec@5 98.820 Loss 0.6729
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.37068009376526

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.5615 (0.5705)	Prec@1 81.250 (80.479)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.5649 (0.5680)	Prec@1 80.469 (80.509)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.5493 (0.5717)	Prec@1 83.594 (80.392)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.5938 (0.5713)	Prec@1 78.906 (80.293)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.4880 (0.5680)	Prec@1 83.594 (80.351)	
Total train loss: 0.5679

Train time: 17.466023445129395
 * Prec@1 76.790 Prec@5 98.760 Loss 0.6665
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.558274507522583

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.5220 (0.5616)	Prec@1 82.422 (80.228)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.5718 (0.5700)	Prec@1 78.516 (79.973)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.5615 (0.5720)	Prec@1 83.203 (79.985)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.5659 (0.5720)	Prec@1 81.250 (80.073)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.4668 (0.5689)	Prec@1 85.547 (80.244)	
Total train loss: 0.5691

Train time: 17.487310647964478
 * Prec@1 76.770 Prec@5 98.790 Loss 0.6650
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.511099338531494

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.6411 (0.5812)	Prec@1 79.688 (79.577)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.5396 (0.5741)	Prec@1 82.031 (79.983)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.5693 (0.5715)	Prec@1 80.469 (80.122)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.4485 (0.5700)	Prec@1 85.547 (80.156)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.5762 (0.5691)	Prec@1 79.297 (80.254)	
Total train loss: 0.5690

Train time: 17.401777744293213
 * Prec@1 76.740 Prec@5 98.780 Loss 0.6670
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.468679189682007

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.5557 (0.5593)	Prec@1 81.641 (80.439)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.5713 (0.5642)	Prec@1 78.125 (80.288)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.5337 (0.5665)	Prec@1 81.250 (80.205)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.5381 (0.5643)	Prec@1 80.078 (80.228)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.5322 (0.5660)	Prec@1 82.422 (80.286)	
Total train loss: 0.5659

Train time: 17.39388680458069
 * Prec@1 76.590 Prec@5 98.750 Loss 0.6670
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.475032806396484

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.5825 (0.5625)	Prec@1 82.031 (80.539)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.5029 (0.5612)	Prec@1 83.594 (80.509)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.5029 (0.5591)	Prec@1 85.938 (80.556)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.6479 (0.5647)	Prec@1 79.688 (80.454)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.5991 (0.5669)	Prec@1 74.609 (80.357)	
Total train loss: 0.5669

Train time: 17.66081976890564
 * Prec@1 76.700 Prec@5 98.800 Loss 0.6675
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.868837594985962

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.5308 (0.5611)	Prec@1 80.859 (80.359)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.5225 (0.5607)	Prec@1 82.422 (80.709)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.6035 (0.5668)	Prec@1 78.516 (80.442)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.5493 (0.5686)	Prec@1 83.594 (80.464)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.5654 (0.5683)	Prec@1 80.469 (80.385)	
Total train loss: 0.5683

Train time: 17.45961856842041
 * Prec@1 76.630 Prec@5 98.790 Loss 0.6660
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.566859245300293

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.5273 (0.5588)	Prec@1 81.250 (81.100)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.5464 (0.5644)	Prec@1 81.641 (80.719)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.5869 (0.5704)	Prec@1 80.078 (80.519)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.5532 (0.5704)	Prec@1 81.250 (80.354)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.6465 (0.5686)	Prec@1 77.344 (80.321)	
Total train loss: 0.5689

Train time: 17.76566243171692
 * Prec@1 76.750 Prec@5 98.840 Loss 0.6641
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.967044830322266

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.5488 (0.5682)	Prec@1 81.250 (80.278)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.5151 (0.5619)	Prec@1 83.594 (80.664)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.5498 (0.5645)	Prec@1 80.078 (80.375)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.6353 (0.5668)	Prec@1 76.953 (80.371)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.6064 (0.5681)	Prec@1 75.781 (80.240)	
Total train loss: 0.5680

Train time: 17.276243209838867
 * Prec@1 76.560 Prec@5 98.780 Loss 0.6685
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.33474349975586

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.4978 (0.5540)	Prec@1 84.766 (81.030)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.4702 (0.5612)	Prec@1 83.984 (80.549)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.5537 (0.5639)	Prec@1 81.250 (80.566)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.6367 (0.5651)	Prec@1 78.125 (80.509)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.5918 (0.5655)	Prec@1 78.906 (80.445)	
Total train loss: 0.5656

Train time: 17.473397731781006
 * Prec@1 76.710 Prec@5 98.820 Loss 0.6660
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 20.66992425918579

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.5244 (0.5812)	Prec@1 81.250 (79.838)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.5737 (0.5735)	Prec@1 80.078 (80.003)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.6270 (0.5708)	Prec@1 78.516 (80.172)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.6255 (0.5688)	Prec@1 78.125 (80.296)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.5430 (0.5679)	Prec@1 83.203 (80.391)	
Total train loss: 0.5681

Train time: 18.17167854309082
 * Prec@1 76.560 Prec@5 98.780 Loss 0.6724
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 22.45521903038025

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.5020 (0.5542)	Prec@1 82.812 (80.950)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.5674 (0.5584)	Prec@1 80.469 (80.609)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.5093 (0.5568)	Prec@1 84.375 (80.632)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.5571 (0.5646)	Prec@1 81.641 (80.361)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.5449 (0.5676)	Prec@1 81.641 (80.282)	
Total train loss: 0.5675

Train time: 17.884186029434204
 * Prec@1 76.890 Prec@5 98.770 Loss 0.6655
Best acc: 77.000
--------------------------------------------------------------------------------
Test time: 21.004474878311157

