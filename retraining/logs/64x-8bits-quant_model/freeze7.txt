
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 50.000 Loss inf
Pre-trained Prec@1 with 7 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.0264 (1.3500)	Prec@1 67.188 (55.268)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.9341 (1.1222)	Prec@1 69.531 (62.350)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.7676 (1.0108)	Prec@1 74.609 (65.628)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.6562 (0.9400)	Prec@1 79.297 (67.949)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.6523 (0.8918)	Prec@1 78.125 (69.569)	
Total train loss: 0.8913

Train time: 204.73699736595154
 * Prec@1 69.740 Prec@5 98.210 Loss 0.8721
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 210.12537598609924

Epoch: [1][38/196]	LR: 0.1	Loss 0.6699 (0.6258)	Prec@1 77.344 (79.117)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.6890 (0.6178)	Prec@1 75.781 (79.006)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.6753 (0.6110)	Prec@1 78.906 (79.150)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.6045 (0.6057)	Prec@1 79.688 (79.304)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.6377 (0.5969)	Prec@1 76.172 (79.553)	
Total train loss: 0.5967

Train time: 35.8387131690979
 * Prec@1 56.520 Prec@5 95.930 Loss 1.3965
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 44.2181077003479

Epoch: [2][38/196]	LR: 0.1	Loss 0.5977 (0.5331)	Prec@1 80.078 (81.781)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.5078 (0.5248)	Prec@1 80.469 (81.956)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.6162 (0.5229)	Prec@1 81.641 (82.098)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.4563 (0.5196)	Prec@1 83.203 (82.179)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.5293 (0.5157)	Prec@1 80.859 (82.278)	
Total train loss: 0.5155

Train time: 47.93911862373352
 * Prec@1 58.760 Prec@5 93.390 Loss 1.2188
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 59.128013610839844

Epoch: [3][38/196]	LR: 0.1	Loss 0.4194 (0.4513)	Prec@1 87.500 (84.265)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.5269 (0.4582)	Prec@1 82.422 (83.999)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.4355 (0.4554)	Prec@1 85.156 (84.148)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4541 (0.4528)	Prec@1 83.594 (84.257)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.5298 (0.4595)	Prec@1 82.812 (84.062)	
Total train loss: 0.4596

Train time: 27.072726488113403
 * Prec@1 64.740 Prec@5 96.980 Loss 1.1963
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 30.01097536087036

Epoch: [4][38/196]	LR: 0.1	Loss 0.3733 (0.3998)	Prec@1 87.109 (86.398)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.4448 (0.4132)	Prec@1 84.766 (85.938)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.4365 (0.4201)	Prec@1 83.594 (85.534)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4180 (0.4225)	Prec@1 85.547 (85.484)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.5576 (0.4253)	Prec@1 81.250 (85.419)	
Total train loss: 0.4254

Train time: 12.960815668106079
 * Prec@1 66.030 Prec@5 97.240 Loss 0.9858
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 16.998707056045532

Epoch: [5][38/196]	LR: 0.1	Loss 0.4001 (0.4016)	Prec@1 87.109 (86.118)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.4636 (0.4013)	Prec@1 84.766 (86.263)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.4036 (0.3987)	Prec@1 86.328 (86.298)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.4675 (0.4035)	Prec@1 81.641 (86.070)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.3020 (0.4019)	Prec@1 90.625 (86.152)	
Total train loss: 0.4018

Train time: 14.400646924972534
 * Prec@1 67.930 Prec@5 97.210 Loss 0.9639
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 17.321560382843018

Epoch: [6][38/196]	LR: 0.1	Loss 0.3440 (0.3410)	Prec@1 85.547 (88.351)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.3962 (0.3540)	Prec@1 86.719 (87.921)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.4192 (0.3597)	Prec@1 86.719 (87.644)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.4436 (0.3679)	Prec@1 82.812 (87.375)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.3225 (0.3708)	Prec@1 86.328 (87.260)	
Total train loss: 0.3710

Train time: 12.209975719451904
 * Prec@1 45.630 Prec@5 91.910 Loss 1.9355
Best acc: 69.740
--------------------------------------------------------------------------------
Test time: 14.804447650909424

Epoch: [7][38/196]	LR: 0.1	Loss 0.4309 (0.3494)	Prec@1 83.984 (88.251)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4082 (0.3498)	Prec@1 83.203 (88.121)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.3584 (0.3564)	Prec@1 88.672 (87.851)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.3271 (0.3567)	Prec@1 89.062 (87.831)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3608 (0.3612)	Prec@1 87.109 (87.740)	
Total train loss: 0.3613

Train time: 13.800281047821045
 * Prec@1 70.880 Prec@5 97.790 Loss 0.9448
Best acc: 70.880
--------------------------------------------------------------------------------
Test time: 16.850956439971924

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2957 (0.3133)	Prec@1 89.453 (89.313)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3225 (0.3059)	Prec@1 89.453 (89.744)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2922 (0.3007)	Prec@1 89.453 (89.924)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3074 (0.2982)	Prec@1 90.234 (89.929)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2266 (0.2950)	Prec@1 91.406 (90.036)	
Total train loss: 0.2949

Train time: 12.462634325027466
 * Prec@1 78.050 Prec@5 98.750 Loss 0.6714
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.288594007492065

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.3396 (0.2721)	Prec@1 88.672 (90.815)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3430 (0.2727)	Prec@1 87.891 (90.890)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2450 (0.2727)	Prec@1 92.578 (90.822)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3013 (0.2745)	Prec@1 90.625 (90.715)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2712 (0.2753)	Prec@1 90.625 (90.731)	
Total train loss: 0.2753

Train time: 13.810405731201172
 * Prec@1 77.370 Prec@5 98.680 Loss 0.6885
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.555164575576782

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.3035 (0.2793)	Prec@1 88.281 (90.445)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3398 (0.2794)	Prec@1 89.453 (90.600)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2949 (0.2765)	Prec@1 91.406 (90.662)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2563 (0.2729)	Prec@1 91.797 (90.718)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2477 (0.2731)	Prec@1 92.188 (90.729)	
Total train loss: 0.2732

Train time: 12.672861337661743
 * Prec@1 77.600 Prec@5 98.570 Loss 0.6870
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.114521741867065

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2549 (0.2700)	Prec@1 91.797 (90.815)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1970 (0.2683)	Prec@1 92.188 (90.790)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2920 (0.2767)	Prec@1 90.234 (90.518)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2576 (0.2765)	Prec@1 91.406 (90.570)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2045 (0.2752)	Prec@1 92.578 (90.667)	
Total train loss: 0.2752

Train time: 12.752312660217285
 * Prec@1 76.970 Prec@5 98.460 Loss 0.7002
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.838813543319702

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.3000 (0.2788)	Prec@1 89.844 (90.685)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2505 (0.2773)	Prec@1 91.797 (90.570)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2659 (0.2746)	Prec@1 91.797 (90.685)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2866 (0.2784)	Prec@1 88.672 (90.602)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2964 (0.2775)	Prec@1 90.234 (90.601)	
Total train loss: 0.2777

Train time: 13.211182832717896
 * Prec@1 77.370 Prec@5 98.590 Loss 0.6934
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.88143277168274

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2705 (0.2814)	Prec@1 91.797 (90.765)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2788 (0.2854)	Prec@1 91.406 (90.450)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2502 (0.2906)	Prec@1 91.016 (90.168)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.3140 (0.2902)	Prec@1 90.625 (90.157)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.3391 (0.2898)	Prec@1 87.891 (90.114)	
Total train loss: 0.2900

Train time: 13.386275053024292
 * Prec@1 77.390 Prec@5 98.560 Loss 0.7070
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.173115730285645

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.2365 (0.2909)	Prec@1 92.578 (90.254)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2991 (0.2900)	Prec@1 88.672 (90.104)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.3591 (0.2902)	Prec@1 85.156 (90.087)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2976 (0.2891)	Prec@1 88.281 (90.147)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2788 (0.2908)	Prec@1 91.016 (90.068)	
Total train loss: 0.2910

Train time: 13.157914876937866
 * Prec@1 77.240 Prec@5 98.690 Loss 0.7065
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.069375038146973

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2795 (0.2913)	Prec@1 89.844 (89.914)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2625 (0.2959)	Prec@1 92.578 (89.959)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.3003 (0.2987)	Prec@1 89.844 (89.744)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2566 (0.3024)	Prec@1 91.016 (89.633)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.2272 (0.3004)	Prec@1 91.797 (89.653)	
Total train loss: 0.3002

Train time: 13.355554819107056
 * Prec@1 76.650 Prec@5 98.460 Loss 0.7178
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.832900524139404

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.2910 (0.2915)	Prec@1 90.625 (89.954)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.4141 (0.3002)	Prec@1 85.156 (89.829)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2830 (0.2968)	Prec@1 90.625 (89.911)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.3074 (0.2974)	Prec@1 90.234 (89.891)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2917 (0.2971)	Prec@1 91.016 (89.872)	
Total train loss: 0.2972

Train time: 13.387609720230103
 * Prec@1 77.150 Prec@5 98.560 Loss 0.7021
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.164913654327393

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2859 (0.2825)	Prec@1 89.453 (90.274)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2115 (0.2855)	Prec@1 93.359 (90.244)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.3123 (0.2889)	Prec@1 89.453 (90.151)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2869 (0.2908)	Prec@1 91.016 (90.052)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2576 (0.2932)	Prec@1 92.578 (89.996)	
Total train loss: 0.2934

Train time: 12.97487187385559
 * Prec@1 76.920 Prec@5 98.560 Loss 0.7085
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.392750024795532

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.3066 (0.2987)	Prec@1 89.844 (89.824)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.3542 (0.2998)	Prec@1 86.719 (89.734)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2791 (0.2957)	Prec@1 91.797 (89.934)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.3142 (0.2966)	Prec@1 88.672 (89.841)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.3276 (0.2948)	Prec@1 87.109 (89.884)	
Total train loss: 0.2948

Train time: 13.277541875839233
 * Prec@1 76.710 Prec@5 98.520 Loss 0.7173
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.52396535873413

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.3333 (0.2958)	Prec@1 90.234 (89.734)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2749 (0.2946)	Prec@1 90.234 (89.849)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.3237 (0.2968)	Prec@1 90.625 (89.744)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2742 (0.2974)	Prec@1 91.406 (89.781)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.2905 (0.2964)	Prec@1 88.281 (89.834)	
Total train loss: 0.2966

Train time: 12.98676347732544
 * Prec@1 77.000 Prec@5 98.600 Loss 0.7090
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.82198190689087

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.3489 (0.2991)	Prec@1 88.672 (89.683)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.3601 (0.2968)	Prec@1 87.891 (89.719)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.3315 (0.2933)	Prec@1 88.281 (89.847)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2549 (0.2961)	Prec@1 92.969 (89.796)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.2712 (0.2938)	Prec@1 91.406 (89.932)	
Total train loss: 0.2937

Train time: 13.190463542938232
 * Prec@1 76.730 Prec@5 98.530 Loss 0.7153
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.797881841659546

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2310 (0.2996)	Prec@1 92.578 (89.784)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.3223 (0.3018)	Prec@1 87.891 (89.694)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2947 (0.2964)	Prec@1 87.500 (89.787)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2512 (0.2960)	Prec@1 92.188 (89.794)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.3259 (0.2959)	Prec@1 89.062 (89.832)	
Total train loss: 0.2961

Train time: 12.971128225326538
 * Prec@1 76.590 Prec@5 98.520 Loss 0.7212
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.623207569122314

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2637 (0.2878)	Prec@1 91.797 (89.974)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2451 (0.2904)	Prec@1 91.016 (89.949)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2939 (0.2880)	Prec@1 91.016 (90.044)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.3240 (0.2882)	Prec@1 89.062 (90.107)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.3093 (0.2924)	Prec@1 88.281 (89.984)	
Total train loss: 0.2925

Train time: 13.37073302268982
 * Prec@1 76.850 Prec@5 98.550 Loss 0.7095
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.423823356628418

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.2920 (0.2844)	Prec@1 90.625 (89.994)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.2820 (0.2901)	Prec@1 89.844 (90.019)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2649 (0.2939)	Prec@1 91.797 (89.917)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2661 (0.2950)	Prec@1 87.500 (89.791)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2715 (0.2951)	Prec@1 91.797 (89.798)	
Total train loss: 0.2949

Train time: 13.037841558456421
 * Prec@1 76.920 Prec@5 98.530 Loss 0.7090
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.966562747955322

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.3193 (0.2963)	Prec@1 90.234 (89.964)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2993 (0.2906)	Prec@1 89.453 (90.239)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2854 (0.2908)	Prec@1 89.453 (90.124)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2607 (0.2919)	Prec@1 92.188 (90.077)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.3164 (0.2941)	Prec@1 89.844 (89.986)	
Total train loss: 0.2942

Train time: 13.342460870742798
 * Prec@1 77.090 Prec@5 98.550 Loss 0.7046
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.969221115112305

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2622 (0.2929)	Prec@1 91.406 (89.884)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2593 (0.2927)	Prec@1 91.797 (89.984)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2708 (0.2938)	Prec@1 90.234 (89.937)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.3596 (0.2917)	Prec@1 87.500 (90.039)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.3254 (0.2929)	Prec@1 88.672 (89.962)	
Total train loss: 0.2931

Train time: 13.00986933708191
 * Prec@1 76.900 Prec@5 98.530 Loss 0.7158
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.079089879989624

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.3313 (0.3088)	Prec@1 89.062 (89.403)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.3696 (0.2983)	Prec@1 87.500 (89.784)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.3159 (0.2962)	Prec@1 87.500 (89.797)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.3193 (0.2947)	Prec@1 86.719 (89.866)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.3196 (0.2939)	Prec@1 91.406 (89.926)	
Total train loss: 0.2937

Train time: 12.800917863845825
 * Prec@1 76.910 Prec@5 98.580 Loss 0.7104
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.856767177581787

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.3391 (0.2869)	Prec@1 89.844 (90.184)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.3240 (0.2886)	Prec@1 89.062 (90.074)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2859 (0.2891)	Prec@1 91.406 (90.174)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.3088 (0.2931)	Prec@1 87.109 (89.981)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2549 (0.2935)	Prec@1 90.234 (89.898)	
Total train loss: 0.2935

Train time: 12.97909426689148
 * Prec@1 76.900 Prec@5 98.550 Loss 0.7104
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.475987672805786

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2334 (0.2841)	Prec@1 93.359 (90.315)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2974 (0.2842)	Prec@1 89.844 (90.244)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2734 (0.2895)	Prec@1 90.625 (89.974)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2754 (0.2917)	Prec@1 92.188 (89.849)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.3235 (0.2936)	Prec@1 89.062 (89.870)	
Total train loss: 0.2938

Train time: 13.420884370803833
 * Prec@1 77.130 Prec@5 98.570 Loss 0.6992
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.159668922424316

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2817 (0.3022)	Prec@1 89.062 (89.643)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2952 (0.2956)	Prec@1 91.016 (90.004)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2264 (0.2936)	Prec@1 93.750 (90.057)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.3909 (0.2919)	Prec@1 84.766 (90.084)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.3489 (0.2935)	Prec@1 88.672 (90.052)	
Total train loss: 0.2937

Train time: 13.183643579483032
 * Prec@1 76.500 Prec@5 98.540 Loss 0.7251
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.880432844161987

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.2834 (0.3002)	Prec@1 91.016 (89.744)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.2595 (0.2895)	Prec@1 89.844 (90.179)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.3008 (0.2901)	Prec@1 90.625 (90.228)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.3523 (0.2909)	Prec@1 86.328 (90.099)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2908 (0.2916)	Prec@1 89.844 (90.042)	
Total train loss: 0.2918

Train time: 12.559552431106567
 * Prec@1 77.380 Prec@5 98.540 Loss 0.6914
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.519002676010132

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.3682 (0.2938)	Prec@1 89.062 (89.854)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.3142 (0.2937)	Prec@1 87.891 (90.014)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.2612 (0.2931)	Prec@1 91.016 (90.024)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.2754 (0.2910)	Prec@1 89.844 (90.134)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.3262 (0.2931)	Prec@1 88.672 (90.036)	
Total train loss: 0.2931

Train time: 13.211304903030396
 * Prec@1 76.740 Prec@5 98.520 Loss 0.7134
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 16.09272265434265

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.2932 (0.3001)	Prec@1 92.578 (90.014)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.2507 (0.2950)	Prec@1 89.844 (89.969)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.3083 (0.2946)	Prec@1 88.672 (89.887)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.2715 (0.2942)	Prec@1 90.234 (89.899)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.3074 (0.2936)	Prec@1 89.453 (89.930)	
Total train loss: 0.2935

Train time: 13.224780321121216
 * Prec@1 76.630 Prec@5 98.520 Loss 0.7188
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.903235912322998

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.2340 (0.2882)	Prec@1 92.188 (90.124)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.3689 (0.2970)	Prec@1 86.719 (89.789)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.3794 (0.2953)	Prec@1 86.328 (89.867)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.3423 (0.2950)	Prec@1 85.547 (89.884)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.2747 (0.2939)	Prec@1 90.234 (89.950)	
Total train loss: 0.2939

Train time: 12.794736623764038
 * Prec@1 76.720 Prec@5 98.540 Loss 0.7158
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.575716018676758

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.3081 (0.2980)	Prec@1 88.281 (89.724)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.3882 (0.2921)	Prec@1 85.938 (89.764)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.2286 (0.2927)	Prec@1 92.188 (89.880)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.3037 (0.2937)	Prec@1 91.406 (89.936)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.2793 (0.2933)	Prec@1 89.844 (89.966)	
Total train loss: 0.2935

Train time: 12.552571058273315
 * Prec@1 76.720 Prec@5 98.530 Loss 0.7139
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.484463453292847

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.2583 (0.2890)	Prec@1 91.406 (90.355)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.3020 (0.2893)	Prec@1 89.453 (90.400)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.3223 (0.2903)	Prec@1 89.844 (90.345)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.3452 (0.2919)	Prec@1 87.109 (90.169)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.2430 (0.2924)	Prec@1 92.578 (90.092)	
Total train loss: 0.2927

Train time: 13.328654766082764
 * Prec@1 76.470 Prec@5 98.490 Loss 0.7236
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.947481393814087

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.2869 (0.3001)	Prec@1 89.062 (89.563)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.2666 (0.2961)	Prec@1 91.016 (89.769)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.2502 (0.2923)	Prec@1 91.797 (89.890)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.3184 (0.2921)	Prec@1 87.891 (90.019)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.2532 (0.2938)	Prec@1 91.016 (89.954)	
Total train loss: 0.2937

Train time: 12.707248210906982
 * Prec@1 76.590 Prec@5 98.550 Loss 0.7173
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.555328369140625

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.3525 (0.2974)	Prec@1 88.672 (89.633)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.2776 (0.2924)	Prec@1 91.406 (89.969)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.2595 (0.2943)	Prec@1 91.406 (89.931)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.3401 (0.2926)	Prec@1 89.062 (89.989)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.3057 (0.2928)	Prec@1 90.625 (89.976)	
Total train loss: 0.2928

Train time: 13.040581226348877
 * Prec@1 76.860 Prec@5 98.520 Loss 0.7114
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.905605792999268

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2659 (0.2911)	Prec@1 90.625 (90.264)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.3225 (0.2937)	Prec@1 89.062 (89.959)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.2827 (0.2953)	Prec@1 89.844 (89.921)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.2385 (0.2937)	Prec@1 92.578 (89.981)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.2871 (0.2930)	Prec@1 89.844 (90.010)	
Total train loss: 0.2931

Train time: 12.730094194412231
 * Prec@1 76.740 Prec@5 98.530 Loss 0.7085
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.685206174850464

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.3699 (0.2976)	Prec@1 86.328 (90.074)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.2949 (0.2969)	Prec@1 89.062 (90.024)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.3054 (0.2969)	Prec@1 89.062 (90.014)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.2211 (0.2961)	Prec@1 91.797 (89.954)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.3042 (0.2941)	Prec@1 90.625 (90.014)	
Total train loss: 0.2942

Train time: 12.834356784820557
 * Prec@1 76.760 Prec@5 98.470 Loss 0.7183
Best acc: 78.050
--------------------------------------------------------------------------------
Test time: 15.528550386428833

