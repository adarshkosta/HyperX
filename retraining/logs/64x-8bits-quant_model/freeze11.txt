
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b-lp/
          pretrained: ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20qfp_cifar10_half_quant_all_w7b_a7b_best.pth.tar ...
Original model accuracy: 89.29999542236328
ResNet_cifar(
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 10.000 Prec@5 50.600 Loss inf
Pre-trained Prec@1 with 11 layers frozen: 10.0 	 Loss: inf

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.9199 (1.3627)	Prec@1 68.359 (54.718)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.7388 (1.1277)	Prec@1 76.172 (61.654)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.6987 (1.0169)	Prec@1 77.344 (65.304)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.6553 (0.9475)	Prec@1 75.391 (67.661)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.5947 (0.8948)	Prec@1 77.734 (69.417)	
Total train loss: 0.8944

Train time: 168.01061725616455
 * Prec@1 44.390 Prec@5 85.990 Loss 1.8174
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 174.41476488113403

Epoch: [1][38/196]	LR: 0.1	Loss 0.5601 (0.6264)	Prec@1 80.078 (78.395)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.6431 (0.6151)	Prec@1 78.125 (78.871)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.6113 (0.6079)	Prec@1 78.125 (79.050)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.6353 (0.6046)	Prec@1 78.125 (79.154)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.6558 (0.5963)	Prec@1 77.734 (79.439)	
Total train loss: 0.5963

Train time: 23.68317151069641
 * Prec@1 40.900 Prec@5 86.260 Loss 1.9219
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 29.23632526397705

Epoch: [2][38/196]	LR: 0.1	Loss 0.4666 (0.5120)	Prec@1 83.203 (81.931)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.4839 (0.5206)	Prec@1 84.766 (81.791)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.5396 (0.5172)	Prec@1 80.859 (82.018)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.5283 (0.5171)	Prec@1 82.422 (82.104)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.5200 (0.5141)	Prec@1 79.297 (82.226)	
Total train loss: 0.5139

Train time: 23.29156804084778
 * Prec@1 43.950 Prec@5 88.420 Loss 1.9502
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 28.44543695449829

Epoch: [3][38/196]	LR: 0.1	Loss 0.4656 (0.4663)	Prec@1 83.984 (84.044)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.4004 (0.4723)	Prec@1 86.719 (83.859)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.5444 (0.4733)	Prec@1 79.297 (83.707)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4417 (0.4688)	Prec@1 82.031 (83.812)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.4788 (0.4692)	Prec@1 81.641 (83.778)	
Total train loss: 0.4695

Train time: 23.33503222465515
 * Prec@1 28.650 Prec@5 85.320 Loss 2.4180
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 27.021323204040527

Epoch: [4][38/196]	LR: 0.1	Loss 0.4233 (0.4280)	Prec@1 86.719 (85.246)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.4272 (0.4368)	Prec@1 85.938 (84.851)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.4329 (0.4372)	Prec@1 85.156 (84.976)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4285 (0.4335)	Prec@1 83.203 (85.064)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.3638 (0.4299)	Prec@1 86.719 (85.196)	
Total train loss: 0.4298

Train time: 20.94201683998108
 * Prec@1 34.590 Prec@5 78.550 Loss 2.4082
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 25.722113370895386

Epoch: [5][38/196]	LR: 0.1	Loss 0.4834 (0.3914)	Prec@1 82.812 (86.168)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.4810 (0.4101)	Prec@1 84.375 (85.707)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.3879 (0.4158)	Prec@1 85.938 (85.564)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.3633 (0.4136)	Prec@1 89.453 (85.680)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.4387 (0.4119)	Prec@1 85.547 (85.765)	
Total train loss: 0.4118

Train time: 22.297280311584473
 * Prec@1 36.470 Prec@5 90.380 Loss 2.5762
Best acc: 44.390
--------------------------------------------------------------------------------
Test time: 26.06624460220337

Epoch: [6][38/196]	LR: 0.1	Loss 0.4158 (0.3889)	Prec@1 84.766 (86.619)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2954 (0.3986)	Prec@1 87.500 (86.153)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.4309 (0.3971)	Prec@1 86.719 (86.308)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.3416 (0.3978)	Prec@1 89.453 (86.195)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.4053 (0.4000)	Prec@1 85.547 (86.090)	
Total train loss: 0.4002

Train time: 19.995243310928345
 * Prec@1 47.590 Prec@5 91.190 Loss 1.7949
Best acc: 47.590
--------------------------------------------------------------------------------
Test time: 24.27983069419861

Epoch: [7][38/196]	LR: 0.1	Loss 0.4111 (0.3979)	Prec@1 83.984 (86.248)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4075 (0.3826)	Prec@1 85.938 (86.839)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.4050 (0.3808)	Prec@1 84.375 (86.896)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.5225 (0.3800)	Prec@1 85.547 (86.856)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3792 (0.3792)	Prec@1 85.938 (86.925)	
Total train loss: 0.3794

Train time: 21.839003324508667
 * Prec@1 39.140 Prec@5 91.540 Loss 1.8604
Best acc: 47.590
--------------------------------------------------------------------------------
Test time: 26.169013261795044

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3008 (0.3345)	Prec@1 89.844 (88.792)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3604 (0.3332)	Prec@1 89.062 (88.807)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3904 (0.3324)	Prec@1 87.109 (88.852)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.4019 (0.3304)	Prec@1 85.547 (88.855)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.3733 (0.3292)	Prec@1 88.672 (88.870)	
Total train loss: 0.3289

Train time: 23.137962102890015
 * Prec@1 50.420 Prec@5 93.330 Loss 1.5898
Best acc: 50.420
--------------------------------------------------------------------------------
Test time: 27.85029697418213

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.3904 (0.3277)	Prec@1 86.328 (89.002)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3276 (0.3280)	Prec@1 87.500 (88.862)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2625 (0.3226)	Prec@1 91.797 (89.052)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3274 (0.3198)	Prec@1 87.891 (89.098)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.3450 (0.3199)	Prec@1 88.281 (89.155)	
Total train loss: 0.3201

Train time: 21.713805675506592
 * Prec@1 51.480 Prec@5 93.080 Loss 1.5732
Best acc: 51.480
--------------------------------------------------------------------------------
Test time: 26.264477729797363

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.3267 (0.3103)	Prec@1 87.109 (89.633)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2681 (0.3095)	Prec@1 92.969 (89.638)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.3066 (0.3118)	Prec@1 89.453 (89.547)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2433 (0.3137)	Prec@1 92.969 (89.408)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.3789 (0.3154)	Prec@1 85.156 (89.307)	
Total train loss: 0.3156

Train time: 22.996747255325317
 * Prec@1 52.360 Prec@5 92.790 Loss 1.5381
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 27.25132131576538

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.3574 (0.3073)	Prec@1 87.500 (89.643)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.3516 (0.3135)	Prec@1 88.281 (89.403)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2705 (0.3137)	Prec@1 89.453 (89.356)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2529 (0.3151)	Prec@1 91.797 (89.333)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.3240 (0.3167)	Prec@1 87.891 (89.253)	
Total train loss: 0.3166

Train time: 19.81985855102539
 * Prec@1 51.800 Prec@5 92.950 Loss 1.5430
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 23.64756202697754

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.3098 (0.3197)	Prec@1 89.062 (89.103)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2715 (0.3186)	Prec@1 91.797 (89.268)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2131 (0.3175)	Prec@1 92.188 (89.189)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2756 (0.3138)	Prec@1 89.062 (89.360)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.3457 (0.3149)	Prec@1 87.109 (89.321)	
Total train loss: 0.3151

Train time: 20.28815746307373
 * Prec@1 52.200 Prec@5 92.940 Loss 1.5469
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.438557863235474

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2546 (0.3211)	Prec@1 92.188 (88.832)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.3416 (0.3140)	Prec@1 87.109 (89.183)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.3032 (0.3156)	Prec@1 88.672 (89.086)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.3286 (0.3139)	Prec@1 88.281 (89.253)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2781 (0.3158)	Prec@1 89.453 (89.191)	
Total train loss: 0.3159

Train time: 20.18010425567627
 * Prec@1 51.910 Prec@5 93.130 Loss 1.5469
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 23.903969764709473

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.3403 (0.3161)	Prec@1 87.891 (89.283)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2837 (0.3163)	Prec@1 89.844 (89.413)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2939 (0.3153)	Prec@1 87.891 (89.390)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3015 (0.3140)	Prec@1 90.234 (89.448)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.3052 (0.3138)	Prec@1 88.672 (89.437)	
Total train loss: 0.3137

Train time: 22.363674640655518
 * Prec@1 50.540 Prec@5 92.400 Loss 1.6191
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.57322359085083

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.3140 (0.3010)	Prec@1 89.453 (89.904)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2969 (0.3101)	Prec@1 89.062 (89.543)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2307 (0.3117)	Prec@1 92.188 (89.413)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.3887 (0.3156)	Prec@1 87.109 (89.220)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.2473 (0.3151)	Prec@1 89.844 (89.287)	
Total train loss: 0.3150

Train time: 21.61419653892517
 * Prec@1 50.530 Prec@5 91.910 Loss 1.6543
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 25.80743956565857

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.2668 (0.3026)	Prec@1 92.578 (89.914)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.2996 (0.3044)	Prec@1 90.234 (89.834)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.3247 (0.3112)	Prec@1 90.625 (89.580)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.3760 (0.3132)	Prec@1 87.109 (89.483)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.3359 (0.3128)	Prec@1 87.500 (89.441)	
Total train loss: 0.3127

Train time: 22.634843111038208
 * Prec@1 50.680 Prec@5 92.300 Loss 1.6191
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.776373863220215

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2600 (0.3100)	Prec@1 91.797 (89.754)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.3513 (0.3139)	Prec@1 85.547 (89.418)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.3477 (0.3164)	Prec@1 89.062 (89.266)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.3479 (0.3150)	Prec@1 88.672 (89.255)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.3040 (0.3142)	Prec@1 90.234 (89.321)	
Total train loss: 0.3142

Train time: 20.42843246459961
 * Prec@1 51.210 Prec@5 92.360 Loss 1.5908
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.106053113937378

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2179 (0.3016)	Prec@1 93.750 (89.974)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.3042 (0.3065)	Prec@1 88.672 (89.663)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2964 (0.3059)	Prec@1 89.844 (89.764)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2937 (0.3090)	Prec@1 88.281 (89.643)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.3369 (0.3119)	Prec@1 89.844 (89.489)	
Total train loss: 0.3122

Train time: 21.764843463897705
 * Prec@1 51.240 Prec@5 92.610 Loss 1.6064
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.27613615989685

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.3594 (0.3198)	Prec@1 87.891 (89.052)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2737 (0.3203)	Prec@1 90.625 (89.213)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.3528 (0.3126)	Prec@1 91.016 (89.487)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.3052 (0.3120)	Prec@1 89.453 (89.478)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.3625 (0.3127)	Prec@1 87.109 (89.395)	
Total train loss: 0.3127

Train time: 21.63686490058899
 * Prec@1 51.870 Prec@5 92.510 Loss 1.5645
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 25.725027561187744

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.3052 (0.3046)	Prec@1 91.016 (89.854)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.2947 (0.3103)	Prec@1 89.844 (89.608)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.3198 (0.3132)	Prec@1 90.625 (89.403)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2991 (0.3109)	Prec@1 90.625 (89.491)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.3323 (0.3140)	Prec@1 89.453 (89.365)	
Total train loss: 0.3142

Train time: 20.68040633201599
 * Prec@1 50.400 Prec@5 92.260 Loss 1.6309
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.99362540245056

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2754 (0.3227)	Prec@1 91.016 (89.032)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.3337 (0.3193)	Prec@1 91.797 (89.138)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.3389 (0.3154)	Prec@1 86.328 (89.356)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.3953 (0.3153)	Prec@1 88.281 (89.330)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.2737 (0.3130)	Prec@1 92.188 (89.387)	
Total train loss: 0.3131

Train time: 20.473729372024536
 * Prec@1 50.360 Prec@5 92.130 Loss 1.6328
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.344652891159058

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2815 (0.3233)	Prec@1 90.234 (89.062)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.3176 (0.3188)	Prec@1 87.891 (89.418)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.3567 (0.3115)	Prec@1 86.328 (89.633)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2620 (0.3125)	Prec@1 91.016 (89.501)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.3164 (0.3131)	Prec@1 89.453 (89.421)	
Total train loss: 0.3132

Train time: 22.281261444091797
 * Prec@1 50.190 Prec@5 92.120 Loss 1.6328
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 27.060420274734497

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.3359 (0.3056)	Prec@1 88.672 (89.794)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.3271 (0.3072)	Prec@1 86.719 (89.658)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.3696 (0.3134)	Prec@1 87.891 (89.350)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.3630 (0.3127)	Prec@1 87.891 (89.373)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.3262 (0.3144)	Prec@1 86.719 (89.269)	
Total train loss: 0.3143

Train time: 21.850253105163574
 * Prec@1 50.770 Prec@5 92.530 Loss 1.6104
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 25.599607944488525

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.3557 (0.3112)	Prec@1 88.281 (89.473)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.3047 (0.3150)	Prec@1 90.234 (89.358)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2849 (0.3133)	Prec@1 89.844 (89.406)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2805 (0.3128)	Prec@1 90.234 (89.328)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.3594 (0.3135)	Prec@1 89.453 (89.321)	
Total train loss: 0.3136

Train time: 21.885161876678467
 * Prec@1 51.090 Prec@5 92.750 Loss 1.5957
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.137788772583008

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.3196 (0.3182)	Prec@1 89.453 (88.992)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2399 (0.3154)	Prec@1 92.969 (89.348)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.3335 (0.3138)	Prec@1 90.234 (89.420)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.3513 (0.3146)	Prec@1 87.891 (89.338)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.3652 (0.3126)	Prec@1 86.719 (89.439)	
Total train loss: 0.3127

Train time: 20.015300989151
 * Prec@1 51.210 Prec@5 92.430 Loss 1.5996
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.057551622390747

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.3850 (0.3112)	Prec@1 87.500 (89.383)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.2949 (0.3094)	Prec@1 87.500 (89.423)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.3259 (0.3103)	Prec@1 90.625 (89.276)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.2949 (0.3116)	Prec@1 91.797 (89.315)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.3379 (0.3131)	Prec@1 89.453 (89.293)	
Total train loss: 0.3131

Train time: 20.70723867416382
 * Prec@1 49.970 Prec@5 92.360 Loss 1.6465
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.69075846672058

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.3152 (0.3116)	Prec@1 92.188 (89.363)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.2986 (0.3168)	Prec@1 88.672 (89.373)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2744 (0.3130)	Prec@1 91.797 (89.443)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2817 (0.3135)	Prec@1 91.406 (89.363)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2397 (0.3120)	Prec@1 90.234 (89.403)	
Total train loss: 0.3120

Train time: 19.988459587097168
 * Prec@1 50.330 Prec@5 92.330 Loss 1.6240
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.69438672065735

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.3132 (0.3170)	Prec@1 89.062 (89.093)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2277 (0.3114)	Prec@1 94.922 (89.348)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.3655 (0.3116)	Prec@1 87.500 (89.450)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2286 (0.3128)	Prec@1 92.188 (89.421)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.3296 (0.3135)	Prec@1 89.453 (89.405)	
Total train loss: 0.3135

Train time: 21.776209831237793
 * Prec@1 51.730 Prec@5 92.610 Loss 1.5732
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.026151180267334

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.3625 (0.2993)	Prec@1 89.453 (89.854)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.3167 (0.3078)	Prec@1 89.062 (89.468)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.3223 (0.3117)	Prec@1 90.625 (89.430)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.3911 (0.3111)	Prec@1 85.156 (89.421)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.2661 (0.3120)	Prec@1 91.016 (89.369)	
Total train loss: 0.3121

Train time: 22.5130877494812
 * Prec@1 51.000 Prec@5 92.360 Loss 1.5947
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 27.140547513961792

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.3267 (0.3019)	Prec@1 89.844 (89.944)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.2522 (0.3086)	Prec@1 88.672 (89.658)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.2886 (0.3098)	Prec@1 90.234 (89.527)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.3557 (0.3098)	Prec@1 89.453 (89.561)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2903 (0.3111)	Prec@1 90.234 (89.391)	
Total train loss: 0.3111

Train time: 22.62642216682434
 * Prec@1 51.020 Prec@5 92.570 Loss 1.5957
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 27.284492254257202

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.3608 (0.3097)	Prec@1 86.328 (89.283)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.3701 (0.3119)	Prec@1 87.109 (89.258)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.3501 (0.3102)	Prec@1 87.109 (89.373)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.3481 (0.3131)	Prec@1 87.500 (89.358)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.2695 (0.3125)	Prec@1 90.625 (89.401)	
Total train loss: 0.3126

Train time: 21.109097242355347
 * Prec@1 50.870 Prec@5 92.570 Loss 1.6113
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.977898359298706

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.3599 (0.3076)	Prec@1 86.719 (89.503)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.2900 (0.3124)	Prec@1 90.234 (89.313)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.3362 (0.3132)	Prec@1 88.281 (89.363)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.3179 (0.3138)	Prec@1 89.844 (89.323)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.3120 (0.3134)	Prec@1 88.672 (89.301)	
Total train loss: 0.3136

Train time: 19.660141229629517
 * Prec@1 50.960 Prec@5 92.400 Loss 1.6006
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.188085794448853

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.2937 (0.3067)	Prec@1 89.844 (89.643)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.3132 (0.3126)	Prec@1 89.844 (89.368)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.3459 (0.3140)	Prec@1 88.672 (89.320)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.3167 (0.3129)	Prec@1 87.500 (89.345)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.2986 (0.3120)	Prec@1 90.234 (89.395)	
Total train loss: 0.3119

Train time: 22.563953638076782
 * Prec@1 51.730 Prec@5 92.710 Loss 1.5742
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 26.375450611114502

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.3506 (0.3185)	Prec@1 87.891 (89.203)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.2712 (0.3118)	Prec@1 90.234 (89.428)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.3406 (0.3119)	Prec@1 87.109 (89.443)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.3750 (0.3124)	Prec@1 85.938 (89.436)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.3323 (0.3121)	Prec@1 88.672 (89.399)	
Total train loss: 0.3123

Train time: 20.354814052581787
 * Prec@1 51.470 Prec@5 92.490 Loss 1.5781
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.716925144195557

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.3267 (0.3157)	Prec@1 88.281 (89.413)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.3440 (0.3179)	Prec@1 89.062 (89.208)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.3147 (0.3167)	Prec@1 89.062 (89.269)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.3223 (0.3170)	Prec@1 89.844 (89.295)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.2607 (0.3137)	Prec@1 91.797 (89.437)	
Total train loss: 0.3138

Train time: 19.768043756484985
 * Prec@1 51.320 Prec@5 92.350 Loss 1.5820
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 23.81448483467102

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.2783 (0.3181)	Prec@1 89.844 (89.032)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.3784 (0.3121)	Prec@1 88.281 (89.338)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.3308 (0.3113)	Prec@1 86.719 (89.330)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.2537 (0.3116)	Prec@1 91.406 (89.365)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.2668 (0.3128)	Prec@1 91.797 (89.315)	
Total train loss: 0.3129

Train time: 21.295756340026855
 * Prec@1 51.140 Prec@5 92.020 Loss 1.5859
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 25.500449180603027

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.2898 (0.3137)	Prec@1 90.234 (89.323)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.3018 (0.3139)	Prec@1 89.062 (89.243)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.3833 (0.3122)	Prec@1 88.281 (89.473)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.3132 (0.3151)	Prec@1 89.844 (89.333)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.3257 (0.3126)	Prec@1 88.281 (89.443)	
Total train loss: 0.3125

Train time: 20.382587671279907
 * Prec@1 50.860 Prec@5 92.450 Loss 1.6143
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 24.151586294174194

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2678 (0.3130)	Prec@1 91.016 (89.173)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.3149 (0.3110)	Prec@1 91.016 (89.408)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.3469 (0.3098)	Prec@1 88.281 (89.570)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.2759 (0.3091)	Prec@1 90.234 (89.563)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.3853 (0.3127)	Prec@1 87.891 (89.425)	
Total train loss: 0.3129

Train time: 11.648167371749878
 * Prec@1 51.770 Prec@5 92.780 Loss 1.5781
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 13.94284176826477

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.3574 (0.3128)	Prec@1 86.719 (89.303)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.3503 (0.3113)	Prec@1 89.453 (89.383)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.3252 (0.3128)	Prec@1 88.672 (89.450)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.3113 (0.3119)	Prec@1 89.453 (89.421)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.3098 (0.3124)	Prec@1 88.672 (89.419)	
Total train loss: 0.3129

Train time: 11.294602632522583
 * Prec@1 50.320 Prec@5 92.430 Loss 1.6230
Best acc: 52.360
--------------------------------------------------------------------------------
Test time: 14.208211421966553

