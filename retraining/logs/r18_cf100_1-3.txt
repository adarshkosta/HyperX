
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 0.790 Prec@5 4.430 Loss 4.6211
Pre-trained Prec@1 with 1 layers frozen: 0.7899999618530273 	 Loss: 4.62109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 3.8145 (4.5094)	Prec@1 15.625 (6.020)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.0078 (3.9422)	Prec@1 33.984 (15.500)	
Epoch: [0][116/196]	LR: 0.002	Loss 2.6074 (3.5551)	Prec@1 43.359 (23.504)	
Epoch: [0][155/196]	LR: 0.002	Loss 2.4023 (3.2798)	Prec@1 51.562 (29.465)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.0742 (3.0730)	Prec@1 57.812 (33.932)	
Total train loss: 3.0716

 * Prec@1 56.210 Prec@5 85.250 Loss 2.0508
Best acc: 56.210
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.0117 (2.0028)	Prec@1 57.031 (57.752)	
Epoch: [1][77/196]	LR: 0.002	Loss 1.8916 (1.9375)	Prec@1 60.156 (59.465)	
Epoch: [1][116/196]	LR: 0.002	Loss 1.6738 (1.8836)	Prec@1 67.188 (60.534)	
Epoch: [1][155/196]	LR: 0.002	Loss 1.8242 (1.8405)	Prec@1 59.766 (61.256)	
Epoch: [1][194/196]	LR: 0.002	Loss 1.6113 (1.8043)	Prec@1 62.500 (61.887)	
Total train loss: 1.8045

 * Prec@1 65.050 Prec@5 91.090 Loss 1.5947
Best acc: 65.050
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 1.5430 (1.5342)	Prec@1 67.188 (67.758)	
Epoch: [2][77/196]	LR: 0.002	Loss 1.3418 (1.5179)	Prec@1 70.312 (67.964)	
Epoch: [2][116/196]	LR: 0.002	Loss 1.5049 (1.4996)	Prec@1 69.141 (68.470)	
Epoch: [2][155/196]	LR: 0.002	Loss 1.4688 (1.4840)	Prec@1 66.016 (68.753)	
Epoch: [2][194/196]	LR: 0.002	Loss 1.5801 (1.4674)	Prec@1 65.625 (68.970)	
Total train loss: 1.4678

 * Prec@1 68.770 Prec@5 92.840 Loss 1.4004
Best acc: 68.770
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.3447 (1.3240)	Prec@1 69.531 (71.885)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.2852 (1.3162)	Prec@1 73.438 (72.095)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.2197 (1.3056)	Prec@1 73.438 (72.326)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.3066 (1.2954)	Prec@1 70.703 (72.536)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.2520 (1.2900)	Prec@1 74.219 (72.508)	
Total train loss: 1.2903

 * Prec@1 71.200 Prec@5 93.450 Loss 1.2910
Best acc: 71.200
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.2656 (1.1741)	Prec@1 70.312 (75.531)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.1963 (1.1715)	Prec@1 72.656 (75.566)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.1514 (1.1653)	Prec@1 76.172 (75.581)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.0527 (1.1618)	Prec@1 78.906 (75.568)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.1885 (1.1582)	Prec@1 73.828 (75.613)	
Total train loss: 1.1586

 * Prec@1 72.670 Prec@5 93.970 Loss 1.2148
Best acc: 72.670
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.1328 (1.0758)	Prec@1 73.047 (77.764)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.1064 (1.0654)	Prec@1 76.562 (77.995)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.0830 (1.0674)	Prec@1 76.953 (77.835)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.0654 (1.0653)	Prec@1 76.562 (77.830)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.0254 (1.0608)	Prec@1 77.734 (77.955)	
Total train loss: 1.0614

 * Prec@1 73.790 Prec@5 94.260 Loss 1.1670
Best acc: 73.790
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.0488 (0.9985)	Prec@1 80.078 (79.527)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.0449 (0.9893)	Prec@1 78.125 (79.718)	
Epoch: [6][116/196]	LR: 0.002	Loss 1.0127 (0.9884)	Prec@1 81.641 (79.694)	
Epoch: [6][155/196]	LR: 0.002	Loss 0.9546 (0.9873)	Prec@1 80.078 (79.645)	
Epoch: [6][194/196]	LR: 0.002	Loss 0.9121 (0.9829)	Prec@1 82.422 (79.641)	
Total train loss: 0.9831

 * Prec@1 74.830 Prec@5 94.450 Loss 1.1240
Best acc: 74.830
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 0.8511 (0.8957)	Prec@1 84.375 (82.101)	
Epoch: [7][77/196]	LR: 0.002	Loss 0.7988 (0.9037)	Prec@1 87.500 (82.011)	
Epoch: [7][116/196]	LR: 0.002	Loss 0.9312 (0.9077)	Prec@1 79.688 (81.804)	
Epoch: [7][155/196]	LR: 0.002	Loss 0.8687 (0.9105)	Prec@1 85.156 (81.766)	
Epoch: [7][194/196]	LR: 0.002	Loss 0.8457 (0.9128)	Prec@1 82.812 (81.647)	
Total train loss: 0.9132

 * Prec@1 75.100 Prec@5 94.650 Loss 1.1006
Best acc: 75.100
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 0.8286 (0.8680)	Prec@1 84.375 (82.883)	
Epoch: [8][77/196]	LR: 0.002	Loss 0.8301 (0.8525)	Prec@1 83.594 (83.358)	
Epoch: [8][116/196]	LR: 0.002	Loss 0.8413 (0.8543)	Prec@1 82.031 (83.176)	
Epoch: [8][155/196]	LR: 0.002	Loss 0.8555 (0.8590)	Prec@1 80.078 (82.880)	
Epoch: [8][194/196]	LR: 0.002	Loss 0.8628 (0.8564)	Prec@1 83.594 (82.937)	
Total train loss: 0.8566

 * Prec@1 75.290 Prec@5 94.850 Loss 1.0791
Best acc: 75.290
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 0.7974 (0.7986)	Prec@1 83.594 (84.976)	
Epoch: [9][77/196]	LR: 0.002	Loss 0.7539 (0.7952)	Prec@1 85.156 (84.956)	
Epoch: [9][116/196]	LR: 0.002	Loss 0.7642 (0.8020)	Prec@1 85.547 (84.692)	
Epoch: [9][155/196]	LR: 0.002	Loss 0.8076 (0.8016)	Prec@1 83.594 (84.613)	
Epoch: [9][194/196]	LR: 0.002	Loss 0.8599 (0.8030)	Prec@1 82.422 (84.557)	
Total train loss: 0.8036

 * Prec@1 75.810 Prec@5 94.770 Loss 1.0723
Best acc: 75.810
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 0.8062 (0.7465)	Prec@1 83.984 (86.448)	
Epoch: [10][77/196]	LR: 0.0004	Loss 0.6655 (0.7472)	Prec@1 87.891 (86.308)	
Epoch: [10][116/196]	LR: 0.0004	Loss 0.7510 (0.7438)	Prec@1 86.328 (86.418)	
Epoch: [10][155/196]	LR: 0.0004	Loss 0.7432 (0.7418)	Prec@1 84.766 (86.436)	
Epoch: [10][194/196]	LR: 0.0004	Loss 0.7744 (0.7418)	Prec@1 86.328 (86.446)	
Total train loss: 0.7421

 * Prec@1 75.930 Prec@5 94.880 Loss 1.0566
Best acc: 75.930
--------------------------------------------------------------------------------
Epoch: [11][38/196]	LR: 0.0004	Loss 0.6924 (0.7329)	Prec@1 86.719 (86.679)	
Epoch: [11][77/196]	LR: 0.0004	Loss 0.7271 (0.7354)	Prec@1 87.500 (86.564)	
Epoch: [11][116/196]	LR: 0.0004	Loss 0.7422 (0.7339)	Prec@1 83.594 (86.659)	
Epoch: [11][155/196]	LR: 0.0004	Loss 0.8018 (0.7377)	Prec@1 82.422 (86.571)	
Epoch: [11][194/196]	LR: 0.0004	Loss 0.7578 (0.7375)	Prec@1 87.109 (86.591)	
Total train loss: 0.7378

 * Prec@1 75.810 Prec@5 94.840 Loss 1.0605
Best acc: 75.930
--------------------------------------------------------------------------------
Epoch: [12][38/196]	LR: 0.0004	Loss 0.7188 (0.7446)	Prec@1 88.672 (86.659)	
Epoch: [12][77/196]	LR: 0.0004	Loss 0.6841 (0.7385)	Prec@1 87.500 (86.709)	
Epoch: [12][116/196]	LR: 0.0004	Loss 0.7905 (0.7392)	Prec@1 82.812 (86.602)	
Epoch: [12][155/196]	LR: 0.0004	Loss 0.8398 (0.7376)	Prec@1 85.156 (86.609)	
Epoch: [12][194/196]	LR: 0.0004	Loss 0.6997 (0.7358)	Prec@1 89.844 (86.705)	
Total train loss: 0.7360

 * Prec@1 75.810 Prec@5 94.900 Loss 1.0547
Best acc: 75.930
--------------------------------------------------------------------------------
Epoch: [13][38/196]	LR: 0.0004	Loss 0.7573 (0.7332)	Prec@1 85.547 (86.769)	
Epoch: [13][77/196]	LR: 0.0004	Loss 0.7661 (0.7278)	Prec@1 85.547 (87.114)	
Epoch: [13][116/196]	LR: 0.0004	Loss 0.7266 (0.7284)	Prec@1 87.500 (87.006)	
Epoch: [13][155/196]	LR: 0.0004	Loss 0.8384 (0.7329)	Prec@1 82.031 (86.761)	
Epoch: [13][194/196]	LR: 0.0004	Loss 0.7256 (0.7327)	Prec@1 87.891 (86.839)	
Total train loss: 0.7332

 * Prec@1 76.040 Prec@5 94.980 Loss 1.0566
Best acc: 76.040
--------------------------------------------------------------------------------
Epoch: [14][38/196]	LR: 0.0004	Loss 0.6812 (0.7153)	Prec@1 87.500 (87.310)	
Epoch: [14][77/196]	LR: 0.0004	Loss 0.6924 (0.7266)	Prec@1 87.109 (86.964)	
Epoch: [14][116/196]	LR: 0.0004	Loss 0.8154 (0.7244)	Prec@1 82.031 (87.036)	
Epoch: [14][155/196]	LR: 0.0004	Loss 0.7129 (0.7246)	Prec@1 85.547 (86.904)	
Epoch: [14][194/196]	LR: 0.0004	Loss 0.7646 (0.7242)	Prec@1 86.328 (86.925)	
Total train loss: 0.7246

 * Prec@1 75.740 Prec@5 94.860 Loss 1.0586
Best acc: 76.040
--------------------------------------------------------------------------------
Epoch: [15][38/196]	LR: 0.0004	Loss 0.6416 (0.7261)	Prec@1 90.234 (86.769)	
Epoch: [15][77/196]	LR: 0.0004	Loss 0.7080 (0.7213)	Prec@1 89.844 (87.094)	
Epoch: [15][116/196]	LR: 0.0004	Loss 0.7866 (0.7270)	Prec@1 86.328 (86.956)	
Epoch: [15][155/196]	LR: 0.0004	Loss 0.7026 (0.7290)	Prec@1 87.500 (86.859)	
Epoch: [15][194/196]	LR: 0.0004	Loss 0.7007 (0.7280)	Prec@1 87.500 (86.795)	
Total train loss: 0.7283

 * Prec@1 76.070 Prec@5 94.840 Loss 1.0557
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [16][38/196]	LR: 0.0004	Loss 0.6602 (0.7161)	Prec@1 87.109 (87.129)	
Epoch: [16][77/196]	LR: 0.0004	Loss 0.7236 (0.7210)	Prec@1 86.719 (87.109)	
Epoch: [16][116/196]	LR: 0.0004	Loss 0.8252 (0.7207)	Prec@1 86.719 (87.103)	
Epoch: [16][155/196]	LR: 0.0004	Loss 0.7358 (0.7223)	Prec@1 88.281 (87.039)	
Epoch: [16][194/196]	LR: 0.0004	Loss 0.6841 (0.7217)	Prec@1 89.062 (87.103)	
Total train loss: 0.7220

 * Prec@1 75.950 Prec@5 94.900 Loss 1.0537
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [17][38/196]	LR: 0.0004	Loss 0.7261 (0.7030)	Prec@1 87.891 (87.720)	
Epoch: [17][77/196]	LR: 0.0004	Loss 0.6997 (0.7033)	Prec@1 88.672 (87.816)	
Epoch: [17][116/196]	LR: 0.0004	Loss 0.8364 (0.7082)	Prec@1 85.547 (87.597)	
Epoch: [17][155/196]	LR: 0.0004	Loss 0.7349 (0.7122)	Prec@1 85.938 (87.407)	
Epoch: [17][194/196]	LR: 0.0004	Loss 0.7192 (0.7163)	Prec@1 88.281 (87.302)	
Total train loss: 0.7167

 * Prec@1 75.910 Prec@5 94.750 Loss 1.0586
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [18][38/196]	LR: 0.0004	Loss 0.7881 (0.7224)	Prec@1 87.891 (87.230)	
Epoch: [18][77/196]	LR: 0.0004	Loss 0.7563 (0.7222)	Prec@1 85.938 (87.029)	
Epoch: [18][116/196]	LR: 0.0004	Loss 0.7900 (0.7164)	Prec@1 85.156 (87.190)	
Epoch: [18][155/196]	LR: 0.0004	Loss 0.6714 (0.7156)	Prec@1 88.672 (87.197)	
Epoch: [18][194/196]	LR: 0.0004	Loss 0.6289 (0.7151)	Prec@1 89.844 (87.274)	
Total train loss: 0.7157

 * Prec@1 75.690 Prec@5 94.890 Loss 1.0596
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [19][38/196]	LR: 0.0004	Loss 0.6963 (0.7251)	Prec@1 87.891 (86.909)	
Epoch: [19][77/196]	LR: 0.0004	Loss 0.6372 (0.7136)	Prec@1 89.844 (87.260)	
Epoch: [19][116/196]	LR: 0.0004	Loss 0.7500 (0.7169)	Prec@1 85.547 (87.216)	
Epoch: [19][155/196]	LR: 0.0004	Loss 0.7041 (0.7151)	Prec@1 89.453 (87.302)	
Epoch: [19][194/196]	LR: 0.0004	Loss 0.7261 (0.7122)	Prec@1 86.719 (87.362)	
Total train loss: 0.7127

 * Prec@1 76.070 Prec@5 94.810 Loss 1.0518
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [20][38/196]	LR: 8e-05	Loss 0.8022 (0.6983)	Prec@1 86.328 (87.700)	
Epoch: [20][77/196]	LR: 8e-05	Loss 0.6313 (0.7050)	Prec@1 88.672 (87.590)	
Epoch: [20][116/196]	LR: 8e-05	Loss 0.7412 (0.7111)	Prec@1 86.328 (87.443)	
Epoch: [20][155/196]	LR: 8e-05	Loss 0.6968 (0.7084)	Prec@1 88.281 (87.568)	
Epoch: [20][194/196]	LR: 8e-05	Loss 0.7178 (0.7081)	Prec@1 87.500 (87.536)	
Total train loss: 0.7081

 * Prec@1 75.860 Prec@5 94.800 Loss 1.0537
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [21][38/196]	LR: 8e-05	Loss 0.7329 (0.7046)	Prec@1 86.328 (87.921)	
Epoch: [21][77/196]	LR: 8e-05	Loss 0.7461 (0.7086)	Prec@1 86.328 (87.635)	
Epoch: [21][116/196]	LR: 8e-05	Loss 0.7207 (0.7114)	Prec@1 88.672 (87.587)	
Epoch: [21][155/196]	LR: 8e-05	Loss 0.6953 (0.7123)	Prec@1 87.891 (87.462)	
Epoch: [21][194/196]	LR: 8e-05	Loss 0.5747 (0.7104)	Prec@1 89.453 (87.426)	
Total train loss: 0.7111

 * Prec@1 75.940 Prec@5 94.850 Loss 1.0566
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [22][38/196]	LR: 8e-05	Loss 0.6348 (0.7033)	Prec@1 88.281 (87.690)	
Epoch: [22][77/196]	LR: 8e-05	Loss 0.6270 (0.7108)	Prec@1 90.625 (87.600)	
Epoch: [22][116/196]	LR: 8e-05	Loss 0.7017 (0.7160)	Prec@1 86.328 (87.343)	
Epoch: [22][155/196]	LR: 8e-05	Loss 0.7856 (0.7107)	Prec@1 83.984 (87.420)	
Epoch: [22][194/196]	LR: 8e-05	Loss 0.6875 (0.7088)	Prec@1 87.500 (87.448)	
Total train loss: 0.7092

 * Prec@1 75.920 Prec@5 94.950 Loss 1.0586
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [23][38/196]	LR: 8e-05	Loss 0.6523 (0.6988)	Prec@1 88.281 (87.891)	
Epoch: [23][77/196]	LR: 8e-05	Loss 0.6870 (0.7066)	Prec@1 88.672 (87.600)	
Epoch: [23][116/196]	LR: 8e-05	Loss 0.7212 (0.7099)	Prec@1 83.203 (87.443)	
Epoch: [23][155/196]	LR: 8e-05	Loss 0.6904 (0.7106)	Prec@1 89.062 (87.430)	
Epoch: [23][194/196]	LR: 8e-05	Loss 0.6333 (0.7082)	Prec@1 90.234 (87.518)	
Total train loss: 0.7087

 * Prec@1 75.930 Prec@5 94.890 Loss 1.0498
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [24][38/196]	LR: 8e-05	Loss 0.7681 (0.7049)	Prec@1 87.500 (87.861)	
Epoch: [24][77/196]	LR: 8e-05	Loss 0.6870 (0.7049)	Prec@1 88.672 (87.670)	
Epoch: [24][116/196]	LR: 8e-05	Loss 0.7578 (0.7087)	Prec@1 86.719 (87.587)	
Epoch: [24][155/196]	LR: 8e-05	Loss 0.6333 (0.7072)	Prec@1 89.844 (87.518)	
Epoch: [24][194/196]	LR: 8e-05	Loss 0.7056 (0.7050)	Prec@1 85.156 (87.560)	
Total train loss: 0.7053

 * Prec@1 75.860 Prec@5 94.800 Loss 1.0566
Best acc: 76.070
--------------------------------------------------------------------------------
Epoch: [25][38/196]	LR: 8e-05	Loss 0.7817 (0.7068)	Prec@1 85.547 (87.370)	
Epoch: [25][77/196]	LR: 8e-05	Loss 0.7251 (0.7083)	Prec@1 87.500 (87.515)	
Epoch: [25][116/196]	LR: 8e-05	Loss 0.6997 (0.7061)	Prec@1 86.719 (87.593)	
Epoch: [25][155/196]	LR: 8e-05	Loss 0.6792 (0.7063)	Prec@1 88.672 (87.605)	
Epoch: [25][194/196]	LR: 8e-05	Loss 0.7397 (0.7062)	Prec@1 85.156 (87.610)	
Total train loss: 0.7068

 * Prec@1 75.890 Prec@5 94.840 Loss 1.0576
Best acc: 76.070
--------------------------------------------------------------------------------
