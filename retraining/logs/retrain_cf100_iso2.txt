
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 68.690 Prec@5 90.190 Loss 1.1846
Pre-trained Prec@1 with 1 layers frozen: 68.68999481201172 	 Loss: 1.1845703125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.7671 (1.1594)	Prec@1 82.031 (67.117)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.0400 (1.0937)	Prec@1 69.531 (69.040)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.0645 (1.0572)	Prec@1 67.188 (70.005)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.8853 (1.0420)	Prec@1 76.562 (70.403)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.7729 (1.0221)	Prec@1 77.344 (70.931)	
Total train loss: 1.0221

 * Prec@1 68.740 Prec@5 90.740 Loss 1.1475
Best acc: 68.740
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.9019 (0.9064)	Prec@1 74.219 (73.998)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.9746 (0.9097)	Prec@1 71.875 (73.773)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.7617 (0.9067)	Prec@1 75.781 (73.892)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.8726 (0.9025)	Prec@1 73.438 (74.139)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.8423 (0.9019)	Prec@1 73.438 (74.175)	
Total train loss: 0.9018

 * Prec@1 68.860 Prec@5 90.480 Loss 1.1426
Best acc: 68.860
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.7993 (0.8480)	Prec@1 76.562 (76.042)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0059 (0.8499)	Prec@1 71.875 (75.691)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.8760 (0.8486)	Prec@1 75.781 (75.868)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.9268 (0.8514)	Prec@1 74.219 (75.861)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.0186 (0.8534)	Prec@1 67.188 (75.849)	
Total train loss: 0.8534

 * Prec@1 69.240 Prec@5 90.680 Loss 1.1348
Best acc: 69.240
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7949 (0.8288)	Prec@1 75.781 (76.562)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8696 (0.8328)	Prec@1 71.875 (76.287)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.6929 (0.8326)	Prec@1 81.250 (76.265)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.0244 (0.8319)	Prec@1 68.750 (76.275)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.1162 (0.8282)	Prec@1 68.750 (76.434)	
Total train loss: 0.8284

 * Prec@1 69.180 Prec@5 90.940 Loss 1.1270
Best acc: 69.240
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.7705 (0.7912)	Prec@1 77.344 (77.454)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.7524 (0.7975)	Prec@1 78.906 (77.264)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.7803 (0.7971)	Prec@1 76.562 (77.177)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.7900 (0.8000)	Prec@1 78.906 (77.211)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.8691 (0.7998)	Prec@1 75.781 (77.290)	
Total train loss: 0.7996

 * Prec@1 69.300 Prec@5 90.930 Loss 1.1270
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.7271 (0.7782)	Prec@1 83.594 (77.664)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.6387 (0.7764)	Prec@1 82.812 (78.005)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.7510 (0.7751)	Prec@1 75.781 (78.075)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.7700 (0.7755)	Prec@1 78.906 (78.210)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.7520 (0.7742)	Prec@1 80.469 (78.275)	
Total train loss: 0.7740

 * Prec@1 69.360 Prec@5 90.940 Loss 1.1289
Best acc: 69.360
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.7666 (0.7680)	Prec@1 76.562 (78.275)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.8511 (0.7660)	Prec@1 75.781 (78.365)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.7461 (0.7719)	Prec@1 77.344 (78.122)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.5972 (0.7700)	Prec@1 81.250 (78.250)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.7788 (0.7715)	Prec@1 79.688 (78.275)	
Total train loss: 0.7716

 * Prec@1 69.280 Prec@5 90.830 Loss 1.1260
Best acc: 69.360
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.7236 (0.7830)	Prec@1 78.906 (77.925)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.7515 (0.7766)	Prec@1 78.906 (77.995)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.6411 (0.7682)	Prec@1 84.375 (78.302)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.8711 (0.7684)	Prec@1 76.562 (78.265)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.6763 (0.7698)	Prec@1 83.594 (78.247)	
Total train loss: 0.7703

 * Prec@1 69.170 Prec@5 90.850 Loss 1.1348
Best acc: 69.360
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.6626 (0.7572)	Prec@1 82.812 (79.117)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.6919 (0.7638)	Prec@1 77.344 (78.551)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.8843 (0.7650)	Prec@1 70.312 (78.432)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.8101 (0.7694)	Prec@1 78.906 (78.265)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.7749 (0.7669)	Prec@1 75.781 (78.353)	
Total train loss: 0.7669

 * Prec@1 69.390 Prec@5 90.860 Loss 1.1270
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.6250 (0.7735)	Prec@1 82.031 (78.235)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.8687 (0.7711)	Prec@1 77.344 (78.451)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.8213 (0.7701)	Prec@1 75.000 (78.442)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.6558 (0.7700)	Prec@1 77.344 (78.308)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.8335 (0.7647)	Prec@1 76.562 (78.562)	
Total train loss: 0.7650

 * Prec@1 69.160 Prec@5 90.930 Loss 1.1318
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.7554 (0.7649)	Prec@1 79.688 (78.636)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.7280 (0.7652)	Prec@1 80.469 (78.686)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.7163 (0.7630)	Prec@1 78.906 (78.659)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.6436 (0.7631)	Prec@1 78.125 (78.601)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.7334 (0.7626)	Prec@1 85.156 (78.642)	
Total train loss: 0.7628

 * Prec@1 69.350 Prec@5 90.870 Loss 1.1260
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.6587 (0.7709)	Prec@1 86.719 (78.526)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.6738 (0.7703)	Prec@1 82.812 (78.576)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.7495 (0.7671)	Prec@1 75.781 (78.532)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.7944 (0.7672)	Prec@1 79.688 (78.606)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.8452 (0.7685)	Prec@1 77.344 (78.472)	
Total train loss: 0.7685

 * Prec@1 69.090 Prec@5 90.880 Loss 1.1318
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.9971 (0.7781)	Prec@1 70.312 (77.614)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.7939 (0.7666)	Prec@1 80.469 (78.130)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.8462 (0.7659)	Prec@1 75.781 (78.215)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.8550 (0.7667)	Prec@1 77.344 (78.338)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.6958 (0.7670)	Prec@1 80.469 (78.381)	
Total train loss: 0.7670

 * Prec@1 69.410 Prec@5 90.860 Loss 1.1279
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.6973 (0.7590)	Prec@1 82.812 (79.087)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.7427 (0.7654)	Prec@1 80.469 (78.661)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.6445 (0.7642)	Prec@1 79.688 (78.435)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.9375 (0.7642)	Prec@1 71.875 (78.468)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.6392 (0.7660)	Prec@1 79.688 (78.327)	
Total train loss: 0.7660

 * Prec@1 69.180 Prec@5 90.820 Loss 1.1299
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.7324 (0.7810)	Prec@1 78.906 (78.075)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.8442 (0.7702)	Prec@1 75.781 (78.230)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.7256 (0.7739)	Prec@1 77.344 (78.152)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.7310 (0.7677)	Prec@1 78.906 (78.278)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.9883 (0.7638)	Prec@1 69.531 (78.448)	
Total train loss: 0.7642

 * Prec@1 69.180 Prec@5 90.790 Loss 1.1279
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.6172 (0.7582)	Prec@1 83.594 (78.496)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 1.0303 (0.7612)	Prec@1 75.781 (78.420)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.8682 (0.7672)	Prec@1 75.781 (78.335)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.7148 (0.7650)	Prec@1 78.125 (78.315)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.8975 (0.7655)	Prec@1 75.781 (78.345)	
Total train loss: 0.7656

 * Prec@1 69.290 Prec@5 90.830 Loss 1.1270
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.5762 (0.7577)	Prec@1 85.156 (78.736)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.8999 (0.7607)	Prec@1 77.344 (78.631)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.7534 (0.7643)	Prec@1 77.344 (78.559)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.8589 (0.7653)	Prec@1 73.438 (78.621)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.9219 (0.7652)	Prec@1 71.875 (78.564)	
Total train loss: 0.7651

 * Prec@1 69.340 Prec@5 90.830 Loss 1.1270
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.7461 (0.7687)	Prec@1 79.688 (78.456)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.7769 (0.7588)	Prec@1 78.125 (78.736)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.7378 (0.7681)	Prec@1 78.125 (78.519)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.5903 (0.7637)	Prec@1 84.375 (78.553)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.7397 (0.7651)	Prec@1 80.469 (78.468)	
Total train loss: 0.7651

 * Prec@1 69.260 Prec@5 90.810 Loss 1.1211
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.7100 (0.7769)	Prec@1 80.469 (78.075)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.7393 (0.7701)	Prec@1 78.906 (78.215)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.9277 (0.7695)	Prec@1 73.438 (78.282)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.7402 (0.7662)	Prec@1 78.125 (78.403)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.8535 (0.7663)	Prec@1 75.000 (78.444)	
Total train loss: 0.7665

 * Prec@1 69.180 Prec@5 90.740 Loss 1.1299
Best acc: 69.410
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.6528 (0.7665)	Prec@1 79.688 (78.225)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.6846 (0.7627)	Prec@1 79.688 (78.230)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.8325 (0.7654)	Prec@1 73.438 (78.332)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.7271 (0.7640)	Prec@1 77.344 (78.393)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.6772 (0.7635)	Prec@1 82.812 (78.452)	
Total train loss: 0.7636

 * Prec@1 69.250 Prec@5 90.860 Loss 1.1279
Best acc: 69.410
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 67.640 Prec@5 89.480 Loss 1.2285
Pre-trained Prec@1 with 3 layers frozen: 67.63999938964844 	 Loss: 1.228515625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5010 (0.5903)	Prec@1 89.062 (83.363)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5737 (0.5821)	Prec@1 85.938 (83.484)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5977 (0.5711)	Prec@1 86.719 (83.834)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.5693 (0.5751)	Prec@1 82.031 (83.709)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.5767 (0.5755)	Prec@1 82.031 (83.718)	
Total train loss: 0.5756

 * Prec@1 69.610 Prec@5 90.340 Loss 1.1699
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.4504 (0.5248)	Prec@1 85.156 (85.256)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.4824 (0.5283)	Prec@1 89.844 (85.051)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6421 (0.5334)	Prec@1 80.469 (84.983)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4534 (0.5402)	Prec@1 86.719 (84.806)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.5645 (0.5420)	Prec@1 85.938 (84.768)	
Total train loss: 0.5421

 * Prec@1 69.310 Prec@5 90.190 Loss 1.1777
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5205 (0.5206)	Prec@1 84.375 (85.657)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.6206 (0.5154)	Prec@1 85.156 (85.958)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.3560 (0.5189)	Prec@1 89.062 (85.761)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5024 (0.5234)	Prec@1 87.500 (85.544)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5527 (0.5248)	Prec@1 82.812 (85.487)	
Total train loss: 0.5251

 * Prec@1 68.960 Prec@5 90.230 Loss 1.1904
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4668 (0.4885)	Prec@1 89.844 (86.769)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6738 (0.5004)	Prec@1 81.250 (86.528)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5864 (0.5041)	Prec@1 85.156 (86.418)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.3716 (0.5026)	Prec@1 92.188 (86.403)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6431 (0.5055)	Prec@1 80.469 (86.324)	
Total train loss: 0.5056

 * Prec@1 69.270 Prec@5 90.220 Loss 1.1914
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4817 (0.4864)	Prec@1 92.188 (87.139)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4202 (0.4815)	Prec@1 89.844 (87.280)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4753 (0.4865)	Prec@1 88.281 (87.169)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5645 (0.4891)	Prec@1 84.375 (86.959)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.3975 (0.4919)	Prec@1 92.188 (86.827)	
Total train loss: 0.4918

 * Prec@1 69.010 Prec@5 90.150 Loss 1.1904
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.4678 (0.4620)	Prec@1 88.281 (88.041)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.3979 (0.4618)	Prec@1 89.844 (88.166)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.5415 (0.4602)	Prec@1 82.031 (88.074)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.3811 (0.4580)	Prec@1 91.406 (88.176)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.3765 (0.4588)	Prec@1 89.062 (88.195)	
Total train loss: 0.4588

 * Prec@1 69.070 Prec@5 89.950 Loss 1.1895
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.3992 (0.4619)	Prec@1 91.406 (87.600)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.4255 (0.4600)	Prec@1 91.406 (87.976)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.4299 (0.4610)	Prec@1 89.062 (88.101)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.4238 (0.4602)	Prec@1 90.625 (88.053)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.5454 (0.4573)	Prec@1 83.594 (88.205)	
Total train loss: 0.4573

 * Prec@1 69.110 Prec@5 90.070 Loss 1.1982
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.3108 (0.4598)	Prec@1 94.531 (88.532)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.4749 (0.4586)	Prec@1 84.375 (88.226)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.4377 (0.4606)	Prec@1 89.062 (88.154)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.4255 (0.4588)	Prec@1 90.625 (88.201)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.4275 (0.4570)	Prec@1 90.625 (88.271)	
Total train loss: 0.4569

 * Prec@1 68.990 Prec@5 90.130 Loss 1.1934
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.5039 (0.4593)	Prec@1 84.375 (88.011)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.4082 (0.4509)	Prec@1 89.062 (88.431)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.3923 (0.4569)	Prec@1 89.844 (88.108)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.4263 (0.4571)	Prec@1 89.062 (88.169)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.3623 (0.4579)	Prec@1 92.969 (88.197)	
Total train loss: 0.4579

 * Prec@1 69.100 Prec@5 89.970 Loss 1.1934
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.3982 (0.4560)	Prec@1 92.188 (88.522)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.4446 (0.4542)	Prec@1 88.281 (88.562)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.3870 (0.4508)	Prec@1 91.406 (88.562)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.4094 (0.4531)	Prec@1 89.062 (88.484)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.4512 (0.4539)	Prec@1 84.375 (88.383)	
Total train loss: 0.4541

 * Prec@1 69.140 Prec@5 89.950 Loss 1.2021
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.4436 (0.4441)	Prec@1 86.719 (88.712)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.3940 (0.4505)	Prec@1 91.406 (88.406)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.3621 (0.4543)	Prec@1 89.844 (88.235)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.3823 (0.4523)	Prec@1 94.531 (88.406)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.4932 (0.4520)	Prec@1 84.375 (88.494)	
Total train loss: 0.4521

 * Prec@1 68.790 Prec@5 89.950 Loss 1.2051
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.4431 (0.4435)	Prec@1 88.281 (88.492)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.4016 (0.4521)	Prec@1 90.625 (88.286)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.4441 (0.4548)	Prec@1 85.938 (88.224)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.5566 (0.4531)	Prec@1 85.938 (88.259)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.4346 (0.4518)	Prec@1 89.844 (88.367)	
Total train loss: 0.4518

 * Prec@1 69.290 Prec@5 89.910 Loss 1.1943
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.4419 (0.4522)	Prec@1 89.062 (88.431)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.4468 (0.4488)	Prec@1 90.625 (88.431)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.5566 (0.4529)	Prec@1 80.469 (88.358)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.4150 (0.4533)	Prec@1 89.844 (88.351)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.5825 (0.4559)	Prec@1 83.594 (88.243)	
Total train loss: 0.4560

 * Prec@1 69.100 Prec@5 90.020 Loss 1.1953
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.5200 (0.4573)	Prec@1 87.500 (88.061)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.4009 (0.4532)	Prec@1 89.062 (88.411)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.4417 (0.4520)	Prec@1 85.156 (88.381)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.4111 (0.4506)	Prec@1 92.969 (88.396)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.4399 (0.4524)	Prec@1 89.062 (88.413)	
Total train loss: 0.4527

 * Prec@1 69.100 Prec@5 90.040 Loss 1.1973
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.3333 (0.4374)	Prec@1 95.312 (89.213)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.4648 (0.4432)	Prec@1 87.500 (88.992)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.4907 (0.4494)	Prec@1 85.156 (88.615)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.4902 (0.4510)	Prec@1 87.500 (88.547)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.5840 (0.4527)	Prec@1 83.594 (88.464)	
Total train loss: 0.4526

 * Prec@1 68.730 Prec@5 90.010 Loss 1.1992
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.4084 (0.4507)	Prec@1 88.281 (88.702)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.4688 (0.4520)	Prec@1 86.719 (88.416)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.5547 (0.4535)	Prec@1 84.375 (88.431)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.4377 (0.4510)	Prec@1 87.500 (88.459)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.4246 (0.4522)	Prec@1 90.625 (88.421)	
Total train loss: 0.4524

 * Prec@1 68.870 Prec@5 89.990 Loss 1.2041
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.3914 (0.4565)	Prec@1 91.406 (88.121)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.3953 (0.4622)	Prec@1 92.188 (87.926)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.4290 (0.4580)	Prec@1 89.844 (88.108)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.4883 (0.4555)	Prec@1 87.500 (88.144)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.5181 (0.4537)	Prec@1 85.938 (88.243)	
Total train loss: 0.4539

 * Prec@1 68.850 Prec@5 90.080 Loss 1.1982
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.4561 (0.4680)	Prec@1 90.625 (87.750)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.4702 (0.4590)	Prec@1 86.719 (88.161)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.5220 (0.4569)	Prec@1 84.375 (88.048)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.5176 (0.4562)	Prec@1 83.594 (88.139)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.4924 (0.4563)	Prec@1 86.719 (88.135)	
Total train loss: 0.4562

 * Prec@1 68.880 Prec@5 89.920 Loss 1.2041
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.4915 (0.4661)	Prec@1 87.500 (87.800)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.5005 (0.4625)	Prec@1 88.281 (88.116)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.4761 (0.4616)	Prec@1 86.719 (88.151)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.4548 (0.4587)	Prec@1 91.406 (88.281)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.4573 (0.4564)	Prec@1 90.625 (88.329)	
Total train loss: 0.4563

 * Prec@1 68.990 Prec@5 90.050 Loss 1.1973
Best acc: 69.610
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.4299 (0.4505)	Prec@1 91.406 (88.411)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.4775 (0.4496)	Prec@1 85.156 (88.477)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.4131 (0.4490)	Prec@1 92.969 (88.418)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.5654 (0.4503)	Prec@1 85.156 (88.389)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.4468 (0.4535)	Prec@1 89.844 (88.341)	
Total train loss: 0.4538

 * Prec@1 69.170 Prec@5 90.080 Loss 1.1982
Best acc: 69.610
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 66.330 Prec@5 88.910 Loss 1.2773
Pre-trained Prec@1 with 5 layers frozen: 66.33000183105469 	 Loss: 1.27734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.6030 (0.6043)	Prec@1 85.156 (82.933)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.8862 (0.6006)	Prec@1 73.438 (82.993)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5771 (0.5961)	Prec@1 85.938 (83.096)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6982 (0.5945)	Prec@1 80.469 (83.110)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.6470 (0.5928)	Prec@1 81.250 (83.179)	
Total train loss: 0.5926

 * Prec@1 69.250 Prec@5 90.290 Loss 1.1816
Best acc: 69.250
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.6079 (0.5636)	Prec@1 82.031 (84.004)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5508 (0.5594)	Prec@1 84.375 (84.370)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5903 (0.5575)	Prec@1 82.812 (84.435)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4878 (0.5560)	Prec@1 85.156 (84.553)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.6182 (0.5562)	Prec@1 82.812 (84.483)	
Total train loss: 0.5558

 * Prec@1 69.490 Prec@5 90.520 Loss 1.1758
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.3740 (0.5290)	Prec@1 92.969 (85.597)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4551 (0.5340)	Prec@1 89.062 (85.181)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5898 (0.5393)	Prec@1 82.812 (84.966)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5776 (0.5348)	Prec@1 80.469 (85.026)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.4939 (0.5363)	Prec@1 85.156 (84.996)	
Total train loss: 0.5363

 * Prec@1 68.760 Prec@5 89.970 Loss 1.1934
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.5107 (0.5038)	Prec@1 86.719 (86.769)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.4116 (0.5108)	Prec@1 89.844 (86.378)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5103 (0.5134)	Prec@1 87.500 (86.288)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5068 (0.5165)	Prec@1 84.375 (86.038)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.4548 (0.5175)	Prec@1 87.500 (85.942)	
Total train loss: 0.5175

 * Prec@1 68.960 Prec@5 90.110 Loss 1.1992
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.5400 (0.4881)	Prec@1 85.156 (86.699)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4385 (0.4928)	Prec@1 89.062 (86.879)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4233 (0.4957)	Prec@1 88.281 (86.655)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.4763 (0.4982)	Prec@1 87.500 (86.704)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.4065 (0.5007)	Prec@1 89.844 (86.514)	
Total train loss: 0.5008

 * Prec@1 68.900 Prec@5 90.090 Loss 1.1992
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.4500 (0.4760)	Prec@1 86.719 (87.740)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.4751 (0.4702)	Prec@1 87.500 (87.816)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.5005 (0.4675)	Prec@1 85.938 (87.911)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.4844 (0.4696)	Prec@1 89.062 (87.846)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.4651 (0.4697)	Prec@1 87.500 (87.804)	
Total train loss: 0.4697

 * Prec@1 68.720 Prec@5 89.970 Loss 1.1953
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.4778 (0.4733)	Prec@1 86.719 (87.670)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.5215 (0.4706)	Prec@1 85.156 (87.816)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.3799 (0.4697)	Prec@1 89.844 (87.700)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.6523 (0.4709)	Prec@1 76.562 (87.668)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.5151 (0.4701)	Prec@1 86.719 (87.772)	
Total train loss: 0.4703

 * Prec@1 68.760 Prec@5 89.890 Loss 1.2051
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.4434 (0.4556)	Prec@1 85.938 (88.241)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.3901 (0.4601)	Prec@1 91.406 (88.056)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.5278 (0.4630)	Prec@1 85.938 (88.001)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.4434 (0.4659)	Prec@1 92.188 (87.873)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.5015 (0.4685)	Prec@1 86.719 (87.724)	
Total train loss: 0.4687

 * Prec@1 68.900 Prec@5 90.140 Loss 1.1973
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.3784 (0.4609)	Prec@1 89.844 (88.001)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.6055 (0.4655)	Prec@1 81.250 (87.770)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.4148 (0.4629)	Prec@1 89.062 (87.817)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.4348 (0.4641)	Prec@1 85.938 (87.795)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.4197 (0.4666)	Prec@1 88.281 (87.754)	
Total train loss: 0.4665

 * Prec@1 68.810 Prec@5 90.020 Loss 1.2070
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.4304 (0.4652)	Prec@1 89.062 (88.071)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.4946 (0.4643)	Prec@1 88.281 (88.251)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.3640 (0.4657)	Prec@1 92.188 (88.101)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.5288 (0.4661)	Prec@1 85.156 (88.018)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.5513 (0.4661)	Prec@1 85.938 (87.937)	
Total train loss: 0.4661

 * Prec@1 68.750 Prec@5 90.000 Loss 1.2021
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.4805 (0.4703)	Prec@1 87.500 (87.911)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.5239 (0.4620)	Prec@1 85.938 (88.011)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.3594 (0.4633)	Prec@1 89.844 (88.051)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.4307 (0.4648)	Prec@1 89.062 (87.951)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.4341 (0.4642)	Prec@1 88.281 (87.929)	
Total train loss: 0.4640

 * Prec@1 68.780 Prec@5 89.980 Loss 1.2031
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.5103 (0.4672)	Prec@1 85.156 (87.891)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.4836 (0.4643)	Prec@1 86.719 (88.006)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.4119 (0.4629)	Prec@1 87.500 (87.904)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.3401 (0.4638)	Prec@1 92.188 (87.883)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.4038 (0.4642)	Prec@1 91.406 (87.881)	
Total train loss: 0.4644

 * Prec@1 68.750 Prec@5 89.910 Loss 1.2080
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.3967 (0.4649)	Prec@1 89.062 (87.921)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.5327 (0.4597)	Prec@1 84.375 (88.106)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.5249 (0.4610)	Prec@1 85.156 (88.061)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.5059 (0.4606)	Prec@1 87.500 (88.118)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.4221 (0.4629)	Prec@1 88.281 (87.991)	
Total train loss: 0.4630

 * Prec@1 68.640 Prec@5 90.170 Loss 1.2051
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.5210 (0.4620)	Prec@1 82.812 (88.061)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.3945 (0.4602)	Prec@1 88.281 (88.121)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.4834 (0.4562)	Prec@1 89.062 (88.325)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.5356 (0.4595)	Prec@1 85.156 (88.194)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.3828 (0.4618)	Prec@1 90.625 (88.129)	
Total train loss: 0.4619

 * Prec@1 68.940 Prec@5 90.170 Loss 1.1973
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.4639 (0.4631)	Prec@1 89.844 (87.971)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.3652 (0.4655)	Prec@1 92.188 (87.856)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.4321 (0.4621)	Prec@1 86.719 (87.881)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.4348 (0.4641)	Prec@1 89.062 (87.851)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.4250 (0.4623)	Prec@1 92.188 (87.917)	
Total train loss: 0.4624

 * Prec@1 68.750 Prec@5 89.950 Loss 1.2070
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.4626 (0.4730)	Prec@1 89.062 (87.620)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.4641 (0.4601)	Prec@1 86.719 (88.306)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.3845 (0.4654)	Prec@1 89.062 (87.961)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.4521 (0.4638)	Prec@1 88.281 (87.963)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.4812 (0.4644)	Prec@1 86.719 (88.011)	
Total train loss: 0.4647

 * Prec@1 68.760 Prec@5 90.030 Loss 1.2012
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.3970 (0.4640)	Prec@1 89.062 (87.871)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.4392 (0.4633)	Prec@1 84.375 (87.956)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.4607 (0.4636)	Prec@1 89.062 (87.974)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.4075 (0.4634)	Prec@1 90.625 (87.988)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.5278 (0.4618)	Prec@1 86.719 (88.043)	
Total train loss: 0.4618

 * Prec@1 68.640 Prec@5 89.900 Loss 1.2090
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.5103 (0.4677)	Prec@1 86.719 (88.111)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.4460 (0.4594)	Prec@1 90.625 (88.321)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.6304 (0.4631)	Prec@1 82.812 (88.188)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.4341 (0.4614)	Prec@1 87.500 (88.199)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.5405 (0.4626)	Prec@1 84.375 (88.173)	
Total train loss: 0.4625

 * Prec@1 68.890 Prec@5 90.050 Loss 1.2002
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.6401 (0.4645)	Prec@1 82.031 (87.861)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.3772 (0.4650)	Prec@1 90.625 (87.770)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.5293 (0.4661)	Prec@1 84.375 (87.810)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.4731 (0.4660)	Prec@1 87.500 (87.876)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.4526 (0.4634)	Prec@1 91.406 (87.899)	
Total train loss: 0.4633

 * Prec@1 68.660 Prec@5 90.000 Loss 1.2031
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.4927 (0.4549)	Prec@1 87.500 (88.572)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.4788 (0.4538)	Prec@1 89.062 (88.401)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.5068 (0.4556)	Prec@1 85.938 (88.405)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.4421 (0.4594)	Prec@1 88.281 (88.279)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.5078 (0.4628)	Prec@1 85.938 (88.119)	
Total train loss: 0.4631

 * Prec@1 68.760 Prec@5 89.920 Loss 1.2061
Best acc: 69.490
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 65.160 Prec@5 88.110 Loss 1.3447
Pre-trained Prec@1 with 7 layers frozen: 65.15999603271484 	 Loss: 1.3447265625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5918 (0.6373)	Prec@1 82.031 (81.921)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.7070 (0.6173)	Prec@1 80.469 (82.462)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5210 (0.6121)	Prec@1 85.156 (82.686)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.5791 (0.6106)	Prec@1 85.156 (82.715)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.6562 (0.6076)	Prec@1 78.906 (82.764)	
Total train loss: 0.6077

 * Prec@1 69.140 Prec@5 90.340 Loss 1.1865
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.5132 (0.5593)	Prec@1 85.156 (84.245)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5576 (0.5652)	Prec@1 84.375 (83.989)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5229 (0.5654)	Prec@1 83.594 (84.018)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6880 (0.5688)	Prec@1 82.031 (83.957)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.5347 (0.5700)	Prec@1 85.156 (83.972)	
Total train loss: 0.5699

 * Prec@1 68.960 Prec@5 90.360 Loss 1.1836
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5317 (0.5487)	Prec@1 86.719 (84.746)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4417 (0.5433)	Prec@1 88.281 (85.026)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5200 (0.5457)	Prec@1 86.719 (84.909)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.4792 (0.5444)	Prec@1 88.281 (84.961)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5342 (0.5447)	Prec@1 82.812 (84.904)	
Total train loss: 0.5450

 * Prec@1 68.890 Prec@5 90.370 Loss 1.1943
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4282 (0.5038)	Prec@1 87.500 (85.978)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.3767 (0.5134)	Prec@1 88.281 (85.912)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.4817 (0.5218)	Prec@1 85.938 (85.787)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6616 (0.5223)	Prec@1 83.594 (85.645)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6206 (0.5251)	Prec@1 78.906 (85.583)	
Total train loss: 0.5253

 * Prec@1 68.660 Prec@5 90.260 Loss 1.2002
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4702 (0.4894)	Prec@1 84.375 (87.009)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4543 (0.4976)	Prec@1 86.719 (86.669)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4473 (0.5042)	Prec@1 91.406 (86.452)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5854 (0.5086)	Prec@1 82.031 (86.301)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5366 (0.5094)	Prec@1 87.500 (86.210)	
Total train loss: 0.5097

 * Prec@1 68.610 Prec@5 89.970 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.4280 (0.4744)	Prec@1 89.844 (87.580)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.4636 (0.4763)	Prec@1 88.281 (87.375)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.4724 (0.4763)	Prec@1 86.719 (87.383)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.5830 (0.4785)	Prec@1 82.031 (87.317)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.4543 (0.4784)	Prec@1 89.062 (87.288)	
Total train loss: 0.4784

 * Prec@1 68.680 Prec@5 89.760 Loss 1.2109
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.5068 (0.4763)	Prec@1 85.156 (87.650)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.4224 (0.4762)	Prec@1 86.719 (87.425)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.4788 (0.4768)	Prec@1 85.938 (87.467)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.4758 (0.4779)	Prec@1 87.500 (87.392)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.4697 (0.4763)	Prec@1 84.375 (87.522)	
Total train loss: 0.4764

 * Prec@1 68.660 Prec@5 89.960 Loss 1.2109
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.4465 (0.4896)	Prec@1 85.156 (86.999)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.5205 (0.4796)	Prec@1 83.594 (87.435)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.4021 (0.4773)	Prec@1 88.281 (87.453)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.3899 (0.4737)	Prec@1 87.500 (87.583)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.4541 (0.4753)	Prec@1 89.844 (87.594)	
Total train loss: 0.4757

 * Prec@1 68.820 Prec@5 90.070 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.3318 (0.4835)	Prec@1 93.750 (87.550)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.4602 (0.4710)	Prec@1 86.719 (87.856)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.5317 (0.4722)	Prec@1 85.156 (87.647)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.4653 (0.4726)	Prec@1 89.062 (87.670)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.5068 (0.4714)	Prec@1 87.500 (87.698)	
Total train loss: 0.4716

 * Prec@1 68.830 Prec@5 90.120 Loss 1.2129
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.5693 (0.4627)	Prec@1 82.031 (88.111)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.4736 (0.4688)	Prec@1 86.719 (87.810)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.3970 (0.4721)	Prec@1 89.844 (87.620)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.5601 (0.4740)	Prec@1 84.375 (87.560)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.4092 (0.4748)	Prec@1 91.406 (87.548)	
Total train loss: 0.4747

 * Prec@1 68.850 Prec@5 89.970 Loss 1.2129
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.4685 (0.4674)	Prec@1 87.500 (87.780)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.4519 (0.4719)	Prec@1 88.281 (87.470)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.4551 (0.4695)	Prec@1 86.719 (87.624)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.4182 (0.4699)	Prec@1 85.938 (87.633)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.5190 (0.4727)	Prec@1 87.500 (87.572)	
Total train loss: 0.4730

 * Prec@1 68.460 Prec@5 89.930 Loss 1.2109
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.3953 (0.4854)	Prec@1 92.188 (87.360)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.3928 (0.4785)	Prec@1 89.844 (87.540)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.4470 (0.4764)	Prec@1 86.719 (87.557)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.4648 (0.4773)	Prec@1 84.375 (87.492)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.5117 (0.4730)	Prec@1 84.375 (87.646)	
Total train loss: 0.4733

 * Prec@1 68.750 Prec@5 89.970 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.4470 (0.4679)	Prec@1 88.281 (87.851)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.5332 (0.4717)	Prec@1 85.938 (87.891)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.4490 (0.4755)	Prec@1 88.281 (87.593)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.4319 (0.4726)	Prec@1 87.500 (87.745)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.4580 (0.4725)	Prec@1 85.938 (87.706)	
Total train loss: 0.4726

 * Prec@1 68.590 Prec@5 89.920 Loss 1.2139
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.4458 (0.4785)	Prec@1 86.719 (87.250)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.4758 (0.4706)	Prec@1 83.594 (87.625)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.3950 (0.4742)	Prec@1 88.281 (87.587)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.4329 (0.4713)	Prec@1 89.844 (87.760)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.4685 (0.4721)	Prec@1 87.500 (87.762)	
Total train loss: 0.4721

 * Prec@1 68.580 Prec@5 89.830 Loss 1.2090
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.4482 (0.4652)	Prec@1 85.938 (87.981)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.4426 (0.4661)	Prec@1 89.844 (87.876)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.5015 (0.4685)	Prec@1 85.938 (87.707)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.5415 (0.4712)	Prec@1 82.031 (87.593)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.5044 (0.4717)	Prec@1 86.719 (87.656)	
Total train loss: 0.4719

 * Prec@1 68.500 Prec@5 90.020 Loss 1.2100
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.3503 (0.4719)	Prec@1 91.406 (88.021)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.3889 (0.4666)	Prec@1 90.625 (88.051)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.5107 (0.4680)	Prec@1 84.375 (87.991)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.3704 (0.4708)	Prec@1 90.625 (87.798)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.5225 (0.4706)	Prec@1 83.594 (87.782)	
Total train loss: 0.4711

 * Prec@1 68.560 Prec@5 89.900 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.5151 (0.4631)	Prec@1 87.500 (88.181)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.3657 (0.4662)	Prec@1 89.844 (87.961)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.3857 (0.4707)	Prec@1 92.188 (87.767)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.3906 (0.4687)	Prec@1 89.062 (87.755)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.4165 (0.4686)	Prec@1 89.844 (87.774)	
Total train loss: 0.4689

 * Prec@1 68.580 Prec@5 89.940 Loss 1.2090
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.4651 (0.4735)	Prec@1 89.844 (87.790)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.3958 (0.4734)	Prec@1 89.062 (87.805)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.4258 (0.4698)	Prec@1 90.625 (87.914)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.4275 (0.4698)	Prec@1 88.281 (87.873)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.4014 (0.4691)	Prec@1 91.406 (87.847)	
Total train loss: 0.4690

 * Prec@1 68.750 Prec@5 90.120 Loss 1.2080
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.3682 (0.4756)	Prec@1 90.625 (87.630)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.5396 (0.4702)	Prec@1 85.938 (87.780)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.5020 (0.4707)	Prec@1 85.938 (87.760)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.3484 (0.4705)	Prec@1 94.531 (87.828)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.4924 (0.4715)	Prec@1 88.281 (87.817)	
Total train loss: 0.4716

 * Prec@1 68.730 Prec@5 89.940 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.3799 (0.4652)	Prec@1 89.844 (88.071)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.3997 (0.4660)	Prec@1 92.969 (88.016)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.6343 (0.4686)	Prec@1 80.469 (87.877)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.6201 (0.4713)	Prec@1 80.469 (87.758)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.3972 (0.4697)	Prec@1 89.844 (87.857)	
Total train loss: 0.4698

 * Prec@1 68.630 Prec@5 90.070 Loss 1.2070
Best acc: 69.140
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 57.720 Prec@5 83.750 Loss 1.7061
Pre-trained Prec@1 with 9 layers frozen: 57.71999740600586 	 Loss: 1.7060546875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.5679 (0.7172)	Prec@1 85.938 (79.307)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5522 (0.6929)	Prec@1 87.500 (79.978)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.8154 (0.6783)	Prec@1 78.906 (80.442)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6543 (0.6684)	Prec@1 80.469 (80.744)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.5371 (0.6665)	Prec@1 85.156 (80.839)	
Total train loss: 0.6663

 * Prec@1 68.520 Prec@5 90.120 Loss 1.1855
Best acc: 68.520
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.7041 (0.5800)	Prec@1 78.125 (83.864)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.7271 (0.5878)	Prec@1 80.469 (83.469)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.7129 (0.5916)	Prec@1 76.562 (83.413)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.5015 (0.5933)	Prec@1 85.156 (83.243)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.6860 (0.5980)	Prec@1 79.688 (83.051)	
Total train loss: 0.5980

 * Prec@1 68.370 Prec@5 89.990 Loss 1.1934
Best acc: 68.520
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5610 (0.5666)	Prec@1 88.281 (84.525)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.4092 (0.5685)	Prec@1 88.281 (84.275)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5654 (0.5736)	Prec@1 82.031 (83.844)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.6304 (0.5763)	Prec@1 80.469 (83.799)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.5337 (0.5740)	Prec@1 83.594 (83.920)	
Total train loss: 0.5740

 * Prec@1 68.530 Prec@5 89.930 Loss 1.1982
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4575 (0.5488)	Prec@1 86.719 (84.716)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.4336 (0.5450)	Prec@1 88.281 (84.991)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5693 (0.5476)	Prec@1 85.156 (84.859)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5781 (0.5489)	Prec@1 85.156 (84.793)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.5229 (0.5514)	Prec@1 84.375 (84.692)	
Total train loss: 0.5515

 * Prec@1 68.140 Prec@5 89.650 Loss 1.2100
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.5303 (0.5053)	Prec@1 87.500 (86.619)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.5918 (0.5177)	Prec@1 82.812 (86.118)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.5405 (0.5219)	Prec@1 85.156 (85.811)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5752 (0.5287)	Prec@1 85.156 (85.477)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.4629 (0.5328)	Prec@1 90.625 (85.339)	
Total train loss: 0.5327

 * Prec@1 68.220 Prec@5 89.960 Loss 1.2080
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.4114 (0.5014)	Prec@1 91.406 (86.558)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.4568 (0.5049)	Prec@1 88.281 (86.463)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.5952 (0.5020)	Prec@1 82.812 (86.468)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.4229 (0.5012)	Prec@1 88.281 (86.506)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.5610 (0.5036)	Prec@1 81.250 (86.490)	
Total train loss: 0.5038

 * Prec@1 68.180 Prec@5 89.650 Loss 1.2129
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.4648 (0.4953)	Prec@1 87.500 (86.829)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.5679 (0.4990)	Prec@1 85.938 (86.849)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.5225 (0.5016)	Prec@1 85.938 (86.675)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.6021 (0.5015)	Prec@1 85.156 (86.614)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.5552 (0.4995)	Prec@1 84.375 (86.679)	
Total train loss: 0.4994

 * Prec@1 67.930 Prec@5 89.550 Loss 1.2158
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.4531 (0.4924)	Prec@1 89.062 (86.949)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.5405 (0.4970)	Prec@1 85.938 (86.714)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.4663 (0.5004)	Prec@1 86.719 (86.655)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.4382 (0.5000)	Prec@1 91.406 (86.699)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.4697 (0.4999)	Prec@1 85.156 (86.695)	
Total train loss: 0.5003

 * Prec@1 68.050 Prec@5 89.660 Loss 1.2236
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.5176 (0.4921)	Prec@1 83.594 (87.159)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.5737 (0.4959)	Prec@1 82.812 (86.819)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.5146 (0.4926)	Prec@1 87.500 (86.983)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.3552 (0.4932)	Prec@1 91.406 (86.871)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.5635 (0.4955)	Prec@1 82.812 (86.861)	
Total train loss: 0.4957

 * Prec@1 68.440 Prec@5 89.920 Loss 1.2139
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.4292 (0.4942)	Prec@1 90.625 (86.619)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.5435 (0.4927)	Prec@1 82.812 (87.044)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.5117 (0.4936)	Prec@1 85.938 (86.899)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.5078 (0.4926)	Prec@1 88.281 (86.984)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.4534 (0.4941)	Prec@1 88.281 (86.945)	
Total train loss: 0.4941

 * Prec@1 68.210 Prec@5 89.810 Loss 1.2109
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.4595 (0.4867)	Prec@1 85.938 (86.789)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.5029 (0.4862)	Prec@1 86.719 (87.114)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.3923 (0.4931)	Prec@1 91.406 (86.919)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.5176 (0.4979)	Prec@1 85.938 (86.714)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.5854 (0.4963)	Prec@1 84.375 (86.787)	
Total train loss: 0.4964

 * Prec@1 68.460 Prec@5 89.740 Loss 1.2119
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.5518 (0.4881)	Prec@1 83.594 (87.410)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.5532 (0.4891)	Prec@1 85.156 (87.345)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.5156 (0.4933)	Prec@1 86.719 (87.223)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.5586 (0.4958)	Prec@1 85.156 (87.039)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.4836 (0.4942)	Prec@1 85.938 (87.011)	
Total train loss: 0.4943

 * Prec@1 68.420 Prec@5 89.900 Loss 1.2080
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.5308 (0.4921)	Prec@1 86.719 (87.220)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.5537 (0.4931)	Prec@1 82.031 (86.824)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.5259 (0.4934)	Prec@1 87.500 (86.859)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.4878 (0.4930)	Prec@1 89.062 (86.821)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.5088 (0.4922)	Prec@1 84.375 (86.869)	
Total train loss: 0.4921

 * Prec@1 68.120 Prec@5 89.810 Loss 1.2158
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.5376 (0.4965)	Prec@1 84.375 (86.879)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.5474 (0.4940)	Prec@1 80.469 (86.959)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.5786 (0.4973)	Prec@1 85.938 (86.812)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.3738 (0.4942)	Prec@1 91.406 (86.947)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.3713 (0.4947)	Prec@1 90.625 (87.011)	
Total train loss: 0.4946

 * Prec@1 68.170 Prec@5 89.730 Loss 1.2148
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.4631 (0.4980)	Prec@1 87.500 (86.629)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.4309 (0.4944)	Prec@1 87.500 (86.819)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.4761 (0.4944)	Prec@1 90.625 (86.842)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.4834 (0.4962)	Prec@1 90.625 (86.729)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.4651 (0.4938)	Prec@1 85.938 (86.881)	
Total train loss: 0.4937

 * Prec@1 67.960 Prec@5 89.720 Loss 1.2139
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.5205 (0.4909)	Prec@1 82.031 (87.059)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.5820 (0.4963)	Prec@1 82.031 (87.029)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.5195 (0.4962)	Prec@1 85.938 (87.003)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.4666 (0.4964)	Prec@1 88.281 (86.909)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.4175 (0.4954)	Prec@1 88.281 (86.871)	
Total train loss: 0.4953

 * Prec@1 68.120 Prec@5 89.860 Loss 1.2129
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.4675 (0.4996)	Prec@1 87.500 (86.809)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.6504 (0.4977)	Prec@1 82.812 (86.749)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.4607 (0.4922)	Prec@1 89.062 (86.936)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.4563 (0.4898)	Prec@1 88.281 (86.997)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.5005 (0.4893)	Prec@1 83.594 (86.975)	
Total train loss: 0.4895

 * Prec@1 68.040 Prec@5 89.730 Loss 1.2148
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.6030 (0.4920)	Prec@1 83.594 (87.230)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.4595 (0.4927)	Prec@1 89.844 (87.064)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.5298 (0.4880)	Prec@1 86.719 (87.173)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.4453 (0.4915)	Prec@1 90.625 (86.947)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.2964 (0.4916)	Prec@1 95.312 (86.869)	
Total train loss: 0.4918

 * Prec@1 68.130 Prec@5 89.730 Loss 1.2139
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.4863 (0.4831)	Prec@1 85.938 (87.370)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.5938 (0.4868)	Prec@1 87.500 (87.205)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.3530 (0.4892)	Prec@1 91.406 (87.089)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.3367 (0.4912)	Prec@1 93.750 (87.099)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.5317 (0.4913)	Prec@1 84.375 (86.971)	
Total train loss: 0.4916

 * Prec@1 68.230 Prec@5 89.910 Loss 1.2129
Best acc: 68.530
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.4453 (0.4901)	Prec@1 89.062 (86.979)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.5137 (0.4903)	Prec@1 87.500 (87.074)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.4949 (0.4908)	Prec@1 82.812 (87.146)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.4851 (0.4903)	Prec@1 86.719 (87.092)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.4556 (0.4937)	Prec@1 89.844 (86.999)	
Total train loss: 0.4936

 * Prec@1 68.150 Prec@5 89.780 Loss 1.2109
Best acc: 68.530
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 52.200 Prec@5 79.060 Loss 2.0078
Pre-trained Prec@1 with 11 layers frozen: 52.19999694824219 	 Loss: 2.0078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.8242 (0.7819)	Prec@1 74.219 (77.113)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5723 (0.7529)	Prec@1 85.938 (78.110)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.7344 (0.7290)	Prec@1 77.344 (78.736)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.7603 (0.7183)	Prec@1 82.812 (79.129)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.7124 (0.7097)	Prec@1 78.906 (79.441)	
Total train loss: 0.7099

 * Prec@1 68.460 Prec@5 89.940 Loss 1.2070
Best acc: 68.460
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.5591 (0.6329)	Prec@1 85.156 (81.681)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.7100 (0.6308)	Prec@1 77.344 (81.916)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6494 (0.6333)	Prec@1 82.812 (81.838)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6475 (0.6343)	Prec@1 79.688 (81.843)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.4077 (0.6330)	Prec@1 90.625 (81.883)	
Total train loss: 0.6331

 * Prec@1 68.130 Prec@5 89.810 Loss 1.2119
Best acc: 68.460
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.6836 (0.5939)	Prec@1 78.906 (83.223)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5630 (0.5911)	Prec@1 84.375 (83.148)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5757 (0.5994)	Prec@1 85.938 (82.873)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.5093 (0.6016)	Prec@1 84.375 (82.767)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.6504 (0.6035)	Prec@1 80.469 (82.730)	
Total train loss: 0.6036

 * Prec@1 68.630 Prec@5 89.620 Loss 1.2090
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4839 (0.5612)	Prec@1 88.281 (84.495)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6089 (0.5747)	Prec@1 85.156 (83.879)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.6636 (0.5774)	Prec@1 83.594 (83.767)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6284 (0.5769)	Prec@1 80.469 (83.849)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.5859 (0.5787)	Prec@1 82.031 (83.710)	
Total train loss: 0.5790

 * Prec@1 68.160 Prec@5 89.740 Loss 1.2217
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.4734 (0.5566)	Prec@1 87.500 (84.395)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.6704 (0.5576)	Prec@1 82.812 (84.290)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.5371 (0.5578)	Prec@1 87.500 (84.368)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.6309 (0.5621)	Prec@1 82.031 (84.317)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5269 (0.5621)	Prec@1 84.375 (84.303)	
Total train loss: 0.5622

 * Prec@1 68.010 Prec@5 89.600 Loss 1.2275
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.5356 (0.5409)	Prec@1 88.281 (85.106)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.6177 (0.5379)	Prec@1 81.250 (85.146)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.5952 (0.5337)	Prec@1 81.250 (85.360)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.4812 (0.5330)	Prec@1 87.500 (85.442)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.4287 (0.5339)	Prec@1 89.844 (85.457)	
Total train loss: 0.5342

 * Prec@1 67.580 Prec@5 89.650 Loss 1.2256
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.6172 (0.5320)	Prec@1 84.375 (85.847)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.6016 (0.5271)	Prec@1 85.938 (85.772)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.7104 (0.5273)	Prec@1 77.344 (85.747)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.4895 (0.5278)	Prec@1 87.500 (85.702)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.6250 (0.5277)	Prec@1 82.031 (85.701)	
Total train loss: 0.5277

 * Prec@1 67.990 Prec@5 89.470 Loss 1.2275
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.5181 (0.5309)	Prec@1 84.375 (85.317)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.5273 (0.5284)	Prec@1 85.156 (85.557)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.6021 (0.5298)	Prec@1 83.594 (85.610)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.6357 (0.5287)	Prec@1 80.469 (85.660)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.4568 (0.5273)	Prec@1 89.844 (85.663)	
Total train loss: 0.5275

 * Prec@1 68.160 Prec@5 89.530 Loss 1.2236
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.5337 (0.5316)	Prec@1 83.594 (85.557)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.4697 (0.5273)	Prec@1 89.062 (85.797)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.4917 (0.5288)	Prec@1 85.938 (85.594)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.6265 (0.5285)	Prec@1 81.250 (85.687)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.4849 (0.5300)	Prec@1 89.062 (85.595)	
Total train loss: 0.5298

 * Prec@1 68.130 Prec@5 89.550 Loss 1.2236
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.5591 (0.5250)	Prec@1 84.375 (85.867)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.6050 (0.5254)	Prec@1 79.688 (85.762)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.6450 (0.5280)	Prec@1 79.688 (85.684)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.5469 (0.5233)	Prec@1 82.812 (85.820)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.5664 (0.5257)	Prec@1 83.594 (85.791)	
Total train loss: 0.5258

 * Prec@1 68.250 Prec@5 89.500 Loss 1.2256
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.6123 (0.5300)	Prec@1 82.812 (85.617)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.5347 (0.5330)	Prec@1 87.500 (85.567)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.5645 (0.5293)	Prec@1 84.375 (85.747)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.5273 (0.5284)	Prec@1 84.375 (85.740)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.4998 (0.5254)	Prec@1 82.812 (85.867)	
Total train loss: 0.5255

 * Prec@1 67.980 Prec@5 89.390 Loss 1.2227
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.5542 (0.5373)	Prec@1 83.594 (85.477)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.5171 (0.5292)	Prec@1 82.812 (85.597)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.4456 (0.5225)	Prec@1 86.719 (85.884)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.4712 (0.5234)	Prec@1 90.625 (85.912)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.5981 (0.5249)	Prec@1 82.031 (85.831)	
Total train loss: 0.5248

 * Prec@1 68.130 Prec@5 89.370 Loss 1.2285
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.4971 (0.5206)	Prec@1 89.062 (85.917)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.4590 (0.5178)	Prec@1 88.281 (86.103)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.4634 (0.5201)	Prec@1 87.500 (85.981)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.5132 (0.5216)	Prec@1 85.156 (85.877)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.3909 (0.5194)	Prec@1 87.500 (85.976)	
Total train loss: 0.5193

 * Prec@1 68.040 Prec@5 89.570 Loss 1.2236
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.4824 (0.5188)	Prec@1 89.844 (86.168)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.5176 (0.5244)	Prec@1 85.156 (85.762)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.5640 (0.5262)	Prec@1 83.594 (85.690)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.5234 (0.5254)	Prec@1 82.031 (85.705)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.5444 (0.5240)	Prec@1 89.062 (85.817)	
Total train loss: 0.5240

 * Prec@1 67.700 Prec@5 89.470 Loss 1.2334
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.4885 (0.5236)	Prec@1 85.938 (85.637)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.4827 (0.5220)	Prec@1 85.938 (85.892)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.6001 (0.5247)	Prec@1 83.594 (85.904)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.4287 (0.5245)	Prec@1 89.062 (85.887)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.6260 (0.5216)	Prec@1 84.375 (85.996)	
Total train loss: 0.5217

 * Prec@1 67.980 Prec@5 89.630 Loss 1.2207
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.4685 (0.5272)	Prec@1 87.500 (85.767)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.5376 (0.5193)	Prec@1 86.719 (86.103)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.4302 (0.5190)	Prec@1 94.531 (86.221)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.5410 (0.5233)	Prec@1 86.719 (86.040)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.5786 (0.5219)	Prec@1 86.719 (86.058)	
Total train loss: 0.5218

 * Prec@1 67.830 Prec@5 89.480 Loss 1.2266
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.5347 (0.5193)	Prec@1 89.062 (86.238)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.6504 (0.5215)	Prec@1 81.250 (86.043)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.5430 (0.5218)	Prec@1 85.156 (86.044)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.4268 (0.5202)	Prec@1 89.844 (86.073)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.5093 (0.5215)	Prec@1 85.938 (86.006)	
Total train loss: 0.5218

 * Prec@1 68.020 Prec@5 89.520 Loss 1.2236
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.5474 (0.5157)	Prec@1 83.594 (86.358)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.5391 (0.5240)	Prec@1 86.719 (85.847)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.4473 (0.5216)	Prec@1 92.188 (85.907)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.4978 (0.5179)	Prec@1 88.281 (85.983)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.6953 (0.5227)	Prec@1 77.344 (85.887)	
Total train loss: 0.5230

 * Prec@1 67.760 Prec@5 89.480 Loss 1.2314
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.4009 (0.5196)	Prec@1 92.188 (86.238)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.5161 (0.5227)	Prec@1 88.281 (85.998)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.4224 (0.5197)	Prec@1 89.062 (86.108)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.5547 (0.5201)	Prec@1 86.719 (86.128)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.5332 (0.5219)	Prec@1 84.375 (85.972)	
Total train loss: 0.5221

 * Prec@1 67.750 Prec@5 89.490 Loss 1.2354
Best acc: 68.630
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.6060 (0.5139)	Prec@1 85.156 (86.138)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.4846 (0.5221)	Prec@1 82.812 (85.742)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.5957 (0.5178)	Prec@1 82.812 (86.001)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.5254 (0.5187)	Prec@1 86.719 (86.080)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.4443 (0.5174)	Prec@1 87.500 (86.044)	
Total train loss: 0.5175

 * Prec@1 68.030 Prec@5 89.600 Loss 1.2295
Best acc: 68.630
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 39.440 Prec@5 67.140 Loss 2.8184
Pre-trained Prec@1 with 13 layers frozen: 39.439998626708984 	 Loss: 2.818359375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.9087 (0.9112)	Prec@1 71.875 (73.908)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.6362 (0.8613)	Prec@1 84.375 (75.055)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.6792 (0.8305)	Prec@1 77.344 (75.808)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.7173 (0.8116)	Prec@1 76.562 (76.417)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.8062 (0.7991)	Prec@1 75.781 (76.707)	
Total train loss: 0.7993

 * Prec@1 67.520 Prec@5 89.810 Loss 1.2363
Best acc: 67.520
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.6851 (0.6865)	Prec@1 79.688 (80.008)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.6318 (0.6946)	Prec@1 84.375 (79.808)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.6152 (0.6881)	Prec@1 82.031 (80.025)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6689 (0.6918)	Prec@1 79.688 (79.870)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.7568 (0.6947)	Prec@1 80.469 (79.850)	
Total train loss: 0.6946

 * Prec@1 67.670 Prec@5 89.950 Loss 1.2227
Best acc: 67.670
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.7266 (0.6409)	Prec@1 76.562 (82.051)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5181 (0.6599)	Prec@1 88.281 (81.420)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.6343 (0.6574)	Prec@1 83.594 (81.390)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.6489 (0.6564)	Prec@1 81.250 (81.345)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.8252 (0.6573)	Prec@1 78.906 (81.178)	
Total train loss: 0.6572

 * Prec@1 67.840 Prec@5 89.650 Loss 1.2354
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7212 (0.6192)	Prec@1 78.906 (82.432)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8037 (0.6328)	Prec@1 77.344 (82.081)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.7603 (0.6324)	Prec@1 78.906 (82.075)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5391 (0.6307)	Prec@1 82.812 (82.076)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6250 (0.6318)	Prec@1 81.250 (81.997)	
Total train loss: 0.6321

 * Prec@1 67.730 Prec@5 89.620 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.6084 (0.6035)	Prec@1 82.812 (82.923)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.5405 (0.6046)	Prec@1 85.156 (82.597)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.6948 (0.6057)	Prec@1 78.125 (82.662)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5239 (0.6084)	Prec@1 87.500 (82.707)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.6553 (0.6115)	Prec@1 80.469 (82.614)	
Total train loss: 0.6114

 * Prec@1 66.970 Prec@5 89.690 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.5859 (0.5886)	Prec@1 83.594 (83.664)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.7822 (0.5874)	Prec@1 75.781 (83.924)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.5049 (0.5811)	Prec@1 85.156 (84.054)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.6509 (0.5805)	Prec@1 82.031 (84.110)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.5024 (0.5815)	Prec@1 89.844 (84.010)	
Total train loss: 0.5817

 * Prec@1 67.580 Prec@5 89.660 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.4712 (0.5833)	Prec@1 85.938 (84.175)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.5806 (0.5771)	Prec@1 87.500 (84.280)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.5840 (0.5720)	Prec@1 80.469 (84.448)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.4805 (0.5728)	Prec@1 86.719 (84.325)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.7061 (0.5760)	Prec@1 78.906 (84.179)	
Total train loss: 0.5762

 * Prec@1 67.490 Prec@5 89.480 Loss 1.2363
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.6543 (0.5928)	Prec@1 82.812 (83.524)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.5928 (0.5865)	Prec@1 78.906 (83.684)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.6230 (0.5816)	Prec@1 82.031 (83.757)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.6221 (0.5803)	Prec@1 80.469 (83.824)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.6504 (0.5800)	Prec@1 77.344 (83.872)	
Total train loss: 0.5801

 * Prec@1 67.420 Prec@5 89.650 Loss 1.2363
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.5835 (0.5620)	Prec@1 85.156 (84.575)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.6489 (0.5678)	Prec@1 80.469 (84.100)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.5269 (0.5714)	Prec@1 86.719 (84.058)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.6348 (0.5740)	Prec@1 79.688 (83.934)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.4541 (0.5768)	Prec@1 89.062 (83.898)	
Total train loss: 0.5767

 * Prec@1 67.510 Prec@5 89.590 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.5552 (0.5783)	Prec@1 85.156 (83.874)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.5752 (0.5728)	Prec@1 82.812 (84.185)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.5645 (0.5717)	Prec@1 84.375 (84.355)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.4353 (0.5760)	Prec@1 88.281 (84.205)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.3965 (0.5710)	Prec@1 91.406 (84.353)	
Total train loss: 0.5711

 * Prec@1 67.590 Prec@5 89.550 Loss 1.2373
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.4871 (0.5643)	Prec@1 85.938 (84.615)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.5356 (0.5702)	Prec@1 87.500 (84.320)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.6685 (0.5706)	Prec@1 79.688 (84.261)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.5449 (0.5707)	Prec@1 85.156 (84.237)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.5513 (0.5692)	Prec@1 81.250 (84.363)	
Total train loss: 0.5691

 * Prec@1 67.420 Prec@5 89.500 Loss 1.2314
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.5732 (0.5679)	Prec@1 84.375 (84.575)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.5762 (0.5634)	Prec@1 83.594 (84.635)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.6831 (0.5659)	Prec@1 78.125 (84.522)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.6641 (0.5691)	Prec@1 78.906 (84.458)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.4724 (0.5703)	Prec@1 89.062 (84.403)	
Total train loss: 0.5707

 * Prec@1 67.340 Prec@5 89.460 Loss 1.2402
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.5693 (0.5672)	Prec@1 82.812 (84.575)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.5239 (0.5662)	Prec@1 86.719 (84.610)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.4534 (0.5671)	Prec@1 87.500 (84.485)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.4824 (0.5690)	Prec@1 89.844 (84.388)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.5781 (0.5697)	Prec@1 82.812 (84.285)	
Total train loss: 0.5696

 * Prec@1 67.620 Prec@5 89.500 Loss 1.2344
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.5566 (0.5684)	Prec@1 82.812 (84.585)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.5381 (0.5684)	Prec@1 85.156 (84.535)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.5508 (0.5663)	Prec@1 86.719 (84.692)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.6270 (0.5684)	Prec@1 84.375 (84.540)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.6675 (0.5682)	Prec@1 77.344 (84.549)	
Total train loss: 0.5684

 * Prec@1 67.390 Prec@5 89.540 Loss 1.2422
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.5386 (0.5821)	Prec@1 86.719 (83.844)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.5552 (0.5742)	Prec@1 85.156 (84.115)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.5557 (0.5702)	Prec@1 84.375 (84.211)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.5786 (0.5704)	Prec@1 85.938 (84.220)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.5688 (0.5705)	Prec@1 86.719 (84.209)	
Total train loss: 0.5706

 * Prec@1 67.260 Prec@5 89.510 Loss 1.2383
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.5728 (0.5652)	Prec@1 85.938 (84.275)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.4692 (0.5683)	Prec@1 87.500 (84.175)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.5400 (0.5658)	Prec@1 82.812 (84.365)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.5713 (0.5689)	Prec@1 85.156 (84.270)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.6514 (0.5691)	Prec@1 84.375 (84.301)	
Total train loss: 0.5692

 * Prec@1 67.540 Prec@5 89.450 Loss 1.2402
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.4309 (0.5605)	Prec@1 91.406 (84.896)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.5596 (0.5649)	Prec@1 82.031 (84.650)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.6484 (0.5686)	Prec@1 84.375 (84.539)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.5698 (0.5687)	Prec@1 82.812 (84.523)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.4724 (0.5685)	Prec@1 88.281 (84.499)	
Total train loss: 0.5685

 * Prec@1 67.750 Prec@5 89.620 Loss 1.2334
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.6226 (0.5641)	Prec@1 84.375 (84.515)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.5889 (0.5677)	Prec@1 86.719 (84.360)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.4880 (0.5673)	Prec@1 85.156 (84.458)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.6294 (0.5680)	Prec@1 83.594 (84.493)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.6143 (0.5713)	Prec@1 79.688 (84.333)	
Total train loss: 0.5715

 * Prec@1 67.310 Prec@5 89.480 Loss 1.2422
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.6133 (0.5748)	Prec@1 78.906 (83.904)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.5747 (0.5757)	Prec@1 85.156 (83.999)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.6328 (0.5738)	Prec@1 82.812 (84.205)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.5366 (0.5743)	Prec@1 86.719 (84.207)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.6860 (0.5721)	Prec@1 78.906 (84.313)	
Total train loss: 0.5723

 * Prec@1 67.500 Prec@5 89.550 Loss 1.2412
Best acc: 67.840
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.7080 (0.5833)	Prec@1 78.906 (83.554)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.4797 (0.5767)	Prec@1 89.062 (83.789)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.5474 (0.5742)	Prec@1 85.156 (84.014)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.5518 (0.5732)	Prec@1 87.500 (84.155)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.5342 (0.5715)	Prec@1 87.500 (84.211)	
Total train loss: 0.5717

 * Prec@1 67.550 Prec@5 89.670 Loss 1.2354
Best acc: 67.840
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 34.890 Prec@5 63.170 Loss 3.1543
Pre-trained Prec@1 with 15 layers frozen: 34.88999938964844 	 Loss: 3.154296875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 1.2178 (1.4186)	Prec@1 66.406 (61.008)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.0605 (1.2883)	Prec@1 67.969 (64.037)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.9521 (1.2309)	Prec@1 74.219 (65.258)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.0850 (1.1930)	Prec@1 70.312 (66.078)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.0000 (1.1686)	Prec@1 71.875 (66.673)	
Total train loss: 1.1685

 * Prec@1 63.740 Prec@5 87.600 Loss 1.3799
Best acc: 63.740
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.8833 (0.9848)	Prec@1 70.312 (71.414)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.0049 (0.9692)	Prec@1 71.875 (71.780)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.0312 (0.9646)	Prec@1 68.750 (71.898)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.8701 (0.9583)	Prec@1 72.656 (72.196)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.7544 (0.9566)	Prec@1 77.344 (72.316)	
Total train loss: 0.9565

 * Prec@1 64.390 Prec@5 88.280 Loss 1.3398
Best acc: 64.390
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.8296 (0.8850)	Prec@1 70.312 (74.659)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0000 (0.8890)	Prec@1 69.531 (74.239)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.0234 (0.8835)	Prec@1 67.969 (74.319)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.1592 (0.8920)	Prec@1 64.844 (73.943)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.0156 (0.8933)	Prec@1 71.875 (73.946)	
Total train loss: 0.8932

 * Prec@1 65.130 Prec@5 88.540 Loss 1.3262
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.7124 (0.8290)	Prec@1 78.906 (75.972)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.7754 (0.8488)	Prec@1 78.906 (75.366)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.8765 (0.8477)	Prec@1 73.438 (75.377)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.6772 (0.8483)	Prec@1 82.031 (75.398)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.7432 (0.8493)	Prec@1 80.469 (75.331)	
Total train loss: 0.8493

 * Prec@1 65.290 Prec@5 88.690 Loss 1.3086
Best acc: 65.290
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.8306 (0.7955)	Prec@1 75.000 (77.484)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.0498 (0.8022)	Prec@1 74.219 (77.013)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.8228 (0.8083)	Prec@1 73.438 (76.679)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.7803 (0.8156)	Prec@1 78.125 (76.452)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.0020 (0.8184)	Prec@1 73.438 (76.354)	
Total train loss: 0.8185

 * Prec@1 65.300 Prec@5 88.520 Loss 1.3164
Best acc: 65.300
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.7681 (0.7777)	Prec@1 78.906 (77.314)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.8027 (0.7890)	Prec@1 76.562 (77.194)	
Epoch: [5][233/391]	LR: 0.0002	Loss 0.7969 (0.7873)	Prec@1 77.344 (77.340)	
Epoch: [5][311/391]	LR: 0.0002	Loss 0.7651 (0.7810)	Prec@1 77.344 (77.632)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.7197 (0.7813)	Prec@1 78.125 (77.552)	
Total train loss: 0.7813

 * Prec@1 65.240 Prec@5 88.640 Loss 1.3047
Best acc: 65.300
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.9624 (0.7660)	Prec@1 73.438 (77.895)	
Epoch: [6][155/391]	LR: 0.0002	Loss 0.7554 (0.7704)	Prec@1 78.906 (77.905)	
Epoch: [6][233/391]	LR: 0.0002	Loss 0.9131 (0.7747)	Prec@1 74.219 (77.684)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.7251 (0.7727)	Prec@1 78.125 (77.714)	
Epoch: [6][389/391]	LR: 0.0002	Loss 0.9712 (0.7773)	Prec@1 71.094 (77.616)	
Total train loss: 0.7774

 * Prec@1 65.410 Prec@5 88.660 Loss 1.3115
Best acc: 65.410
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 0.6758 (0.7579)	Prec@1 85.156 (78.295)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.7437 (0.7679)	Prec@1 77.344 (77.995)	
Epoch: [7][233/391]	LR: 0.0002	Loss 0.7715 (0.7728)	Prec@1 75.781 (77.905)	
Epoch: [7][311/391]	LR: 0.0002	Loss 0.6973 (0.7751)	Prec@1 79.688 (77.822)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.6108 (0.7759)	Prec@1 81.250 (77.808)	
Total train loss: 0.7763

 * Prec@1 65.520 Prec@5 88.660 Loss 1.2969
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 0.8164 (0.7785)	Prec@1 74.219 (77.754)	
Epoch: [8][155/391]	LR: 0.0002	Loss 0.9194 (0.7713)	Prec@1 70.312 (77.729)	
Epoch: [8][233/391]	LR: 0.0002	Loss 0.7783 (0.7692)	Prec@1 76.562 (77.865)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.7212 (0.7694)	Prec@1 75.781 (77.815)	
Epoch: [8][389/391]	LR: 0.0002	Loss 0.8721 (0.7704)	Prec@1 75.000 (77.762)	
Total train loss: 0.7703

 * Prec@1 65.430 Prec@5 88.540 Loss 1.3047
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.7974 (0.7825)	Prec@1 78.125 (77.394)	
Epoch: [9][155/391]	LR: 0.0002	Loss 0.7192 (0.7680)	Prec@1 80.469 (77.875)	
Epoch: [9][233/391]	LR: 0.0002	Loss 0.8149 (0.7696)	Prec@1 75.781 (77.891)	
Epoch: [9][311/391]	LR: 0.0002	Loss 0.7710 (0.7678)	Prec@1 78.906 (78.012)	
Epoch: [9][389/391]	LR: 0.0002	Loss 0.9482 (0.7694)	Prec@1 70.312 (77.993)	
Total train loss: 0.7692

 * Prec@1 65.160 Prec@5 88.570 Loss 1.3105
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 0.7803 (0.7581)	Prec@1 81.250 (78.255)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.9121 (0.7640)	Prec@1 65.625 (78.180)	
Epoch: [10][233/391]	LR: 4e-05	Loss 0.7407 (0.7667)	Prec@1 82.031 (77.975)	
Epoch: [10][311/391]	LR: 4e-05	Loss 0.8120 (0.7684)	Prec@1 78.906 (77.892)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.6782 (0.7679)	Prec@1 82.812 (77.893)	
Total train loss: 0.7679

 * Prec@1 65.270 Prec@5 88.660 Loss 1.3174
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 0.9067 (0.7696)	Prec@1 71.094 (78.165)	
Epoch: [11][155/391]	LR: 4e-05	Loss 0.7031 (0.7754)	Prec@1 77.344 (77.985)	
Epoch: [11][233/391]	LR: 4e-05	Loss 0.7446 (0.7792)	Prec@1 79.688 (77.708)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.8730 (0.7767)	Prec@1 73.438 (77.784)	
Epoch: [11][389/391]	LR: 4e-05	Loss 0.8442 (0.7720)	Prec@1 77.344 (77.889)	
Total train loss: 0.7720

 * Prec@1 65.330 Prec@5 88.510 Loss 1.3086
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.7002 (0.7625)	Prec@1 79.688 (78.596)	
Epoch: [12][155/391]	LR: 4e-05	Loss 0.7271 (0.7679)	Prec@1 77.344 (78.215)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.8838 (0.7700)	Prec@1 74.219 (78.042)	
Epoch: [12][311/391]	LR: 4e-05	Loss 0.7573 (0.7691)	Prec@1 75.781 (78.032)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.6494 (0.7654)	Prec@1 82.812 (78.117)	
Total train loss: 0.7653

 * Prec@1 65.460 Prec@5 88.590 Loss 1.3096
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.6143 (0.7641)	Prec@1 79.688 (78.115)	
Epoch: [13][155/391]	LR: 4e-05	Loss 0.7725 (0.7696)	Prec@1 79.688 (77.769)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.7412 (0.7729)	Prec@1 79.688 (77.911)	
Epoch: [13][311/391]	LR: 4e-05	Loss 0.6431 (0.7693)	Prec@1 82.812 (77.975)	
Epoch: [13][389/391]	LR: 4e-05	Loss 0.7197 (0.7646)	Prec@1 82.031 (78.007)	
Total train loss: 0.7651

 * Prec@1 65.340 Prec@5 88.460 Loss 1.3145
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 0.7788 (0.7826)	Prec@1 75.781 (77.284)	
Epoch: [14][155/391]	LR: 4e-05	Loss 0.6743 (0.7715)	Prec@1 80.469 (77.729)	
Epoch: [14][233/391]	LR: 4e-05	Loss 0.9038 (0.7758)	Prec@1 73.438 (77.608)	
Epoch: [14][311/391]	LR: 4e-05	Loss 0.7310 (0.7735)	Prec@1 79.688 (77.637)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.5728 (0.7696)	Prec@1 84.375 (77.750)	
Total train loss: 0.7695

 * Prec@1 65.380 Prec@5 88.710 Loss 1.3105
Best acc: 65.520
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 0.8081 (0.7546)	Prec@1 77.344 (78.446)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.6353 (0.7651)	Prec@1 84.375 (77.925)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 0.8125 (0.7657)	Prec@1 75.781 (78.125)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 0.5947 (0.7639)	Prec@1 85.156 (78.065)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 0.9521 (0.7653)	Prec@1 73.438 (78.127)	
Total train loss: 0.7659

 * Prec@1 65.800 Prec@5 88.640 Loss 1.3076
Best acc: 65.800
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 0.7632 (0.7517)	Prec@1 77.344 (78.285)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 0.6577 (0.7678)	Prec@1 76.562 (77.689)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 0.8125 (0.7647)	Prec@1 75.781 (77.908)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 0.7461 (0.7674)	Prec@1 78.906 (77.935)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 0.7681 (0.7693)	Prec@1 77.344 (77.887)	
Total train loss: 0.7696

 * Prec@1 65.530 Prec@5 88.460 Loss 1.3076
Best acc: 65.800
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 0.7202 (0.7638)	Prec@1 75.000 (77.885)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 0.6943 (0.7648)	Prec@1 83.594 (78.010)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 0.7847 (0.7663)	Prec@1 79.688 (77.995)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 0.8296 (0.7628)	Prec@1 78.125 (78.153)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.6582 (0.7650)	Prec@1 79.688 (78.067)	
Total train loss: 0.7651

 * Prec@1 65.380 Prec@5 88.420 Loss 1.3135
Best acc: 65.800
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.7563 (0.7678)	Prec@1 77.344 (78.195)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.7568 (0.7681)	Prec@1 78.125 (78.010)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 0.7266 (0.7712)	Prec@1 75.000 (77.841)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 0.6777 (0.7700)	Prec@1 82.812 (77.885)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 0.6304 (0.7680)	Prec@1 81.250 (77.955)	
Total train loss: 0.7680

 * Prec@1 65.320 Prec@5 88.620 Loss 1.3066
Best acc: 65.800
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 0.7896 (0.7626)	Prec@1 75.000 (77.764)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.5923 (0.7594)	Prec@1 85.156 (78.240)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.6367 (0.7657)	Prec@1 87.500 (77.981)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 0.8867 (0.7655)	Prec@1 75.000 (77.980)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 0.8164 (0.7696)	Prec@1 75.781 (77.895)	
Total train loss: 0.7697

 * Prec@1 65.040 Prec@5 88.690 Loss 1.3115
Best acc: 65.800
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 27.520 Prec@5 57.080 Loss 3.8301
Pre-trained Prec@1 with 17 layers frozen: 27.51999855041504 	 Loss: 3.830078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 1.4170 (1.7274)	Prec@1 61.719 (54.417)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.3203 (1.6011)	Prec@1 60.938 (56.791)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.3730 (1.5251)	Prec@1 58.594 (58.423)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.3291 (1.4773)	Prec@1 63.281 (59.398)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.2969 (1.4444)	Prec@1 63.281 (60.072)	
Total train loss: 1.4453

 * Prec@1 60.060 Prec@5 86.140 Loss 1.5088
Best acc: 60.060
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 1.3818 (1.2407)	Prec@1 60.938 (64.854)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.1562 (1.2395)	Prec@1 69.531 (65.124)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.3457 (1.2350)	Prec@1 56.250 (65.218)	
Epoch: [1][311/391]	LR: 0.001	Loss 1.1113 (1.2351)	Prec@1 64.844 (65.167)	
Epoch: [1][389/391]	LR: 0.001	Loss 1.1807 (1.2255)	Prec@1 68.750 (65.363)	
Total train loss: 1.2258

 * Prec@1 62.010 Prec@5 86.890 Loss 1.4297
Best acc: 62.010
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 1.2949 (1.1608)	Prec@1 64.062 (67.037)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.0479 (1.1622)	Prec@1 67.188 (66.892)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.2305 (1.1617)	Prec@1 67.969 (66.887)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.1670 (1.1603)	Prec@1 67.969 (66.897)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.1816 (1.1570)	Prec@1 67.188 (67.003)	
Total train loss: 1.1566

 * Prec@1 62.390 Prec@5 87.260 Loss 1.4062
Best acc: 62.390
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 1.2686 (1.1422)	Prec@1 67.969 (67.218)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.8477 (1.1232)	Prec@1 77.344 (67.934)	
Epoch: [3][233/391]	LR: 0.001	Loss 1.2285 (1.1275)	Prec@1 68.750 (67.755)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.0928 (1.1189)	Prec@1 68.750 (67.939)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.3691 (1.1157)	Prec@1 60.938 (68.069)	
Total train loss: 1.1159

 * Prec@1 62.790 Prec@5 87.510 Loss 1.3838
Best acc: 62.790
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 1.1387 (1.0874)	Prec@1 67.969 (68.790)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.2109 (1.0922)	Prec@1 64.844 (68.850)	
Epoch: [4][233/391]	LR: 0.001	Loss 1.0029 (1.0909)	Prec@1 74.219 (68.944)	
Epoch: [4][311/391]	LR: 0.001	Loss 1.1533 (1.0887)	Prec@1 66.406 (68.955)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.0820 (1.0875)	Prec@1 69.531 (68.840)	
Total train loss: 1.0874

 * Prec@1 63.140 Prec@5 87.530 Loss 1.3711
Best acc: 63.140
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 0.8926 (1.0650)	Prec@1 74.219 (69.211)	
Epoch: [5][155/391]	LR: 0.0002	Loss 0.9717 (1.0562)	Prec@1 73.438 (69.521)	
Epoch: [5][233/391]	LR: 0.0002	Loss 1.0869 (1.0574)	Prec@1 69.531 (69.758)	
Epoch: [5][311/391]	LR: 0.0002	Loss 1.0303 (1.0526)	Prec@1 73.438 (69.822)	
Epoch: [5][389/391]	LR: 0.0002	Loss 0.9512 (1.0521)	Prec@1 75.000 (70.020)	
Total train loss: 1.0519

 * Prec@1 63.110 Prec@5 87.540 Loss 1.3789
Best acc: 63.140
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 0.9736 (1.0637)	Prec@1 71.875 (69.952)	
Epoch: [6][155/391]	LR: 0.0002	Loss 1.0469 (1.0500)	Prec@1 67.188 (70.102)	
Epoch: [6][233/391]	LR: 0.0002	Loss 1.1279 (1.0514)	Prec@1 67.188 (69.965)	
Epoch: [6][311/391]	LR: 0.0002	Loss 0.9443 (1.0465)	Prec@1 70.312 (70.127)	
Epoch: [6][389/391]	LR: 0.0002	Loss 1.0215 (1.0499)	Prec@1 73.438 (69.982)	
Total train loss: 1.0502

 * Prec@1 63.310 Prec@5 87.330 Loss 1.3818
Best acc: 63.310
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 1.0264 (1.0494)	Prec@1 73.438 (70.142)	
Epoch: [7][155/391]	LR: 0.0002	Loss 0.9893 (1.0546)	Prec@1 72.656 (69.872)	
Epoch: [7][233/391]	LR: 0.0002	Loss 1.1992 (1.0485)	Prec@1 64.062 (69.878)	
Epoch: [7][311/391]	LR: 0.0002	Loss 1.1045 (1.0524)	Prec@1 71.875 (69.912)	
Epoch: [7][389/391]	LR: 0.0002	Loss 0.8638 (1.0475)	Prec@1 77.344 (69.964)	
Total train loss: 1.0480

 * Prec@1 63.480 Prec@5 87.440 Loss 1.3711
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 1.1211 (1.0637)	Prec@1 67.188 (69.361)	
Epoch: [8][155/391]	LR: 0.0002	Loss 1.0205 (1.0484)	Prec@1 71.094 (70.067)	
Epoch: [8][233/391]	LR: 0.0002	Loss 1.0898 (1.0468)	Prec@1 70.312 (70.169)	
Epoch: [8][311/391]	LR: 0.0002	Loss 0.9834 (1.0439)	Prec@1 75.000 (70.272)	
Epoch: [8][389/391]	LR: 0.0002	Loss 1.0273 (1.0416)	Prec@1 70.312 (70.331)	
Total train loss: 1.0421

 * Prec@1 63.450 Prec@5 87.600 Loss 1.3711
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 0.9814 (1.0221)	Prec@1 73.438 (70.533)	
Epoch: [9][155/391]	LR: 0.0002	Loss 1.0381 (1.0349)	Prec@1 67.969 (70.488)	
Epoch: [9][233/391]	LR: 0.0002	Loss 1.1533 (1.0370)	Prec@1 66.406 (70.349)	
Epoch: [9][311/391]	LR: 0.0002	Loss 1.2012 (1.0408)	Prec@1 64.062 (70.290)	
Epoch: [9][389/391]	LR: 0.0002	Loss 1.0449 (1.0419)	Prec@1 67.188 (70.256)	
Total train loss: 1.0421

 * Prec@1 63.280 Prec@5 87.650 Loss 1.3779
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 1.1348 (1.0509)	Prec@1 65.625 (69.972)	
Epoch: [10][155/391]	LR: 4e-05	Loss 0.8252 (1.0528)	Prec@1 76.562 (69.817)	
Epoch: [10][233/391]	LR: 4e-05	Loss 1.0156 (1.0517)	Prec@1 67.969 (69.788)	
Epoch: [10][311/391]	LR: 4e-05	Loss 1.1494 (1.0432)	Prec@1 64.062 (70.020)	
Epoch: [10][389/391]	LR: 4e-05	Loss 0.8301 (1.0437)	Prec@1 72.656 (70.050)	
Total train loss: 1.0435

 * Prec@1 63.310 Prec@5 87.570 Loss 1.3740
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 1.0107 (1.0481)	Prec@1 71.094 (70.072)	
Epoch: [11][155/391]	LR: 4e-05	Loss 1.0986 (1.0469)	Prec@1 70.312 (70.147)	
Epoch: [11][233/391]	LR: 4e-05	Loss 1.1963 (1.0414)	Prec@1 62.500 (70.216)	
Epoch: [11][311/391]	LR: 4e-05	Loss 0.9712 (1.0392)	Prec@1 72.656 (70.358)	
Epoch: [11][389/391]	LR: 4e-05	Loss 1.0469 (1.0419)	Prec@1 72.656 (70.317)	
Total train loss: 1.0418

 * Prec@1 63.120 Prec@5 87.620 Loss 1.3711
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 0.9258 (1.0422)	Prec@1 75.000 (70.022)	
Epoch: [12][155/391]	LR: 4e-05	Loss 1.1211 (1.0318)	Prec@1 69.531 (70.428)	
Epoch: [12][233/391]	LR: 4e-05	Loss 0.9849 (1.0394)	Prec@1 75.000 (70.323)	
Epoch: [12][311/391]	LR: 4e-05	Loss 1.1768 (1.0377)	Prec@1 69.531 (70.388)	
Epoch: [12][389/391]	LR: 4e-05	Loss 0.8091 (1.0412)	Prec@1 80.469 (70.302)	
Total train loss: 1.0416

 * Prec@1 63.310 Prec@5 87.540 Loss 1.3730
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 0.9141 (1.0323)	Prec@1 72.656 (70.503)	
Epoch: [13][155/391]	LR: 4e-05	Loss 1.2363 (1.0387)	Prec@1 69.531 (70.348)	
Epoch: [13][233/391]	LR: 4e-05	Loss 0.9585 (1.0424)	Prec@1 72.656 (70.246)	
Epoch: [13][311/391]	LR: 4e-05	Loss 1.1650 (1.0379)	Prec@1 68.750 (70.380)	
Epoch: [13][389/391]	LR: 4e-05	Loss 1.1133 (1.0398)	Prec@1 72.656 (70.290)	
Total train loss: 1.0401

 * Prec@1 63.280 Prec@5 87.380 Loss 1.3750
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 1.0225 (1.0401)	Prec@1 67.188 (70.182)	
Epoch: [14][155/391]	LR: 4e-05	Loss 1.2451 (1.0408)	Prec@1 64.844 (70.358)	
Epoch: [14][233/391]	LR: 4e-05	Loss 1.2236 (1.0382)	Prec@1 64.844 (70.266)	
Epoch: [14][311/391]	LR: 4e-05	Loss 1.1162 (1.0393)	Prec@1 67.969 (70.420)	
Epoch: [14][389/391]	LR: 4e-05	Loss 0.9805 (1.0377)	Prec@1 74.219 (70.415)	
Total train loss: 1.0375

 * Prec@1 63.350 Prec@5 87.580 Loss 1.3750
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 1.0547 (1.0333)	Prec@1 71.875 (70.292)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 0.9277 (1.0312)	Prec@1 72.656 (70.473)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 1.0625 (1.0340)	Prec@1 67.969 (70.519)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 1.0068 (1.0320)	Prec@1 73.438 (70.483)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 1.1533 (1.0382)	Prec@1 66.406 (70.325)	
Total train loss: 1.0384

 * Prec@1 63.070 Prec@5 87.620 Loss 1.3799
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 1.1719 (1.0537)	Prec@1 66.406 (69.942)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 1.0576 (1.0501)	Prec@1 67.188 (69.862)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 1.0303 (1.0420)	Prec@1 62.500 (70.102)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 1.0127 (1.0372)	Prec@1 71.094 (70.297)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 1.1895 (1.0393)	Prec@1 62.500 (70.272)	
Total train loss: 1.0394

 * Prec@1 63.240 Prec@5 87.580 Loss 1.3711
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 1.0488 (1.0360)	Prec@1 70.312 (70.262)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 1.0234 (1.0476)	Prec@1 67.969 (70.217)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 1.0293 (1.0442)	Prec@1 68.750 (70.319)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 1.0146 (1.0399)	Prec@1 76.562 (70.363)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 0.9678 (1.0408)	Prec@1 71.875 (70.379)	
Total train loss: 1.0409

 * Prec@1 63.210 Prec@5 87.570 Loss 1.3770
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 0.9834 (1.0323)	Prec@1 71.094 (70.443)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 0.9370 (1.0395)	Prec@1 75.000 (70.227)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 1.3242 (1.0399)	Prec@1 60.156 (70.206)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 1.2539 (1.0411)	Prec@1 65.625 (70.185)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 1.1123 (1.0433)	Prec@1 68.750 (70.152)	
Total train loss: 1.0432

 * Prec@1 63.470 Prec@5 87.660 Loss 1.3711
Best acc: 63.480
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 1.1602 (1.0401)	Prec@1 65.625 (70.042)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 0.8706 (1.0422)	Prec@1 72.656 (70.007)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 0.9619 (1.0459)	Prec@1 71.875 (69.969)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 1.0127 (1.0421)	Prec@1 75.781 (70.150)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 1.2217 (1.0435)	Prec@1 67.969 (70.180)	
Total train loss: 1.0434

 * Prec@1 63.400 Prec@5 87.590 Loss 1.3740
Best acc: 63.480
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 23.800 Prec@5 53.360 Loss 4.3320
Pre-trained Prec@1 with 19 layers frozen: 23.799999237060547 	 Loss: 4.33203125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 2.1738 (2.0161)	Prec@1 42.969 (47.796)	
Epoch: [0][155/391]	LR: 0.001	Loss 1.9492 (1.9646)	Prec@1 50.000 (48.873)	
Epoch: [0][233/391]	LR: 0.001	Loss 1.9678 (1.9191)	Prec@1 54.688 (49.893)	
Epoch: [0][311/391]	LR: 0.001	Loss 1.8262 (1.8914)	Prec@1 52.344 (50.503)	
Epoch: [0][389/391]	LR: 0.001	Loss 1.6475 (1.8676)	Prec@1 58.594 (50.917)	
Total train loss: 1.8675

 * Prec@1 52.860 Prec@5 80.250 Loss 1.8545
Best acc: 52.860
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 1.7881 (1.7169)	Prec@1 54.688 (54.046)	
Epoch: [1][155/391]	LR: 0.001	Loss 1.6396 (1.7092)	Prec@1 53.125 (54.147)	
Epoch: [1][233/391]	LR: 0.001	Loss 1.6367 (1.7094)	Prec@1 57.031 (54.284)	
Epoch: [1][311/391]	LR: 0.001	Loss 1.6084 (1.7079)	Prec@1 58.594 (54.269)	
Epoch: [1][389/391]	LR: 0.001	Loss 1.7373 (1.6997)	Prec@1 57.812 (54.479)	
Total train loss: 1.7000

 * Prec@1 54.540 Prec@5 81.930 Loss 1.7607
Best acc: 54.540
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 1.8398 (1.6494)	Prec@1 51.562 (55.759)	
Epoch: [2][155/391]	LR: 0.001	Loss 1.6426 (1.6473)	Prec@1 57.031 (55.414)	
Epoch: [2][233/391]	LR: 0.001	Loss 1.6729 (1.6378)	Prec@1 55.469 (55.639)	
Epoch: [2][311/391]	LR: 0.001	Loss 1.5127 (1.6379)	Prec@1 58.594 (55.724)	
Epoch: [2][389/391]	LR: 0.001	Loss 1.8604 (1.6381)	Prec@1 51.562 (55.813)	
Total train loss: 1.6377

 * Prec@1 55.310 Prec@5 82.820 Loss 1.7061
Best acc: 55.310
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 1.5645 (1.5883)	Prec@1 58.594 (56.861)	
Epoch: [3][155/391]	LR: 0.001	Loss 1.4922 (1.5919)	Prec@1 55.469 (56.976)	
Epoch: [3][233/391]	LR: 0.001	Loss 1.5312 (1.5925)	Prec@1 57.031 (57.028)	
Epoch: [3][311/391]	LR: 0.001	Loss 1.4082 (1.5941)	Prec@1 61.719 (56.916)	
Epoch: [3][389/391]	LR: 0.001	Loss 1.7168 (1.5946)	Prec@1 55.469 (56.921)	
Total train loss: 1.5947

 * Prec@1 55.970 Prec@5 83.540 Loss 1.6709
Best acc: 55.970
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 1.7285 (1.5579)	Prec@1 56.250 (57.572)	
Epoch: [4][155/391]	LR: 0.001	Loss 1.6338 (1.5729)	Prec@1 56.250 (57.021)	
Epoch: [4][233/391]	LR: 0.001	Loss 1.6279 (1.5685)	Prec@1 57.812 (57.315)	
Epoch: [4][311/391]	LR: 0.001	Loss 1.4580 (1.5718)	Prec@1 57.031 (57.161)	
Epoch: [4][389/391]	LR: 0.001	Loss 1.5498 (1.5693)	Prec@1 57.812 (57.344)	
Total train loss: 1.5692

 * Prec@1 56.840 Prec@5 83.810 Loss 1.6387
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0002	Loss 1.5957 (1.5553)	Prec@1 60.938 (58.113)	
Epoch: [5][155/391]	LR: 0.0002	Loss 1.5938 (1.5484)	Prec@1 61.719 (58.238)	
Epoch: [5][233/391]	LR: 0.0002	Loss 1.3086 (1.5580)	Prec@1 68.750 (57.999)	
Epoch: [5][311/391]	LR: 0.0002	Loss 1.5107 (1.5545)	Prec@1 64.062 (58.118)	
Epoch: [5][389/391]	LR: 0.0002	Loss 1.4766 (1.5517)	Prec@1 61.719 (58.023)	
Total train loss: 1.5518

 * Prec@1 56.400 Prec@5 83.510 Loss 1.6465
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0002	Loss 1.6094 (1.5589)	Prec@1 55.469 (57.883)	
Epoch: [6][155/391]	LR: 0.0002	Loss 1.3076 (1.5590)	Prec@1 64.844 (57.853)	
Epoch: [6][233/391]	LR: 0.0002	Loss 1.4932 (1.5549)	Prec@1 55.469 (58.053)	
Epoch: [6][311/391]	LR: 0.0002	Loss 1.4209 (1.5558)	Prec@1 58.594 (57.918)	
Epoch: [6][389/391]	LR: 0.0002	Loss 1.7188 (1.5515)	Prec@1 53.125 (57.945)	
Total train loss: 1.5516

 * Prec@1 56.570 Prec@5 83.790 Loss 1.6465
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0002	Loss 1.6045 (1.5659)	Prec@1 57.812 (57.622)	
Epoch: [7][155/391]	LR: 0.0002	Loss 1.5117 (1.5569)	Prec@1 57.812 (57.923)	
Epoch: [7][233/391]	LR: 0.0002	Loss 1.4238 (1.5609)	Prec@1 64.844 (57.749)	
Epoch: [7][311/391]	LR: 0.0002	Loss 1.4824 (1.5514)	Prec@1 57.812 (57.978)	
Epoch: [7][389/391]	LR: 0.0002	Loss 1.5713 (1.5474)	Prec@1 61.719 (58.115)	
Total train loss: 1.5475

 * Prec@1 56.730 Prec@5 83.690 Loss 1.6514
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0002	Loss 1.6621 (1.5732)	Prec@1 55.469 (57.382)	
Epoch: [8][155/391]	LR: 0.0002	Loss 1.7480 (1.5465)	Prec@1 54.688 (58.283)	
Epoch: [8][233/391]	LR: 0.0002	Loss 1.4658 (1.5488)	Prec@1 63.281 (57.856)	
Epoch: [8][311/391]	LR: 0.0002	Loss 1.4561 (1.5523)	Prec@1 56.250 (57.835)	
Epoch: [8][389/391]	LR: 0.0002	Loss 1.4180 (1.5482)	Prec@1 60.938 (57.849)	
Total train loss: 1.5489

 * Prec@1 56.710 Prec@5 83.900 Loss 1.6406
Best acc: 56.840
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0002	Loss 1.5225 (1.5527)	Prec@1 60.938 (57.592)	
Epoch: [9][155/391]	LR: 0.0002	Loss 1.3125 (1.5612)	Prec@1 67.188 (57.627)	
Epoch: [9][233/391]	LR: 0.0002	Loss 1.5537 (1.5547)	Prec@1 57.812 (57.766)	
Epoch: [9][311/391]	LR: 0.0002	Loss 1.5547 (1.5507)	Prec@1 50.000 (57.860)	
Epoch: [9][389/391]	LR: 0.0002	Loss 1.5527 (1.5474)	Prec@1 57.812 (57.991)	
Total train loss: 1.5477

 * Prec@1 56.990 Prec@5 83.810 Loss 1.6377
Best acc: 56.990
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 4e-05	Loss 1.4111 (1.5226)	Prec@1 60.156 (59.155)	
Epoch: [10][155/391]	LR: 4e-05	Loss 1.5420 (1.5298)	Prec@1 54.688 (58.684)	
Epoch: [10][233/391]	LR: 4e-05	Loss 1.5400 (1.5428)	Prec@1 57.031 (58.430)	
Epoch: [10][311/391]	LR: 4e-05	Loss 1.6025 (1.5451)	Prec@1 58.594 (58.263)	
Epoch: [10][389/391]	LR: 4e-05	Loss 1.5840 (1.5479)	Prec@1 53.906 (58.247)	
Total train loss: 1.5482

 * Prec@1 56.800 Prec@5 83.820 Loss 1.6436
Best acc: 56.990
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 4e-05	Loss 1.4541 (1.5574)	Prec@1 58.594 (57.732)	
Epoch: [11][155/391]	LR: 4e-05	Loss 1.6123 (1.5537)	Prec@1 57.812 (57.818)	
Epoch: [11][233/391]	LR: 4e-05	Loss 1.5283 (1.5558)	Prec@1 64.062 (57.819)	
Epoch: [11][311/391]	LR: 4e-05	Loss 1.4844 (1.5493)	Prec@1 54.688 (57.935)	
Epoch: [11][389/391]	LR: 4e-05	Loss 1.6846 (1.5448)	Prec@1 54.688 (58.077)	
Total train loss: 1.5448

 * Prec@1 56.840 Prec@5 83.880 Loss 1.6406
Best acc: 56.990
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 4e-05	Loss 1.5449 (1.5499)	Prec@1 56.250 (57.843)	
Epoch: [12][155/391]	LR: 4e-05	Loss 1.4395 (1.5353)	Prec@1 62.500 (58.454)	
Epoch: [12][233/391]	LR: 4e-05	Loss 1.4346 (1.5367)	Prec@1 62.500 (58.504)	
Epoch: [12][311/391]	LR: 4e-05	Loss 1.5176 (1.5409)	Prec@1 57.812 (58.459)	
Epoch: [12][389/391]	LR: 4e-05	Loss 1.4414 (1.5457)	Prec@1 58.594 (58.273)	
Total train loss: 1.5456

 * Prec@1 57.150 Prec@5 83.770 Loss 1.6377
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 4e-05	Loss 1.5674 (1.5669)	Prec@1 54.688 (57.462)	
Epoch: [13][155/391]	LR: 4e-05	Loss 1.4395 (1.5515)	Prec@1 60.156 (58.018)	
Epoch: [13][233/391]	LR: 4e-05	Loss 1.6494 (1.5510)	Prec@1 58.594 (58.043)	
Epoch: [13][311/391]	LR: 4e-05	Loss 1.6133 (1.5499)	Prec@1 57.031 (58.073)	
Epoch: [13][389/391]	LR: 4e-05	Loss 1.6270 (1.5478)	Prec@1 55.469 (58.041)	
Total train loss: 1.5476

 * Prec@1 56.820 Prec@5 83.700 Loss 1.6367
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 4e-05	Loss 1.5225 (1.5471)	Prec@1 58.594 (57.772)	
Epoch: [14][155/391]	LR: 4e-05	Loss 1.4990 (1.5511)	Prec@1 53.125 (57.612)	
Epoch: [14][233/391]	LR: 4e-05	Loss 1.4932 (1.5460)	Prec@1 60.938 (57.993)	
Epoch: [14][311/391]	LR: 4e-05	Loss 1.3887 (1.5493)	Prec@1 64.844 (58.030)	
Epoch: [14][389/391]	LR: 4e-05	Loss 1.4307 (1.5451)	Prec@1 65.625 (58.167)	
Total train loss: 1.5457

 * Prec@1 56.640 Prec@5 83.840 Loss 1.6406
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 8.000000000000001e-06	Loss 1.5137 (1.5374)	Prec@1 57.031 (57.853)	
Epoch: [15][155/391]	LR: 8.000000000000001e-06	Loss 1.5352 (1.5432)	Prec@1 57.031 (58.033)	
Epoch: [15][233/391]	LR: 8.000000000000001e-06	Loss 1.4668 (1.5446)	Prec@1 60.156 (58.223)	
Epoch: [15][311/391]	LR: 8.000000000000001e-06	Loss 1.5918 (1.5423)	Prec@1 53.906 (58.228)	
Epoch: [15][389/391]	LR: 8.000000000000001e-06	Loss 1.6494 (1.5488)	Prec@1 50.781 (58.117)	
Total train loss: 1.5488

 * Prec@1 56.720 Prec@5 83.830 Loss 1.6455
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 8.000000000000001e-06	Loss 1.5000 (1.5752)	Prec@1 53.906 (56.981)	
Epoch: [16][155/391]	LR: 8.000000000000001e-06	Loss 1.5938 (1.5586)	Prec@1 55.469 (57.672)	
Epoch: [16][233/391]	LR: 8.000000000000001e-06	Loss 1.5371 (1.5561)	Prec@1 62.500 (57.555)	
Epoch: [16][311/391]	LR: 8.000000000000001e-06	Loss 1.7559 (1.5531)	Prec@1 56.250 (57.702)	
Epoch: [16][389/391]	LR: 8.000000000000001e-06	Loss 1.6641 (1.5486)	Prec@1 53.906 (57.941)	
Total train loss: 1.5487

 * Prec@1 56.520 Prec@5 83.840 Loss 1.6406
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 8.000000000000001e-06	Loss 1.6182 (1.5266)	Prec@1 54.688 (58.724)	
Epoch: [17][155/391]	LR: 8.000000000000001e-06	Loss 1.4580 (1.5389)	Prec@1 59.375 (58.083)	
Epoch: [17][233/391]	LR: 8.000000000000001e-06	Loss 1.6865 (1.5476)	Prec@1 52.344 (57.963)	
Epoch: [17][311/391]	LR: 8.000000000000001e-06	Loss 1.4512 (1.5454)	Prec@1 58.594 (58.005)	
Epoch: [17][389/391]	LR: 8.000000000000001e-06	Loss 1.1787 (1.5464)	Prec@1 62.500 (57.979)	
Total train loss: 1.5464

 * Prec@1 56.810 Prec@5 83.990 Loss 1.6338
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 8.000000000000001e-06	Loss 1.6309 (1.5491)	Prec@1 55.469 (58.474)	
Epoch: [18][155/391]	LR: 8.000000000000001e-06	Loss 1.7109 (1.5457)	Prec@1 55.469 (58.308)	
Epoch: [18][233/391]	LR: 8.000000000000001e-06	Loss 1.6562 (1.5492)	Prec@1 57.812 (57.939)	
Epoch: [18][311/391]	LR: 8.000000000000001e-06	Loss 1.6377 (1.5504)	Prec@1 58.594 (57.883)	
Epoch: [18][389/391]	LR: 8.000000000000001e-06	Loss 1.3174 (1.5490)	Prec@1 64.844 (57.917)	
Total train loss: 1.5489

 * Prec@1 56.960 Prec@5 83.920 Loss 1.6328
Best acc: 57.150
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 8.000000000000001e-06	Loss 1.3057 (1.5472)	Prec@1 63.281 (58.193)	
Epoch: [19][155/391]	LR: 8.000000000000001e-06	Loss 1.2734 (1.5531)	Prec@1 67.969 (57.762)	
Epoch: [19][233/391]	LR: 8.000000000000001e-06	Loss 1.4512 (1.5620)	Prec@1 62.500 (57.686)	
Epoch: [19][311/391]	LR: 8.000000000000001e-06	Loss 1.5322 (1.5549)	Prec@1 59.375 (57.797)	
Epoch: [19][389/391]	LR: 8.000000000000001e-06	Loss 1.8516 (1.5491)	Prec@1 49.219 (57.959)	
Total train loss: 1.5488

 * Prec@1 56.970 Prec@5 83.930 Loss 1.6367
Best acc: 57.150
--------------------------------------------------------------------------------
