
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 3
Savedir:  ../pretrained_models/frozen/x64-8b/sram/cifar100/resnet18
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar100/resnet18/train/relu3
Test path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar100/resnet18/test/relu3
ResNet18(
  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=512, out_features=100, bias=False)
  (bn19): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 1.270 Prec@5 5.780 Loss 4.5977
Avg Loading time: 6.9449 seconds
Avg Batch time: 6.9908 seconds

Pre-trained Prec@1 with 3 layers frozen: 1.2699999809265137 	 Loss: 4.59765625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	DT: 0.000 (5.866)	BT: 0.076 (5.957)	Loss 1.8174 (2.8142)	Prec@1 60.156 (37.580)	
Epoch: [0][155/391]	LR: 0.01	DT: 0.000 (6.443)	BT: 0.076 (6.531)	Loss 1.4199 (2.2384)	Prec@1 69.531 (49.134)	
Epoch: [0][233/391]	LR: 0.01	DT: 4.028 (6.931)	BT: 4.127 (7.018)	Loss 1.3369 (1.9552)	Prec@1 69.531 (54.564)	
Epoch: [0][311/391]	LR: 0.01	DT: 0.000 (6.695)	BT: 0.073 (6.781)	Loss 1.1680 (1.7822)	Prec@1 70.312 (57.767)	
Epoch: [0][389/391]	LR: 0.01	DT: 0.000 (6.746)	BT: 0.073 (6.832)	Loss 1.0078 (1.6556)	Prec@1 74.219 (60.090)	
Total train loss: 1.6545
Avg Loading time: 6.7283 seconds
Avg Batch time: 6.8142 seconds

Train time: 2664.47505736351
 * Prec@1 73.060 Prec@5 94.270 Loss 1.0107
Avg Loading time: 6.3262 seconds
Avg Batch time: 6.3595 seconds

Best acc: 73.060
--------------------------------------------------------------------------------
Test time: 503.5187158584595

Epoch: [1][77/391]	LR: 0.01	DT: 0.000 (4.481)	BT: 0.076 (4.562)	Loss 0.8584 (0.8484)	Prec@1 78.125 (78.175)	
Epoch: [1][155/391]	LR: 0.01	DT: 0.000 (4.581)	BT: 0.067 (4.663)	Loss 0.8882 (0.8450)	Prec@1 79.688 (77.910)	
Epoch: [1][233/391]	LR: 0.01	DT: 0.000 (4.690)	BT: 0.082 (4.772)	Loss 0.6772 (0.8409)	Prec@1 81.250 (77.808)	
Epoch: [1][311/391]	LR: 0.01	DT: 0.000 (4.640)	BT: 0.074 (4.721)	Loss 0.9805 (0.8382)	Prec@1 71.094 (77.709)	
Epoch: [1][389/391]	LR: 0.01	DT: 0.000 (4.618)	BT: 0.074 (4.700)	Loss 0.8623 (0.8346)	Prec@1 75.781 (77.676)	
Total train loss: 0.8345
Avg Loading time: 4.6061 seconds
Avg Batch time: 4.6884 seconds

Train time: 1833.2698543071747
 * Prec@1 75.550 Prec@5 94.690 Loss 0.9121
Avg Loading time: 4.5729 seconds
Avg Batch time: 4.6067 seconds

Best acc: 75.550
--------------------------------------------------------------------------------
Test time: 365.09138345718384

Epoch: [2][77/391]	LR: 0.01	DT: 0.000 (4.886)	BT: 0.074 (4.974)	Loss 0.5547 (0.5188)	Prec@1 85.156 (86.699)	
Epoch: [2][155/391]	LR: 0.01	DT: 0.000 (5.501)	BT: 0.084 (5.589)	Loss 0.5200 (0.5177)	Prec@1 81.250 (86.564)	
Epoch: [2][233/391]	LR: 0.01	DT: 6.553 (5.703)	BT: 6.652 (5.792)	Loss 0.3857 (0.5188)	Prec@1 88.281 (86.408)	
Epoch: [2][311/391]	LR: 0.01	DT: 0.000 (5.448)	BT: 0.073 (5.537)	Loss 0.4856 (0.5242)	Prec@1 85.938 (86.138)	
Epoch: [2][389/391]	LR: 0.01	DT: 0.000 (5.133)	BT: 0.072 (5.221)	Loss 0.4983 (0.5335)	Prec@1 83.594 (85.851)	
Total train loss: 0.5335
Avg Loading time: 5.1199 seconds
Avg Batch time: 5.2077 seconds

Train time: 2036.3108246326447
 * Prec@1 76.500 Prec@5 94.630 Loss 0.8882
Avg Loading time: 3.1761 seconds
Avg Batch time: 3.2108 seconds

Best acc: 76.500
--------------------------------------------------------------------------------
Test time: 254.83038353919983

Epoch: [3][77/391]	LR: 0.01	DT: 0.000 (3.763)	BT: 0.072 (3.849)	Loss 0.2861 (0.3316)	Prec@1 95.312 (92.097)	
Epoch: [3][155/391]	LR: 0.01	DT: 0.000 (4.340)	BT: 0.073 (4.426)	Loss 0.3079 (0.3290)	Prec@1 92.188 (92.052)	
Epoch: [3][233/391]	LR: 0.01	DT: 16.000 (4.779)	BT: 16.107 (4.865)	Loss 0.3853 (0.3337)	Prec@1 89.844 (91.920)	
Epoch: [3][311/391]	LR: 0.01	DT: 0.000 (4.920)	BT: 0.074 (5.006)	Loss 0.3875 (0.3390)	Prec@1 92.969 (91.714)	
Epoch: [3][389/391]	LR: 0.01	DT: 0.000 (5.165)	BT: 0.077 (5.251)	Loss 0.3242 (0.3447)	Prec@1 90.625 (91.422)	
Total train loss: 0.3450
Avg Loading time: 5.1518 seconds
Avg Batch time: 5.2381 seconds

Train time: 2048.1962344646454
 * Prec@1 76.750 Prec@5 94.360 Loss 0.9048
Avg Loading time: 5.9699 seconds
Avg Batch time: 6.0086 seconds

Best acc: 76.750
--------------------------------------------------------------------------------
Test time: 475.85839915275574

Epoch: [4][77/391]	LR: 0.01	DT: 0.000 (4.052)	BT: 0.084 (4.145)	Loss 0.2195 (0.2168)	Prec@1 96.094 (95.473)	
Epoch: [4][155/391]	LR: 0.01	DT: 0.000 (4.115)	BT: 0.089 (4.206)	Loss 0.1929 (0.2041)	Prec@1 95.312 (95.773)	
Epoch: [4][233/391]	LR: 0.01	DT: 0.858 (4.285)	BT: 0.965 (4.375)	Loss 0.1904 (0.2043)	Prec@1 94.531 (95.753)	
Epoch: [4][311/391]	LR: 0.01	DT: 0.000 (4.477)	BT: 0.080 (4.566)	Loss 0.3000 (0.2094)	Prec@1 92.188 (95.523)	
Epoch: [4][389/391]	LR: 0.01	DT: 0.000 (4.736)	BT: 0.076 (4.826)	Loss 0.2625 (0.2145)	Prec@1 92.969 (95.280)	
Total train loss: 0.2146
Avg Loading time: 4.7236 seconds
Avg Batch time: 4.8137 seconds

Train time: 1882.2703104019165
 * Prec@1 76.920 Prec@5 94.060 Loss 0.9097
Avg Loading time: 5.8618 seconds
Avg Batch time: 5.8982 seconds

Best acc: 76.920
--------------------------------------------------------------------------------
Test time: 467.24504590034485

Epoch: [5][77/391]	LR: 0.01	DT: 0.000 (4.420)	BT: 0.072 (4.508)	Loss 0.1605 (0.1434)	Prec@1 96.094 (97.266)	
Epoch: [5][155/391]	LR: 0.01	DT: 0.000 (4.199)	BT: 0.075 (4.287)	Loss 0.1976 (0.1446)	Prec@1 95.312 (97.236)	
Epoch: [5][233/391]	LR: 0.01	DT: 12.780 (4.029)	BT: 12.877 (4.116)	Loss 0.1576 (0.1428)	Prec@1 97.656 (97.339)	
Epoch: [5][311/391]	LR: 0.01	DT: 0.000 (3.968)	BT: 0.071 (4.055)	Loss 0.0933 (0.1458)	Prec@1 100.000 (97.241)	
Epoch: [5][389/391]	LR: 0.01	DT: 0.000 (4.288)	BT: 0.082 (4.376)	Loss 0.1766 (0.1488)	Prec@1 95.312 (97.099)	
Total train loss: 0.1490
Avg Loading time: 4.2766 seconds
Avg Batch time: 4.3646 seconds

Train time: 1706.6433818340302
 * Prec@1 77.090 Prec@5 93.430 Loss 0.9639
Avg Loading time: 5.2522 seconds
Avg Batch time: 5.2871 seconds

Best acc: 77.090
--------------------------------------------------------------------------------
Test time: 418.89142751693726

Epoch: [6][77/391]	LR: 0.01	DT: 0.282 (4.683)	BT: 0.366 (4.769)	Loss 0.1445 (0.1123)	Prec@1 96.875 (98.127)	
Epoch: [6][155/391]	LR: 0.01	DT: 0.000 (5.225)	BT: 0.073 (5.311)	Loss 0.0657 (0.1098)	Prec@1 100.000 (98.192)	
Epoch: [6][233/391]	LR: 0.01	DT: 0.000 (5.273)	BT: 0.082 (5.359)	Loss 0.0894 (0.1085)	Prec@1 97.656 (98.150)	
Epoch: [6][311/391]	LR: 0.01	DT: 1.778 (5.145)	BT: 1.874 (5.231)	Loss 0.1066 (0.1100)	Prec@1 97.656 (98.117)	
Epoch: [6][389/391]	LR: 0.01	DT: 0.000 (5.126)	BT: 0.080 (5.212)	Loss 0.1196 (0.1105)	Prec@1 98.438 (98.131)	
Total train loss: 0.1105
Avg Loading time: 5.1125 seconds
Avg Batch time: 5.1988 seconds

Train time: 2032.8274052143097
 * Prec@1 77.630 Prec@5 93.480 Loss 0.9399
Avg Loading time: 4.4014 seconds
Avg Batch time: 4.4376 seconds

Best acc: 77.630
--------------------------------------------------------------------------------
Test time: 351.79732489585876

Epoch: [7][77/391]	LR: 0.01	DT: 0.000 (3.390)	BT: 0.075 (3.482)	Loss 0.0613 (0.0859)	Prec@1 100.000 (98.838)	
Epoch: [7][155/391]	LR: 0.01	DT: 5.273 (4.027)	BT: 5.369 (4.116)	Loss 0.0974 (0.0818)	Prec@1 98.438 (98.868)	
Epoch: [7][233/391]	LR: 0.01	DT: 0.000 (4.589)	BT: 0.089 (4.679)	Loss 0.0919 (0.0817)	Prec@1 97.656 (98.865)	
Epoch: [7][311/391]	LR: 0.01	DT: 0.000 (4.818)	BT: 0.074 (4.907)	Loss 0.0972 (0.0824)	Prec@1 97.656 (98.851)	
Epoch: [7][389/391]	LR: 0.01	DT: 0.000 (4.927)	BT: 0.074 (5.015)	Loss 0.0587 (0.0835)	Prec@1 100.000 (98.796)	
Total train loss: 0.0836
Avg Loading time: 4.9146 seconds
Avg Batch time: 5.0026 seconds

Train time: 1956.099497795105
 * Prec@1 77.550 Prec@5 93.290 Loss 0.9512
Avg Loading time: 4.6860 seconds
Avg Batch time: 4.7204 seconds

Best acc: 77.630
--------------------------------------------------------------------------------
Test time: 373.5676553249359

Epoch: [8][77/391]	LR: 0.01	DT: 2.957 (3.488)	BT: 3.058 (3.579)	Loss 0.0854 (0.0727)	Prec@1 98.438 (99.008)	
Epoch: [8][155/391]	LR: 0.01	DT: 5.949 (3.980)	BT: 6.047 (4.072)	Loss 0.0562 (0.0693)	Prec@1 100.000 (99.124)	
Epoch: [8][233/391]	LR: 0.01	DT: 8.795 (4.644)	BT: 8.892 (4.736)	Loss 0.1615 (0.0682)	Prec@1 96.094 (99.159)	
Epoch: [8][311/391]	LR: 0.01	DT: 0.000 (4.988)	BT: 0.082 (5.081)	Loss 0.0802 (0.0694)	Prec@1 99.219 (99.106)	
Epoch: [8][389/391]	LR: 0.01	DT: 2.363 (5.287)	BT: 2.466 (5.381)	Loss 0.0572 (0.0700)	Prec@1 99.219 (99.077)	
Total train loss: 0.0701
Avg Loading time: 5.2736 seconds
Avg Batch time: 5.3675 seconds

Train time: 2098.80358171463
 * Prec@1 77.820 Prec@5 93.270 Loss 0.9414
Avg Loading time: 6.5084 seconds
Avg Batch time: 6.5458 seconds

Best acc: 77.820
--------------------------------------------------------------------------------
Test time: 518.2486608028412

Epoch: [9][77/391]	LR: 0.01	DT: 0.000 (4.128)	BT: 0.078 (4.216)	Loss 0.0866 (0.0634)	Prec@1 97.656 (99.229)	
Epoch: [9][155/391]	LR: 0.01	DT: 1.519 (4.394)	BT: 1.600 (4.484)	Loss 0.0573 (0.0625)	Prec@1 99.219 (99.204)	
Epoch: [9][233/391]	LR: 0.01	DT: 11.930 (4.357)	BT: 12.030 (4.446)	Loss 0.1044 (0.0619)	Prec@1 96.875 (99.222)	
Epoch: [9][311/391]	LR: 0.01	DT: 0.000 (4.318)	BT: 0.073 (4.407)	Loss 0.0697 (0.0623)	Prec@1 99.219 (99.201)	
Epoch: [9][389/391]	LR: 0.01	DT: 0.000 (4.544)	BT: 0.070 (4.633)	Loss 0.0602 (0.0617)	Prec@1 99.219 (99.207)	
Total train loss: 0.0619
Avg Loading time: 4.5328 seconds
Avg Batch time: 4.6209 seconds

Train time: 1806.8705654144287
 * Prec@1 77.440 Prec@5 93.310 Loss 0.9648
Avg Loading time: 5.9898 seconds
Avg Batch time: 6.0249 seconds

Best acc: 77.820
--------------------------------------------------------------------------------
Test time: 476.62069368362427

Epoch: [10][77/391]	LR: 0.002	DT: 0.000 (5.399)	BT: 0.077 (5.486)	Loss 0.0263 (0.0480)	Prec@1 100.000 (99.489)	
Epoch: [10][155/391]	LR: 0.002	DT: 0.000 (5.156)	BT: 0.073 (5.243)	Loss 0.0451 (0.0471)	Prec@1 100.000 (99.529)	
Epoch: [10][233/391]	LR: 0.002	DT: 4.912 (4.702)	BT: 5.001 (4.789)	Loss 0.0213 (0.0452)	Prec@1 100.000 (99.599)	
Epoch: [10][311/391]	LR: 0.002	DT: 0.000 (4.543)	BT: 0.118 (4.631)	Loss 0.0254 (0.0443)	Prec@1 100.000 (99.612)	
Epoch: [10][389/391]	LR: 0.002	DT: 0.000 (4.690)	BT: 0.079 (4.779)	Loss 0.0308 (0.0439)	Prec@1 99.219 (99.633)	
Total train loss: 0.0439
Avg Loading time: 4.6782 seconds
Avg Batch time: 4.7674 seconds

Train time: 1864.1376898288727
 * Prec@1 78.570 Prec@5 93.600 Loss 0.9209
Avg Loading time: 5.1551 seconds
Avg Batch time: 5.1924 seconds

Best acc: 78.570
--------------------------------------------------------------------------------
Test time: 411.339213848114

Epoch: [11][77/391]	LR: 0.002	DT: 0.000 (4.632)	BT: 0.076 (4.722)	Loss 0.0473 (0.0388)	Prec@1 100.000 (99.730)	
Epoch: [11][155/391]	LR: 0.002	DT: 0.000 (5.146)	BT: 0.078 (5.235)	Loss 0.0400 (0.0384)	Prec@1 99.219 (99.770)	
Epoch: [11][233/391]	LR: 0.002	DT: 0.000 (5.353)	BT: 0.089 (5.442)	Loss 0.0444 (0.0380)	Prec@1 99.219 (99.746)	
Epoch: [11][311/391]	LR: 0.002	DT: 0.000 (5.199)	BT: 0.079 (5.288)	Loss 0.0321 (0.0373)	Prec@1 100.000 (99.762)	
Epoch: [11][389/391]	LR: 0.002	DT: 0.000 (5.222)	BT: 0.073 (5.310)	Loss 0.0264 (0.0375)	Prec@1 100.000 (99.764)	
Total train loss: 0.0375
Avg Loading time: 5.2083 seconds
Avg Batch time: 5.2964 seconds

Train time: 2070.9634635448456
 * Prec@1 78.780 Prec@5 93.840 Loss 0.9097
Avg Loading time: 4.2037 seconds
Avg Batch time: 4.2428 seconds

Best acc: 78.780
--------------------------------------------------------------------------------
Test time: 336.4146330356598

Epoch: [12][77/391]	LR: 0.002	DT: 0.000 (3.684)	BT: 0.076 (3.772)	Loss 0.0305 (0.0335)	Prec@1 100.000 (99.890)	
Epoch: [12][155/391]	LR: 0.002	DT: 0.000 (3.893)	BT: 0.076 (3.984)	Loss 0.0242 (0.0337)	Prec@1 100.000 (99.820)	
Epoch: [12][233/391]	LR: 0.002	DT: 6.504 (4.567)	BT: 6.605 (4.659)	Loss 0.0213 (0.0342)	Prec@1 100.000 (99.816)	
Epoch: [12][311/391]	LR: 0.002	DT: 5.351 (5.050)	BT: 5.453 (5.142)	Loss 0.0266 (0.0337)	Prec@1 100.000 (99.825)	
Epoch: [12][389/391]	LR: 0.002	DT: 0.000 (5.313)	BT: 0.074 (5.405)	Loss 0.0390 (0.0340)	Prec@1 100.000 (99.820)	
Total train loss: 0.0340
Avg Loading time: 5.2995 seconds
Avg Batch time: 5.3912 seconds

Train time: 2108.0783879756927
 * Prec@1 78.950 Prec@5 93.660 Loss 0.8984
Avg Loading time: 3.3308 seconds
Avg Batch time: 3.3677 seconds

Best acc: 78.950
--------------------------------------------------------------------------------
Test time: 267.24312233924866

Epoch: [13][77/391]	LR: 0.002	DT: 0.000 (3.204)	BT: 0.076 (3.296)	Loss 0.0310 (0.0326)	Prec@1 100.000 (99.810)	
Epoch: [13][155/391]	LR: 0.002	DT: 0.000 (3.488)	BT: 0.075 (3.580)	Loss 0.0205 (0.0334)	Prec@1 100.000 (99.820)	
Epoch: [13][233/391]	LR: 0.002	DT: 0.000 (4.158)	BT: 0.088 (4.249)	Loss 0.0277 (0.0330)	Prec@1 100.000 (99.826)	
Epoch: [13][311/391]	LR: 0.002	DT: 9.084 (4.514)	BT: 9.180 (4.603)	Loss 0.0364 (0.0330)	Prec@1 100.000 (99.827)	
Epoch: [13][389/391]	LR: 0.002	DT: 0.000 (4.939)	BT: 0.074 (5.029)	Loss 0.0384 (0.0328)	Prec@1 99.219 (99.826)	
Total train loss: 0.0329
Avg Loading time: 4.9263 seconds
Avg Batch time: 5.0161 seconds

Train time: 1961.39160490036
 * Prec@1 79.020 Prec@5 93.620 Loss 0.9058
Avg Loading time: 6.9888 seconds
Avg Batch time: 7.0254 seconds

Best acc: 79.020
--------------------------------------------------------------------------------
Test time: 556.1713860034943

Epoch: [14][77/391]	LR: 0.002	DT: 0.000 (5.345)	BT: 0.078 (5.437)	Loss 0.0388 (0.0328)	Prec@1 99.219 (99.830)	
Epoch: [14][155/391]	LR: 0.002	DT: 4.826 (5.047)	BT: 4.925 (5.139)	Loss 0.0187 (0.0321)	Prec@1 100.000 (99.840)	
Epoch: [14][233/391]	LR: 0.002	DT: 1.794 (4.946)	BT: 1.895 (5.038)	Loss 0.0334 (0.0314)	Prec@1 100.000 (99.860)	
Epoch: [14][311/391]	LR: 0.002	DT: 0.000 (4.914)	BT: 0.080 (5.007)	Loss 0.0309 (0.0319)	Prec@1 100.000 (99.842)	
Epoch: [14][389/391]	LR: 0.002	DT: 0.278 (5.039)	BT: 0.371 (5.133)	Loss 0.0335 (0.0317)	Prec@1 100.000 (99.852)	
Total train loss: 0.0318
Avg Loading time: 5.0264 seconds
Avg Batch time: 5.1204 seconds

Train time: 2002.1935210227966
 * Prec@1 79.240 Prec@5 93.720 Loss 0.8999
Avg Loading time: 6.6448 seconds
Avg Batch time: 6.6814 seconds

Best acc: 79.240
--------------------------------------------------------------------------------
Test time: 528.9586319923401

Epoch: [15][77/391]	LR: 0.002	DT: 0.313 (5.619)	BT: 0.399 (5.707)	Loss 0.0385 (0.0316)	Prec@1 100.000 (99.770)	
Epoch: [15][155/391]	LR: 0.002	DT: 0.742 (5.997)	BT: 0.832 (6.087)	Loss 0.0319 (0.0310)	Prec@1 100.000 (99.825)	
Epoch: [15][233/391]	LR: 0.002	DT: 0.000 (5.317)	BT: 0.094 (5.407)	Loss 0.0279 (0.0311)	Prec@1 100.000 (99.853)	
Epoch: [15][311/391]	LR: 0.002	DT: 0.000 (4.916)	BT: 0.088 (5.008)	Loss 0.0165 (0.0309)	Prec@1 100.000 (99.855)	
Epoch: [15][389/391]	LR: 0.002	DT: 0.000 (4.952)	BT: 0.081 (5.045)	Loss 0.0319 (0.0306)	Prec@1 99.219 (99.858)	
Total train loss: 0.0307
Avg Loading time: 4.9397 seconds
Avg Batch time: 5.0323 seconds

Train time: 1967.7463595867157
 * Prec@1 79.010 Prec@5 93.570 Loss 0.9014
Avg Loading time: 6.4900 seconds
Avg Batch time: 6.5314 seconds

Best acc: 79.240
--------------------------------------------------------------------------------
Test time: 516.6326971054077

Epoch: [16][77/391]	LR: 0.002	DT: 0.000 (5.499)	BT: 0.076 (5.593)	Loss 0.0360 (0.0290)	Prec@1 100.000 (99.900)	
Epoch: [16][155/391]	LR: 0.002	DT: 0.000 (6.368)	BT: 0.089 (6.460)	Loss 0.0257 (0.0277)	Prec@1 100.000 (99.910)	
Epoch: [16][233/391]	LR: 0.002	DT: 10.836 (6.425)	BT: 10.934 (6.517)	Loss 0.0354 (0.0285)	Prec@1 99.219 (99.890)	
Epoch: [16][311/391]	LR: 0.002	DT: 1.556 (6.377)	BT: 1.646 (6.467)	Loss 0.0380 (0.0287)	Prec@1 100.000 (99.900)	
Epoch: [16][389/391]	LR: 0.002	DT: 0.000 (6.131)	BT: 0.074 (6.221)	Loss 0.0408 (0.0288)	Prec@1 100.000 (99.888)	
Total train loss: 0.0289
Avg Loading time: 6.1151 seconds
Avg Batch time: 6.2054 seconds

Train time: 2426.3997480869293
 * Prec@1 79.190 Prec@5 93.690 Loss 0.8936
Avg Loading time: 3.5902 seconds
Avg Batch time: 3.6275 seconds

Best acc: 79.240
--------------------------------------------------------------------------------
Test time: 287.26031970977783

Epoch: [17][77/391]	LR: 0.002	DT: 0.000 (4.089)	BT: 0.076 (4.181)	Loss 0.0359 (0.0273)	Prec@1 100.000 (99.920)	
Epoch: [17][155/391]	LR: 0.002	DT: 2.701 (3.592)	BT: 2.798 (3.684)	Loss 0.0354 (0.0278)	Prec@1 100.000 (99.905)	
Epoch: [17][233/391]	LR: 0.002	DT: 0.000 (3.403)	BT: 0.091 (3.497)	Loss 0.0407 (0.0285)	Prec@1 99.219 (99.903)	
Epoch: [17][311/391]	LR: 0.002	DT: 0.000 (3.588)	BT: 0.090 (3.681)	Loss 0.0316 (0.0285)	Prec@1 100.000 (99.917)	
Epoch: [17][389/391]	LR: 0.002	DT: 0.000 (3.861)	BT: 0.086 (3.953)	Loss 0.0201 (0.0283)	Prec@1 100.000 (99.920)	
Total train loss: 0.0283
Avg Loading time: 3.8512 seconds
Avg Batch time: 3.9434 seconds

Train time: 1541.9824802875519
 * Prec@1 79.050 Prec@5 93.720 Loss 0.9014
Avg Loading time: 4.6857 seconds
Avg Batch time: 4.7229 seconds

Best acc: 79.240
--------------------------------------------------------------------------------
Test time: 374.22100615501404

