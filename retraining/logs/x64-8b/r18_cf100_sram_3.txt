
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode: sram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
Savedir:  ../pretrained_models/frozen/x64-8b/sram/cifar100/resnet18
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar100/resnet18/train/relu3
Test path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar100/resnet18/test/relu3
ResNet18(
  (conv4): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): QConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=512, out_features=100, bias=False)
  (bn19): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 1.330 Prec@5 5.770 Loss 4.6016
Avg Loading time: 7.3996 seconds
Avg Batch time: 7.4709 seconds

Pre-trained Prec@1 with 3 layers frozen: 1.3299999237060547 	 Loss: 4.6015625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	DT: 0.000 (13.268)	BT: 0.169 (13.459)	Loss 4.2344 (4.6947)	Prec@1 7.031 (3.225)	
Epoch: [0][155/391]	LR: 0.001	DT: 0.000 (13.198)	BT: 0.167 (13.386)	Loss 4.0859 (4.4020)	Prec@1 7.812 (6.295)	
Epoch: [0][233/391]	LR: 0.001	DT: 0.000 (12.817)	BT: 0.184 (13.003)	Loss 4.1523 (4.2797)	Prec@1 7.031 (7.836)	
Epoch: [0][311/391]	LR: 0.001	DT: 0.000 (11.892)	BT: 0.174 (12.078)	Loss 3.8945 (4.2157)	Prec@1 20.312 (8.794)	
Epoch: [0][389/391]	LR: 0.001	DT: 5.491 (11.068)	BT: 5.679 (11.254)	Loss 3.9746 (4.1772)	Prec@1 14.844 (9.505)	
Total train loss: 4.1769
Avg Loading time: 11.0400 seconds
Avg Batch time: 11.2255 seconds

Train time: 4389.355434656143
 * Prec@1 12.990 Prec@5 34.270 Loss 4.0078
Avg Loading time: 8.6760 seconds
Avg Batch time: 8.7425 seconds

Best acc: 12.990
--------------------------------------------------------------------------------
Test time: 692.9631795883179

Epoch: [1][77/391]	LR: 0.001	DT: 0.000 (4.588)	BT: 0.172 (4.767)	Loss 4.0352 (3.9894)	Prec@1 13.281 (13.552)	
Epoch: [1][155/391]	LR: 0.001	DT: 0.000 (5.202)	BT: 0.173 (5.382)	Loss 3.9121 (3.9848)	Prec@1 12.500 (13.777)	
Epoch: [1][233/391]	LR: 0.001	DT: 0.000 (5.950)	BT: 0.180 (6.130)	Loss 3.8047 (3.9731)	Prec@1 19.531 (14.133)	
Epoch: [1][311/391]	LR: 0.001	DT: 0.000 (6.144)	BT: 0.174 (6.324)	Loss 3.9492 (3.9650)	Prec@1 18.750 (14.513)	
Epoch: [1][389/391]	LR: 0.001	DT: 0.000 (6.437)	BT: 0.174 (6.617)	Loss 3.9453 (3.9583)	Prec@1 16.406 (14.794)	
Total train loss: 3.9584
Avg Loading time: 6.4208 seconds
Avg Batch time: 6.6008 seconds

Train time: 2581.052812099457
 * Prec@1 15.770 Prec@5 38.160 Loss 3.9297
Avg Loading time: 8.8503 seconds
Avg Batch time: 8.9113 seconds

Best acc: 15.770
--------------------------------------------------------------------------------
Test time: 706.4238007068634

Epoch: [2][77/391]	LR: 0.001	DT: 0.000 (6.592)	BT: 0.172 (6.776)	Loss 3.9023 (3.9094)	Prec@1 20.312 (15.815)	
Epoch: [2][155/391]	LR: 0.001	DT: 0.000 (6.737)	BT: 0.170 (6.921)	Loss 3.7539 (3.9022)	Prec@1 19.531 (16.316)	
Epoch: [2][233/391]	LR: 0.001	DT: 5.379 (6.174)	BT: 5.557 (6.357)	Loss 3.9238 (3.8981)	Prec@1 15.625 (16.383)	
Epoch: [2][311/391]	LR: 0.001	DT: 0.000 (6.135)	BT: 0.174 (6.318)	Loss 3.8477 (3.8924)	Prec@1 17.188 (16.642)	
Epoch: [2][389/391]	LR: 0.001	DT: 0.000 (6.428)	BT: 0.172 (6.611)	Loss 3.8379 (3.8924)	Prec@1 16.406 (16.683)	
Total train loss: 3.8921
Avg Loading time: 6.4114 seconds
Avg Batch time: 6.5944 seconds

Train time: 2578.506001472473
 * Prec@1 17.240 Prec@5 41.070 Loss 3.8750
Avg Loading time: 8.3318 seconds
Avg Batch time: 8.3904 seconds

Best acc: 17.240
--------------------------------------------------------------------------------
Test time: 663.8762693405151

Epoch: [3][77/391]	LR: 0.001	DT: 0.000 (6.557)	BT: 0.174 (6.738)	Loss 3.9414 (3.8611)	Prec@1 12.500 (17.808)	
Epoch: [3][155/391]	LR: 0.001	DT: 0.000 (6.645)	BT: 0.170 (6.827)	Loss 3.8594 (3.8658)	Prec@1 21.875 (17.663)	
Epoch: [3][233/391]	LR: 0.001	DT: 0.000 (6.803)	BT: 0.183 (6.984)	Loss 3.8926 (3.8632)	Prec@1 15.625 (17.678)	
Epoch: [3][311/391]	LR: 0.001	DT: 0.000 (6.575)	BT: 0.171 (6.756)	Loss 3.8770 (3.8650)	Prec@1 16.406 (17.711)	
Epoch: [3][389/391]	LR: 0.001	DT: 0.000 (6.282)	BT: 0.176 (6.463)	Loss 3.8906 (3.8641)	Prec@1 15.625 (17.841)	
Total train loss: 3.8639
Avg Loading time: 6.2658 seconds
Avg Batch time: 6.4468 seconds

Train time: 2520.8094992637634
 * Prec@1 18.010 Prec@5 42.900 Loss 3.8633
Avg Loading time: 5.9782 seconds
Avg Batch time: 6.0382 seconds

Best acc: 18.010
--------------------------------------------------------------------------------
Test time: 478.06607580184937

Epoch: [4][77/391]	LR: 0.001	DT: 0.000 (7.136)	BT: 0.169 (7.320)	Loss 3.8730 (3.8731)	Prec@1 17.969 (18.129)	
Epoch: [4][155/391]	LR: 0.001	DT: 0.000 (7.111)	BT: 0.174 (7.295)	Loss 3.8477 (3.8684)	Prec@1 17.188 (18.540)	
Epoch: [4][233/391]	LR: 0.001	DT: 16.181 (7.146)	BT: 16.368 (7.330)	Loss 3.8965 (3.8700)	Prec@1 14.844 (18.523)	
Epoch: [4][311/391]	LR: 0.001	DT: 0.000 (7.006)	BT: 0.170 (7.189)	Loss 4.0078 (3.8682)	Prec@1 11.719 (18.605)	
Epoch: [4][389/391]	LR: 0.001	DT: 0.000 (7.037)	BT: 0.172 (7.220)	Loss 3.7207 (3.8654)	Prec@1 21.875 (18.702)	
Total train loss: 3.8651
Avg Loading time: 7.0186 seconds
Avg Batch time: 7.2022 seconds

Train time: 2816.2201964855194
 * Prec@1 18.950 Prec@5 44.190 Loss 3.8535
Avg Loading time: 6.8324 seconds
Avg Batch time: 6.8953 seconds

Best acc: 18.950
--------------------------------------------------------------------------------
Test time: 546.2353835105896

Epoch: [5][77/391]	LR: 0.001	DT: 0.000 (4.660)	BT: 0.172 (4.840)	Loss 3.8008 (3.8465)	Prec@1 24.219 (19.371)	
Epoch: [5][155/391]	LR: 0.001	DT: 0.000 (5.240)	BT: 0.170 (5.420)	Loss 3.7891 (3.8419)	Prec@1 17.969 (19.551)	
Epoch: [5][233/391]	LR: 0.001	DT: 16.008 (5.986)	BT: 16.204 (6.168)	Loss 3.8594 (3.8465)	Prec@1 23.438 (19.154)	
Epoch: [5][311/391]	LR: 0.001	DT: 0.000 (6.139)	BT: 0.172 (6.321)	Loss 3.9941 (3.8459)	Prec@1 17.188 (19.293)	
Epoch: [5][389/391]	LR: 0.001	DT: 0.000 (6.330)	BT: 0.174 (6.512)	Loss 3.8555 (3.8452)	Prec@1 19.531 (19.271)	
Total train loss: 3.8455
Avg Loading time: 6.3139 seconds
Avg Batch time: 6.4958 seconds

Train time: 2539.9373972415924
 * Prec@1 19.140 Prec@5 45.330 Loss 3.8359
Avg Loading time: 8.1671 seconds
Avg Batch time: 8.2365 seconds

Best acc: 19.140
--------------------------------------------------------------------------------
Test time: 652.7090151309967

Epoch: [6][77/391]	LR: 0.001	DT: 0.000 (6.925)	BT: 0.176 (7.108)	Loss 3.7695 (3.8337)	Prec@1 23.438 (19.571)	
Epoch: [6][155/391]	LR: 0.001	DT: 0.000 (6.268)	BT: 0.176 (6.449)	Loss 3.6777 (3.8249)	Prec@1 22.656 (19.862)	
Epoch: [6][233/391]	LR: 0.001	DT: 0.000 (6.006)	BT: 0.183 (6.187)	Loss 3.8066 (3.8239)	Prec@1 17.969 (20.015)	
Epoch: [6][311/391]	LR: 0.001	DT: 0.000 (5.832)	BT: 0.171 (6.013)	Loss 3.7363 (3.8186)	Prec@1 21.875 (19.867)	
Epoch: [6][389/391]	LR: 0.001	DT: 0.000 (6.154)	BT: 0.173 (6.335)	Loss 3.6953 (3.8119)	Prec@1 18.750 (19.962)	
Total train loss: 3.8117
Avg Loading time: 6.1383 seconds
Avg Batch time: 6.3190 seconds

Train time: 2470.820249557495
 * Prec@1 20.140 Prec@5 46.370 Loss 3.7891
Avg Loading time: 7.7907 seconds
Avg Batch time: 7.8479 seconds

Best acc: 20.140
--------------------------------------------------------------------------------
Test time: 621.0641057491302

Epoch: [7][77/391]	LR: 0.001	DT: 0.000 (7.673)	BT: 0.171 (7.857)	Loss 3.8281 (3.7920)	Prec@1 17.969 (20.753)	
Epoch: [7][155/391]	LR: 0.001	DT: 1.749 (7.718)	BT: 1.927 (7.903)	Loss 3.6934 (3.7860)	Prec@1 26.562 (20.628)	
Epoch: [7][233/391]	LR: 0.001	DT: 10.101 (7.700)	BT: 10.285 (7.885)	Loss 3.8320 (3.7876)	Prec@1 14.844 (20.272)	
Epoch: [7][311/391]	LR: 0.001	DT: 0.000 (7.176)	BT: 0.172 (7.360)	Loss 3.7871 (3.7873)	Prec@1 18.750 (20.277)	
Epoch: [7][389/391]	LR: 0.001	DT: 1.013 (6.655)	BT: 1.186 (6.838)	Loss 3.8320 (3.7875)	Prec@1 17.969 (20.298)	
Total train loss: 3.7877
Avg Loading time: 6.6381 seconds
Avg Batch time: 6.8210 seconds

Train time: 2667.109488964081
