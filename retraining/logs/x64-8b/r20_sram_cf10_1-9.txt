
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv2): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 91.700 Prec@5 99.660 Loss 0.3235
Pre-trained Prec@1 with 1 layers frozen: 91.69999694824219 	 Loss: 0.323486328125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0357 (0.0393)	Prec@1 99.219 (98.858)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.0381 (0.0490)	Prec@1 98.438 (98.443)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.0507 (0.0553)	Prec@1 98.828 (98.231)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.0587 (0.0603)	Prec@1 98.047 (98.042)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.0999 (0.0686)	Prec@1 96.094 (97.788)	
Total train loss: 0.0687

Train time: 782.1342453956604
 * Prec@1 71.540 Prec@5 96.840 Loss 0.9287
Best acc: 71.540
--------------------------------------------------------------------------------
Test time: 815.0199460983276

Epoch: [1][38/196]	LR: 0.1	Loss 0.0938 (0.1240)	Prec@1 96.484 (95.703)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.0483 (0.1124)	Prec@1 99.219 (96.154)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1884 (0.1188)	Prec@1 92.578 (95.977)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.2244 (0.1210)	Prec@1 92.969 (95.843)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1354 (0.1227)	Prec@1 95.703 (95.727)	
Total train loss: 0.1226

Train time: 76.69274473190308
 * Prec@1 88.980 Prec@5 99.570 Loss 0.3667
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 81.50464630126953

Epoch: [2][38/196]	LR: 0.1	Loss 0.2144 (0.1209)	Prec@1 92.578 (95.623)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1282 (0.1325)	Prec@1 95.312 (95.237)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1185 (0.1410)	Prec@1 95.312 (95.009)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1493 (0.1432)	Prec@1 95.312 (94.979)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.0950 (0.1430)	Prec@1 95.703 (95.002)	
Total train loss: 0.1431

Train time: 23.16402769088745
 * Prec@1 42.820 Prec@5 87.540 Loss 1.7344
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 27.699970245361328

Epoch: [3][38/196]	LR: 0.1	Loss 0.1549 (0.1558)	Prec@1 96.875 (94.651)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.1440 (0.1535)	Prec@1 93.750 (94.606)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1199 (0.1502)	Prec@1 95.312 (94.651)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1169 (0.1454)	Prec@1 97.266 (94.869)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1071 (0.1450)	Prec@1 96.875 (94.856)	
Total train loss: 0.1449

Train time: 23.26658320426941
 * Prec@1 88.020 Prec@5 99.310 Loss 0.4275
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 27.072550296783447

Epoch: [4][38/196]	LR: 0.1	Loss 0.1072 (0.1239)	Prec@1 96.484 (95.793)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.1677 (0.1339)	Prec@1 94.531 (95.398)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.1197 (0.1356)	Prec@1 96.094 (95.339)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1361 (0.1367)	Prec@1 94.141 (95.260)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.1600 (0.1357)	Prec@1 94.141 (95.260)	
Total train loss: 0.1358

Train time: 23.664432525634766
 * Prec@1 56.180 Prec@5 89.910 Loss 2.2168
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 29.533235549926758

Epoch: [5][38/196]	LR: 0.1	Loss 0.1289 (0.1153)	Prec@1 95.703 (96.104)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.1306 (0.1151)	Prec@1 96.484 (96.234)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.1823 (0.1246)	Prec@1 93.359 (95.837)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.1892 (0.1378)	Prec@1 94.141 (95.323)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.1852 (0.1494)	Prec@1 94.922 (94.848)	
Total train loss: 0.1494

Train time: 23.408215045928955
 * Prec@1 22.020 Prec@5 73.860 Loss inf
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 26.965056896209717

Epoch: [6][38/196]	LR: 0.1	Loss 0.1276 (0.1433)	Prec@1 96.094 (95.022)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.1350 (0.1372)	Prec@1 96.875 (95.318)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.1342 (0.1378)	Prec@1 95.312 (95.212)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.2318 (0.1457)	Prec@1 91.797 (94.924)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.1246 (0.1488)	Prec@1 95.312 (94.770)	
Total train loss: 0.1488

Train time: 22.64083194732666
 * Prec@1 73.580 Prec@5 96.540 Loss 0.8833
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 26.91350293159485

Epoch: [7][38/196]	LR: 0.1	Loss 0.1755 (0.1267)	Prec@1 94.531 (95.523)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.1942 (0.1544)	Prec@1 94.531 (94.641)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2109 (0.1677)	Prec@1 92.188 (94.231)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.1564 (0.1759)	Prec@1 94.141 (93.958)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.2310 (0.1784)	Prec@1 92.188 (93.862)	
Total train loss: 0.1785

Train time: 22.768609523773193
 * Prec@1 72.700 Prec@5 97.500 Loss 0.8208
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 26.686392545700073

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2288 (0.1804)	Prec@1 91.797 (93.600)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.1792 (0.1742)	Prec@1 92.188 (93.900)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1927 (0.1675)	Prec@1 95.312 (94.197)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.1954 (0.1650)	Prec@1 93.359 (94.308)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1610 (0.1646)	Prec@1 94.141 (94.347)	
Total train loss: 0.1647

Train time: 22.848365545272827
 * Prec@1 88.760 Prec@5 99.600 Loss 0.3733
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 26.81519103050232

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1608 (0.1505)	Prec@1 93.750 (94.872)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1044 (0.1480)	Prec@1 97.266 (94.917)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1323 (0.1476)	Prec@1 95.703 (94.929)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1494 (0.1485)	Prec@1 94.531 (94.947)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.1658 (0.1480)	Prec@1 93.359 (94.992)	
Total train loss: 0.1480

Train time: 22.173665285110474
 * Prec@1 88.770 Prec@5 99.550 Loss 0.3674
Best acc: 88.980
--------------------------------------------------------------------------------
Test time: 25.943012475967407

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1270 (0.1440)	Prec@1 94.922 (95.242)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1324 (0.1430)	Prec@1 97.656 (95.197)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1102 (0.1436)	Prec@1 97.266 (95.226)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1443 (0.1431)	Prec@1 94.922 (95.227)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.1741 (0.1440)	Prec@1 95.703 (95.196)	
Total train loss: 0.1443

Train time: 23.181267023086548
 * Prec@1 89.010 Prec@5 99.570 Loss 0.3655
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.78305196762085

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1451 (0.1467)	Prec@1 94.141 (95.212)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1748 (0.1436)	Prec@1 94.141 (95.232)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1415 (0.1434)	Prec@1 94.922 (95.206)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1293 (0.1416)	Prec@1 94.531 (95.260)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1115 (0.1413)	Prec@1 94.922 (95.264)	
Total train loss: 0.1413

Train time: 23.739794969558716
 * Prec@1 88.670 Prec@5 99.530 Loss 0.3701
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.593968868255615

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1360 (0.1404)	Prec@1 95.312 (95.132)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0998 (0.1365)	Prec@1 98.047 (95.338)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1193 (0.1386)	Prec@1 96.875 (95.236)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1346 (0.1392)	Prec@1 95.312 (95.255)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1835 (0.1395)	Prec@1 91.797 (95.248)	
Total train loss: 0.1399

Train time: 23.71317768096924
 * Prec@1 88.770 Prec@5 99.590 Loss 0.3679
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.870506286621094

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1320 (0.1344)	Prec@1 96.484 (95.583)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1730 (0.1398)	Prec@1 92.578 (95.252)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1388 (0.1394)	Prec@1 96.875 (95.333)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1236 (0.1380)	Prec@1 96.875 (95.368)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1082 (0.1381)	Prec@1 97.656 (95.377)	
Total train loss: 0.1382

Train time: 22.56051540374756
 * Prec@1 88.740 Prec@5 99.510 Loss 0.3665
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 26.592079877853394

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1290 (0.1380)	Prec@1 94.531 (95.403)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1259 (0.1376)	Prec@1 94.922 (95.338)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1700 (0.1390)	Prec@1 93.750 (95.383)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1477 (0.1385)	Prec@1 93.750 (95.440)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1611 (0.1396)	Prec@1 94.531 (95.411)	
Total train loss: 0.1396

Train time: 22.738725423812866
 * Prec@1 88.960 Prec@5 99.540 Loss 0.3643
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 26.7762234210968

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1471 (0.1367)	Prec@1 95.703 (95.583)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1284 (0.1398)	Prec@1 95.703 (95.307)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1415 (0.1412)	Prec@1 94.922 (95.216)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1263 (0.1407)	Prec@1 96.484 (95.235)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1699 (0.1403)	Prec@1 93.359 (95.268)	
Total train loss: 0.1403

Train time: 23.17218542098999
 * Prec@1 88.910 Prec@5 99.540 Loss 0.3623
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 26.97960376739502

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1622 (0.1374)	Prec@1 94.531 (95.473)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1777 (0.1395)	Prec@1 92.969 (95.338)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0928 (0.1396)	Prec@1 97.266 (95.296)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1249 (0.1372)	Prec@1 96.094 (95.383)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1516 (0.1391)	Prec@1 93.750 (95.365)	
Total train loss: 0.1392

Train time: 23.671980142593384
 * Prec@1 88.860 Prec@5 99.550 Loss 0.3652
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.320241689682007

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1219 (0.1382)	Prec@1 94.922 (95.333)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1401 (0.1408)	Prec@1 94.922 (95.182)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1310 (0.1395)	Prec@1 95.312 (95.292)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1606 (0.1391)	Prec@1 96.094 (95.305)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1322 (0.1388)	Prec@1 96.094 (95.357)	
Total train loss: 0.1389

Train time: 23.098742723464966
 * Prec@1 88.890 Prec@5 99.550 Loss 0.3650
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 26.596211194992065

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1655 (0.1431)	Prec@1 93.359 (95.062)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1333 (0.1442)	Prec@1 96.875 (95.072)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1376 (0.1401)	Prec@1 94.922 (95.189)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1082 (0.1381)	Prec@1 95.312 (95.275)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1361 (0.1390)	Prec@1 96.094 (95.262)	
Total train loss: 0.1392

Train time: 23.35357165336609
 * Prec@1 88.720 Prec@5 99.530 Loss 0.3638
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.37311053276062

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1498 (0.1315)	Prec@1 94.922 (95.643)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1450 (0.1354)	Prec@1 95.312 (95.613)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1510 (0.1384)	Prec@1 93.750 (95.386)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2007 (0.1390)	Prec@1 92.188 (95.335)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1580 (0.1388)	Prec@1 94.531 (95.347)	
Total train loss: 0.1389

Train time: 23.3430495262146
 * Prec@1 88.740 Prec@5 99.520 Loss 0.3645
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.54596495628357

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1069 (0.1385)	Prec@1 96.484 (95.343)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1038 (0.1386)	Prec@1 97.266 (95.358)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2072 (0.1400)	Prec@1 91.797 (95.309)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1860 (0.1400)	Prec@1 94.141 (95.343)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1610 (0.1395)	Prec@1 94.922 (95.319)	
Total train loss: 0.1396

Train time: 23.486204862594604
 * Prec@1 88.680 Prec@5 99.530 Loss 0.3645
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.573058128356934

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1205 (0.1335)	Prec@1 94.922 (95.643)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1670 (0.1363)	Prec@1 92.969 (95.413)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1287 (0.1368)	Prec@1 95.703 (95.379)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1420 (0.1383)	Prec@1 96.094 (95.340)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1371 (0.1375)	Prec@1 94.531 (95.359)	
Total train loss: 0.1376

Train time: 23.245814323425293
 * Prec@1 88.750 Prec@5 99.550 Loss 0.3633
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.170408964157104

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1226 (0.1406)	Prec@1 96.484 (95.192)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1582 (0.1405)	Prec@1 94.141 (95.297)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1180 (0.1384)	Prec@1 96.484 (95.346)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1445 (0.1378)	Prec@1 96.094 (95.405)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1100 (0.1375)	Prec@1 96.875 (95.429)	
Total train loss: 0.1375

Train time: 23.06208848953247
 * Prec@1 88.630 Prec@5 99.570 Loss 0.3640
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.65605854988098

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1382 (0.1321)	Prec@1 94.922 (95.593)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1969 (0.1384)	Prec@1 94.531 (95.413)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1562 (0.1389)	Prec@1 94.922 (95.406)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0789 (0.1384)	Prec@1 98.828 (95.428)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1243 (0.1377)	Prec@1 96.484 (95.425)	
Total train loss: 0.1378

Train time: 23.981054544448853
 * Prec@1 88.650 Prec@5 99.550 Loss 0.3638
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.731220483779907

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1801 (0.1349)	Prec@1 92.578 (95.323)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1469 (0.1366)	Prec@1 95.312 (95.413)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1300 (0.1353)	Prec@1 95.703 (95.499)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1121 (0.1374)	Prec@1 96.875 (95.373)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1371 (0.1382)	Prec@1 94.922 (95.349)	
Total train loss: 0.1383

Train time: 23.702550888061523
 * Prec@1 88.620 Prec@5 99.530 Loss 0.3633
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.933478355407715

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1605 (0.1353)	Prec@1 93.750 (95.633)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1287 (0.1387)	Prec@1 96.094 (95.523)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1689 (0.1378)	Prec@1 95.312 (95.496)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1750 (0.1385)	Prec@1 92.969 (95.450)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1500 (0.1379)	Prec@1 94.531 (95.401)	
Total train loss: 0.1381

Train time: 23.767460823059082
 * Prec@1 88.800 Prec@5 99.550 Loss 0.3635
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.92712378501892

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1332 (0.1391)	Prec@1 94.922 (95.292)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0994 (0.1386)	Prec@1 96.875 (95.398)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1276 (0.1386)	Prec@1 95.703 (95.379)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1001 (0.1366)	Prec@1 97.266 (95.463)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1214 (0.1377)	Prec@1 95.703 (95.407)	
Total train loss: 0.1377

Train time: 23.89460325241089
 * Prec@1 88.640 Prec@5 99.560 Loss 0.3645
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.012116193771362

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1299 (0.1415)	Prec@1 95.703 (95.192)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1231 (0.1374)	Prec@1 95.312 (95.433)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1855 (0.1383)	Prec@1 93.359 (95.426)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1547 (0.1382)	Prec@1 95.312 (95.393)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1118 (0.1386)	Prec@1 96.875 (95.399)	
Total train loss: 0.1386

Train time: 24.439220190048218
 * Prec@1 88.670 Prec@5 99.560 Loss 0.3655
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.15048384666443

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1589 (0.1368)	Prec@1 95.312 (95.453)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1290 (0.1391)	Prec@1 96.875 (95.297)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1074 (0.1391)	Prec@1 96.875 (95.329)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1292 (0.1390)	Prec@1 95.312 (95.343)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1862 (0.1386)	Prec@1 92.578 (95.361)	
Total train loss: 0.1388

Train time: 23.8514666557312
 * Prec@1 88.780 Prec@5 99.550 Loss 0.3616
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.42000937461853

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1198 (0.1338)	Prec@1 94.922 (95.583)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1692 (0.1364)	Prec@1 94.922 (95.468)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1422 (0.1386)	Prec@1 94.141 (95.366)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1423 (0.1383)	Prec@1 94.531 (95.373)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1263 (0.1375)	Prec@1 96.094 (95.431)	
Total train loss: 0.1374

Train time: 23.503438711166382
 * Prec@1 88.650 Prec@5 99.560 Loss 0.3633
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.302173376083374

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.1242 (0.1385)	Prec@1 95.312 (95.363)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.0986 (0.1404)	Prec@1 98.047 (95.373)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.1290 (0.1398)	Prec@1 96.484 (95.366)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.1632 (0.1409)	Prec@1 94.922 (95.285)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.0942 (0.1385)	Prec@1 97.266 (95.357)	
Total train loss: 0.1385

Train time: 23.639658451080322
 * Prec@1 88.720 Prec@5 99.550 Loss 0.3645
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.64138650894165

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.1409 (0.1398)	Prec@1 93.750 (95.202)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.1041 (0.1406)	Prec@1 97.656 (95.292)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1533 (0.1369)	Prec@1 95.312 (95.489)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.0920 (0.1375)	Prec@1 97.656 (95.523)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1390 (0.1379)	Prec@1 94.531 (95.493)	
Total train loss: 0.1378

Train time: 23.205219745635986
 * Prec@1 88.640 Prec@5 99.560 Loss 0.3630
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.421551942825317

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1229 (0.1333)	Prec@1 97.266 (95.653)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1233 (0.1396)	Prec@1 95.312 (95.373)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1515 (0.1387)	Prec@1 94.922 (95.409)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1061 (0.1379)	Prec@1 95.703 (95.460)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1048 (0.1375)	Prec@1 97.266 (95.477)	
Total train loss: 0.1374

Train time: 23.86134934425354
 * Prec@1 88.710 Prec@5 99.540 Loss 0.3630
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.08332657814026

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.1387 (0.1370)	Prec@1 96.875 (95.403)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.1257 (0.1376)	Prec@1 95.312 (95.282)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1376 (0.1361)	Prec@1 94.531 (95.463)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.1605 (0.1375)	Prec@1 94.922 (95.418)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1600 (0.1380)	Prec@1 93.359 (95.453)	
Total train loss: 0.1381

Train time: 23.560676097869873
 * Prec@1 88.860 Prec@5 99.560 Loss 0.3604
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.297647953033447

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.1177 (0.1338)	Prec@1 96.484 (95.583)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.1061 (0.1318)	Prec@1 96.094 (95.638)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.1304 (0.1339)	Prec@1 96.875 (95.596)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.1356 (0.1355)	Prec@1 96.094 (95.563)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1757 (0.1373)	Prec@1 92.969 (95.463)	
Total train loss: 0.1374

Train time: 23.4020094871521
 * Prec@1 88.660 Prec@5 99.550 Loss 0.3645
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.115825653076172

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.1595 (0.1362)	Prec@1 94.141 (95.443)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.0825 (0.1336)	Prec@1 98.047 (95.473)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.0946 (0.1372)	Prec@1 96.875 (95.366)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1527 (0.1385)	Prec@1 95.703 (95.350)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1825 (0.1386)	Prec@1 93.750 (95.367)	
Total train loss: 0.1386

Train time: 23.491918563842773
 * Prec@1 88.620 Prec@5 99.500 Loss 0.3647
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.226595163345337

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1351 (0.1395)	Prec@1 94.922 (95.333)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1187 (0.1352)	Prec@1 96.875 (95.478)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1036 (0.1353)	Prec@1 97.266 (95.530)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1195 (0.1365)	Prec@1 95.703 (95.455)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1678 (0.1379)	Prec@1 93.359 (95.367)	
Total train loss: 0.1378

Train time: 23.892619609832764
 * Prec@1 88.760 Prec@5 99.570 Loss 0.3604
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.022216320037842

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1498 (0.1413)	Prec@1 94.531 (95.172)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.0884 (0.1420)	Prec@1 98.047 (95.247)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1641 (0.1386)	Prec@1 93.750 (95.409)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1998 (0.1388)	Prec@1 94.141 (95.430)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.1403 (0.1373)	Prec@1 95.312 (95.437)	
Total train loss: 0.1372

Train time: 24.16137456893921
 * Prec@1 88.720 Prec@5 99.540 Loss 0.3618
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 28.467214345932007

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.1302 (0.1417)	Prec@1 94.922 (95.302)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1285 (0.1410)	Prec@1 94.141 (95.292)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.1108 (0.1411)	Prec@1 96.484 (95.292)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1111 (0.1406)	Prec@1 96.484 (95.267)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.1208 (0.1393)	Prec@1 95.703 (95.367)	
Total train loss: 0.1394

Train time: 22.871001482009888
 * Prec@1 88.860 Prec@5 99.560 Loss 0.3633
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 27.259942293167114

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.0981 (0.1317)	Prec@1 97.266 (95.683)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1473 (0.1346)	Prec@1 95.703 (95.603)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.1591 (0.1335)	Prec@1 94.141 (95.600)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1400 (0.1377)	Prec@1 96.094 (95.460)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1565 (0.1379)	Prec@1 93.359 (95.415)	
Total train loss: 0.1377

Train time: 24.081815242767334
 * Prec@1 88.750 Prec@5 99.550 Loss 0.3618
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 26.996163606643677


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 90.740 Prec@5 99.670 Loss 0.3484
Pre-trained Prec@1 with 3 layers frozen: 90.73999786376953 	 Loss: 0.348388671875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0520 (0.0554)	Prec@1 98.047 (98.267)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.0743 (0.0611)	Prec@1 96.875 (97.907)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.1510 (0.0715)	Prec@1 94.922 (97.599)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1259 (0.0789)	Prec@1 95.312 (97.323)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.0812 (0.0882)	Prec@1 96.094 (96.971)	
Total train loss: 0.0883

Train time: 271.12162470817566
 * Prec@1 80.570 Prec@5 98.620 Loss 0.6362
Best acc: 80.570
--------------------------------------------------------------------------------
Test time: 293.74135160446167

Epoch: [1][38/196]	LR: 0.1	Loss 0.1104 (0.1016)	Prec@1 95.312 (96.384)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1129 (0.1046)	Prec@1 96.094 (96.229)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1863 (0.1126)	Prec@1 92.969 (95.960)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1219 (0.1146)	Prec@1 96.094 (95.901)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1401 (0.1220)	Prec@1 94.922 (95.689)	
Total train loss: 0.1221

Train time: 40.61313486099243
 * Prec@1 81.690 Prec@5 98.010 Loss 0.8232
Best acc: 81.690
--------------------------------------------------------------------------------
Test time: 45.36200189590454

Epoch: [2][38/196]	LR: 0.1	Loss 0.1460 (0.1225)	Prec@1 95.312 (95.903)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1819 (0.1337)	Prec@1 93.750 (95.423)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1769 (0.1547)	Prec@1 92.969 (94.598)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1935 (0.1635)	Prec@1 92.578 (94.308)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.0866 (0.1631)	Prec@1 97.656 (94.303)	
Total train loss: 0.1632

Train time: 21.887918949127197
 * Prec@1 73.880 Prec@5 98.120 Loss 0.8223
Best acc: 81.690
--------------------------------------------------------------------------------
Test time: 26.664434671401978

Epoch: [3][38/196]	LR: 0.1	Loss 0.1375 (0.1483)	Prec@1 95.703 (94.922)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.1010 (0.1275)	Prec@1 97.656 (95.663)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1740 (0.1348)	Prec@1 93.359 (95.339)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1571 (0.1394)	Prec@1 94.531 (95.137)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1236 (0.1404)	Prec@1 96.484 (95.114)	
Total train loss: 0.1404

Train time: 22.557548999786377
 * Prec@1 87.150 Prec@5 99.260 Loss 0.4104
Best acc: 87.150
--------------------------------------------------------------------------------
Test time: 26.9019455909729

Epoch: [4][38/196]	LR: 0.1	Loss 0.1564 (0.1377)	Prec@1 93.359 (95.242)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.1245 (0.1329)	Prec@1 95.312 (95.323)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.1105 (0.1377)	Prec@1 95.703 (95.035)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1348 (0.1522)	Prec@1 94.531 (94.606)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.2340 (0.1616)	Prec@1 91.016 (94.289)	
Total train loss: 0.1616

Train time: 22.950605154037476
 * Prec@1 79.180 Prec@5 99.020 Loss 0.6997
Best acc: 87.150
--------------------------------------------------------------------------------
Test time: 28.190417289733887

Epoch: [5][38/196]	LR: 0.1	Loss 0.1860 (0.1608)	Prec@1 92.969 (94.311)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.2080 (0.1691)	Prec@1 92.578 (94.086)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.1630 (0.1661)	Prec@1 92.969 (94.204)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.1942 (0.1745)	Prec@1 95.312 (93.890)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.2661 (0.1789)	Prec@1 91.016 (93.758)	
Total train loss: 0.1789

Train time: 21.36175847053528
 * Prec@1 66.200 Prec@5 94.480 Loss 1.3115
Best acc: 87.150
--------------------------------------------------------------------------------
Test time: 26.095149278640747

Epoch: [6][38/196]	LR: 0.1	Loss 0.2074 (0.1893)	Prec@1 92.969 (93.520)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.1841 (0.2087)	Prec@1 93.750 (92.904)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.2351 (0.2217)	Prec@1 91.406 (92.391)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.1763 (0.2206)	Prec@1 94.141 (92.373)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.1636 (0.2188)	Prec@1 94.531 (92.388)	
Total train loss: 0.2191

Train time: 21.54226303100586
 * Prec@1 83.590 Prec@5 98.910 Loss 0.5879
Best acc: 87.150
--------------------------------------------------------------------------------
Test time: 26.23488736152649

Epoch: [7][38/196]	LR: 0.1	Loss 0.2463 (0.1930)	Prec@1 91.406 (92.889)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.2520 (0.1990)	Prec@1 92.578 (92.989)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2012 (0.1982)	Prec@1 92.578 (93.042)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.1276 (0.1959)	Prec@1 95.312 (93.147)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3345 (0.2050)	Prec@1 87.500 (92.837)	
Total train loss: 0.2054

Train time: 22.051247358322144
 * Prec@1 64.390 Prec@5 96.530 Loss 1.4805
Best acc: 87.150
--------------------------------------------------------------------------------
Test time: 26.135685682296753

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2131 (0.2350)	Prec@1 91.797 (92.027)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2278 (0.2139)	Prec@1 89.844 (92.608)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1813 (0.2050)	Prec@1 94.531 (92.852)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2496 (0.2006)	Prec@1 90.625 (92.969)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1775 (0.1986)	Prec@1 93.359 (93.055)	
Total train loss: 0.1985

Train time: 22.004277229309082
 * Prec@1 87.860 Prec@5 99.440 Loss 0.3855
Best acc: 87.860
--------------------------------------------------------------------------------
Test time: 27.32121992111206

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1077 (0.1717)	Prec@1 96.484 (94.201)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1423 (0.1687)	Prec@1 95.312 (94.156)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1669 (0.1698)	Prec@1 93.750 (93.994)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1248 (0.1695)	Prec@1 96.094 (94.058)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.1753 (0.1703)	Prec@1 92.969 (94.008)	
Total train loss: 0.1704

Train time: 21.567241668701172
 * Prec@1 88.130 Prec@5 99.510 Loss 0.3860
Best acc: 88.130
--------------------------------------------------------------------------------
Test time: 25.5941321849823

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1754 (0.1576)	Prec@1 94.141 (94.471)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1129 (0.1600)	Prec@1 96.484 (94.446)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1548 (0.1604)	Prec@1 95.312 (94.488)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1232 (0.1588)	Prec@1 96.094 (94.501)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.1605 (0.1586)	Prec@1 93.359 (94.479)	
Total train loss: 0.1587

Train time: 22.658467292785645
 * Prec@1 87.640 Prec@5 99.510 Loss 0.4087
Best acc: 88.130
--------------------------------------------------------------------------------
Test time: 27.572205305099487

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2075 (0.1543)	Prec@1 92.188 (94.661)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1698 (0.1564)	Prec@1 96.094 (94.601)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1432 (0.1592)	Prec@1 95.312 (94.481)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1970 (0.1588)	Prec@1 92.578 (94.514)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0970 (0.1583)	Prec@1 96.484 (94.509)	
Total train loss: 0.1584

Train time: 22.463644981384277
 * Prec@1 88.360 Prec@5 99.490 Loss 0.3821
Best acc: 88.360
--------------------------------------------------------------------------------
Test time: 27.39023232460022

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1305 (0.1502)	Prec@1 94.922 (94.742)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1821 (0.1538)	Prec@1 94.141 (94.681)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1962 (0.1524)	Prec@1 92.578 (94.722)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1512 (0.1514)	Prec@1 94.531 (94.757)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1460 (0.1511)	Prec@1 95.312 (94.794)	
Total train loss: 0.1512

Train time: 22.977330684661865
 * Prec@1 88.310 Prec@5 99.570 Loss 0.3899
Best acc: 88.360
--------------------------------------------------------------------------------
Test time: 27.885372638702393

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1235 (0.1454)	Prec@1 95.703 (94.822)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1246 (0.1463)	Prec@1 95.312 (94.902)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1486 (0.1470)	Prec@1 94.531 (94.872)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1963 (0.1458)	Prec@1 92.188 (94.877)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2041 (0.1472)	Prec@1 92.188 (94.812)	
Total train loss: 0.1473

Train time: 22.448951482772827
 * Prec@1 88.450 Prec@5 99.530 Loss 0.3772
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 26.973491668701172

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1186 (0.1458)	Prec@1 95.312 (94.922)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1748 (0.1446)	Prec@1 92.578 (95.042)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1450 (0.1458)	Prec@1 94.141 (94.982)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1399 (0.1461)	Prec@1 94.922 (94.987)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1565 (0.1456)	Prec@1 94.922 (95.004)	
Total train loss: 0.1457

Train time: 22.182753324508667
 * Prec@1 87.930 Prec@5 99.530 Loss 0.4011
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 27.62985134124756

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1278 (0.1416)	Prec@1 93.750 (94.902)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1411 (0.1450)	Prec@1 96.484 (94.877)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2123 (0.1453)	Prec@1 91.797 (94.875)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2155 (0.1470)	Prec@1 91.406 (94.807)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0911 (0.1469)	Prec@1 98.047 (94.884)	
Total train loss: 0.1469

Train time: 21.52631664276123
 * Prec@1 88.170 Prec@5 99.500 Loss 0.3860
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 25.726330041885376

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1564 (0.1432)	Prec@1 95.312 (94.952)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1930 (0.1447)	Prec@1 93.359 (94.917)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1409 (0.1433)	Prec@1 94.141 (94.919)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2101 (0.1450)	Prec@1 92.188 (94.897)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1423 (0.1435)	Prec@1 96.875 (95.008)	
Total train loss: 0.1436

Train time: 21.914214611053467
 * Prec@1 88.270 Prec@5 99.490 Loss 0.3872
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 26.988656997680664

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1149 (0.1465)	Prec@1 96.484 (95.032)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1144 (0.1467)	Prec@1 96.094 (94.857)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1561 (0.1430)	Prec@1 94.531 (95.119)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1281 (0.1442)	Prec@1 94.141 (95.007)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1956 (0.1438)	Prec@1 92.578 (95.052)	
Total train loss: 0.1438

Train time: 22.57662010192871
 * Prec@1 88.350 Prec@5 99.500 Loss 0.3833
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 27.36334991455078

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1925 (0.1426)	Prec@1 94.531 (95.353)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1488 (0.1439)	Prec@1 96.484 (95.222)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1517 (0.1445)	Prec@1 96.484 (95.092)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1158 (0.1449)	Prec@1 95.312 (95.065)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1403 (0.1435)	Prec@1 95.312 (95.070)	
Total train loss: 0.1435

Train time: 22.403239011764526
 * Prec@1 88.370 Prec@5 99.510 Loss 0.3838
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 27.201332330703735

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1135 (0.1384)	Prec@1 96.875 (95.222)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1306 (0.1401)	Prec@1 95.312 (95.122)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1882 (0.1445)	Prec@1 93.359 (95.012)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1176 (0.1435)	Prec@1 97.266 (95.082)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0927 (0.1424)	Prec@1 97.266 (95.130)	
Total train loss: 0.1424

Train time: 21.926498651504517
 * Prec@1 88.260 Prec@5 99.540 Loss 0.3865
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 26.19508457183838

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1469 (0.1488)	Prec@1 94.922 (95.162)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1061 (0.1447)	Prec@1 96.875 (95.177)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1747 (0.1430)	Prec@1 92.969 (95.159)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2288 (0.1428)	Prec@1 92.188 (95.167)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1125 (0.1419)	Prec@1 97.266 (95.200)	
Total train loss: 0.1421

Train time: 22.140734672546387
 * Prec@1 88.240 Prec@5 99.530 Loss 0.3855
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 27.69057559967041

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1221 (0.1420)	Prec@1 95.703 (95.122)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1406 (0.1397)	Prec@1 95.312 (95.227)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1512 (0.1412)	Prec@1 95.312 (95.176)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1392 (0.1437)	Prec@1 95.703 (95.082)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1167 (0.1434)	Prec@1 96.875 (95.058)	
Total train loss: 0.1435

Train time: 21.4874529838562
 * Prec@1 88.290 Prec@5 99.510 Loss 0.3853
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 25.430943489074707

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1619 (0.1399)	Prec@1 94.141 (95.022)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1890 (0.1467)	Prec@1 93.359 (94.767)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1234 (0.1445)	Prec@1 95.703 (94.952)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1445 (0.1440)	Prec@1 94.531 (95.017)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1881 (0.1431)	Prec@1 93.750 (95.078)	
Total train loss: 0.1432

Train time: 22.083855628967285
 * Prec@1 88.360 Prec@5 99.520 Loss 0.3850
Best acc: 88.450
--------------------------------------------------------------------------------
Test time: 27.08405590057373

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1371 (0.1397)	Prec@1 96.094 (95.473)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1125 (0.1394)	Prec@1 96.094 (95.413)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1126 (0.1423)	Prec@1 97.266 (95.206)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1935 (0.1424)	Prec@1 92.969 (95.207)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1373 (0.1419)	Prec@1 95.312 (95.184)	
Total train loss: 0.1421

Train time: 22.433436393737793
 * Prec@1 88.470 Prec@5 99.480 Loss 0.3804
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.152199029922485

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1669 (0.1381)	Prec@1 94.141 (95.082)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1409 (0.1403)	Prec@1 95.703 (95.072)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1571 (0.1417)	Prec@1 93.359 (95.082)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1755 (0.1416)	Prec@1 93.359 (95.075)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1530 (0.1417)	Prec@1 96.094 (95.116)	
Total train loss: 0.1419

Train time: 21.83760690689087
 * Prec@1 88.170 Prec@5 99.490 Loss 0.3857
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 26.59248447418213

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1547 (0.1462)	Prec@1 95.703 (94.872)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1136 (0.1436)	Prec@1 96.094 (95.097)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1353 (0.1453)	Prec@1 96.875 (95.009)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1605 (0.1447)	Prec@1 93.750 (95.017)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1564 (0.1439)	Prec@1 95.312 (95.064)	
Total train loss: 0.1441

Train time: 21.46523928642273
 * Prec@1 88.230 Prec@5 99.500 Loss 0.3845
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 25.904670238494873

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1587 (0.1412)	Prec@1 94.141 (95.152)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1444 (0.1410)	Prec@1 95.312 (95.062)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1461 (0.1405)	Prec@1 94.531 (95.119)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1321 (0.1411)	Prec@1 96.484 (95.107)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1221 (0.1425)	Prec@1 96.484 (95.066)	
Total train loss: 0.1427

Train time: 22.645634174346924
 * Prec@1 88.280 Prec@5 99.510 Loss 0.3867
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.918350219726562

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1619 (0.1430)	Prec@1 94.531 (95.363)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1488 (0.1455)	Prec@1 94.531 (95.077)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1057 (0.1443)	Prec@1 96.875 (95.089)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1417 (0.1439)	Prec@1 95.312 (95.045)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1877 (0.1432)	Prec@1 94.531 (95.092)	
Total train loss: 0.1434

Train time: 21.562803268432617
 * Prec@1 88.330 Prec@5 99.520 Loss 0.3828
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 25.804783582687378

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1324 (0.1493)	Prec@1 97.266 (94.692)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1526 (0.1439)	Prec@1 94.141 (95.037)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1039 (0.1425)	Prec@1 95.703 (95.079)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1260 (0.1426)	Prec@1 95.312 (95.097)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1338 (0.1420)	Prec@1 95.703 (95.118)	
Total train loss: 0.1422

Train time: 23.069466829299927
 * Prec@1 88.330 Prec@5 99.500 Loss 0.3838
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 28.040544033050537

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1656 (0.1382)	Prec@1 94.531 (95.323)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1472 (0.1424)	Prec@1 95.312 (95.147)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1639 (0.1426)	Prec@1 94.141 (95.112)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1565 (0.1432)	Prec@1 94.922 (95.120)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1296 (0.1442)	Prec@1 96.094 (95.064)	
Total train loss: 0.1441

Train time: 23.07567286491394
 * Prec@1 88.280 Prec@5 99.510 Loss 0.3855
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.524759769439697

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.1450 (0.1485)	Prec@1 94.922 (94.872)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.1315 (0.1472)	Prec@1 94.922 (94.847)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.0913 (0.1441)	Prec@1 96.484 (94.949)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.1689 (0.1440)	Prec@1 93.750 (95.032)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.1510 (0.1437)	Prec@1 94.141 (95.098)	
Total train loss: 0.1435

Train time: 22.274670839309692
 * Prec@1 88.300 Prec@5 99.530 Loss 0.3843
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.030205488204956

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.1493 (0.1454)	Prec@1 94.922 (94.972)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.1445 (0.1419)	Prec@1 95.312 (95.152)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1521 (0.1415)	Prec@1 96.094 (95.142)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.1678 (0.1427)	Prec@1 94.531 (95.085)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1312 (0.1429)	Prec@1 95.312 (95.074)	
Total train loss: 0.1428

Train time: 22.42238998413086
 * Prec@1 88.410 Prec@5 99.510 Loss 0.3833
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 26.67051124572754

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1466 (0.1403)	Prec@1 94.922 (94.932)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1378 (0.1425)	Prec@1 94.922 (94.987)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1542 (0.1449)	Prec@1 93.750 (94.922)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1185 (0.1446)	Prec@1 95.312 (94.944)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1242 (0.1434)	Prec@1 96.094 (95.008)	
Total train loss: 0.1434

Train time: 23.24695897102356
 * Prec@1 88.350 Prec@5 99.520 Loss 0.3850
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 28.6790292263031

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.1625 (0.1371)	Prec@1 96.484 (95.403)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.1123 (0.1410)	Prec@1 96.875 (95.192)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1715 (0.1415)	Prec@1 93.750 (95.119)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.1591 (0.1432)	Prec@1 94.922 (95.052)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1388 (0.1429)	Prec@1 96.484 (95.064)	
Total train loss: 0.1429

Train time: 21.88026523590088
 * Prec@1 88.330 Prec@5 99.520 Loss 0.3821
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 26.41670298576355

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.1392 (0.1480)	Prec@1 94.531 (94.692)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.0905 (0.1459)	Prec@1 98.047 (94.912)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.1299 (0.1446)	Prec@1 96.094 (94.972)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.0995 (0.1427)	Prec@1 96.484 (95.095)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1348 (0.1430)	Prec@1 97.266 (95.098)	
Total train loss: 0.1431

Train time: 22.911506414413452
 * Prec@1 88.220 Prec@5 99.500 Loss 0.3835
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.958525896072388

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.0951 (0.1452)	Prec@1 98.047 (94.882)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.1444 (0.1458)	Prec@1 95.312 (94.907)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.1295 (0.1458)	Prec@1 96.094 (94.935)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1565 (0.1445)	Prec@1 93.750 (95.022)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1504 (0.1433)	Prec@1 94.531 (95.042)	
Total train loss: 0.1433

Train time: 22.66129970550537
 * Prec@1 88.340 Prec@5 99.520 Loss 0.3821
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.1902072429657

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1561 (0.1476)	Prec@1 94.531 (94.872)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1288 (0.1447)	Prec@1 96.094 (95.112)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1470 (0.1442)	Prec@1 94.531 (95.106)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1453 (0.1440)	Prec@1 94.141 (95.120)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1761 (0.1436)	Prec@1 93.750 (95.088)	
Total train loss: 0.1437

Train time: 22.424282550811768
 * Prec@1 88.390 Prec@5 99.510 Loss 0.3809
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.446497440338135

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1285 (0.1372)	Prec@1 95.703 (95.333)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.1440 (0.1393)	Prec@1 94.531 (95.217)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1404 (0.1426)	Prec@1 95.703 (95.042)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1306 (0.1421)	Prec@1 94.922 (95.107)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.2039 (0.1415)	Prec@1 92.188 (95.132)	
Total train loss: 0.1418

Train time: 22.103760957717896
 * Prec@1 88.400 Prec@5 99.520 Loss 0.3835
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 26.413753747940063

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.1932 (0.1467)	Prec@1 92.969 (94.962)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1049 (0.1429)	Prec@1 97.656 (95.087)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.0999 (0.1433)	Prec@1 97.266 (95.002)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1504 (0.1446)	Prec@1 94.531 (94.942)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.1277 (0.1439)	Prec@1 95.312 (94.968)	
Total train loss: 0.1440

Train time: 22.7910053730011
 * Prec@1 88.400 Prec@5 99.510 Loss 0.3826
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 27.387410640716553

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.1659 (0.1487)	Prec@1 94.141 (94.902)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1072 (0.1458)	Prec@1 97.266 (94.997)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.1173 (0.1444)	Prec@1 96.094 (95.019)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1212 (0.1450)	Prec@1 96.484 (94.972)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1399 (0.1438)	Prec@1 95.312 (95.028)	
Total train loss: 0.1438

Train time: 22.11940050125122
 * Prec@1 88.280 Prec@5 99.520 Loss 0.3823
Best acc: 88.470
--------------------------------------------------------------------------------
Test time: 26.161954641342163


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 89.160 Prec@5 99.580 Loss 0.4265
Pre-trained Prec@1 with 5 layers frozen: 89.15999603271484 	 Loss: 0.426513671875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0573 (0.0738)	Prec@1 97.656 (97.456)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.0656 (0.0864)	Prec@1 97.656 (96.865)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.1254 (0.0932)	Prec@1 94.922 (96.695)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1047 (0.1019)	Prec@1 95.703 (96.382)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.0995 (0.1047)	Prec@1 96.484 (96.288)	
Total train loss: 0.1048

Train time: 242.14608478546143
 * Prec@1 75.510 Prec@5 98.230 Loss 0.8906
Best acc: 75.510
--------------------------------------------------------------------------------
Test time: 251.92840814590454

Epoch: [1][38/196]	LR: 0.1	Loss 0.0926 (0.0957)	Prec@1 96.875 (96.615)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.0871 (0.0958)	Prec@1 96.875 (96.565)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.0856 (0.1014)	Prec@1 97.266 (96.391)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1252 (0.1125)	Prec@1 94.141 (95.979)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1606 (0.1215)	Prec@1 93.750 (95.643)	
Total train loss: 0.1215

Train time: 34.47673511505127
 * Prec@1 66.180 Prec@5 96.830 Loss 1.5361
Best acc: 75.510
--------------------------------------------------------------------------------
Test time: 38.96468210220337

Epoch: [2][38/196]	LR: 0.1	Loss 0.1440 (0.1348)	Prec@1 94.141 (95.072)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1625 (0.1430)	Prec@1 96.094 (94.972)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1282 (0.1382)	Prec@1 96.484 (95.159)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1836 (0.1387)	Prec@1 92.578 (95.152)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1034 (0.1398)	Prec@1 95.312 (95.120)	
Total train loss: 0.1399

Train time: 21.96985650062561
 * Prec@1 64.910 Prec@5 93.280 Loss 1.5107
Best acc: 75.510
--------------------------------------------------------------------------------
Test time: 26.99147343635559

Epoch: [3][38/196]	LR: 0.1	Loss 0.0955 (0.1176)	Prec@1 96.094 (95.954)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.2212 (0.1382)	Prec@1 92.578 (95.207)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.2122 (0.1486)	Prec@1 92.969 (94.832)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.2073 (0.1570)	Prec@1 92.969 (94.501)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1779 (0.1647)	Prec@1 92.578 (94.183)	
Total train loss: 0.1649

Train time: 22.23550009727478
 * Prec@1 81.290 Prec@5 98.780 Loss 0.6323
Best acc: 81.290
--------------------------------------------------------------------------------
Test time: 26.012195825576782

Epoch: [4][38/196]	LR: 0.1	Loss 0.2148 (0.1665)	Prec@1 90.234 (94.020)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.1610 (0.1624)	Prec@1 92.969 (94.010)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.1203 (0.1588)	Prec@1 95.703 (94.221)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1759 (0.1518)	Prec@1 93.750 (94.484)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.1801 (0.1555)	Prec@1 92.578 (94.323)	
Total train loss: 0.1556

Train time: 22.53610324859619
 * Prec@1 44.930 Prec@5 89.980 Loss 4.9023
Best acc: 81.290
--------------------------------------------------------------------------------
Test time: 26.941113710403442

Epoch: [5][38/196]	LR: 0.1	Loss 0.1610 (0.1853)	Prec@1 94.531 (93.279)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.2312 (0.1715)	Prec@1 91.797 (93.745)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.1423 (0.1660)	Prec@1 94.531 (93.990)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.1631 (0.1614)	Prec@1 93.750 (94.143)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.1831 (0.1622)	Prec@1 94.141 (94.105)	
Total train loss: 0.1623

Train time: 21.468273401260376
 * Prec@1 78.460 Prec@5 98.200 Loss 1.0771
Best acc: 81.290
--------------------------------------------------------------------------------
Test time: 25.71886968612671

Epoch: [6][38/196]	LR: 0.1	Loss 0.1820 (0.1574)	Prec@1 92.969 (94.281)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.1422 (0.1600)	Prec@1 94.141 (94.251)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.2871 (0.1692)	Prec@1 87.891 (93.890)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.1638 (0.1737)	Prec@1 93.750 (93.740)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.1432 (0.1777)	Prec@1 95.312 (93.624)	
Total train loss: 0.1781

Train time: 21.75016212463379
 * Prec@1 76.130 Prec@5 96.360 Loss 0.7476
Best acc: 81.290
--------------------------------------------------------------------------------
Test time: 26.19168519973755

Epoch: [7][38/196]	LR: 0.1	Loss 0.1378 (0.1846)	Prec@1 95.312 (93.289)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.1820 (0.1792)	Prec@1 92.578 (93.575)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.1766 (0.1841)	Prec@1 94.141 (93.319)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.2048 (0.1915)	Prec@1 91.797 (93.094)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.2129 (0.1917)	Prec@1 91.406 (93.067)	
Total train loss: 0.1920

Train time: 21.295006036758423
 * Prec@1 67.550 Prec@5 94.830 Loss 1.1572
Best acc: 81.290
--------------------------------------------------------------------------------
Test time: 25.331873655319214

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.1964 (0.1728)	Prec@1 93.750 (93.980)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.1194 (0.1575)	Prec@1 96.094 (94.426)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1049 (0.1559)	Prec@1 96.094 (94.418)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.1208 (0.1523)	Prec@1 96.484 (94.639)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1978 (0.1497)	Prec@1 92.578 (94.702)	
Total train loss: 0.1497

Train time: 20.654034852981567
 * Prec@1 88.420 Prec@5 99.570 Loss 0.3911
Best acc: 88.420
--------------------------------------------------------------------------------
Test time: 25.436668634414673

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1615 (0.1317)	Prec@1 93.359 (95.583)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1063 (0.1306)	Prec@1 95.703 (95.473)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1233 (0.1319)	Prec@1 95.703 (95.423)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1317 (0.1317)	Prec@1 94.922 (95.390)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.1449 (0.1321)	Prec@1 94.141 (95.415)	
Total train loss: 0.1323

Train time: 21.50166893005371
 * Prec@1 88.690 Prec@5 99.570 Loss 0.3843
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.333679914474487

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1359 (0.1288)	Prec@1 95.312 (95.713)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1803 (0.1286)	Prec@1 93.359 (95.493)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1044 (0.1275)	Prec@1 96.484 (95.506)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1115 (0.1281)	Prec@1 96.875 (95.548)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.1403 (0.1299)	Prec@1 94.531 (95.465)	
Total train loss: 0.1298

Train time: 20.97298812866211
 * Prec@1 88.580 Prec@5 99.590 Loss 0.3855
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.2563533782959

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1183 (0.1276)	Prec@1 96.484 (95.573)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1158 (0.1274)	Prec@1 96.094 (95.618)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1324 (0.1306)	Prec@1 95.312 (95.543)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1930 (0.1309)	Prec@1 93.750 (95.553)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0784 (0.1332)	Prec@1 98.047 (95.491)	
Total train loss: 0.1333

Train time: 21.22305154800415
 * Prec@1 88.490 Prec@5 99.590 Loss 0.3933
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.495731115341187

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1956 (0.1328)	Prec@1 94.531 (95.543)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0969 (0.1320)	Prec@1 96.484 (95.548)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1410 (0.1325)	Prec@1 95.312 (95.509)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1154 (0.1325)	Prec@1 96.484 (95.518)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1306 (0.1342)	Prec@1 96.094 (95.397)	
Total train loss: 0.1342

Train time: 22.623353958129883
 * Prec@1 88.280 Prec@5 99.620 Loss 0.3889
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 27.242180347442627

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1083 (0.1293)	Prec@1 96.094 (95.503)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1549 (0.1346)	Prec@1 94.531 (95.237)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1431 (0.1326)	Prec@1 94.531 (95.343)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1088 (0.1336)	Prec@1 95.703 (95.302)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1047 (0.1336)	Prec@1 97.656 (95.339)	
Total train loss: 0.1336

Train time: 22.089943408966064
 * Prec@1 88.610 Prec@5 99.570 Loss 0.3918
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.164665699005127

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0803 (0.1358)	Prec@1 98.047 (95.513)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1110 (0.1339)	Prec@1 96.484 (95.438)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1265 (0.1341)	Prec@1 95.703 (95.383)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1417 (0.1364)	Prec@1 95.703 (95.292)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1414 (0.1374)	Prec@1 95.703 (95.226)	
Total train loss: 0.1374

Train time: 22.080108404159546
 * Prec@1 88.440 Prec@5 99.590 Loss 0.3911
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 27.378896236419678

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1300 (0.1427)	Prec@1 94.141 (95.022)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1659 (0.1381)	Prec@1 92.969 (95.157)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1421 (0.1410)	Prec@1 94.141 (94.979)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1598 (0.1417)	Prec@1 94.531 (94.937)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1470 (0.1415)	Prec@1 93.359 (94.996)	
Total train loss: 0.1416

Train time: 22.441191911697388
 * Prec@1 88.350 Prec@5 99.540 Loss 0.3965
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.25821542739868

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0992 (0.1394)	Prec@1 96.875 (95.443)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1251 (0.1363)	Prec@1 95.703 (95.503)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1583 (0.1376)	Prec@1 93.359 (95.356)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1462 (0.1391)	Prec@1 94.922 (95.295)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1346 (0.1382)	Prec@1 95.312 (95.329)	
Total train loss: 0.1383

Train time: 21.035271644592285
 * Prec@1 88.410 Prec@5 99.540 Loss 0.3945
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.597135543823242

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1833 (0.1343)	Prec@1 92.969 (95.643)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1481 (0.1349)	Prec@1 93.750 (95.573)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1466 (0.1385)	Prec@1 94.141 (95.336)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1772 (0.1390)	Prec@1 94.141 (95.307)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1026 (0.1385)	Prec@1 96.484 (95.282)	
Total train loss: 0.1388

Train time: 21.21378207206726
 * Prec@1 88.370 Prec@5 99.560 Loss 0.3960
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.929867029190063

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1392 (0.1364)	Prec@1 95.703 (95.282)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1450 (0.1360)	Prec@1 95.312 (95.388)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1257 (0.1350)	Prec@1 95.703 (95.433)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1400 (0.1364)	Prec@1 95.312 (95.360)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1542 (0.1394)	Prec@1 95.703 (95.204)	
Total train loss: 0.1395

Train time: 21.451685667037964
 * Prec@1 88.250 Prec@5 99.490 Loss 0.3967
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.8429012298584

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0944 (0.1438)	Prec@1 97.266 (94.912)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1231 (0.1391)	Prec@1 95.312 (95.147)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0773 (0.1364)	Prec@1 97.656 (95.319)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1597 (0.1385)	Prec@1 91.797 (95.222)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1385 (0.1387)	Prec@1 94.141 (95.250)	
Total train loss: 0.1387

Train time: 20.973232746124268
 * Prec@1 88.350 Prec@5 99.540 Loss 0.3970
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.017550706863403

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1061 (0.1424)	Prec@1 96.094 (95.052)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1528 (0.1431)	Prec@1 94.141 (95.052)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1288 (0.1413)	Prec@1 97.266 (95.129)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1646 (0.1391)	Prec@1 93.359 (95.217)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1055 (0.1392)	Prec@1 94.922 (95.190)	
Total train loss: 0.1395

Train time: 20.687370538711548
 * Prec@1 88.390 Prec@5 99.520 Loss 0.3936
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.69721221923828

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1471 (0.1394)	Prec@1 94.531 (95.082)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1892 (0.1396)	Prec@1 93.359 (95.187)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1158 (0.1409)	Prec@1 95.703 (95.129)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1162 (0.1386)	Prec@1 96.484 (95.160)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1249 (0.1375)	Prec@1 95.703 (95.194)	
Total train loss: 0.1375

Train time: 21.003884077072144
 * Prec@1 88.440 Prec@5 99.560 Loss 0.3909
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.3875675201416

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0983 (0.1369)	Prec@1 97.656 (95.373)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1100 (0.1385)	Prec@1 96.875 (95.182)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1970 (0.1402)	Prec@1 92.188 (95.136)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1559 (0.1393)	Prec@1 94.531 (95.182)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1298 (0.1389)	Prec@1 96.094 (95.210)	
Total train loss: 0.1391

Train time: 21.52779722213745
 * Prec@1 88.360 Prec@5 99.580 Loss 0.3943
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.157312393188477

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1565 (0.1430)	Prec@1 92.969 (95.012)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1510 (0.1404)	Prec@1 93.750 (95.097)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1409 (0.1374)	Prec@1 93.750 (95.186)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1146 (0.1379)	Prec@1 96.094 (95.180)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1630 (0.1388)	Prec@1 94.531 (95.200)	
Total train loss: 0.1388

Train time: 21.619765281677246
 * Prec@1 88.370 Prec@5 99.510 Loss 0.3928
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.40742325782776

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1362 (0.1392)	Prec@1 94.922 (94.872)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1202 (0.1408)	Prec@1 95.312 (94.942)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1409 (0.1397)	Prec@1 95.703 (95.085)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1549 (0.1393)	Prec@1 94.531 (95.125)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1455 (0.1396)	Prec@1 94.531 (95.120)	
Total train loss: 0.1398

Train time: 21.46663188934326
 * Prec@1 88.440 Prec@5 99.560 Loss 0.3931
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.05720853805542

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1534 (0.1326)	Prec@1 94.531 (95.473)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1720 (0.1333)	Prec@1 92.188 (95.398)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1300 (0.1377)	Prec@1 96.875 (95.262)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1713 (0.1387)	Prec@1 93.359 (95.190)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1389 (0.1388)	Prec@1 94.531 (95.214)	
Total train loss: 0.1390

Train time: 21.222362756729126
 * Prec@1 88.480 Prec@5 99.570 Loss 0.3918
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.19153356552124

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1230 (0.1321)	Prec@1 94.922 (95.453)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1781 (0.1352)	Prec@1 91.797 (95.338)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1089 (0.1360)	Prec@1 95.312 (95.316)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1511 (0.1372)	Prec@1 94.531 (95.260)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1422 (0.1380)	Prec@1 93.750 (95.226)	
Total train loss: 0.1379

Train time: 21.348756313323975
 * Prec@1 88.350 Prec@5 99.560 Loss 0.3958
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.400838136672974

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1311 (0.1443)	Prec@1 94.922 (94.892)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1696 (0.1407)	Prec@1 93.359 (95.157)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1725 (0.1384)	Prec@1 92.578 (95.179)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1086 (0.1386)	Prec@1 96.094 (95.227)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1422 (0.1396)	Prec@1 95.703 (95.224)	
Total train loss: 0.1397

Train time: 22.23755669593811
 * Prec@1 88.390 Prec@5 99.570 Loss 0.3948
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.382383823394775

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1404 (0.1349)	Prec@1 96.094 (95.132)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1421 (0.1379)	Prec@1 94.922 (95.087)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1376 (0.1388)	Prec@1 94.141 (95.126)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1862 (0.1366)	Prec@1 94.922 (95.270)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0979 (0.1376)	Prec@1 96.875 (95.276)	
Total train loss: 0.1378

Train time: 22.47635054588318
 * Prec@1 88.340 Prec@5 99.560 Loss 0.3962
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 27.025415420532227

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0952 (0.1387)	Prec@1 96.875 (95.373)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1550 (0.1429)	Prec@1 94.141 (95.057)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1176 (0.1397)	Prec@1 95.703 (95.176)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1705 (0.1388)	Prec@1 94.141 (95.182)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1255 (0.1388)	Prec@1 95.703 (95.232)	
Total train loss: 0.1388

Train time: 21.42635703086853
 * Prec@1 88.320 Prec@5 99.540 Loss 0.3977
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 25.775451183319092

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.1644 (0.1436)	Prec@1 93.359 (94.832)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.1034 (0.1399)	Prec@1 96.094 (95.152)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.1353 (0.1394)	Prec@1 95.312 (95.186)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.1246 (0.1390)	Prec@1 94.141 (95.152)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.1482 (0.1387)	Prec@1 94.922 (95.166)	
Total train loss: 0.1386

Train time: 22.329355001449585
 * Prec@1 88.350 Prec@5 99.550 Loss 0.3938
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.981199264526367

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.1510 (0.1383)	Prec@1 95.312 (95.112)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.1382 (0.1369)	Prec@1 95.703 (95.187)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1282 (0.1394)	Prec@1 96.875 (95.192)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.1515 (0.1380)	Prec@1 94.141 (95.255)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1552 (0.1381)	Prec@1 94.922 (95.270)	
Total train loss: 0.1381

Train time: 22.42953610420227
 * Prec@1 88.450 Prec@5 99.530 Loss 0.3921
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.32001304626465

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1532 (0.1356)	Prec@1 93.359 (95.423)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1158 (0.1358)	Prec@1 96.094 (95.353)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1564 (0.1367)	Prec@1 94.531 (95.336)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1462 (0.1387)	Prec@1 95.703 (95.280)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1379 (0.1387)	Prec@1 95.312 (95.244)	
Total train loss: 0.1387

Train time: 21.566619634628296
 * Prec@1 88.330 Prec@5 99.530 Loss 0.3943
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.443317413330078

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.1487 (0.1425)	Prec@1 94.922 (95.072)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.1213 (0.1413)	Prec@1 95.703 (95.007)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1556 (0.1387)	Prec@1 94.141 (95.146)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.1403 (0.1377)	Prec@1 94.531 (95.232)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1575 (0.1386)	Prec@1 94.531 (95.180)	
Total train loss: 0.1386

Train time: 21.903218984603882
 * Prec@1 88.470 Prec@5 99.570 Loss 0.3955
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.106101751327515

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.1622 (0.1389)	Prec@1 94.141 (95.262)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.1591 (0.1353)	Prec@1 93.750 (95.433)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.1534 (0.1368)	Prec@1 93.359 (95.292)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.1375 (0.1392)	Prec@1 95.312 (95.180)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1755 (0.1391)	Prec@1 95.703 (95.194)	
Total train loss: 0.1393

Train time: 22.09285283088684
 * Prec@1 88.400 Prec@5 99.560 Loss 0.3933
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.686490535736084

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.1511 (0.1371)	Prec@1 94.531 (95.302)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.1371 (0.1403)	Prec@1 93.750 (95.182)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.1254 (0.1383)	Prec@1 96.875 (95.199)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.0792 (0.1378)	Prec@1 99.219 (95.255)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1584 (0.1388)	Prec@1 92.969 (95.224)	
Total train loss: 0.1389

Train time: 22.241865634918213
 * Prec@1 88.340 Prec@5 99.570 Loss 0.3928
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.97570013999939

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1862 (0.1440)	Prec@1 93.359 (95.082)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1195 (0.1373)	Prec@1 97.656 (95.232)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1879 (0.1393)	Prec@1 94.141 (95.159)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1051 (0.1396)	Prec@1 97.266 (95.200)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.0842 (0.1393)	Prec@1 96.484 (95.178)	
Total train loss: 0.1393

Train time: 21.60144305229187
 * Prec@1 88.250 Prec@5 99.600 Loss 0.3948
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.381314277648926

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1420 (0.1436)	Prec@1 96.094 (95.052)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.1276 (0.1446)	Prec@1 95.312 (94.907)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1674 (0.1427)	Prec@1 93.750 (95.019)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1538 (0.1410)	Prec@1 94.922 (95.077)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.1331 (0.1407)	Prec@1 94.922 (95.106)	
Total train loss: 0.1409

Train time: 20.88659930229187
 * Prec@1 88.320 Prec@5 99.590 Loss 0.3931
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 24.7871356010437

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.1877 (0.1374)	Prec@1 93.750 (95.032)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1426 (0.1382)	Prec@1 93.359 (94.997)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.1152 (0.1393)	Prec@1 96.875 (94.999)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1732 (0.1385)	Prec@1 94.141 (95.090)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.0989 (0.1382)	Prec@1 96.875 (95.124)	
Total train loss: 0.1384

Train time: 21.01430034637451
 * Prec@1 88.320 Prec@5 99.530 Loss 0.3948
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 26.078633069992065

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.1178 (0.1383)	Prec@1 97.266 (95.252)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1492 (0.1421)	Prec@1 95.312 (95.142)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.1523 (0.1411)	Prec@1 94.141 (95.132)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1390 (0.1392)	Prec@1 95.312 (95.172)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1571 (0.1393)	Prec@1 95.703 (95.174)	
Total train loss: 0.1393

Train time: 20.46412706375122
 * Prec@1 88.360 Prec@5 99.550 Loss 0.3953
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 22.93011784553528


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 81.380 Prec@5 98.950 Loss 0.8594
Pre-trained Prec@1 with 7 layers frozen: 81.37999725341797 	 Loss: 0.859375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0644 (0.1048)	Prec@1 98.047 (96.615)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.1233 (0.1170)	Prec@1 94.922 (96.079)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.1842 (0.1253)	Prec@1 93.750 (95.706)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1611 (0.1364)	Prec@1 94.531 (95.285)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.1464 (0.1418)	Prec@1 94.531 (95.026)	
Total train loss: 0.1417

Train time: 351.755882024765
 * Prec@1 79.310 Prec@5 98.750 Loss 0.9097
Best acc: 79.310
--------------------------------------------------------------------------------
Test time: 373.47844529151917

Epoch: [1][38/196]	LR: 0.1	Loss 0.1168 (0.1222)	Prec@1 96.094 (95.563)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1801 (0.1244)	Prec@1 92.969 (95.583)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.2062 (0.1368)	Prec@1 93.359 (95.156)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1798 (0.1385)	Prec@1 93.750 (95.115)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.2085 (0.1400)	Prec@1 92.188 (95.048)	
Total train loss: 0.1401

Train time: 56.46822237968445
 * Prec@1 66.530 Prec@5 95.050 Loss 1.1895
Best acc: 79.310
--------------------------------------------------------------------------------
Test time: 60.91270685195923

Epoch: [2][38/196]	LR: 0.1	Loss 0.2023 (0.1590)	Prec@1 94.141 (94.431)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1057 (0.1478)	Prec@1 94.922 (94.782)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1317 (0.1445)	Prec@1 94.922 (94.885)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1403 (0.1470)	Prec@1 96.484 (94.817)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1581 (0.1504)	Prec@1 93.750 (94.708)	
Total train loss: 0.1505

Train time: 23.192835569381714
 * Prec@1 85.810 Prec@5 99.260 Loss 0.4663
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 27.305891752243042

Epoch: [3][38/196]	LR: 0.1	Loss 0.1832 (0.2039)	Prec@1 92.969 (92.979)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.2340 (0.2072)	Prec@1 91.406 (92.798)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1342 (0.2020)	Prec@1 96.094 (92.932)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1792 (0.1886)	Prec@1 92.969 (93.377)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.2013 (0.1840)	Prec@1 94.141 (93.552)	
Total train loss: 0.1840

Train time: 22.64659857749939
 * Prec@1 82.820 Prec@5 98.740 Loss 0.5879
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 26.327004194259644

Epoch: [4][38/196]	LR: 0.1	Loss 0.1678 (0.1387)	Prec@1 92.578 (95.112)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.2084 (0.1496)	Prec@1 93.750 (94.692)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.2267 (0.1748)	Prec@1 91.016 (93.800)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1564 (0.1868)	Prec@1 95.703 (93.399)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.2451 (0.1894)	Prec@1 92.188 (93.349)	
Total train loss: 0.1895

Train time: 21.273934602737427
 * Prec@1 78.590 Prec@5 97.230 Loss 0.7144
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 26.18521738052368

Epoch: [5][38/196]	LR: 0.1	Loss 0.2502 (0.2093)	Prec@1 91.797 (92.798)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.2311 (0.2109)	Prec@1 92.969 (92.834)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2135 (0.2178)	Prec@1 92.578 (92.471)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.2952 (0.2308)	Prec@1 89.844 (92.027)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.2100 (0.2332)	Prec@1 92.969 (91.971)	
Total train loss: 0.2331

Train time: 22.709086656570435
 * Prec@1 78.800 Prec@5 98.470 Loss 0.8081
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 26.603271484375

Epoch: [6][38/196]	LR: 0.1	Loss 0.1854 (0.1958)	Prec@1 93.359 (93.229)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2119 (0.2062)	Prec@1 91.406 (92.768)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.1943 (0.2031)	Prec@1 91.797 (92.885)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.2871 (0.2108)	Prec@1 87.500 (92.553)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2379 (0.2197)	Prec@1 91.406 (92.284)	
Total train loss: 0.2199

Train time: 22.56707191467285
 * Prec@1 76.180 Prec@5 98.000 Loss 0.8008
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 27.885608911514282

Epoch: [7][38/196]	LR: 0.1	Loss 0.2683 (0.2520)	Prec@1 89.062 (91.126)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.3291 (0.2612)	Prec@1 87.891 (90.740)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2617 (0.2673)	Prec@1 91.016 (90.545)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.2080 (0.2677)	Prec@1 93.359 (90.617)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3455 (0.2802)	Prec@1 86.719 (90.226)	
Total train loss: 0.2807

Train time: 25.196229696273804
 * Prec@1 40.960 Prec@5 82.810 Loss 1.9951
Best acc: 85.810
--------------------------------------------------------------------------------
Test time: 30.519384622573853

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2522 (0.2784)	Prec@1 90.625 (90.224)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2849 (0.2722)	Prec@1 89.453 (90.630)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2307 (0.2642)	Prec@1 91.016 (90.875)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2444 (0.2594)	Prec@1 92.188 (91.046)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2393 (0.2587)	Prec@1 91.406 (91.112)	
Total train loss: 0.2585

Train time: 25.420817852020264
 * Prec@1 86.530 Prec@5 99.480 Loss 0.4104
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 32.63671922683716

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2507 (0.2343)	Prec@1 89.062 (91.797)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3350 (0.2390)	Prec@1 90.234 (91.732)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2440 (0.2388)	Prec@1 91.797 (91.747)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2424 (0.2395)	Prec@1 91.406 (91.717)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2157 (0.2417)	Prec@1 92.188 (91.577)	
Total train loss: 0.2416

Train time: 27.075444221496582
 * Prec@1 86.270 Prec@5 99.380 Loss 0.4341
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 32.36858081817627

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2549 (0.2435)	Prec@1 91.016 (91.707)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2898 (0.2502)	Prec@1 89.453 (91.461)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2313 (0.2496)	Prec@1 91.406 (91.426)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1993 (0.2461)	Prec@1 92.969 (91.582)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2612 (0.2470)	Prec@1 91.797 (91.506)	
Total train loss: 0.2470

Train time: 26.887688398361206
 * Prec@1 86.430 Prec@5 99.440 Loss 0.4185
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 32.3611376285553

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2401 (0.2348)	Prec@1 93.359 (91.747)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.2333 (0.2367)	Prec@1 91.797 (91.722)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2437 (0.2448)	Prec@1 90.625 (91.473)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2278 (0.2443)	Prec@1 92.969 (91.496)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2351 (0.2453)	Prec@1 90.625 (91.490)	
Total train loss: 0.2454

Train time: 28.342471837997437
 * Prec@1 86.450 Prec@5 99.410 Loss 0.4231
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 33.45095133781433

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.3528 (0.2502)	Prec@1 88.672 (91.386)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2172 (0.2511)	Prec@1 93.359 (91.366)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2544 (0.2523)	Prec@1 91.016 (91.256)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3196 (0.2512)	Prec@1 89.062 (91.321)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2659 (0.2488)	Prec@1 88.672 (91.398)	
Total train loss: 0.2490

Train time: 26.074721813201904
 * Prec@1 86.170 Prec@5 99.370 Loss 0.4248
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 32.92880153656006

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2382 (0.2371)	Prec@1 92.578 (91.506)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1976 (0.2352)	Prec@1 94.922 (91.762)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1978 (0.2378)	Prec@1 92.578 (91.597)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.2094 (0.2397)	Prec@1 94.531 (91.546)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2512 (0.2412)	Prec@1 91.797 (91.478)	
Total train loss: 0.2414

Train time: 25.65397810935974
 * Prec@1 86.000 Prec@5 99.360 Loss 0.4404
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 31.437204122543335

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.2433 (0.2429)	Prec@1 91.797 (91.276)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2229 (0.2451)	Prec@1 93.359 (91.226)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.3330 (0.2486)	Prec@1 87.891 (91.249)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2432 (0.2500)	Prec@1 89.062 (91.256)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2751 (0.2514)	Prec@1 89.844 (91.190)	
Total train loss: 0.2514

Train time: 25.220679759979248
 * Prec@1 86.170 Prec@5 99.410 Loss 0.4255
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 30.57293200492859

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2563 (0.2330)	Prec@1 92.188 (92.198)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2812 (0.2353)	Prec@1 90.234 (91.747)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2307 (0.2389)	Prec@1 91.406 (91.647)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2072 (0.2420)	Prec@1 92.969 (91.479)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.2788 (0.2516)	Prec@1 90.625 (91.210)	
Total train loss: 0.2517

Train time: 26.42829918861389
 * Prec@1 84.730 Prec@5 99.230 Loss 0.4780
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 32.5392382144928

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.3098 (0.2742)	Prec@1 91.016 (90.375)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.4133 (0.2805)	Prec@1 84.375 (90.059)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.3127 (0.2818)	Prec@1 87.891 (90.024)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2808 (0.2825)	Prec@1 92.188 (90.054)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2375 (0.2799)	Prec@1 89.453 (90.128)	
Total train loss: 0.2800

Train time: 25.9544837474823
 * Prec@1 84.810 Prec@5 99.200 Loss 0.4768
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 31.139400959014893

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.3142 (0.2743)	Prec@1 91.797 (90.365)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2318 (0.2717)	Prec@1 91.406 (90.460)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.2935 (0.2767)	Prec@1 89.062 (90.191)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2179 (0.2762)	Prec@1 93.359 (90.294)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2534 (0.2768)	Prec@1 91.797 (90.226)	
Total train loss: 0.2772

Train time: 26.698717832565308
 * Prec@1 84.950 Prec@5 99.220 Loss 0.4736
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 31.419833660125732

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1986 (0.2744)	Prec@1 91.797 (90.304)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.2844 (0.2771)	Prec@1 89.453 (90.269)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2810 (0.2748)	Prec@1 90.625 (90.345)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2527 (0.2777)	Prec@1 90.234 (90.182)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.3430 (0.2782)	Prec@1 87.891 (90.180)	
Total train loss: 0.2781

Train time: 21.61994695663452
 * Prec@1 84.870 Prec@5 99.230 Loss 0.4768
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.491793870925903

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.3218 (0.2805)	Prec@1 89.062 (90.114)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2708 (0.2785)	Prec@1 89.062 (90.244)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.3110 (0.2776)	Prec@1 89.062 (90.181)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2159 (0.2768)	Prec@1 93.750 (90.249)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.3030 (0.2777)	Prec@1 89.844 (90.230)	
Total train loss: 0.2777

Train time: 20.945656538009644
 * Prec@1 84.810 Prec@5 99.260 Loss 0.4773
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 24.6896755695343

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2544 (0.2763)	Prec@1 90.625 (90.234)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.2986 (0.2743)	Prec@1 87.891 (90.400)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2803 (0.2776)	Prec@1 91.016 (90.198)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.3091 (0.2803)	Prec@1 87.500 (90.072)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.2195 (0.2767)	Prec@1 91.797 (90.162)	
Total train loss: 0.2766

Train time: 22.275279998779297
 * Prec@1 84.970 Prec@5 99.240 Loss 0.4712
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 28.251616954803467

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2183 (0.2687)	Prec@1 91.406 (90.104)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2703 (0.2713)	Prec@1 91.016 (90.244)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2546 (0.2726)	Prec@1 90.234 (90.321)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1681 (0.2768)	Prec@1 93.359 (90.197)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.2957 (0.2768)	Prec@1 89.844 (90.216)	
Total train loss: 0.2769

Train time: 21.546393632888794
 * Prec@1 84.790 Prec@5 99.270 Loss 0.4795
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 25.308902263641357

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2090 (0.2711)	Prec@1 92.969 (90.495)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2274 (0.2712)	Prec@1 92.969 (90.530)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2485 (0.2684)	Prec@1 92.188 (90.739)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2847 (0.2703)	Prec@1 89.453 (90.610)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2769 (0.2720)	Prec@1 89.453 (90.543)	
Total train loss: 0.2724

Train time: 21.892624139785767
 * Prec@1 85.040 Prec@5 99.250 Loss 0.4724
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.694660186767578

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.3650 (0.2686)	Prec@1 87.109 (90.575)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.2585 (0.2711)	Prec@1 90.625 (90.184)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2722 (0.2733)	Prec@1 92.969 (90.174)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2372 (0.2739)	Prec@1 91.406 (90.172)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.3242 (0.2755)	Prec@1 90.625 (90.190)	
Total train loss: 0.2755

Train time: 21.62294864654541
 * Prec@1 84.860 Prec@5 99.250 Loss 0.4731
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.73923921585083

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.3123 (0.2744)	Prec@1 87.500 (90.214)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2114 (0.2713)	Prec@1 92.578 (90.515)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2832 (0.2754)	Prec@1 89.453 (90.248)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.3252 (0.2759)	Prec@1 89.062 (90.292)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.3054 (0.2755)	Prec@1 89.453 (90.284)	
Total train loss: 0.2756

Train time: 20.387145280838013
 * Prec@1 85.140 Prec@5 99.250 Loss 0.4736
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 24.872973442077637

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2080 (0.2743)	Prec@1 92.578 (90.284)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2849 (0.2730)	Prec@1 89.453 (90.410)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2258 (0.2726)	Prec@1 91.797 (90.401)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.3870 (0.2738)	Prec@1 85.156 (90.299)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.2573 (0.2736)	Prec@1 91.406 (90.315)	
Total train loss: 0.2738

Train time: 20.625194787979126
 * Prec@1 85.050 Prec@5 99.230 Loss 0.4712
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 24.961416244506836

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.2803 (0.2762)	Prec@1 89.453 (90.445)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.2598 (0.2739)	Prec@1 90.234 (90.320)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2493 (0.2767)	Prec@1 92.188 (90.268)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.2260 (0.2743)	Prec@1 91.797 (90.307)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.2593 (0.2748)	Prec@1 92.188 (90.321)	
Total train loss: 0.2747

Train time: 19.918569803237915
 * Prec@1 85.040 Prec@5 99.240 Loss 0.4749
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 25.961095571517944

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.3660 (0.2707)	Prec@1 86.719 (90.495)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.3398 (0.2721)	Prec@1 87.500 (90.450)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2988 (0.2676)	Prec@1 88.672 (90.572)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.3459 (0.2723)	Prec@1 87.891 (90.422)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.3242 (0.2739)	Prec@1 86.328 (90.373)	
Total train loss: 0.2738

Train time: 21.772599935531616
 * Prec@1 84.980 Prec@5 99.220 Loss 0.4731
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 27.202627658843994

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2220 (0.2658)	Prec@1 91.406 (90.615)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2573 (0.2627)	Prec@1 90.625 (90.830)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.3074 (0.2702)	Prec@1 87.500 (90.505)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2678 (0.2743)	Prec@1 91.406 (90.387)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.3174 (0.2750)	Prec@1 89.062 (90.304)	
Total train loss: 0.2754

Train time: 21.243610382080078
 * Prec@1 84.890 Prec@5 99.250 Loss 0.4756
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.105820178985596

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2561 (0.2722)	Prec@1 91.016 (90.535)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2651 (0.2738)	Prec@1 92.188 (90.360)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2788 (0.2742)	Prec@1 90.625 (90.304)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.3071 (0.2737)	Prec@1 89.453 (90.327)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.2827 (0.2748)	Prec@1 89.844 (90.290)	
Total train loss: 0.2749

Train time: 19.71176242828369
 * Prec@1 84.850 Prec@5 99.270 Loss 0.4731
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 25.036869764328003

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.3081 (0.2740)	Prec@1 89.844 (90.675)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.1630 (0.2747)	Prec@1 94.531 (90.645)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.2262 (0.2766)	Prec@1 92.188 (90.375)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.3611 (0.2755)	Prec@1 85.938 (90.337)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2245 (0.2748)	Prec@1 91.016 (90.294)	
Total train loss: 0.2747

Train time: 21.757154941558838
 * Prec@1 85.000 Prec@5 99.230 Loss 0.4729
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 27.459263801574707

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.3245 (0.2751)	Prec@1 88.281 (90.505)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.2932 (0.2704)	Prec@1 89.062 (90.650)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1644 (0.2734)	Prec@1 95.312 (90.471)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.3491 (0.2715)	Prec@1 87.500 (90.455)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.2974 (0.2736)	Prec@1 88.281 (90.403)	
Total train loss: 0.2740

Train time: 22.6416974067688
 * Prec@1 84.940 Prec@5 99.230 Loss 0.4719
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 27.30592107772827

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.3394 (0.2776)	Prec@1 89.062 (90.154)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.2800 (0.2783)	Prec@1 90.625 (90.149)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.2996 (0.2785)	Prec@1 89.062 (90.071)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.3391 (0.2779)	Prec@1 88.281 (90.144)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.2817 (0.2754)	Prec@1 89.844 (90.246)	
Total train loss: 0.2755

Train time: 21.73806381225586
 * Prec@1 84.790 Prec@5 99.230 Loss 0.4719
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 28.921263217926025

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.3298 (0.2655)	Prec@1 88.281 (90.515)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.3311 (0.2776)	Prec@1 90.234 (90.169)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.3030 (0.2788)	Prec@1 91.016 (90.171)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.3037 (0.2778)	Prec@1 89.062 (90.209)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.2920 (0.2751)	Prec@1 90.625 (90.337)	
Total train loss: 0.2750

Train time: 25.381205558776855
 * Prec@1 85.020 Prec@5 99.240 Loss 0.4707
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 30.02608561515808

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.2588 (0.2753)	Prec@1 89.844 (90.425)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.2620 (0.2783)	Prec@1 90.625 (90.234)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.2433 (0.2751)	Prec@1 90.625 (90.431)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.3079 (0.2773)	Prec@1 90.234 (90.297)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.2482 (0.2763)	Prec@1 92.969 (90.351)	
Total train loss: 0.2766

Train time: 26.469459533691406
 * Prec@1 84.900 Prec@5 99.270 Loss 0.4751
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 31.75387191772461

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.2435 (0.2824)	Prec@1 90.234 (89.874)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.2363 (0.2763)	Prec@1 90.625 (90.164)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.3257 (0.2755)	Prec@1 90.234 (90.218)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.3599 (0.2754)	Prec@1 89.844 (90.239)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.2087 (0.2756)	Prec@1 90.625 (90.188)	
Total train loss: 0.2756

Train time: 22.239068031311035
 * Prec@1 85.030 Prec@5 99.250 Loss 0.4700
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 27.016531705856323

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.3193 (0.2789)	Prec@1 88.672 (90.134)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.2842 (0.2746)	Prec@1 90.234 (90.219)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.2327 (0.2720)	Prec@1 91.406 (90.348)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.2556 (0.2720)	Prec@1 90.234 (90.410)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.2175 (0.2746)	Prec@1 93.750 (90.325)	
Total train loss: 0.2745

Train time: 22.091113567352295
 * Prec@1 84.870 Prec@5 99.260 Loss 0.4736
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 27.177456617355347

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.3284 (0.2787)	Prec@1 87.891 (90.294)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.3152 (0.2797)	Prec@1 87.891 (90.184)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.2935 (0.2752)	Prec@1 91.016 (90.304)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.3523 (0.2730)	Prec@1 87.109 (90.402)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.3447 (0.2740)	Prec@1 88.672 (90.393)	
Total train loss: 0.2743

Train time: 21.334341764450073
 * Prec@1 84.840 Prec@5 99.230 Loss 0.4744
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.49793267250061

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2600 (0.2809)	Prec@1 89.844 (90.134)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.2690 (0.2786)	Prec@1 89.453 (90.144)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.3010 (0.2782)	Prec@1 91.406 (90.087)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.2993 (0.2757)	Prec@1 88.281 (90.144)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.3215 (0.2737)	Prec@1 87.891 (90.250)	
Total train loss: 0.2735

Train time: 21.378522157669067
 * Prec@1 84.900 Prec@5 99.260 Loss 0.4717
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 26.153083562850952

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.3455 (0.2808)	Prec@1 85.547 (89.994)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.3494 (0.2780)	Prec@1 91.016 (90.224)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.2422 (0.2775)	Prec@1 91.406 (90.271)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1964 (0.2753)	Prec@1 93.359 (90.367)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.3000 (0.2737)	Prec@1 89.844 (90.427)	
Total train loss: 0.2739

Train time: 21.286818504333496
 * Prec@1 85.000 Prec@5 99.250 Loss 0.4736
Best acc: 86.530
--------------------------------------------------------------------------------
Test time: 25.453169107437134


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 54.660 Prec@5 92.430 Loss 2.1191
Pre-trained Prec@1 with 9 layers frozen: 54.65999984741211 	 Loss: 2.119140625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.2089 (0.2970)	Prec@1 91.797 (90.264)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.3110 (0.2754)	Prec@1 89.844 (90.670)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2313 (0.2658)	Prec@1 94.141 (90.996)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1847 (0.2597)	Prec@1 94.531 (91.056)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.1674 (0.2522)	Prec@1 94.141 (91.276)	
Total train loss: 0.2521

Train time: 403.14357829093933
 * Prec@1 83.970 Prec@5 98.860 Loss 0.5229
Best acc: 83.970
--------------------------------------------------------------------------------
Test time: 409.0175380706787

Epoch: [1][38/196]	LR: 0.1	Loss 0.1119 (0.1896)	Prec@1 97.266 (93.279)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1740 (0.1889)	Prec@1 94.922 (93.334)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1761 (0.1904)	Prec@1 92.578 (93.269)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1882 (0.1966)	Prec@1 92.969 (93.119)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1829 (0.1955)	Prec@1 93.359 (93.165)	
Total train loss: 0.1956

Train time: 21.56459093093872
 * Prec@1 80.900 Prec@5 98.700 Loss 0.6914
Best acc: 83.970
--------------------------------------------------------------------------------
Test time: 25.782415628433228

Epoch: [2][38/196]	LR: 0.1	Loss 0.2240 (0.2030)	Prec@1 91.406 (92.778)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.2489 (0.2113)	Prec@1 91.797 (92.493)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1378 (0.2049)	Prec@1 95.312 (92.688)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1707 (0.2044)	Prec@1 95.312 (92.743)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.2247 (0.2066)	Prec@1 93.359 (92.766)	
Total train loss: 0.2067

Train time: 19.478137254714966
 * Prec@1 82.050 Prec@5 98.800 Loss 0.5928
Best acc: 83.970
--------------------------------------------------------------------------------
Test time: 23.205065965652466

Epoch: [3][38/196]	LR: 0.1	Loss 0.1760 (0.1744)	Prec@1 94.531 (93.760)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.2664 (0.1749)	Prec@1 89.453 (93.865)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1954 (0.1836)	Prec@1 92.969 (93.613)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1780 (0.1962)	Prec@1 92.578 (93.089)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.2168 (0.2016)	Prec@1 92.578 (92.855)	
Total train loss: 0.2016

Train time: 18.426513671875
 * Prec@1 74.940 Prec@5 97.440 Loss 0.8062
Best acc: 83.970
--------------------------------------------------------------------------------
Test time: 22.80691409111023

Epoch: [4][38/196]	LR: 0.1	Loss 0.1650 (0.2006)	Prec@1 93.750 (92.879)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.2578 (0.2053)	Prec@1 90.234 (92.738)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.1421 (0.2019)	Prec@1 95.312 (92.829)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.2661 (0.2098)	Prec@1 91.797 (92.618)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.1973 (0.2109)	Prec@1 93.359 (92.572)	
Total train loss: 0.2110

Train time: 19.780442714691162
 * Prec@1 85.410 Prec@5 99.340 Loss 0.4548
Best acc: 85.410
--------------------------------------------------------------------------------
Test time: 23.489108324050903

Epoch: [5][38/196]	LR: 0.1	Loss 0.2773 (0.2295)	Prec@1 89.453 (92.137)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.1980 (0.2263)	Prec@1 93.359 (92.122)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2096 (0.2189)	Prec@1 92.188 (92.314)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.1683 (0.2198)	Prec@1 94.141 (92.260)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.2358 (0.2198)	Prec@1 93.359 (92.268)	
Total train loss: 0.2199

Train time: 19.264424324035645
 * Prec@1 51.150 Prec@5 94.250 Loss 2.7500
Best acc: 85.410
--------------------------------------------------------------------------------
Test time: 23.52932095527649

Epoch: [6][38/196]	LR: 0.1	Loss 0.1875 (0.2320)	Prec@1 93.750 (91.947)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2115 (0.2281)	Prec@1 91.797 (91.987)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.3093 (0.2287)	Prec@1 88.672 (91.894)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.1993 (0.2259)	Prec@1 93.359 (91.942)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2925 (0.2295)	Prec@1 90.234 (91.889)	
Total train loss: 0.2295

Train time: 20.86704134941101
 * Prec@1 70.580 Prec@5 96.260 Loss 1.1611
Best acc: 85.410
--------------------------------------------------------------------------------
Test time: 24.781667709350586

Epoch: [7][38/196]	LR: 0.1	Loss 0.2024 (0.2181)	Prec@1 92.969 (92.478)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.2111 (0.2135)	Prec@1 92.188 (92.493)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.1879 (0.2143)	Prec@1 93.359 (92.378)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.2700 (0.2100)	Prec@1 92.578 (92.571)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.1576 (0.2040)	Prec@1 94.141 (92.776)	
Total train loss: 0.2040

Train time: 19.673346042633057
 * Prec@1 85.090 Prec@5 99.190 Loss 0.5312
Best acc: 85.410
--------------------------------------------------------------------------------
Test time: 23.261587858200073

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.1423 (0.1417)	Prec@1 95.703 (95.192)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.1394 (0.1349)	Prec@1 95.703 (95.373)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1176 (0.1339)	Prec@1 97.266 (95.493)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.1920 (0.1331)	Prec@1 93.359 (95.483)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1204 (0.1310)	Prec@1 96.094 (95.555)	
Total train loss: 0.1309

Train time: 18.264354705810547
 * Prec@1 88.400 Prec@5 99.550 Loss 0.3838
Best acc: 88.400
--------------------------------------------------------------------------------
Test time: 21.812650442123413

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1324 (0.1185)	Prec@1 94.141 (96.014)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1429 (0.1208)	Prec@1 94.922 (95.969)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1210 (0.1227)	Prec@1 98.047 (95.930)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1031 (0.1241)	Prec@1 97.266 (95.871)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.1289 (0.1224)	Prec@1 95.703 (95.909)	
Total train loss: 0.1224

Train time: 18.9323410987854
 * Prec@1 88.500 Prec@5 99.520 Loss 0.3855
Best acc: 88.500
--------------------------------------------------------------------------------
Test time: 24.31877303123474

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1202 (0.1201)	Prec@1 94.141 (95.964)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1177 (0.1149)	Prec@1 95.703 (96.179)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1426 (0.1168)	Prec@1 95.312 (96.167)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1344 (0.1188)	Prec@1 95.312 (96.026)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.1215 (0.1198)	Prec@1 96.484 (96.008)	
Total train loss: 0.1198

Train time: 21.203644037246704
 * Prec@1 88.490 Prec@5 99.520 Loss 0.3818
Best acc: 88.500
--------------------------------------------------------------------------------
Test time: 24.845497131347656

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0840 (0.1106)	Prec@1 97.266 (96.524)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1188 (0.1148)	Prec@1 95.703 (96.289)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1475 (0.1152)	Prec@1 93.359 (96.237)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1044 (0.1170)	Prec@1 96.875 (96.176)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0910 (0.1180)	Prec@1 97.266 (96.118)	
Total train loss: 0.1180

Train time: 18.818891048431396
 * Prec@1 88.510 Prec@5 99.540 Loss 0.3848
Best acc: 88.510
--------------------------------------------------------------------------------
Test time: 23.0989990234375

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1316 (0.1208)	Prec@1 94.922 (95.933)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1359 (0.1182)	Prec@1 94.922 (96.099)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0840 (0.1197)	Prec@1 96.094 (95.980)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1048 (0.1181)	Prec@1 96.484 (96.079)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1166 (0.1179)	Prec@1 97.266 (96.074)	
Total train loss: 0.1179

Train time: 18.632318258285522
 * Prec@1 88.490 Prec@5 99.510 Loss 0.3838
Best acc: 88.510
--------------------------------------------------------------------------------
Test time: 22.884867906570435

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1256 (0.1184)	Prec@1 96.484 (96.064)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1406 (0.1155)	Prec@1 95.703 (96.234)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1713 (0.1180)	Prec@1 91.797 (96.130)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1537 (0.1177)	Prec@1 94.531 (96.119)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0781 (0.1175)	Prec@1 96.875 (96.096)	
Total train loss: 0.1175

Train time: 19.79071283340454
 * Prec@1 88.430 Prec@5 99.540 Loss 0.3870
Best acc: 88.510
--------------------------------------------------------------------------------
Test time: 24.336459398269653

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0683 (0.1112)	Prec@1 98.828 (96.364)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1099 (0.1175)	Prec@1 96.094 (96.199)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1373 (0.1197)	Prec@1 94.922 (96.064)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1426 (0.1194)	Prec@1 94.141 (96.026)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1254 (0.1192)	Prec@1 94.922 (96.028)	
Total train loss: 0.1193

Train time: 20.374814987182617
 * Prec@1 88.550 Prec@5 99.500 Loss 0.3862
Best acc: 88.550
--------------------------------------------------------------------------------
Test time: 25.25850009918213

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1277 (0.1175)	Prec@1 95.703 (95.964)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1046 (0.1142)	Prec@1 96.094 (96.194)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0942 (0.1165)	Prec@1 97.266 (96.117)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1179 (0.1168)	Prec@1 95.703 (96.101)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1320 (0.1171)	Prec@1 96.094 (96.068)	
Total train loss: 0.1171

Train time: 20.47285747528076
 * Prec@1 88.400 Prec@5 99.530 Loss 0.3901
Best acc: 88.550
--------------------------------------------------------------------------------
Test time: 25.209256172180176

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1002 (0.1108)	Prec@1 97.266 (96.344)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1481 (0.1132)	Prec@1 94.141 (96.234)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1461 (0.1148)	Prec@1 96.094 (96.144)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1647 (0.1156)	Prec@1 93.750 (96.084)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1080 (0.1166)	Prec@1 97.266 (96.064)	
Total train loss: 0.1167

Train time: 20.26342797279358
 * Prec@1 88.570 Prec@5 99.570 Loss 0.3872
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 24.071321725845337

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1180 (0.1123)	Prec@1 95.312 (96.294)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1191 (0.1142)	Prec@1 95.703 (96.194)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0910 (0.1147)	Prec@1 98.047 (96.171)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1102 (0.1151)	Prec@1 96.484 (96.226)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0922 (0.1153)	Prec@1 96.875 (96.186)	
Total train loss: 0.1154

Train time: 19.426279067993164
 * Prec@1 88.500 Prec@5 99.540 Loss 0.3862
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 22.655786752700806

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1525 (0.1199)	Prec@1 94.922 (96.024)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1017 (0.1139)	Prec@1 97.266 (96.324)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0932 (0.1137)	Prec@1 97.266 (96.254)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1555 (0.1151)	Prec@1 93.359 (96.176)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1014 (0.1163)	Prec@1 96.094 (96.116)	
Total train loss: 0.1162

Train time: 19.576876163482666
 * Prec@1 88.390 Prec@5 99.570 Loss 0.3875
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 24.75353193283081

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1289 (0.1174)	Prec@1 95.312 (96.014)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1246 (0.1184)	Prec@1 95.703 (96.029)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1324 (0.1168)	Prec@1 94.922 (96.120)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0774 (0.1174)	Prec@1 98.047 (96.076)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0862 (0.1170)	Prec@1 96.875 (96.096)	
Total train loss: 0.1169

Train time: 21.665046453475952
 * Prec@1 88.560 Prec@5 99.530 Loss 0.3877
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 26.381153106689453

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1237 (0.1159)	Prec@1 95.703 (96.214)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1214 (0.1157)	Prec@1 95.312 (96.164)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0815 (0.1156)	Prec@1 97.266 (96.130)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1420 (0.1167)	Prec@1 94.531 (96.091)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1719 (0.1164)	Prec@1 93.359 (96.106)	
Total train loss: 0.1164

Train time: 19.62142276763916
 * Prec@1 88.450 Prec@5 99.520 Loss 0.3914
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 23.980746269226074

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1152 (0.1175)	Prec@1 96.484 (95.984)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1285 (0.1161)	Prec@1 94.922 (96.109)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1044 (0.1148)	Prec@1 96.484 (96.181)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1437 (0.1156)	Prec@1 94.922 (96.169)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0809 (0.1154)	Prec@1 98.438 (96.188)	
Total train loss: 0.1155

Train time: 20.229759216308594
 * Prec@1 88.550 Prec@5 99.500 Loss 0.3879
Best acc: 88.570
--------------------------------------------------------------------------------
Test time: 26.213960647583008

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1002 (0.1147)	Prec@1 96.875 (96.314)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1409 (0.1163)	Prec@1 95.703 (96.119)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1447 (0.1149)	Prec@1 94.141 (96.191)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0823 (0.1150)	Prec@1 97.266 (96.204)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1272 (0.1170)	Prec@1 96.484 (96.120)	
Total train loss: 0.1170

Train time: 19.177210330963135
 * Prec@1 88.610 Prec@5 99.510 Loss 0.3889
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 24.508419513702393

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1295 (0.1115)	Prec@1 96.094 (96.364)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1166 (0.1135)	Prec@1 94.531 (96.279)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1122 (0.1128)	Prec@1 96.875 (96.324)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0773 (0.1153)	Prec@1 97.266 (96.226)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1235 (0.1162)	Prec@1 94.922 (96.150)	
Total train loss: 0.1162

Train time: 19.79562497138977
 * Prec@1 88.450 Prec@5 99.530 Loss 0.3889
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 22.84510827064514

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1031 (0.1198)	Prec@1 97.266 (96.194)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0961 (0.1158)	Prec@1 97.656 (96.099)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1428 (0.1147)	Prec@1 94.922 (96.157)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1261 (0.1160)	Prec@1 96.484 (96.121)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1099 (0.1153)	Prec@1 96.484 (96.174)	
Total train loss: 0.1155

Train time: 19.39853048324585
 * Prec@1 88.450 Prec@5 99.560 Loss 0.3877
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.106077432632446

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1265 (0.1144)	Prec@1 96.875 (96.304)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0966 (0.1122)	Prec@1 96.484 (96.359)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1060 (0.1151)	Prec@1 96.484 (96.177)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1148 (0.1147)	Prec@1 95.703 (96.141)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1031 (0.1155)	Prec@1 96.484 (96.102)	
Total train loss: 0.1156

Train time: 19.35057020187378
 * Prec@1 88.320 Prec@5 99.570 Loss 0.3911
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.378374338150024

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0855 (0.1178)	Prec@1 96.484 (96.314)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1113 (0.1145)	Prec@1 96.875 (96.369)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1125 (0.1138)	Prec@1 94.922 (96.307)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0916 (0.1162)	Prec@1 96.875 (96.244)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1154 (0.1160)	Prec@1 96.484 (96.232)	
Total train loss: 0.1160

Train time: 18.982856273651123
 * Prec@1 88.460 Prec@5 99.540 Loss 0.3884
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 24.332356452941895

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1395 (0.1177)	Prec@1 95.312 (96.014)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1134 (0.1180)	Prec@1 96.484 (96.044)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1064 (0.1182)	Prec@1 95.703 (96.104)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1326 (0.1171)	Prec@1 95.703 (96.139)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0958 (0.1159)	Prec@1 97.266 (96.160)	
Total train loss: 0.1159

Train time: 20.272313594818115
 * Prec@1 88.490 Prec@5 99.540 Loss 0.3865
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 24.697224855422974

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1351 (0.1224)	Prec@1 94.141 (95.883)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1070 (0.1180)	Prec@1 97.266 (96.049)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0873 (0.1166)	Prec@1 97.266 (96.070)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1196 (0.1165)	Prec@1 96.094 (96.041)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1333 (0.1159)	Prec@1 94.141 (96.102)	
Total train loss: 0.1160

Train time: 20.606518507003784
 * Prec@1 88.370 Prec@5 99.520 Loss 0.3901
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 24.39918875694275

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0984 (0.1113)	Prec@1 96.484 (96.334)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0956 (0.1148)	Prec@1 96.875 (96.149)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1354 (0.1170)	Prec@1 96.484 (96.080)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0990 (0.1170)	Prec@1 97.656 (96.076)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1006 (0.1159)	Prec@1 96.875 (96.122)	
Total train loss: 0.1158

Train time: 19.830268383026123
 * Prec@1 88.530 Prec@5 99.530 Loss 0.3882
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 24.019899606704712

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.0852 (0.1135)	Prec@1 97.266 (96.274)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.1304 (0.1135)	Prec@1 94.922 (96.334)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.1423 (0.1158)	Prec@1 94.531 (96.137)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.0938 (0.1164)	Prec@1 97.266 (96.141)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.0996 (0.1155)	Prec@1 96.484 (96.174)	
Total train loss: 0.1157

Train time: 20.358632564544678
 * Prec@1 88.540 Prec@5 99.560 Loss 0.3865
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 25.080504894256592

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.0786 (0.1175)	Prec@1 97.656 (96.094)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.1538 (0.1182)	Prec@1 95.312 (96.164)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1123 (0.1175)	Prec@1 96.484 (96.124)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.1138 (0.1166)	Prec@1 97.266 (96.121)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1245 (0.1161)	Prec@1 94.922 (96.132)	
Total train loss: 0.1162

Train time: 20.02771282196045
 * Prec@1 88.460 Prec@5 99.530 Loss 0.3911
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.75916051864624

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1283 (0.1170)	Prec@1 94.922 (96.204)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1243 (0.1146)	Prec@1 96.094 (96.169)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1008 (0.1163)	Prec@1 96.875 (96.107)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.0842 (0.1149)	Prec@1 97.656 (96.186)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1312 (0.1146)	Prec@1 94.922 (96.214)	
Total train loss: 0.1146

Train time: 19.532870531082153
 * Prec@1 88.400 Prec@5 99.510 Loss 0.3877
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 25.215314865112305

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.0941 (0.1137)	Prec@1 96.094 (96.324)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.0981 (0.1139)	Prec@1 96.875 (96.329)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1249 (0.1130)	Prec@1 96.094 (96.368)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.0936 (0.1142)	Prec@1 96.484 (96.299)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1175 (0.1156)	Prec@1 94.922 (96.198)	
Total train loss: 0.1155

Train time: 18.407055854797363
 * Prec@1 88.550 Prec@5 99.530 Loss 0.3875
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.094588041305542

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.1195 (0.1129)	Prec@1 96.094 (96.204)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.0629 (0.1097)	Prec@1 98.828 (96.379)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.0813 (0.1118)	Prec@1 96.484 (96.314)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.0899 (0.1132)	Prec@1 97.266 (96.297)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1449 (0.1140)	Prec@1 94.531 (96.262)	
Total train loss: 0.1142

Train time: 19.336697340011597
 * Prec@1 88.410 Prec@5 99.530 Loss 0.3906
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.20073437690735

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.1017 (0.1162)	Prec@1 95.703 (96.234)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.0918 (0.1154)	Prec@1 96.875 (96.164)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.1410 (0.1165)	Prec@1 94.922 (96.124)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.0998 (0.1161)	Prec@1 96.484 (96.196)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.0994 (0.1166)	Prec@1 96.484 (96.146)	
Total train loss: 0.1166

Train time: 19.724886178970337
 * Prec@1 88.590 Prec@5 99.550 Loss 0.3865
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.69321632385254

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1148 (0.1179)	Prec@1 96.094 (95.994)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1620 (0.1158)	Prec@1 94.141 (96.119)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1301 (0.1146)	Prec@1 94.922 (96.184)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1461 (0.1147)	Prec@1 94.922 (96.201)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1556 (0.1161)	Prec@1 92.969 (96.160)	
Total train loss: 0.1161

Train time: 19.818591833114624
 * Prec@1 88.530 Prec@5 99.520 Loss 0.3894
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.519561529159546

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1624 (0.1149)	Prec@1 93.750 (96.154)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.1570 (0.1140)	Prec@1 94.922 (96.294)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1088 (0.1147)	Prec@1 95.703 (96.231)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.0848 (0.1154)	Prec@1 96.875 (96.196)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.0952 (0.1159)	Prec@1 96.875 (96.150)	
Total train loss: 0.1160

Train time: 18.70155119895935
 * Prec@1 88.460 Prec@5 99.550 Loss 0.3884
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.219554901123047

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.1148 (0.1199)	Prec@1 95.703 (96.074)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1312 (0.1175)	Prec@1 95.312 (96.099)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.1002 (0.1176)	Prec@1 96.484 (96.070)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1014 (0.1168)	Prec@1 96.875 (96.131)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.0851 (0.1161)	Prec@1 97.266 (96.166)	
Total train loss: 0.1160

Train time: 18.771445512771606
 * Prec@1 88.510 Prec@5 99.530 Loss 0.3843
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 23.813883066177368

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.0822 (0.1130)	Prec@1 97.266 (96.424)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1008 (0.1147)	Prec@1 96.875 (96.284)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.0984 (0.1158)	Prec@1 96.094 (96.184)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.1467 (0.1156)	Prec@1 94.141 (96.181)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1163 (0.1155)	Prec@1 95.312 (96.168)	
Total train loss: 0.1155

Train time: 19.727651357650757
 * Prec@1 88.400 Prec@5 99.520 Loss 0.3877
Best acc: 88.610
--------------------------------------------------------------------------------
Test time: 25.147634506225586

