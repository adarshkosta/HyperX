
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 55.460 Prec@5 82.280 Loss 1.8350
Pre-trained Prec@1 with 11 layers frozen: 55.459999084472656 	 Loss: 1.8349609375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6523 (1.2965)	Prec@1 54.297 (63.331)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.4160 (1.3687)	Prec@1 58.984 (61.523)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.2979 (1.3604)	Prec@1 64.453 (61.836)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.3574 (1.3394)	Prec@1 62.109 (62.362)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.1221 (1.3153)	Prec@1 72.266 (62.819)	
Total train loss: 1.3150

Train time: 1517.7073657512665
 * Prec@1 20.780 Prec@5 44.110 Loss 4.3555
Best acc: 20.780
--------------------------------------------------------------------------------
Test time: 1522.2081410884857

Epoch: [1][38/196]	LR: 0.1	Loss 1.0986 (1.0362)	Prec@1 67.969 (70.132)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0225 (1.0288)	Prec@1 71.484 (70.378)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1152 (1.0396)	Prec@1 67.969 (69.745)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0684 (1.0430)	Prec@1 70.703 (69.759)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0557 (1.0412)	Prec@1 68.750 (69.780)	
Total train loss: 1.0413

Train time: 22.605336666107178
 * Prec@1 52.070 Prec@5 80.730 Loss 1.9150
Best acc: 52.070
--------------------------------------------------------------------------------
Test time: 26.492543935775757

Epoch: [2][38/196]	LR: 0.1	Loss 0.8950 (0.8637)	Prec@1 77.734 (75.020)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9819 (0.8720)	Prec@1 71.484 (74.594)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9175 (0.8952)	Prec@1 73.047 (73.882)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.1455 (0.9221)	Prec@1 66.406 (73.077)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.8574 (0.9311)	Prec@1 72.266 (72.778)	
Total train loss: 0.9311

Train time: 21.062420129776
 * Prec@1 48.060 Prec@5 78.000 Loss 2.4512
Best acc: 52.070
--------------------------------------------------------------------------------
Test time: 25.015687942504883

Epoch: [3][38/196]	LR: 0.1	Loss 0.7529 (0.7776)	Prec@1 79.297 (77.183)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8818 (0.7871)	Prec@1 72.656 (76.763)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.7744 (0.7959)	Prec@1 80.078 (76.409)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9336 (0.8048)	Prec@1 74.219 (76.024)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.8735 (0.8086)	Prec@1 72.656 (75.966)	
Total train loss: 0.8089

Train time: 20.4572491645813
 * Prec@1 50.020 Prec@5 78.760 Loss 2.5215
Best acc: 52.070
--------------------------------------------------------------------------------
Test time: 23.76346707344055

Epoch: [4][38/196]	LR: 0.1	Loss 0.7505 (0.7178)	Prec@1 76.172 (79.026)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7358 (0.7321)	Prec@1 77.344 (78.085)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7363 (0.7354)	Prec@1 76.172 (78.045)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.6777 (0.7380)	Prec@1 80.469 (78.160)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.7891 (0.7418)	Prec@1 76.953 (78.053)	
Total train loss: 0.7422

Train time: 19.426692724227905
 * Prec@1 32.740 Prec@5 60.120 Loss 3.3691
Best acc: 52.070
--------------------------------------------------------------------------------
Test time: 23.24990677833557

Epoch: [5][38/196]	LR: 0.1	Loss 0.6196 (0.6404)	Prec@1 80.078 (80.258)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.7466 (0.6557)	Prec@1 76.562 (80.063)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7705 (0.6726)	Prec@1 73.438 (79.474)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7188 (0.6810)	Prec@1 78.906 (79.134)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.6528 (0.6932)	Prec@1 80.078 (78.876)	
Total train loss: 0.6934

Train time: 19.37239408493042
 * Prec@1 38.730 Prec@5 68.270 Loss 2.6484
Best acc: 52.070
--------------------------------------------------------------------------------
Test time: 23.263272047042847

Epoch: [6][38/196]	LR: 0.1	Loss 0.6177 (0.6524)	Prec@1 77.734 (80.098)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5933 (0.6286)	Prec@1 82.422 (81.120)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6851 (0.6361)	Prec@1 77.344 (80.759)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6792 (0.6467)	Prec@1 78.516 (80.381)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.5625 (0.6477)	Prec@1 83.984 (80.427)	
Total train loss: 0.6479

Train time: 19.713709354400635
 * Prec@1 56.160 Prec@5 82.980 Loss 1.7988
Best acc: 56.160
--------------------------------------------------------------------------------
Test time: 23.45607089996338

Epoch: [7][38/196]	LR: 0.1	Loss 0.5142 (0.5433)	Prec@1 84.766 (83.744)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4939 (0.5630)	Prec@1 87.500 (83.048)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6016 (0.5720)	Prec@1 82.031 (82.782)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6846 (0.5883)	Prec@1 79.297 (82.292)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6738 (0.5961)	Prec@1 78.125 (82.091)	
Total train loss: 0.5965

Train time: 18.905714988708496
 * Prec@1 55.910 Prec@5 83.640 Loss 1.8193
Best acc: 56.160
--------------------------------------------------------------------------------
Test time: 22.140472888946533

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.5723 (0.5336)	Prec@1 82.812 (84.265)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.5225 (0.5242)	Prec@1 83.984 (84.565)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4338 (0.5107)	Prec@1 89.453 (85.240)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.4854 (0.5034)	Prec@1 85.938 (85.399)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4456 (0.4985)	Prec@1 87.500 (85.609)	
Total train loss: 0.4989

Train time: 18.64802861213684
 * Prec@1 63.590 Prec@5 87.690 Loss 1.4473
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 22.802623748779297

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.4937 (0.4806)	Prec@1 86.328 (85.917)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.4829 (0.4727)	Prec@1 85.156 (86.478)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.4714 (0.4732)	Prec@1 85.156 (86.535)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.4631 (0.4712)	Prec@1 86.328 (86.616)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.5854 (0.4741)	Prec@1 82.422 (86.526)	
Total train loss: 0.4744

Train time: 18.65860867500305
 * Prec@1 63.180 Prec@5 87.820 Loss 1.4580
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 21.93369150161743

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.4309 (0.4733)	Prec@1 90.234 (86.589)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.5093 (0.4752)	Prec@1 87.109 (86.378)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.4849 (0.4805)	Prec@1 84.766 (86.111)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.4775 (0.4820)	Prec@1 87.109 (86.030)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5522 (0.4847)	Prec@1 85.938 (86.048)	
Total train loss: 0.4848

Train time: 18.540942668914795
 * Prec@1 63.150 Prec@5 87.890 Loss 1.4609
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 22.508586645126343

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.4475 (0.4745)	Prec@1 86.328 (86.609)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.5430 (0.4753)	Prec@1 87.109 (86.574)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.4187 (0.4810)	Prec@1 88.281 (86.245)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.5239 (0.4863)	Prec@1 86.328 (86.125)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.4578 (0.4898)	Prec@1 87.891 (85.958)	
Total train loss: 0.4901

Train time: 19.1965651512146
 * Prec@1 62.860 Prec@5 87.560 Loss 1.4717
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.135918855667114

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.5093 (0.5004)	Prec@1 84.375 (85.687)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5698 (0.4984)	Prec@1 85.156 (85.877)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5166 (0.4961)	Prec@1 83.594 (85.804)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3987 (0.5043)	Prec@1 90.625 (85.532)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5195 (0.5070)	Prec@1 85.938 (85.411)	
Total train loss: 0.5072

Train time: 20.06424379348755
 * Prec@1 62.900 Prec@5 87.680 Loss 1.4727
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.95690608024597

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.5933 (0.5363)	Prec@1 82.812 (84.075)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5317 (0.5351)	Prec@1 81.641 (84.070)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.5693 (0.5398)	Prec@1 85.938 (84.151)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.5713 (0.5430)	Prec@1 81.250 (83.974)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.5791 (0.5446)	Prec@1 84.766 (83.906)	
Total train loss: 0.5448

Train time: 19.619253158569336
 * Prec@1 62.700 Prec@5 87.710 Loss 1.4883
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.06436252593994

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.5654 (0.5605)	Prec@1 81.641 (83.353)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.5776 (0.5632)	Prec@1 81.641 (83.358)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.5664 (0.5671)	Prec@1 81.641 (83.220)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.5684 (0.5621)	Prec@1 82.812 (83.466)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.5107 (0.5571)	Prec@1 84.375 (83.652)	
Total train loss: 0.5573

Train time: 19.531383991241455
 * Prec@1 62.440 Prec@5 87.520 Loss 1.4844
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.620476722717285

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.6914 (0.5532)	Prec@1 78.906 (83.794)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.6362 (0.5570)	Prec@1 80.469 (83.799)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.5166 (0.5560)	Prec@1 84.375 (83.640)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.5908 (0.5603)	Prec@1 83.203 (83.484)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.5698 (0.5646)	Prec@1 80.859 (83.287)	
Total train loss: 0.5647

Train time: 19.75874376296997
 * Prec@1 62.520 Prec@5 87.500 Loss 1.4795
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.174943685531616

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.5425 (0.5714)	Prec@1 83.984 (83.293)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.6045 (0.5672)	Prec@1 81.250 (83.428)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.5498 (0.5721)	Prec@1 84.375 (83.330)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.7139 (0.5717)	Prec@1 82.812 (83.281)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.6597 (0.5729)	Prec@1 81.250 (83.231)	
Total train loss: 0.5728

Train time: 20.248772621154785
 * Prec@1 62.570 Prec@5 87.660 Loss 1.4844
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.88946795463562

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.5474 (0.5655)	Prec@1 84.375 (83.363)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.5435 (0.5635)	Prec@1 84.375 (83.213)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.5483 (0.5676)	Prec@1 80.469 (82.996)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.6416 (0.5672)	Prec@1 78.516 (83.113)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.5879 (0.5701)	Prec@1 81.250 (83.085)	
Total train loss: 0.5703

Train time: 20.349398374557495
 * Prec@1 62.520 Prec@5 87.560 Loss 1.4844
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 24.291504383087158

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.5254 (0.5672)	Prec@1 83.984 (83.133)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.5806 (0.5645)	Prec@1 82.422 (83.313)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.5781 (0.5683)	Prec@1 82.031 (83.150)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5698 (0.5712)	Prec@1 80.859 (83.133)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.6050 (0.5704)	Prec@1 83.594 (83.193)	
Total train loss: 0.5703

Train time: 20.1950204372406
 * Prec@1 62.760 Prec@5 87.500 Loss 1.4775
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.950436115264893

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.5479 (0.5665)	Prec@1 83.594 (83.223)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.5957 (0.5677)	Prec@1 84.375 (83.213)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.5894 (0.5704)	Prec@1 85.938 (83.220)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.4387 (0.5683)	Prec@1 87.109 (83.226)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.5435 (0.5674)	Prec@1 84.766 (83.255)	
Total train loss: 0.5673

Train time: 19.786917686462402
 * Prec@1 62.780 Prec@5 87.590 Loss 1.4756
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.472007513046265

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.5278 (0.5624)	Prec@1 86.328 (83.444)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.6001 (0.5659)	Prec@1 82.422 (83.263)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.4841 (0.5663)	Prec@1 85.547 (83.397)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.5444 (0.5674)	Prec@1 81.641 (83.336)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.5977 (0.5693)	Prec@1 81.641 (83.281)	
Total train loss: 0.5697

Train time: 21.101333379745483
 * Prec@1 62.590 Prec@5 87.380 Loss 1.4883
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 25.341394424438477

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.5518 (0.5738)	Prec@1 82.812 (82.792)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.6152 (0.5693)	Prec@1 81.250 (83.248)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.4912 (0.5661)	Prec@1 84.375 (83.360)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.6504 (0.5685)	Prec@1 80.469 (83.326)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.5850 (0.5717)	Prec@1 81.641 (83.197)	
Total train loss: 0.5719

Train time: 20.55280113220215
 * Prec@1 62.640 Prec@5 87.470 Loss 1.4834
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.989494800567627

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.5991 (0.5750)	Prec@1 80.859 (83.153)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.5942 (0.5740)	Prec@1 82.812 (82.973)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5400 (0.5714)	Prec@1 87.109 (83.090)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.5469 (0.5692)	Prec@1 81.641 (83.143)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.6201 (0.5686)	Prec@1 80.078 (83.285)	
Total train loss: 0.5687

Train time: 20.561606884002686
 * Prec@1 62.370 Prec@5 87.530 Loss 1.4844
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 24.86513662338257

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.5620 (0.5624)	Prec@1 83.203 (83.734)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.5259 (0.5670)	Prec@1 81.250 (83.233)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.5977 (0.5725)	Prec@1 83.984 (83.023)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.5161 (0.5713)	Prec@1 85.156 (83.088)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.5273 (0.5698)	Prec@1 81.641 (83.151)	
Total train loss: 0.5695

Train time: 19.58971858024597
 * Prec@1 62.570 Prec@5 87.530 Loss 1.4814
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 22.98954463005066

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.5225 (0.5675)	Prec@1 84.766 (83.544)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.5908 (0.5670)	Prec@1 83.203 (83.258)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.6812 (0.5661)	Prec@1 80.859 (83.263)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.6323 (0.5680)	Prec@1 81.250 (83.233)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.5532 (0.5694)	Prec@1 83.203 (83.141)	
Total train loss: 0.5695

Train time: 18.79059886932373
 * Prec@1 62.470 Prec@5 87.610 Loss 1.4893
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 21.95529532432556

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.5273 (0.5728)	Prec@1 82.812 (82.772)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.5454 (0.5723)	Prec@1 84.766 (82.958)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.6035 (0.5661)	Prec@1 82.812 (83.200)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.6025 (0.5662)	Prec@1 83.203 (83.243)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5503 (0.5683)	Prec@1 83.984 (83.171)	
Total train loss: 0.5686

Train time: 18.598665475845337
 * Prec@1 62.540 Prec@5 87.420 Loss 1.4854
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 22.59684991836548

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.6157 (0.5624)	Prec@1 82.031 (83.754)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.5679 (0.5627)	Prec@1 84.766 (83.639)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.4846 (0.5658)	Prec@1 85.156 (83.507)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.5259 (0.5662)	Prec@1 84.375 (83.333)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.4919 (0.5684)	Prec@1 85.938 (83.293)	
Total train loss: 0.5687

Train time: 20.682547092437744
 * Prec@1 62.470 Prec@5 87.620 Loss 1.4805
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 24.04022455215454

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.5249 (0.5612)	Prec@1 84.375 (83.423)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.5132 (0.5704)	Prec@1 85.938 (83.058)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.6392 (0.5727)	Prec@1 80.078 (83.040)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.5767 (0.5692)	Prec@1 83.594 (83.163)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.4556 (0.5708)	Prec@1 87.500 (83.099)	
Total train loss: 0.5712

Train time: 19.957040309906006
 * Prec@1 62.650 Prec@5 87.470 Loss 1.4824
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 22.867610454559326

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.5215 (0.5597)	Prec@1 85.547 (83.614)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.6016 (0.5604)	Prec@1 83.203 (83.594)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.5825 (0.5646)	Prec@1 80.078 (83.410)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.5742 (0.5661)	Prec@1 82.031 (83.356)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.5718 (0.5685)	Prec@1 81.641 (83.307)	
Total train loss: 0.5689

Train time: 18.81894850730896
 * Prec@1 62.670 Prec@5 87.630 Loss 1.4824
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.49933886528015

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.4631 (0.5686)	Prec@1 87.109 (82.963)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.5820 (0.5704)	Prec@1 82.031 (83.163)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.5312 (0.5721)	Prec@1 82.812 (83.113)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.4771 (0.5714)	Prec@1 87.109 (83.133)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.5708 (0.5715)	Prec@1 84.375 (83.125)	
Total train loss: 0.5717

Train time: 20.702619075775146
 * Prec@1 62.630 Prec@5 87.580 Loss 1.4844
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.943082809448242

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.6025 (0.5714)	Prec@1 83.203 (83.484)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.5332 (0.5665)	Prec@1 83.594 (83.474)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.5752 (0.5647)	Prec@1 83.984 (83.387)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.5117 (0.5674)	Prec@1 84.766 (83.353)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.5161 (0.5657)	Prec@1 85.547 (83.458)	
Total train loss: 0.5657

Train time: 17.921581029891968
 * Prec@1 62.740 Prec@5 87.510 Loss 1.4893
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 20.826040267944336

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.4910 (0.5587)	Prec@1 85.156 (83.484)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.5537 (0.5711)	Prec@1 82.422 (83.098)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.6221 (0.5699)	Prec@1 80.469 (83.153)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.5728 (0.5675)	Prec@1 80.469 (83.318)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.5503 (0.5692)	Prec@1 84.375 (83.289)	
Total train loss: 0.5696

Train time: 19.79592490196228
 * Prec@1 62.560 Prec@5 87.550 Loss 1.4863
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 24.19130539894104

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.5454 (0.5679)	Prec@1 83.984 (83.303)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.5342 (0.5691)	Prec@1 82.031 (83.469)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.6084 (0.5685)	Prec@1 80.859 (83.370)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.5400 (0.5679)	Prec@1 85.938 (83.363)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.5396 (0.5675)	Prec@1 81.641 (83.383)	
Total train loss: 0.5676

Train time: 20.551837682724
 * Prec@1 62.780 Prec@5 87.560 Loss 1.4766
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.89579725265503

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.5615 (0.5609)	Prec@1 82.422 (83.674)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.5181 (0.5656)	Prec@1 85.938 (83.449)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.5942 (0.5715)	Prec@1 82.031 (83.206)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.6064 (0.5676)	Prec@1 80.859 (83.331)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.5347 (0.5693)	Prec@1 82.812 (83.249)	
Total train loss: 0.5696

Train time: 20.75197458267212
 * Prec@1 62.370 Prec@5 87.580 Loss 1.4824
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 25.224882125854492

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.5103 (0.5539)	Prec@1 86.719 (83.864)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.4670 (0.5579)	Prec@1 86.328 (83.859)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.5669 (0.5627)	Prec@1 82.031 (83.691)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.6318 (0.5675)	Prec@1 83.984 (83.541)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.6528 (0.5678)	Prec@1 78.125 (83.389)	
Total train loss: 0.5679

Train time: 19.99755859375
 * Prec@1 62.670 Prec@5 87.580 Loss 1.4775
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.33385181427002

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.6279 (0.5834)	Prec@1 80.859 (82.792)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.6094 (0.5799)	Prec@1 82.031 (82.898)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.5811 (0.5764)	Prec@1 83.594 (82.983)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.5957 (0.5721)	Prec@1 81.250 (83.141)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.5513 (0.5701)	Prec@1 86.719 (83.185)	
Total train loss: 0.5702

Train time: 19.686612129211426
 * Prec@1 62.590 Prec@5 87.570 Loss 1.4902
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.0726056098938

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.6431 (0.5759)	Prec@1 81.641 (83.303)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.6621 (0.5700)	Prec@1 80.078 (83.018)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.5400 (0.5690)	Prec@1 82.422 (83.046)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.5898 (0.5719)	Prec@1 82.812 (82.978)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.5786 (0.5686)	Prec@1 81.641 (83.125)	
Total train loss: 0.5688

Train time: 19.80174684524536
 * Prec@1 62.430 Prec@5 87.540 Loss 1.4854
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 24.131099939346313

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.6216 (0.5603)	Prec@1 80.859 (83.614)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.5669 (0.5585)	Prec@1 83.984 (83.589)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.5752 (0.5613)	Prec@1 86.328 (83.537)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.6133 (0.5650)	Prec@1 84.766 (83.431)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.6167 (0.5687)	Prec@1 81.641 (83.309)	
Total train loss: 0.5691

Train time: 19.96162724494934
 * Prec@1 62.490 Prec@5 87.540 Loss 1.4834
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 23.34068012237549

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.6328 (0.5763)	Prec@1 82.031 (82.823)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.6992 (0.5710)	Prec@1 77.734 (83.033)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.5396 (0.5755)	Prec@1 84.766 (82.846)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.5366 (0.5684)	Prec@1 84.766 (83.118)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.6045 (0.5685)	Prec@1 82.031 (83.073)	
Total train loss: 0.5686

Train time: 21.002779960632324
 * Prec@1 62.730 Prec@5 87.510 Loss 1.4766
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 25.244532585144043

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.5200 (0.5608)	Prec@1 81.641 (83.323)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.5815 (0.5644)	Prec@1 83.594 (83.273)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.5840 (0.5671)	Prec@1 83.203 (83.220)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.6611 (0.5690)	Prec@1 79.297 (83.243)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.4998 (0.5682)	Prec@1 86.719 (83.335)	
Total train loss: 0.5684

Train time: 21.55353593826294
 * Prec@1 62.510 Prec@5 87.400 Loss 1.4893
Best acc: 63.590
--------------------------------------------------------------------------------
Test time: 26.424861192703247


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 49.630 Prec@5 77.380 Loss 2.1680
Pre-trained Prec@1 with 13 layers frozen: 49.62999725341797 	 Loss: 2.16796875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.4043 (1.2362)	Prec@1 57.031 (64.814)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3418 (1.2986)	Prec@1 61.328 (63.026)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.2881 (1.3046)	Prec@1 63.281 (62.901)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.2920 (1.2973)	Prec@1 65.625 (63.136)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2627 (1.2846)	Prec@1 64.844 (63.456)	
Total train loss: 1.2851

Train time: 246.2424337863922
 * Prec@1 54.680 Prec@5 82.830 Loss 1.7012
Best acc: 54.680
--------------------------------------------------------------------------------
Test time: 250.55357432365417

Epoch: [1][38/196]	LR: 0.1	Loss 0.8467 (1.0052)	Prec@1 76.562 (71.264)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0723 (1.0085)	Prec@1 70.312 (71.014)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1719 (1.0269)	Prec@1 69.531 (70.232)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0449 (1.0303)	Prec@1 69.922 (70.127)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0898 (1.0275)	Prec@1 70.312 (70.166)	
Total train loss: 1.0275

Train time: 19.709376096725464
 * Prec@1 49.450 Prec@5 78.670 Loss 2.3867
Best acc: 54.680
--------------------------------------------------------------------------------
Test time: 23.630311250686646

Epoch: [2][38/196]	LR: 0.1	Loss 0.9150 (0.8821)	Prec@1 75.391 (74.269)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9165 (0.8687)	Prec@1 73.047 (74.820)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9409 (0.8759)	Prec@1 72.656 (74.416)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.7671 (0.8861)	Prec@1 76.953 (74.124)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9409 (0.8861)	Prec@1 71.094 (74.050)	
Total train loss: 0.8865

Train time: 20.145623445510864
 * Prec@1 59.770 Prec@5 86.790 Loss 1.4717
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 24.227192878723145

Epoch: [3][38/196]	LR: 0.1	Loss 0.7979 (0.7779)	Prec@1 76.953 (76.883)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.9072 (0.7945)	Prec@1 71.484 (76.252)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8237 (0.8243)	Prec@1 74.219 (75.444)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.7861 (0.8393)	Prec@1 75.781 (75.033)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9751 (0.8379)	Prec@1 71.484 (75.152)	
Total train loss: 0.8381

Train time: 18.698787450790405
 * Prec@1 56.000 Prec@5 83.500 Loss 1.7773
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 22.771254062652588

Epoch: [4][38/196]	LR: 0.1	Loss 0.7559 (0.6980)	Prec@1 80.469 (79.818)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7544 (0.7197)	Prec@1 80.078 (78.951)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7388 (0.7381)	Prec@1 82.812 (78.389)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8608 (0.7459)	Prec@1 72.656 (77.980)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.7783 (0.7527)	Prec@1 75.781 (77.708)	
Total train loss: 0.7526

Train time: 20.044586181640625
 * Prec@1 52.220 Prec@5 80.750 Loss 1.9453
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 23.788228511810303

Epoch: [5][38/196]	LR: 0.1	Loss 0.8179 (0.6975)	Prec@1 75.781 (79.407)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6899 (0.6877)	Prec@1 79.297 (79.627)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.6831 (0.6983)	Prec@1 80.078 (79.157)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7207 (0.7023)	Prec@1 74.609 (78.936)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8237 (0.7106)	Prec@1 72.656 (78.744)	
Total train loss: 0.7109

Train time: 18.851176261901855
 * Prec@1 35.210 Prec@5 61.270 Loss 2.9160
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 22.684950828552246

Epoch: [6][38/196]	LR: 0.1	Loss 0.7949 (0.6649)	Prec@1 76.172 (79.928)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.6899 (0.6463)	Prec@1 78.516 (80.198)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6885 (0.6518)	Prec@1 79.688 (80.091)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6338 (0.6518)	Prec@1 82.422 (80.201)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7354 (0.6623)	Prec@1 78.516 (79.784)	
Total train loss: 0.6625

Train time: 17.361275911331177
 * Prec@1 56.100 Prec@5 83.540 Loss 1.7197
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 21.596184730529785

Epoch: [7][38/196]	LR: 0.1	Loss 0.5581 (0.6095)	Prec@1 83.594 (82.272)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.6597 (0.6097)	Prec@1 80.859 (82.026)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6382 (0.6175)	Prec@1 80.859 (81.494)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6338 (0.6169)	Prec@1 78.516 (81.566)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6255 (0.6234)	Prec@1 83.203 (81.328)	
Total train loss: 0.6239

Train time: 19.658609867095947
 * Prec@1 50.650 Prec@5 78.990 Loss 2.4824
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 23.452590227127075

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.4768 (0.5358)	Prec@1 86.328 (83.854)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.4546 (0.5316)	Prec@1 87.109 (84.130)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4158 (0.5245)	Prec@1 89.062 (84.458)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.5215 (0.5212)	Prec@1 84.766 (84.670)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.5068 (0.5244)	Prec@1 83.984 (84.559)	
Total train loss: 0.5246

Train time: 19.890934228897095
 * Prec@1 64.610 Prec@5 89.210 Loss 1.3652
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.213024616241455

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.5073 (0.5273)	Prec@1 85.156 (84.475)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.4778 (0.5251)	Prec@1 87.500 (84.590)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.5254 (0.5245)	Prec@1 83.203 (84.625)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.4771 (0.5216)	Prec@1 86.328 (84.688)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.5039 (0.5207)	Prec@1 85.547 (84.720)	
Total train loss: 0.5209

Train time: 19.90078067779541
 * Prec@1 64.940 Prec@5 89.160 Loss 1.3516
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 24.414286136627197

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.5708 (0.5190)	Prec@1 84.766 (84.866)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.4697 (0.5150)	Prec@1 88.281 (85.111)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.4934 (0.5132)	Prec@1 85.156 (85.053)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.5483 (0.5133)	Prec@1 83.984 (85.056)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5508 (0.5142)	Prec@1 83.984 (85.032)	
Total train loss: 0.5146

Train time: 18.78442096710205
 * Prec@1 60.900 Prec@5 86.380 Loss 1.5977
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.660863399505615

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.5781 (0.5256)	Prec@1 82.812 (84.685)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.4060 (0.5222)	Prec@1 90.234 (84.665)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.4863 (0.5312)	Prec@1 87.109 (84.415)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.5317 (0.5322)	Prec@1 84.375 (84.413)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.4922 (0.5372)	Prec@1 85.938 (84.187)	
Total train loss: 0.5376

Train time: 19.77566623687744
 * Prec@1 64.440 Prec@5 88.680 Loss 1.3828
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 24.088032484054565

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.5127 (0.5406)	Prec@1 85.547 (84.175)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5010 (0.5412)	Prec@1 83.984 (83.929)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5596 (0.5450)	Prec@1 83.203 (83.868)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.4529 (0.5461)	Prec@1 86.328 (83.819)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5459 (0.5459)	Prec@1 84.766 (83.804)	
Total train loss: 0.5459

Train time: 18.86331057548523
 * Prec@1 64.160 Prec@5 88.850 Loss 1.3828
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.014464616775513

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.5518 (0.5390)	Prec@1 80.859 (83.864)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5811 (0.5373)	Prec@1 83.203 (84.019)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.5405 (0.5346)	Prec@1 84.375 (84.201)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.4785 (0.5368)	Prec@1 85.547 (84.092)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.5811 (0.5377)	Prec@1 81.641 (84.127)	
Total train loss: 0.5379

Train time: 19.443984985351562
 * Prec@1 64.620 Prec@5 88.790 Loss 1.3701
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.31257915496826

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.4417 (0.5476)	Prec@1 86.719 (83.874)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.5352 (0.5403)	Prec@1 83.984 (83.979)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.4961 (0.5333)	Prec@1 86.719 (84.201)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.5400 (0.5343)	Prec@1 83.594 (84.205)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.5415 (0.5345)	Prec@1 85.547 (84.203)	
Total train loss: 0.5348

Train time: 19.04843807220459
 * Prec@1 64.320 Prec@5 88.720 Loss 1.3857
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.05310034751892

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.5229 (0.5481)	Prec@1 83.984 (83.924)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.4971 (0.5397)	Prec@1 85.938 (84.120)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.4856 (0.5370)	Prec@1 85.547 (84.158)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.6724 (0.5391)	Prec@1 80.859 (84.054)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.5049 (0.5443)	Prec@1 85.547 (83.848)	
Total train loss: 0.5444

Train time: 18.60478711128235
 * Prec@1 64.560 Prec@5 88.810 Loss 1.3799
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.699159860610962

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.4807 (0.5302)	Prec@1 87.500 (84.375)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.5425 (0.5392)	Prec@1 83.594 (84.150)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.5537 (0.5384)	Prec@1 85.547 (84.168)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.4824 (0.5372)	Prec@1 84.375 (84.145)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.4644 (0.5401)	Prec@1 85.547 (84.089)	
Total train loss: 0.5403

Train time: 19.113208770751953
 * Prec@1 64.270 Prec@5 88.620 Loss 1.3955
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.714064598083496

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.5054 (0.5355)	Prec@1 85.938 (84.225)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.5571 (0.5354)	Prec@1 80.859 (84.225)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.5332 (0.5354)	Prec@1 83.984 (84.138)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.5835 (0.5388)	Prec@1 79.688 (83.957)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.4685 (0.5425)	Prec@1 87.109 (83.878)	
Total train loss: 0.5428

Train time: 20.004304885864258
 * Prec@1 64.240 Prec@5 88.650 Loss 1.3936
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 24.079333066940308

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.6890 (0.5415)	Prec@1 80.078 (83.824)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.6040 (0.5436)	Prec@1 78.516 (83.904)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.5400 (0.5448)	Prec@1 82.812 (83.864)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5239 (0.5413)	Prec@1 86.328 (84.039)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.5215 (0.5404)	Prec@1 85.938 (83.970)	
Total train loss: 0.5410

Train time: 18.892200708389282
 * Prec@1 64.130 Prec@5 88.450 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.90108823776245

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.4780 (0.5395)	Prec@1 88.281 (83.924)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.4790 (0.5403)	Prec@1 85.938 (84.070)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.4434 (0.5429)	Prec@1 86.328 (83.914)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.4695 (0.5420)	Prec@1 87.109 (83.977)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.5420 (0.5415)	Prec@1 85.547 (84.010)	
Total train loss: 0.5416

Train time: 18.07313299179077
 * Prec@1 64.210 Prec@5 88.590 Loss 1.3926
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 21.422036170959473

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.5420 (0.5290)	Prec@1 84.766 (84.435)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.5322 (0.5371)	Prec@1 83.984 (84.085)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.5928 (0.5370)	Prec@1 83.594 (84.065)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.5562 (0.5426)	Prec@1 85.156 (83.867)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.6860 (0.5406)	Prec@1 79.297 (84.018)	
Total train loss: 0.5407

Train time: 17.676767110824585
 * Prec@1 64.160 Prec@5 88.720 Loss 1.3965
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 21.773530960083008

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.5308 (0.5461)	Prec@1 83.984 (83.644)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.5210 (0.5401)	Prec@1 85.156 (83.744)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.4939 (0.5430)	Prec@1 85.938 (83.861)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.5703 (0.5411)	Prec@1 83.203 (83.977)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.5127 (0.5399)	Prec@1 84.766 (84.030)	
Total train loss: 0.5399

Train time: 18.79687809944153
 * Prec@1 64.290 Prec@5 88.660 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.762600660324097

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.5439 (0.5434)	Prec@1 81.250 (83.814)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.5322 (0.5383)	Prec@1 83.203 (84.165)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5781 (0.5412)	Prec@1 83.203 (84.014)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.4619 (0.5411)	Prec@1 85.156 (84.042)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.5532 (0.5416)	Prec@1 81.641 (84.036)	
Total train loss: 0.5416

Train time: 18.57188105583191
 * Prec@1 64.450 Prec@5 88.710 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.0117449760437

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.4817 (0.5533)	Prec@1 87.891 (83.353)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.4980 (0.5458)	Prec@1 87.109 (83.754)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.4978 (0.5433)	Prec@1 86.328 (83.834)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.4651 (0.5424)	Prec@1 86.328 (83.867)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.4727 (0.5408)	Prec@1 85.156 (83.880)	
Total train loss: 0.5407

Train time: 18.907665729522705
 * Prec@1 64.240 Prec@5 88.630 Loss 1.3975
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.91388773918152

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.5464 (0.5371)	Prec@1 82.422 (84.265)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.5728 (0.5378)	Prec@1 82.812 (84.235)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.5186 (0.5374)	Prec@1 83.984 (84.151)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.5488 (0.5363)	Prec@1 82.422 (84.267)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.5435 (0.5395)	Prec@1 83.203 (84.159)	
Total train loss: 0.5394

Train time: 19.95528554916382
 * Prec@1 64.340 Prec@5 88.770 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 24.31297755241394

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.5068 (0.5511)	Prec@1 87.109 (83.694)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.5527 (0.5450)	Prec@1 85.938 (83.984)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.4937 (0.5413)	Prec@1 84.375 (83.948)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.4988 (0.5398)	Prec@1 84.766 (83.949)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5693 (0.5419)	Prec@1 85.547 (83.920)	
Total train loss: 0.5419

Train time: 17.826520204544067
 * Prec@1 64.300 Prec@5 88.700 Loss 1.3975
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 21.188772678375244

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.4722 (0.5354)	Prec@1 87.500 (83.874)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.5132 (0.5421)	Prec@1 85.156 (83.909)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.5024 (0.5395)	Prec@1 86.328 (84.075)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.6265 (0.5394)	Prec@1 83.203 (83.994)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.4893 (0.5405)	Prec@1 84.766 (84.018)	
Total train loss: 0.5407

Train time: 18.064462900161743
 * Prec@1 64.210 Prec@5 88.640 Loss 1.4004
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.161532878875732

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.5156 (0.5399)	Prec@1 86.719 (83.984)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.4993 (0.5403)	Prec@1 85.938 (83.754)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.5527 (0.5399)	Prec@1 82.031 (83.764)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.5034 (0.5387)	Prec@1 83.984 (83.877)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.5830 (0.5402)	Prec@1 82.422 (83.860)	
Total train loss: 0.5404

Train time: 18.81180214881897
 * Prec@1 64.210 Prec@5 88.670 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.06099534034729

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.5474 (0.5327)	Prec@1 82.422 (84.115)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.5449 (0.5339)	Prec@1 83.984 (84.285)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.5757 (0.5367)	Prec@1 83.203 (84.175)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.5195 (0.5420)	Prec@1 83.984 (83.864)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.5747 (0.5429)	Prec@1 87.109 (83.978)	
Total train loss: 0.5433

Train time: 18.70729637145996
 * Prec@1 64.230 Prec@5 88.690 Loss 1.3936
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.05024743080139

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.5981 (0.5239)	Prec@1 81.641 (84.615)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.4231 (0.5313)	Prec@1 86.328 (84.190)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.5049 (0.5373)	Prec@1 87.500 (84.095)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.4541 (0.5380)	Prec@1 86.719 (84.062)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.4854 (0.5407)	Prec@1 85.156 (83.970)	
Total train loss: 0.5410

Train time: 19.610021114349365
 * Prec@1 64.250 Prec@5 88.710 Loss 1.3916
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.709945678710938

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.5083 (0.5375)	Prec@1 85.156 (84.305)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.5981 (0.5398)	Prec@1 83.984 (84.105)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.4214 (0.5396)	Prec@1 87.109 (84.008)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.5576 (0.5413)	Prec@1 82.031 (83.984)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.5376 (0.5419)	Prec@1 85.547 (83.974)	
Total train loss: 0.5423

Train time: 18.564045429229736
 * Prec@1 64.300 Prec@5 88.740 Loss 1.3945
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.65187978744507

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.4355 (0.5571)	Prec@1 86.328 (83.724)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.5034 (0.5462)	Prec@1 84.766 (83.854)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.6201 (0.5450)	Prec@1 81.641 (83.774)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.5996 (0.5446)	Prec@1 80.078 (83.794)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.5444 (0.5442)	Prec@1 85.156 (83.832)	
Total train loss: 0.5444

Train time: 20.39973020553589
 * Prec@1 64.190 Prec@5 88.630 Loss 1.3965
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 24.04651713371277

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.4868 (0.5463)	Prec@1 87.891 (83.824)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.4336 (0.5417)	Prec@1 86.719 (83.879)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.5791 (0.5411)	Prec@1 83.984 (83.898)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.5723 (0.5405)	Prec@1 83.984 (84.054)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.5488 (0.5411)	Prec@1 82.422 (83.914)	
Total train loss: 0.5411

Train time: 19.56941318511963
 * Prec@1 64.310 Prec@5 88.590 Loss 1.3965
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.620094060897827

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.5015 (0.5511)	Prec@1 83.203 (83.293)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.5645 (0.5491)	Prec@1 82.812 (83.489)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.5171 (0.5517)	Prec@1 85.156 (83.510)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.5293 (0.5456)	Prec@1 83.594 (83.762)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.4790 (0.5443)	Prec@1 85.156 (83.828)	
Total train loss: 0.5443

Train time: 19.84244155883789
 * Prec@1 64.340 Prec@5 88.770 Loss 1.3906
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.750460147857666

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.4531 (0.5284)	Prec@1 87.500 (84.685)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.6157 (0.5387)	Prec@1 80.078 (84.160)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.5806 (0.5387)	Prec@1 81.250 (84.198)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.6265 (0.5406)	Prec@1 82.031 (84.062)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.4326 (0.5407)	Prec@1 87.500 (84.012)	
Total train loss: 0.5410

Train time: 19.597425937652588
 * Prec@1 64.370 Prec@5 88.710 Loss 1.3926
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.97344136238098

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.5068 (0.5392)	Prec@1 87.891 (84.105)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.6289 (0.5403)	Prec@1 81.641 (84.125)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.5044 (0.5414)	Prec@1 84.766 (84.024)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.5967 (0.5429)	Prec@1 82.422 (84.047)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.5503 (0.5404)	Prec@1 83.203 (84.127)	
Total train loss: 0.5403

Train time: 19.518536806106567
 * Prec@1 64.450 Prec@5 88.610 Loss 1.3945
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.352454662322998

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.4983 (0.5541)	Prec@1 86.328 (83.544)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.5396 (0.5395)	Prec@1 82.812 (83.914)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.5229 (0.5401)	Prec@1 85.156 (84.008)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.4629 (0.5436)	Prec@1 84.766 (83.959)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.5503 (0.5421)	Prec@1 83.203 (83.968)	
Total train loss: 0.5425

Train time: 18.627821683883667
 * Prec@1 64.390 Prec@5 88.820 Loss 1.3906
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.463770627975464

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.5083 (0.5444)	Prec@1 87.891 (84.054)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.5508 (0.5430)	Prec@1 82.031 (83.899)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.4583 (0.5458)	Prec@1 86.719 (83.671)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.4236 (0.5456)	Prec@1 87.891 (83.661)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.5044 (0.5420)	Prec@1 83.594 (83.854)	
Total train loss: 0.5423

Train time: 18.670288801193237
 * Prec@1 64.360 Prec@5 88.860 Loss 1.3994
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 21.702454805374146

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.6123 (0.5497)	Prec@1 81.250 (83.413)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.5581 (0.5432)	Prec@1 82.812 (83.844)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.4238 (0.5431)	Prec@1 88.281 (83.891)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.4958 (0.5400)	Prec@1 83.594 (84.009)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.4697 (0.5385)	Prec@1 85.938 (84.083)	
Total train loss: 0.5387

Train time: 19.18770432472229
 * Prec@1 64.200 Prec@5 88.740 Loss 1.4033
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 23.124802112579346

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.6372 (0.5364)	Prec@1 80.078 (83.914)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.5518 (0.5346)	Prec@1 82.422 (84.105)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.5127 (0.5396)	Prec@1 86.328 (83.898)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.5596 (0.5392)	Prec@1 83.594 (83.837)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.6572 (0.5407)	Prec@1 76.562 (83.902)	
Total train loss: 0.5409

Train time: 18.508957624435425
 * Prec@1 64.160 Prec@5 88.830 Loss 1.3926
Best acc: 64.940
--------------------------------------------------------------------------------
Test time: 22.984205722808838


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 47.810 Prec@5 75.630 Loss 2.3027
Pre-trained Prec@1 with 15 layers frozen: 47.80999755859375 	 Loss: 2.302734375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.2842 (1.3165)	Prec@1 61.328 (62.881)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3135 (1.3255)	Prec@1 62.891 (62.375)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.1426 (1.3053)	Prec@1 67.578 (62.944)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1484 (1.2809)	Prec@1 65.234 (63.517)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2295 (1.2676)	Prec@1 64.453 (63.750)	
Total train loss: 1.2670

Train time: 439.8855314254761
 * Prec@1 56.690 Prec@5 84.810 Loss 1.6299
Best acc: 56.690
--------------------------------------------------------------------------------
Test time: 443.941447019577

Epoch: [1][38/196]	LR: 0.1	Loss 0.9160 (1.0191)	Prec@1 72.656 (70.623)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.9995 (1.0348)	Prec@1 69.531 (70.172)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.9736 (1.0331)	Prec@1 71.484 (70.202)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1250 (1.0309)	Prec@1 65.625 (70.212)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.1748 (1.0299)	Prec@1 65.625 (70.166)	
Total train loss: 1.0297

Train time: 20.6543447971344
 * Prec@1 58.230 Prec@5 85.320 Loss 1.5908
Best acc: 58.230
--------------------------------------------------------------------------------
Test time: 24.36566972732544

Epoch: [2][38/196]	LR: 0.1	Loss 0.8257 (0.8917)	Prec@1 75.391 (73.538)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0557 (0.9089)	Prec@1 66.797 (72.992)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9355 (0.9266)	Prec@1 72.266 (72.543)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.9951 (0.9317)	Prec@1 68.750 (72.396)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9346 (0.9366)	Prec@1 73.828 (72.322)	
Total train loss: 0.9368

Train time: 18.090182781219482
 * Prec@1 56.530 Prec@5 83.700 Loss 1.7744
Best acc: 58.230
--------------------------------------------------------------------------------
Test time: 21.63635516166687

Epoch: [3][38/196]	LR: 0.1	Loss 0.8271 (0.8128)	Prec@1 78.516 (76.202)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.0605 (0.8310)	Prec@1 70.312 (75.441)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.7271 (0.8413)	Prec@1 76.953 (75.140)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9102 (0.8514)	Prec@1 73.828 (74.735)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9697 (0.8582)	Prec@1 72.266 (74.517)	
Total train loss: 0.8581

Train time: 17.26547408103943
 * Prec@1 57.420 Prec@5 84.770 Loss 1.7021
Best acc: 58.230
--------------------------------------------------------------------------------
Test time: 21.546927213668823

Epoch: [4][38/196]	LR: 0.1	Loss 0.7529 (0.7602)	Prec@1 76.562 (77.163)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8892 (0.7804)	Prec@1 75.000 (76.753)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8428 (0.7942)	Prec@1 75.391 (76.325)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8389 (0.8055)	Prec@1 76.953 (75.969)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8633 (0.8311)	Prec@1 71.875 (75.176)	
Total train loss: 0.8312

Train time: 17.250622510910034
 * Prec@1 58.640 Prec@5 85.110 Loss 1.6191
Best acc: 58.640
--------------------------------------------------------------------------------
Test time: 20.701763153076172

Epoch: [5][38/196]	LR: 0.1	Loss 0.7651 (0.7551)	Prec@1 76.953 (77.865)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.8374 (0.7728)	Prec@1 76.953 (77.058)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7993 (0.7751)	Prec@1 74.219 (76.816)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7188 (0.7801)	Prec@1 78.516 (76.600)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8525 (0.7859)	Prec@1 74.219 (76.390)	
Total train loss: 0.7861

Train time: 18.42625641822815
 * Prec@1 55.900 Prec@5 82.470 Loss 1.7695
Best acc: 58.640
--------------------------------------------------------------------------------
Test time: 22.561429500579834

Epoch: [6][38/196]	LR: 0.1	Loss 0.6851 (0.6985)	Prec@1 78.906 (79.287)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.8105 (0.7065)	Prec@1 73.438 (78.846)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.9702 (0.7224)	Prec@1 71.875 (78.322)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7505 (0.7299)	Prec@1 78.125 (78.050)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.8066 (0.7473)	Prec@1 76.953 (77.436)	
Total train loss: 0.7475

Train time: 18.85060214996338
 * Prec@1 44.470 Prec@5 73.300 Loss 2.5605
Best acc: 58.640
--------------------------------------------------------------------------------
Test time: 22.77608060836792

Epoch: [7][38/196]	LR: 0.1	Loss 0.6289 (0.6530)	Prec@1 80.859 (80.769)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.7266 (0.6643)	Prec@1 80.078 (80.303)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6660 (0.7141)	Prec@1 78.125 (78.703)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.8237 (0.7324)	Prec@1 72.266 (77.987)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.8525 (0.7491)	Prec@1 76.953 (77.454)	
Total train loss: 0.7491

Train time: 18.42648482322693
 * Prec@1 57.090 Prec@5 84.630 Loss 1.6621
Best acc: 58.640
--------------------------------------------------------------------------------
Test time: 22.38578963279724

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.5986 (0.6372)	Prec@1 82.422 (80.709)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.5088 (0.6362)	Prec@1 84.375 (80.874)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.6250 (0.6199)	Prec@1 79.297 (81.364)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.5864 (0.6176)	Prec@1 81.641 (81.498)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.7515 (0.6162)	Prec@1 75.391 (81.510)	
Total train loss: 0.6164

Train time: 17.254316806793213
 * Prec@1 64.430 Prec@5 88.760 Loss 1.3779
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 20.665509939193726

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.6860 (0.5991)	Prec@1 77.734 (82.011)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.5913 (0.5951)	Prec@1 81.641 (82.106)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.5469 (0.5966)	Prec@1 83.594 (82.178)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.6367 (0.6000)	Prec@1 81.641 (82.131)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.7051 (0.6015)	Prec@1 80.469 (82.063)	
Total train loss: 0.6015

Train time: 18.049748420715332
 * Prec@1 64.430 Prec@5 88.910 Loss 1.3721
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.487139225006104

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.5542 (0.5951)	Prec@1 84.375 (82.402)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.5825 (0.5960)	Prec@1 83.594 (82.171)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.4751 (0.5993)	Prec@1 85.938 (82.018)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.5933 (0.5918)	Prec@1 82.812 (82.367)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5186 (0.5928)	Prec@1 83.984 (82.348)	
Total train loss: 0.5930

Train time: 17.81313443183899
 * Prec@1 64.430 Prec@5 88.640 Loss 1.3838
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.275632858276367

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.5552 (0.5776)	Prec@1 82.422 (82.993)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.7168 (0.5825)	Prec@1 77.734 (82.652)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.5918 (0.5887)	Prec@1 83.203 (82.522)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.5811 (0.5855)	Prec@1 82.031 (82.532)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.6401 (0.5876)	Prec@1 81.250 (82.460)	
Total train loss: 0.5878

Train time: 19.043888092041016
 * Prec@1 64.290 Prec@5 88.710 Loss 1.3799
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 23.010541915893555

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.4431 (0.5852)	Prec@1 86.719 (82.442)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5781 (0.5867)	Prec@1 82.812 (82.337)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5059 (0.5856)	Prec@1 84.375 (82.395)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.5713 (0.5828)	Prec@1 82.031 (82.432)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5347 (0.5857)	Prec@1 84.375 (82.326)	
Total train loss: 0.5863

Train time: 17.96537137031555
 * Prec@1 64.280 Prec@5 88.680 Loss 1.4004
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.140337228775024

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.5747 (0.5838)	Prec@1 82.422 (82.422)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5781 (0.5854)	Prec@1 83.203 (82.392)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.5620 (0.5868)	Prec@1 82.812 (82.432)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.7051 (0.5870)	Prec@1 78.516 (82.447)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.6255 (0.5855)	Prec@1 80.469 (82.490)	
Total train loss: 0.5857

Train time: 18.434640645980835
 * Prec@1 64.230 Prec@5 88.750 Loss 1.4043
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.479851007461548

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.7725 (0.5872)	Prec@1 73.828 (82.332)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.5117 (0.5848)	Prec@1 85.938 (82.477)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.6147 (0.5868)	Prec@1 80.469 (82.469)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.6304 (0.5853)	Prec@1 77.344 (82.497)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.5249 (0.5868)	Prec@1 82.031 (82.430)	
Total train loss: 0.5875

Train time: 19.484973192214966
 * Prec@1 64.320 Prec@5 88.420 Loss 1.4004
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.86093282699585

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.4163 (0.5745)	Prec@1 88.672 (82.632)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.6777 (0.5797)	Prec@1 81.250 (82.587)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.6445 (0.5803)	Prec@1 78.516 (82.595)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.6143 (0.5800)	Prec@1 81.641 (82.597)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.6548 (0.5851)	Prec@1 78.125 (82.432)	
Total train loss: 0.5849

Train time: 17.227173566818237
 * Prec@1 63.740 Prec@5 88.120 Loss 1.4219
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.596242666244507

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.5386 (0.5985)	Prec@1 84.766 (81.821)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.5381 (0.5896)	Prec@1 81.641 (82.292)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.4709 (0.5891)	Prec@1 85.938 (82.248)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.5669 (0.5852)	Prec@1 81.641 (82.257)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.7075 (0.5840)	Prec@1 78.906 (82.392)	
Total train loss: 0.5839

Train time: 16.881131410598755
 * Prec@1 63.800 Prec@5 88.300 Loss 1.4248
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 19.96031093597412

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.5205 (0.5812)	Prec@1 84.766 (82.372)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.5391 (0.5810)	Prec@1 81.250 (82.457)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.4729 (0.5849)	Prec@1 85.547 (82.405)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.6206 (0.5885)	Prec@1 82.812 (82.297)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.5503 (0.5848)	Prec@1 85.547 (82.358)	
Total train loss: 0.5848

Train time: 17.821027040481567
 * Prec@1 63.880 Prec@5 88.300 Loss 1.4238
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.806474208831787

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.5430 (0.5840)	Prec@1 84.375 (82.622)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.5933 (0.5807)	Prec@1 79.297 (82.652)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.5239 (0.5820)	Prec@1 87.891 (82.582)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.6895 (0.5802)	Prec@1 80.859 (82.670)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.6255 (0.5815)	Prec@1 79.688 (82.642)	
Total train loss: 0.5819

Train time: 18.438220977783203
 * Prec@1 63.910 Prec@5 88.390 Loss 1.4287
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.286490440368652

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.5854 (0.5894)	Prec@1 82.031 (82.512)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.5239 (0.5888)	Prec@1 84.375 (82.512)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.6782 (0.5830)	Prec@1 80.078 (82.535)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.5508 (0.5809)	Prec@1 84.375 (82.605)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.6104 (0.5806)	Prec@1 78.906 (82.622)	
Total train loss: 0.5809

Train time: 16.738504886627197
 * Prec@1 63.780 Prec@5 88.400 Loss 1.4199
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 20.69946837425232

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.6265 (0.5928)	Prec@1 80.859 (81.971)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.5020 (0.5879)	Prec@1 86.719 (82.217)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.5342 (0.5852)	Prec@1 85.156 (82.372)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.6245 (0.5850)	Prec@1 82.031 (82.409)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.4644 (0.5835)	Prec@1 86.719 (82.494)	
Total train loss: 0.5835

Train time: 17.066424131393433
 * Prec@1 63.980 Prec@5 88.180 Loss 1.4287
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 20.37932777404785

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.5864 (0.5807)	Prec@1 83.984 (82.532)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.5601 (0.5839)	Prec@1 82.422 (82.457)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.6797 (0.5862)	Prec@1 80.469 (82.409)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.5210 (0.5815)	Prec@1 83.594 (82.555)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.5947 (0.5821)	Prec@1 80.469 (82.534)	
Total train loss: 0.5821

Train time: 18.836861610412598
 * Prec@1 63.740 Prec@5 88.320 Loss 1.4189
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 23.345135927200317

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.5776 (0.5862)	Prec@1 82.422 (82.812)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.6772 (0.5932)	Prec@1 78.906 (82.492)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5518 (0.5909)	Prec@1 80.859 (82.425)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.7012 (0.5883)	Prec@1 78.516 (82.444)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.4944 (0.5843)	Prec@1 85.547 (82.562)	
Total train loss: 0.5846

Train time: 18.21448278427124
 * Prec@1 63.780 Prec@5 88.180 Loss 1.4229
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.741631031036377

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.6094 (0.5766)	Prec@1 82.812 (82.652)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.6968 (0.5833)	Prec@1 78.906 (82.412)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.4983 (0.5852)	Prec@1 85.547 (82.288)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.6143 (0.5828)	Prec@1 81.641 (82.387)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.5781 (0.5811)	Prec@1 81.250 (82.458)	
Total train loss: 0.5812

Train time: 18.97595453262329
 * Prec@1 63.880 Prec@5 88.290 Loss 1.4180
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.799976348876953

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.6343 (0.5869)	Prec@1 80.859 (82.272)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.5674 (0.5885)	Prec@1 81.641 (82.312)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.5444 (0.5836)	Prec@1 84.375 (82.412)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.5283 (0.5827)	Prec@1 83.984 (82.369)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.6172 (0.5823)	Prec@1 82.812 (82.390)	
Total train loss: 0.5827

Train time: 19.140998125076294
 * Prec@1 63.930 Prec@5 88.390 Loss 1.4209
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 23.272376537322998

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.6523 (0.5914)	Prec@1 79.297 (82.372)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.5898 (0.5901)	Prec@1 84.375 (82.557)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.5063 (0.5898)	Prec@1 82.422 (82.575)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.5098 (0.5831)	Prec@1 85.156 (82.657)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5601 (0.5805)	Prec@1 85.547 (82.638)	
Total train loss: 0.5810

Train time: 18.9071261882782
 * Prec@1 63.680 Prec@5 88.440 Loss 1.4229
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.841875791549683

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.7119 (0.6019)	Prec@1 78.906 (82.071)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.5264 (0.5835)	Prec@1 85.156 (82.382)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.5718 (0.5865)	Prec@1 83.984 (82.215)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.6128 (0.5829)	Prec@1 81.641 (82.377)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.5352 (0.5817)	Prec@1 83.984 (82.444)	
Total train loss: 0.5819

Train time: 18.732296466827393
 * Prec@1 63.840 Prec@5 88.370 Loss 1.4229
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.015904664993286

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.5586 (0.5794)	Prec@1 83.984 (82.362)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.7134 (0.5885)	Prec@1 78.906 (82.146)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.5537 (0.5859)	Prec@1 83.594 (82.292)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.5898 (0.5810)	Prec@1 82.812 (82.510)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.5923 (0.5809)	Prec@1 82.422 (82.490)	
Total train loss: 0.5810

Train time: 16.774418115615845
 * Prec@1 63.900 Prec@5 88.280 Loss 1.4180
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.403151035308838

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.5864 (0.5856)	Prec@1 81.250 (82.682)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.5728 (0.5869)	Prec@1 83.594 (82.572)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.6226 (0.5849)	Prec@1 82.031 (82.515)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.4441 (0.5843)	Prec@1 85.938 (82.467)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.4841 (0.5815)	Prec@1 87.109 (82.608)	
Total train loss: 0.5816

Train time: 17.93349051475525
 * Prec@1 63.910 Prec@5 88.410 Loss 1.4229
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.330749988555908

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.4995 (0.5799)	Prec@1 84.766 (82.622)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.6753 (0.5814)	Prec@1 78.125 (82.667)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.5664 (0.5819)	Prec@1 80.078 (82.609)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.4961 (0.5788)	Prec@1 84.375 (82.700)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.6162 (0.5811)	Prec@1 82.422 (82.570)	
Total train loss: 0.5814

Train time: 19.089787483215332
 * Prec@1 63.970 Prec@5 88.470 Loss 1.4219
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 23.117616653442383

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.5845 (0.5802)	Prec@1 83.594 (82.622)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.5732 (0.5754)	Prec@1 82.422 (82.582)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.5244 (0.5812)	Prec@1 85.547 (82.415)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.6284 (0.5822)	Prec@1 81.641 (82.464)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.5557 (0.5824)	Prec@1 83.594 (82.538)	
Total train loss: 0.5826

Train time: 17.531922578811646
 * Prec@1 63.640 Prec@5 88.370 Loss 1.4258
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.474923849105835

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.6162 (0.5810)	Prec@1 82.812 (82.572)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.7598 (0.5762)	Prec@1 76.953 (82.732)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.5352 (0.5757)	Prec@1 81.641 (82.782)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.5454 (0.5782)	Prec@1 81.641 (82.687)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.5464 (0.5810)	Prec@1 83.984 (82.652)	
Total train loss: 0.5809

Train time: 16.833308458328247
 * Prec@1 63.940 Prec@5 88.390 Loss 1.4150
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.029316663742065

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.5586 (0.5828)	Prec@1 82.812 (82.332)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.5122 (0.5867)	Prec@1 83.984 (82.282)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.5029 (0.5813)	Prec@1 85.156 (82.442)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.4954 (0.5824)	Prec@1 85.938 (82.482)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.5444 (0.5840)	Prec@1 82.812 (82.422)	
Total train loss: 0.5840

Train time: 18.368374347686768
 * Prec@1 63.910 Prec@5 88.390 Loss 1.4199
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.607341289520264

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.6255 (0.5763)	Prec@1 80.469 (82.562)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.7910 (0.5881)	Prec@1 73.438 (82.287)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.5278 (0.5842)	Prec@1 83.203 (82.545)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.5884 (0.5851)	Prec@1 82.812 (82.520)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.5864 (0.5829)	Prec@1 80.078 (82.686)	
Total train loss: 0.5830

Train time: 16.183817863464355
 * Prec@1 64.170 Prec@5 88.450 Loss 1.4258
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 20.846091985702515

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.6592 (0.5912)	Prec@1 80.078 (82.242)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.6079 (0.5910)	Prec@1 81.250 (82.021)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.5532 (0.5813)	Prec@1 82.422 (82.442)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.6660 (0.5845)	Prec@1 78.906 (82.364)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.5542 (0.5839)	Prec@1 83.984 (82.442)	
Total train loss: 0.5837

Train time: 18.590375661849976
 * Prec@1 63.750 Prec@5 88.260 Loss 1.4219
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 22.00716495513916

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.5654 (0.5815)	Prec@1 84.766 (83.033)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.5903 (0.5815)	Prec@1 79.688 (82.893)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.5400 (0.5831)	Prec@1 83.594 (82.749)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.5454 (0.5816)	Prec@1 84.375 (82.825)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.4871 (0.5819)	Prec@1 84.766 (82.722)	
Total train loss: 0.5821

Train time: 17.851852893829346
 * Prec@1 63.800 Prec@5 88.310 Loss 1.4297
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.765517234802246

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.6304 (0.5741)	Prec@1 79.688 (82.542)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.5845 (0.5819)	Prec@1 83.203 (82.457)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.4951 (0.5856)	Prec@1 85.156 (82.285)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.5542 (0.5846)	Prec@1 80.859 (82.347)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.5801 (0.5823)	Prec@1 81.641 (82.458)	
Total train loss: 0.5829

Train time: 17.210571765899658
 * Prec@1 63.690 Prec@5 88.320 Loss 1.4229
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.897377014160156

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.6030 (0.6005)	Prec@1 80.469 (81.871)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.6904 (0.5856)	Prec@1 80.469 (82.232)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.4822 (0.5842)	Prec@1 87.500 (82.345)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.5332 (0.5850)	Prec@1 83.984 (82.342)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.5356 (0.5833)	Prec@1 84.766 (82.394)	
Total train loss: 0.5833

Train time: 17.40926742553711
 * Prec@1 63.730 Prec@5 88.330 Loss 1.4180
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.434409141540527

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.6230 (0.5909)	Prec@1 82.422 (82.462)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.6240 (0.5856)	Prec@1 82.031 (82.607)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.5195 (0.5875)	Prec@1 86.719 (82.409)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.5337 (0.5871)	Prec@1 83.594 (82.424)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.5342 (0.5838)	Prec@1 85.938 (82.456)	
Total train loss: 0.5841

Train time: 17.939408779144287
 * Prec@1 63.690 Prec@5 88.400 Loss 1.4170
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 21.13912582397461

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.6992 (0.5670)	Prec@1 79.297 (83.243)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.5386 (0.5770)	Prec@1 82.812 (82.757)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.6279 (0.5826)	Prec@1 82.031 (82.732)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.5967 (0.5844)	Prec@1 80.859 (82.622)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.5107 (0.5819)	Prec@1 85.938 (82.708)	
Total train loss: 0.5820

Train time: 23.8038227558136
 * Prec@1 63.730 Prec@5 88.390 Loss 1.4248
Best acc: 64.430
--------------------------------------------------------------------------------
Test time: 31.03753924369812


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 42.980 Prec@5 72.450 Loss 2.6445
Pre-trained Prec@1 with 17 layers frozen: 42.97999954223633 	 Loss: 2.64453125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.3271 (1.3186)	Prec@1 60.938 (63.131)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.2656 (1.2767)	Prec@1 66.797 (64.123)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.1084 (1.2470)	Prec@1 71.875 (64.747)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.0674 (1.2352)	Prec@1 66.797 (64.921)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.0742 (1.2139)	Prec@1 69.531 (65.497)	
Total train loss: 1.2137

Train time: 189.42379093170166
 * Prec@1 58.150 Prec@5 85.370 Loss 1.6279
Best acc: 58.150
--------------------------------------------------------------------------------
Test time: 193.41141271591187

Epoch: [1][38/196]	LR: 0.1	Loss 0.8579 (1.0138)	Prec@1 75.391 (70.783)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0176 (1.0065)	Prec@1 71.094 (70.708)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.0361 (1.0071)	Prec@1 69.531 (70.760)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1055 (1.0099)	Prec@1 68.359 (70.756)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0029 (1.0132)	Prec@1 71.875 (70.587)	
Total train loss: 1.0134

Train time: 18.250534534454346
 * Prec@1 59.700 Prec@5 85.390 Loss 1.5137
Best acc: 59.700
--------------------------------------------------------------------------------
Test time: 22.081061124801636

Epoch: [2][38/196]	LR: 0.1	Loss 0.9526 (0.9291)	Prec@1 71.094 (72.546)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0508 (0.9245)	Prec@1 67.969 (72.746)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9810 (0.9345)	Prec@1 72.266 (72.616)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.8657 (0.9440)	Prec@1 75.000 (72.321)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9746 (0.9441)	Prec@1 69.922 (72.288)	
Total train loss: 0.9444

Train time: 19.338732719421387
 * Prec@1 62.250 Prec@5 87.260 Loss 1.4395
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 22.538525819778442

Epoch: [3][38/196]	LR: 0.1	Loss 0.7603 (0.8737)	Prec@1 77.344 (74.379)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8613 (0.8850)	Prec@1 73.047 (73.783)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9673 (0.8851)	Prec@1 71.875 (73.811)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9277 (0.8892)	Prec@1 73.828 (73.690)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.7393 (0.8902)	Prec@1 78.125 (73.764)	
Total train loss: 0.8903

Train time: 15.530358791351318
 * Prec@1 61.310 Prec@5 86.680 Loss 1.5244
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 19.820464849472046

Epoch: [4][38/196]	LR: 0.1	Loss 0.8052 (0.8295)	Prec@1 73.438 (75.611)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8672 (0.8528)	Prec@1 75.781 (75.110)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7456 (0.8491)	Prec@1 79.297 (75.040)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8320 (0.8467)	Prec@1 75.000 (75.060)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.7549 (0.8498)	Prec@1 76.953 (74.936)	
Total train loss: 0.8500

Train time: 18.748395204544067
 * Prec@1 59.150 Prec@5 86.000 Loss 1.6387
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 22.10598611831665

Epoch: [5][38/196]	LR: 0.1	Loss 0.7690 (0.8330)	Prec@1 78.516 (75.391)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.9248 (0.8271)	Prec@1 72.266 (75.496)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.8184 (0.8336)	Prec@1 74.609 (75.427)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.8267 (0.8324)	Prec@1 77.344 (75.338)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8896 (0.8318)	Prec@1 73.047 (75.425)	
Total train loss: 0.8326

Train time: 17.28585910797119
 * Prec@1 60.270 Prec@5 86.230 Loss 1.5830
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 21.632384777069092

Epoch: [6][38/196]	LR: 0.1	Loss 0.7515 (0.7629)	Prec@1 76.172 (77.063)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.6431 (0.7673)	Prec@1 80.078 (77.143)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.7847 (0.7845)	Prec@1 77.344 (76.506)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.8979 (0.7901)	Prec@1 69.531 (76.390)	
Epoch: [6][194/196]	LR: 0.1	Loss 1.0273 (0.7943)	Prec@1 69.141 (76.290)	
Total train loss: 0.7950

Train time: 17.726545810699463
 * Prec@1 53.950 Prec@5 81.430 Loss 2.4551
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 21.501880168914795

Epoch: [7][38/196]	LR: 0.1	Loss 0.8379 (0.7733)	Prec@1 74.609 (76.653)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.8564 (0.7776)	Prec@1 74.609 (76.793)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.7427 (0.7780)	Prec@1 78.516 (76.900)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.8330 (0.7877)	Prec@1 76.562 (76.535)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.8809 (0.7924)	Prec@1 72.266 (76.344)	
Total train loss: 0.7927

Train time: 17.878140449523926
 * Prec@1 60.180 Prec@5 85.710 Loss 1.7197
Best acc: 62.250
--------------------------------------------------------------------------------
Test time: 21.68043541908264

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.6758 (0.7321)	Prec@1 80.078 (77.724)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.7031 (0.7162)	Prec@1 78.125 (78.345)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.6113 (0.7064)	Prec@1 82.031 (78.739)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.6504 (0.7016)	Prec@1 80.469 (78.969)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.5591 (0.7014)	Prec@1 82.812 (79.004)	
Total train loss: 0.7016

Train time: 17.35189127922058
 * Prec@1 63.830 Prec@5 88.000 Loss 1.4180
Best acc: 63.830
--------------------------------------------------------------------------------
Test time: 20.957849502563477

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.6558 (0.6869)	Prec@1 79.688 (79.257)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.7100 (0.6889)	Prec@1 78.906 (79.287)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.7412 (0.6910)	Prec@1 80.859 (79.380)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.7793 (0.6926)	Prec@1 77.344 (79.359)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.6479 (0.6940)	Prec@1 82.031 (79.359)	
Total train loss: 0.6941

Train time: 16.95145845413208
 * Prec@1 63.840 Prec@5 88.120 Loss 1.4180
Best acc: 63.840
--------------------------------------------------------------------------------
Test time: 21.50209617614746

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.6387 (0.6796)	Prec@1 81.250 (79.838)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.7905 (0.6881)	Prec@1 76.562 (79.227)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.7363 (0.6902)	Prec@1 78.906 (79.230)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.6548 (0.6979)	Prec@1 79.297 (78.986)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.8477 (0.6976)	Prec@1 75.000 (79.145)	
Total train loss: 0.6977

Train time: 18.704615831375122
 * Prec@1 63.690 Prec@5 88.130 Loss 1.4180
Best acc: 63.840
--------------------------------------------------------------------------------
Test time: 22.022552967071533

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.5884 (0.6982)	Prec@1 82.422 (79.407)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.8599 (0.6886)	Prec@1 73.047 (79.723)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.7544 (0.6965)	Prec@1 76.953 (79.450)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.6665 (0.6946)	Prec@1 83.203 (79.495)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.6182 (0.6947)	Prec@1 83.984 (79.451)	
Total train loss: 0.6946

Train time: 17.41155195236206
 * Prec@1 63.750 Prec@5 88.120 Loss 1.4238
Best acc: 63.840
--------------------------------------------------------------------------------
Test time: 21.816158533096313

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.5620 (0.6618)	Prec@1 85.938 (80.729)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.6890 (0.6826)	Prec@1 77.344 (80.073)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.6387 (0.6947)	Prec@1 80.469 (79.517)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.5571 (0.6971)	Prec@1 80.469 (79.387)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.7271 (0.6946)	Prec@1 78.906 (79.499)	
Total train loss: 0.6948

Train time: 19.087345838546753
 * Prec@1 63.700 Prec@5 88.090 Loss 1.4180
Best acc: 63.840
--------------------------------------------------------------------------------
Test time: 23.438910961151123

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.8013 (0.6907)	Prec@1 79.297 (79.708)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.7114 (0.6926)	Prec@1 79.297 (79.482)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.7964 (0.6967)	Prec@1 76.562 (79.310)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.6538 (0.7001)	Prec@1 80.859 (79.162)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.6074 (0.6957)	Prec@1 80.859 (79.187)	
Total train loss: 0.6960

Train time: 19.268879890441895
 * Prec@1 63.120 Prec@5 87.310 Loss 1.4199
Best acc: 63.840
--------------------------------------------------------------------------------
Test time: 23.6931631565094

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.7114 (0.6837)	Prec@1 77.734 (79.848)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.7256 (0.6833)	Prec@1 79.688 (79.923)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.7266 (0.6868)	Prec@1 74.609 (79.768)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.5947 (0.6852)	Prec@1 81.641 (79.768)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.7437 (0.6864)	Prec@1 79.297 (79.772)	
Total train loss: 0.6867

Train time: 18.698572874069214
 * Prec@1 63.940 Prec@5 88.010 Loss 1.4209
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 22.23346734046936

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.6968 (0.6941)	Prec@1 77.734 (79.617)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.6001 (0.6940)	Prec@1 80.469 (79.592)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.7334 (0.6878)	Prec@1 78.125 (79.617)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.6587 (0.6883)	Prec@1 80.078 (79.667)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.7129 (0.6904)	Prec@1 77.734 (79.617)	
Total train loss: 0.6907

Train time: 16.534984350204468
 * Prec@1 63.630 Prec@5 88.210 Loss 1.4160
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.703970193862915

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.6372 (0.7069)	Prec@1 82.812 (78.946)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.7710 (0.7103)	Prec@1 77.344 (78.701)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.7236 (0.7050)	Prec@1 80.469 (78.890)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.6465 (0.7078)	Prec@1 81.250 (78.816)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.6597 (0.7067)	Prec@1 81.250 (78.854)	
Total train loss: 0.7069

Train time: 17.14747929573059
 * Prec@1 63.650 Prec@5 88.110 Loss 1.4238
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.267826318740845

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.6899 (0.7215)	Prec@1 78.516 (78.556)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.7100 (0.7115)	Prec@1 81.250 (78.876)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.7876 (0.7098)	Prec@1 77.734 (79.040)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.6914 (0.7077)	Prec@1 78.906 (78.971)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.5674 (0.7073)	Prec@1 85.156 (78.996)	
Total train loss: 0.7078

Train time: 19.460805416107178
 * Prec@1 63.700 Prec@5 88.240 Loss 1.4092
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 23.298547983169556

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.6973 (0.7140)	Prec@1 78.906 (78.726)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.7476 (0.7109)	Prec@1 78.516 (78.716)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.6748 (0.7084)	Prec@1 79.688 (78.753)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.6768 (0.7055)	Prec@1 78.906 (78.981)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.7944 (0.7081)	Prec@1 75.391 (78.856)	
Total train loss: 0.7082

Train time: 18.4427011013031
 * Prec@1 63.650 Prec@5 88.140 Loss 1.4170
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 22.594752311706543

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.7041 (0.7004)	Prec@1 78.906 (78.656)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.7905 (0.6989)	Prec@1 76.172 (78.766)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.7163 (0.7061)	Prec@1 78.906 (78.689)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.5947 (0.7067)	Prec@1 80.078 (78.754)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.7861 (0.7073)	Prec@1 76.953 (78.766)	
Total train loss: 0.7074

Train time: 17.532755851745605
 * Prec@1 63.320 Prec@5 88.180 Loss 1.4180
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.72046160697937

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.7070 (0.7018)	Prec@1 76.562 (79.227)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.6807 (0.7152)	Prec@1 80.859 (78.651)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.6313 (0.7069)	Prec@1 80.078 (78.913)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.7075 (0.7020)	Prec@1 78.516 (79.132)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.7217 (0.7064)	Prec@1 76.172 (78.962)	
Total train loss: 0.7062

Train time: 20.160480976104736
 * Prec@1 63.620 Prec@5 88.160 Loss 1.4141
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 23.356255054473877

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.7637 (0.7055)	Prec@1 78.516 (79.157)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.7451 (0.7019)	Prec@1 78.906 (78.991)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.7095 (0.7063)	Prec@1 77.344 (78.886)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.6572 (0.7055)	Prec@1 78.125 (78.921)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.8809 (0.7056)	Prec@1 73.047 (78.910)	
Total train loss: 0.7062

Train time: 17.203375816345215
 * Prec@1 63.540 Prec@5 87.940 Loss 1.4268
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.670443534851074

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.6587 (0.7151)	Prec@1 78.906 (78.225)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.7080 (0.7068)	Prec@1 80.078 (78.511)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.6680 (0.7056)	Prec@1 79.297 (78.706)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.7407 (0.7041)	Prec@1 75.391 (78.769)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.8506 (0.7075)	Prec@1 76.953 (78.692)	
Total train loss: 0.7075

Train time: 17.279984951019287
 * Prec@1 63.420 Prec@5 88.030 Loss 1.4219
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.69583010673523

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.6548 (0.7046)	Prec@1 80.859 (78.896)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.7153 (0.7077)	Prec@1 77.344 (78.711)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.6152 (0.7030)	Prec@1 82.422 (78.936)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.6626 (0.7064)	Prec@1 79.297 (78.829)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.7031 (0.7062)	Prec@1 78.906 (78.840)	
Total train loss: 0.7065

Train time: 16.412109851837158
 * Prec@1 63.650 Prec@5 88.120 Loss 1.4180
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.023439645767212

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.7617 (0.7094)	Prec@1 80.078 (79.127)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.7744 (0.7003)	Prec@1 78.516 (79.227)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.7344 (0.7013)	Prec@1 79.688 (79.073)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.6890 (0.7040)	Prec@1 80.078 (78.969)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.7764 (0.7030)	Prec@1 78.125 (79.018)	
Total train loss: 0.7032

Train time: 16.984440565109253
 * Prec@1 63.660 Prec@5 88.170 Loss 1.4189
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.863479375839233

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.6440 (0.6946)	Prec@1 79.688 (79.167)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.6987 (0.7011)	Prec@1 77.344 (78.946)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.7817 (0.7061)	Prec@1 79.688 (79.063)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.6660 (0.7053)	Prec@1 77.734 (78.991)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.7749 (0.7065)	Prec@1 76.562 (78.836)	
Total train loss: 0.7064

Train time: 16.57869267463684
 * Prec@1 63.360 Prec@5 88.070 Loss 1.4229
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.56336283683777

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.8286 (0.7288)	Prec@1 74.219 (78.165)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.6899 (0.7116)	Prec@1 81.641 (78.641)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.6694 (0.7066)	Prec@1 77.734 (78.846)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.6626 (0.7089)	Prec@1 78.906 (78.856)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.7637 (0.7064)	Prec@1 76.172 (78.866)	
Total train loss: 0.7068

Train time: 17.218480587005615
 * Prec@1 63.380 Prec@5 88.080 Loss 1.4268
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.84610915184021

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.6455 (0.7132)	Prec@1 80.469 (78.826)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.7090 (0.7065)	Prec@1 78.906 (78.816)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.6709 (0.7085)	Prec@1 79.297 (78.803)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.6797 (0.7070)	Prec@1 78.516 (78.901)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.6069 (0.7060)	Prec@1 81.250 (78.922)	
Total train loss: 0.7063

Train time: 17.127542972564697
 * Prec@1 63.570 Prec@5 87.990 Loss 1.4199
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.284731149673462

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.6079 (0.7046)	Prec@1 81.641 (78.756)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.8330 (0.6990)	Prec@1 77.344 (78.896)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.6626 (0.7031)	Prec@1 81.250 (78.923)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.6304 (0.7051)	Prec@1 80.469 (78.946)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.6133 (0.7037)	Prec@1 82.031 (79.083)	
Total train loss: 0.7041

Train time: 17.085742950439453
 * Prec@1 63.470 Prec@5 88.320 Loss 1.4160
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.94461727142334

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.7378 (0.7084)	Prec@1 76.953 (78.896)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.6357 (0.7022)	Prec@1 80.859 (79.097)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.6030 (0.7020)	Prec@1 81.250 (79.113)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.7158 (0.7025)	Prec@1 79.297 (79.046)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.6270 (0.7062)	Prec@1 79.688 (78.936)	
Total train loss: 0.7062

Train time: 16.103305101394653
 * Prec@1 63.590 Prec@5 88.200 Loss 1.4150
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 19.69290828704834

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.6953 (0.7138)	Prec@1 79.688 (78.526)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.7109 (0.7067)	Prec@1 80.859 (78.911)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.7637 (0.7068)	Prec@1 76.562 (78.906)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.6538 (0.7048)	Prec@1 82.031 (78.964)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.5586 (0.7038)	Prec@1 83.594 (78.978)	
Total train loss: 0.7042

Train time: 16.028390407562256
 * Prec@1 63.470 Prec@5 88.110 Loss 1.4170
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 19.64745783805847

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.7368 (0.7024)	Prec@1 78.906 (78.986)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.8130 (0.7011)	Prec@1 76.172 (79.046)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.8101 (0.7026)	Prec@1 78.516 (79.036)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.8022 (0.7067)	Prec@1 74.219 (78.926)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.8433 (0.7041)	Prec@1 73.047 (78.948)	
Total train loss: 0.7045

Train time: 17.490015983581543
 * Prec@1 63.340 Prec@5 88.200 Loss 1.4209
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.27263593673706

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.6230 (0.7052)	Prec@1 80.078 (78.996)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.6748 (0.7055)	Prec@1 78.516 (78.901)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.6899 (0.7028)	Prec@1 78.516 (78.953)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.6646 (0.7096)	Prec@1 81.250 (78.771)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.7944 (0.7069)	Prec@1 76.562 (78.872)	
Total train loss: 0.7073

Train time: 17.211722373962402
 * Prec@1 63.390 Prec@5 88.120 Loss 1.4189
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.32689142227173

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.7881 (0.7074)	Prec@1 78.516 (78.646)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.7900 (0.7101)	Prec@1 76.172 (78.621)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.7666 (0.7067)	Prec@1 76.562 (78.860)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.8247 (0.7108)	Prec@1 76.172 (78.806)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.7856 (0.7074)	Prec@1 75.781 (78.920)	
Total train loss: 0.7074

Train time: 16.16935110092163
 * Prec@1 63.440 Prec@5 88.190 Loss 1.4199
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.865024089813232

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.6816 (0.7129)	Prec@1 78.906 (78.666)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.8052 (0.7085)	Prec@1 76.172 (78.921)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.8018 (0.7130)	Prec@1 76.953 (78.622)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.6689 (0.7098)	Prec@1 81.250 (78.746)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.7881 (0.7056)	Prec@1 78.516 (78.928)	
Total train loss: 0.7062

Train time: 17.38257598876953
 * Prec@1 63.620 Prec@5 88.160 Loss 1.4229
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.858572483062744

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.8921 (0.7079)	Prec@1 72.656 (79.046)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.7358 (0.7114)	Prec@1 78.516 (78.836)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.6343 (0.7088)	Prec@1 80.078 (78.933)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.6006 (0.7073)	Prec@1 83.594 (78.921)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.6660 (0.7061)	Prec@1 81.641 (78.994)	
Total train loss: 0.7062

Train time: 18.404528379440308
 * Prec@1 63.560 Prec@5 88.060 Loss 1.4170
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 22.270996809005737

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.5815 (0.7007)	Prec@1 82.812 (79.537)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.6123 (0.7003)	Prec@1 81.250 (79.497)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.8179 (0.7065)	Prec@1 75.391 (79.190)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.5737 (0.7067)	Prec@1 82.812 (79.107)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.6636 (0.7053)	Prec@1 78.516 (79.095)	
Total train loss: 0.7051

Train time: 17.920870304107666
 * Prec@1 63.690 Prec@5 88.140 Loss 1.4170
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.88027024269104

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.7559 (0.6915)	Prec@1 75.000 (79.087)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.7847 (0.7016)	Prec@1 78.516 (78.921)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.6440 (0.7009)	Prec@1 80.078 (78.940)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.7266 (0.7065)	Prec@1 76.562 (78.816)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.8301 (0.7048)	Prec@1 74.219 (78.900)	
Total train loss: 0.7051

Train time: 17.90199875831604
 * Prec@1 63.730 Prec@5 88.150 Loss 1.4199
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 21.624605894088745

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.8403 (0.7216)	Prec@1 76.172 (78.776)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.6675 (0.7154)	Prec@1 78.125 (78.921)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.7314 (0.7092)	Prec@1 78.906 (78.893)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.6704 (0.7049)	Prec@1 79.297 (79.051)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.7241 (0.7042)	Prec@1 79.688 (79.085)	
Total train loss: 0.7044

Train time: 16.93942904472351
 * Prec@1 63.400 Prec@5 88.150 Loss 1.4160
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 20.632909297943115

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.6685 (0.7039)	Prec@1 78.516 (79.157)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.7197 (0.7064)	Prec@1 78.125 (78.951)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.7290 (0.7018)	Prec@1 78.516 (79.083)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.7271 (0.7015)	Prec@1 77.734 (79.064)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.7285 (0.7050)	Prec@1 78.906 (78.976)	
Total train loss: 0.7053

Train time: 12.909656286239624
 * Prec@1 63.360 Prec@5 88.150 Loss 1.4229
Best acc: 63.940
--------------------------------------------------------------------------------
Test time: 15.684534311294556


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 39.850 Prec@5 70.000 Loss 2.9316
Pre-trained Prec@1 with 19 layers frozen: 39.849998474121094 	 Loss: 2.931640625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.2900 (1.3311)	Prec@1 65.625 (63.291)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.0762 (1.2827)	Prec@1 70.703 (64.558)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.2295 (1.2655)	Prec@1 67.188 (64.967)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1787 (1.2432)	Prec@1 64.453 (65.460)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2227 (1.2346)	Prec@1 66.406 (65.569)	
Total train loss: 1.2347

Train time: 267.97235107421875
 * Prec@1 61.310 Prec@5 86.210 Loss 1.4756
Best acc: 61.310
--------------------------------------------------------------------------------
Test time: 271.1567804813385

Epoch: [1][38/196]	LR: 0.1	Loss 1.1074 (1.1441)	Prec@1 66.406 (67.969)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0889 (1.1327)	Prec@1 67.578 (68.254)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1289 (1.1399)	Prec@1 65.625 (67.765)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0107 (1.1401)	Prec@1 67.188 (67.683)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2412 (1.1377)	Prec@1 64.062 (67.718)	
Total train loss: 1.1378

Train time: 14.430127143859863
 * Prec@1 61.960 Prec@5 86.930 Loss 1.4326
Best acc: 61.960
--------------------------------------------------------------------------------
Test time: 17.293349504470825

Epoch: [2][38/196]	LR: 0.1	Loss 1.1211 (1.0771)	Prec@1 69.531 (70.032)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9658 (1.0830)	Prec@1 71.484 (69.631)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.0801 (1.0973)	Prec@1 70.312 (69.117)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0723 (1.0990)	Prec@1 66.797 (68.875)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.1357 (1.1004)	Prec@1 67.578 (68.776)	
Total train loss: 1.1002

Train time: 13.008805751800537
 * Prec@1 62.450 Prec@5 87.060 Loss 1.4316
Best acc: 62.450
--------------------------------------------------------------------------------
Test time: 15.450348615646362

Epoch: [3][38/196]	LR: 0.1	Loss 1.0703 (1.0691)	Prec@1 71.484 (69.091)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.9985 (1.0636)	Prec@1 75.781 (69.697)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9873 (1.0660)	Prec@1 71.094 (69.488)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.0420 (1.0709)	Prec@1 69.922 (69.183)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9404 (1.0768)	Prec@1 72.656 (69.127)	
Total train loss: 1.0767

Train time: 11.40468716621399
 * Prec@1 62.670 Prec@5 87.110 Loss 1.4268
Best acc: 62.670
--------------------------------------------------------------------------------
Test time: 14.362377643585205

Epoch: [4][38/196]	LR: 0.1	Loss 1.0244 (1.0366)	Prec@1 69.531 (69.992)	
Epoch: [4][77/196]	LR: 0.1	Loss 1.0234 (1.0520)	Prec@1 69.922 (69.867)	
Epoch: [4][116/196]	LR: 0.1	Loss 1.0791 (1.0688)	Prec@1 67.188 (69.558)	
Epoch: [4][155/196]	LR: 0.1	Loss 1.1816 (1.0760)	Prec@1 63.672 (69.266)	
Epoch: [4][194/196]	LR: 0.1	Loss 1.1562 (1.0838)	Prec@1 65.234 (69.069)	
Total train loss: 1.0844

Train time: 12.419721126556396
 * Prec@1 62.040 Prec@5 86.930 Loss 1.4658
Best acc: 62.670
--------------------------------------------------------------------------------
Test time: 15.376418352127075

Epoch: [5][38/196]	LR: 0.1	Loss 1.0586 (1.0729)	Prec@1 70.703 (69.371)	
Epoch: [5][77/196]	LR: 0.1	Loss 1.1045 (1.0684)	Prec@1 69.531 (69.481)	
Epoch: [5][116/196]	LR: 0.1	Loss 1.0420 (1.0685)	Prec@1 70.312 (69.324)	
Epoch: [5][155/196]	LR: 0.1	Loss 1.0625 (1.0628)	Prec@1 71.484 (69.461)	
Epoch: [5][194/196]	LR: 0.1	Loss 1.0107 (1.0638)	Prec@1 72.266 (69.419)	
Total train loss: 1.0635

Train time: 12.242833852767944
 * Prec@1 62.720 Prec@5 87.230 Loss 1.4414
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.132229566574097

Epoch: [6][38/196]	LR: 0.1	Loss 1.1113 (1.0445)	Prec@1 67.969 (69.321)	
Epoch: [6][77/196]	LR: 0.1	Loss 1.1299 (1.0509)	Prec@1 68.750 (69.311)	
Epoch: [6][116/196]	LR: 0.1	Loss 1.0645 (1.0516)	Prec@1 67.188 (69.491)	
Epoch: [6][155/196]	LR: 0.1	Loss 1.1357 (1.0519)	Prec@1 65.625 (69.461)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.9702 (1.0611)	Prec@1 72.656 (69.195)	
Total train loss: 1.0612

Train time: 13.295366764068604
 * Prec@1 61.010 Prec@5 86.890 Loss 1.4756
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.771068096160889

Epoch: [7][38/196]	LR: 0.1	Loss 1.0498 (1.0537)	Prec@1 71.094 (69.762)	
Epoch: [7][77/196]	LR: 0.1	Loss 1.0137 (1.0519)	Prec@1 73.047 (69.782)	
Epoch: [7][116/196]	LR: 0.1	Loss 1.1221 (1.0492)	Prec@1 66.797 (69.758)	
Epoch: [7][155/196]	LR: 0.1	Loss 1.1543 (1.0514)	Prec@1 70.312 (69.641)	
Epoch: [7][194/196]	LR: 0.1	Loss 1.0527 (1.0573)	Prec@1 69.531 (69.495)	
Total train loss: 1.0572

Train time: 11.770722150802612
 * Prec@1 62.650 Prec@5 87.260 Loss 1.4521
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.744524240493774

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.9385 (1.0131)	Prec@1 72.656 (70.753)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 1.0674 (1.0310)	Prec@1 66.797 (70.257)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.9536 (1.0360)	Prec@1 71.094 (70.069)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 1.0703 (1.0331)	Prec@1 69.531 (70.155)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 1.0488 (1.0311)	Prec@1 65.625 (70.194)	
Total train loss: 1.0312

Train time: 12.226469993591309
 * Prec@1 62.480 Prec@5 87.260 Loss 1.4434
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.192937850952148

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 1.1914 (1.0391)	Prec@1 66.406 (69.912)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.9717 (1.0360)	Prec@1 71.094 (69.942)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.8423 (1.0284)	Prec@1 76.953 (70.216)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 1.0156 (1.0277)	Prec@1 74.609 (70.348)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 1.0293 (1.0305)	Prec@1 68.359 (70.252)	
Total train loss: 1.0312

Train time: 12.112459421157837
 * Prec@1 62.410 Prec@5 87.210 Loss 1.4453
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.023406267166138

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 1.0303 (1.0420)	Prec@1 69.141 (70.042)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 1.1621 (1.0255)	Prec@1 66.797 (70.353)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.9995 (1.0309)	Prec@1 70.703 (70.146)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.9824 (1.0354)	Prec@1 73.828 (70.025)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.9565 (1.0319)	Prec@1 69.922 (70.160)	
Total train loss: 1.0319

Train time: 13.165467500686646
 * Prec@1 62.410 Prec@5 87.170 Loss 1.4473
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.604880094528198

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.9766 (1.0502)	Prec@1 73.438 (69.722)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 1.1562 (1.0486)	Prec@1 69.141 (69.862)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.9878 (1.0442)	Prec@1 69.141 (70.005)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 1.1660 (1.0458)	Prec@1 67.188 (70.080)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 1.0879 (1.0463)	Prec@1 68.359 (70.086)	
Total train loss: 1.0466

Train time: 12.193575143814087
 * Prec@1 62.090 Prec@5 87.080 Loss 1.4580
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.167659997940063

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 1.1621 (1.0694)	Prec@1 67.578 (69.692)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 1.1104 (1.0733)	Prec@1 67.969 (69.707)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 1.1230 (1.0665)	Prec@1 68.359 (70.062)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 1.0078 (1.0736)	Prec@1 74.609 (69.844)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 1.1455 (1.0738)	Prec@1 68.359 (69.950)	
Total train loss: 1.0740

Train time: 12.055352926254272
 * Prec@1 61.390 Prec@5 86.900 Loss 1.4629
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.964293479919434

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 1.2949 (1.0876)	Prec@1 65.234 (69.611)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 1.1650 (1.0919)	Prec@1 67.969 (69.641)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 1.0996 (1.0956)	Prec@1 71.094 (69.471)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 1.2129 (1.1161)	Prec@1 73.828 (69.156)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 1.1240 (1.1314)	Prec@1 68.359 (69.002)	
Total train loss: 1.1318

Train time: 11.97962737083435
 * Prec@1 60.970 Prec@5 86.680 Loss 1.4668
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.826206922531128

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 1.3047 (1.2635)	Prec@1 71.484 (67.808)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 1.2441 (1.2623)	Prec@1 69.141 (68.254)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 1.2227 (1.2578)	Prec@1 69.922 (68.496)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 1.2666 (1.2569)	Prec@1 72.656 (68.650)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 1.2900 (1.2616)	Prec@1 68.359 (68.556)	
Total train loss: 1.2616

Train time: 12.618552923202515
 * Prec@1 60.790 Prec@5 86.590 Loss 1.4883
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.930771350860596

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 1.3516 (1.3070)	Prec@1 64.062 (68.269)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 1.2578 (1.3028)	Prec@1 68.750 (68.314)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 1.3379 (1.2962)	Prec@1 67.188 (68.640)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 1.1465 (1.2952)	Prec@1 70.312 (68.537)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 1.3477 (1.2934)	Prec@1 67.188 (68.608)	
Total train loss: 1.2937

Train time: 12.165944576263428
 * Prec@1 60.820 Prec@5 86.610 Loss 1.4814
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.023126125335693

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 1.3105 (1.2814)	Prec@1 67.578 (69.441)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 1.2217 (1.2922)	Prec@1 73.438 (68.885)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 1.2744 (1.2927)	Prec@1 65.234 (68.850)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 1.3330 (1.2918)	Prec@1 66.797 (68.732)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 1.3271 (1.2894)	Prec@1 69.141 (68.812)	
Total train loss: 1.2894

Train time: 12.096094131469727
 * Prec@1 60.870 Prec@5 86.630 Loss 1.4883
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.472935914993286

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 1.3076 (1.2861)	Prec@1 67.969 (68.540)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 1.2686 (1.2847)	Prec@1 67.969 (68.700)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 1.4150 (1.2888)	Prec@1 64.844 (68.480)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 1.3086 (1.2906)	Prec@1 69.922 (68.590)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 1.2598 (1.2884)	Prec@1 69.531 (68.690)	
Total train loss: 1.2884

Train time: 11.486665487289429
 * Prec@1 60.820 Prec@5 86.570 Loss 1.4805
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.846867084503174

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 1.2393 (1.2751)	Prec@1 70.312 (68.710)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 1.3037 (1.2848)	Prec@1 68.359 (68.705)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 1.2324 (1.2863)	Prec@1 68.750 (68.640)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 1.3438 (1.2858)	Prec@1 66.016 (68.620)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 1.2783 (1.2885)	Prec@1 69.922 (68.584)	
Total train loss: 1.2884

Train time: 12.359323263168335
 * Prec@1 60.770 Prec@5 86.680 Loss 1.4805
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.659595966339111

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 1.2256 (1.3018)	Prec@1 67.578 (68.249)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 1.2451 (1.2899)	Prec@1 69.922 (68.795)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 1.3359 (1.2894)	Prec@1 66.406 (68.626)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 1.2725 (1.2912)	Prec@1 73.438 (68.682)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 1.3223 (1.2868)	Prec@1 67.578 (68.668)	
Total train loss: 1.2867

Train time: 12.162668228149414
 * Prec@1 60.820 Prec@5 86.520 Loss 1.4844
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.025792121887207

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 1.2168 (1.2789)	Prec@1 69.531 (68.460)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 1.3447 (1.2879)	Prec@1 70.312 (68.580)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 1.2744 (1.2926)	Prec@1 69.531 (68.733)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 1.4014 (1.2893)	Prec@1 65.625 (68.755)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 1.2686 (1.2868)	Prec@1 71.484 (68.838)	
Total train loss: 1.2868

Train time: 12.246506452560425
 * Prec@1 60.760 Prec@5 86.650 Loss 1.4844
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.636499643325806

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 1.2910 (1.2831)	Prec@1 68.359 (69.111)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 1.2070 (1.2957)	Prec@1 70.703 (68.500)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 1.3232 (1.2872)	Prec@1 64.453 (68.687)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 1.2354 (1.2843)	Prec@1 70.703 (68.995)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 1.3379 (1.2874)	Prec@1 68.750 (68.866)	
Total train loss: 1.2875

Train time: 11.686171531677246
 * Prec@1 60.750 Prec@5 86.560 Loss 1.4844
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.592469692230225

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 1.3193 (1.2764)	Prec@1 67.969 (68.880)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 1.2227 (1.2955)	Prec@1 70.703 (68.239)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 1.3193 (1.2959)	Prec@1 71.094 (68.316)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 1.3359 (1.2909)	Prec@1 67.578 (68.500)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 1.3213 (1.2884)	Prec@1 70.312 (68.588)	
Total train loss: 1.2885

Train time: 11.88788890838623
 * Prec@1 60.830 Prec@5 86.570 Loss 1.4883
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.730677127838135

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 1.2666 (1.2854)	Prec@1 71.484 (68.710)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 1.3330 (1.2976)	Prec@1 67.969 (68.209)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 1.2441 (1.2967)	Prec@1 69.141 (68.132)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 1.2812 (1.2896)	Prec@1 69.141 (68.359)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 1.2314 (1.2871)	Prec@1 72.266 (68.620)	
Total train loss: 1.2870

Train time: 12.068979978561401
 * Prec@1 60.710 Prec@5 86.600 Loss 1.4854
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.918999910354614

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 1.4102 (1.2970)	Prec@1 64.062 (68.820)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 1.2656 (1.2896)	Prec@1 71.094 (68.920)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 1.3047 (1.2842)	Prec@1 69.141 (69.167)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 1.2656 (1.2854)	Prec@1 69.141 (68.885)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 1.2510 (1.2859)	Prec@1 68.359 (68.800)	
Total train loss: 1.2860

Train time: 12.3191397190094
 * Prec@1 60.920 Prec@5 86.610 Loss 1.4814
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.64646029472351

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 1.3818 (1.2815)	Prec@1 67.188 (68.710)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 1.1348 (1.2777)	Prec@1 72.266 (68.805)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 1.2734 (1.2797)	Prec@1 68.359 (68.870)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 1.4600 (1.2854)	Prec@1 64.453 (68.843)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 1.2422 (1.2879)	Prec@1 66.797 (68.688)	
Total train loss: 1.2877

Train time: 11.840267181396484
 * Prec@1 60.700 Prec@5 86.540 Loss 1.4805
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.744527816772461

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 1.3379 (1.2940)	Prec@1 66.406 (68.440)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 1.2852 (1.2903)	Prec@1 69.531 (68.635)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 1.2754 (1.2946)	Prec@1 69.922 (68.620)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 1.2598 (1.2919)	Prec@1 71.094 (68.875)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 1.3027 (1.2884)	Prec@1 66.797 (68.886)	
Total train loss: 1.2885

Train time: 12.025738716125488
 * Prec@1 61.000 Prec@5 86.630 Loss 1.4746
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.412327289581299

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 1.3066 (1.2905)	Prec@1 67.188 (68.890)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 1.3564 (1.2811)	Prec@1 61.719 (68.880)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 1.3984 (1.2850)	Prec@1 66.797 (68.864)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 1.4082 (1.2867)	Prec@1 61.328 (68.742)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 1.2432 (1.2864)	Prec@1 71.484 (68.772)	
Total train loss: 1.2864

Train time: 11.669624328613281
 * Prec@1 60.700 Prec@5 86.570 Loss 1.4863
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.048047304153442

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 1.3203 (1.2870)	Prec@1 66.406 (68.429)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 1.3818 (1.2876)	Prec@1 67.578 (68.655)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 1.2100 (1.2854)	Prec@1 70.703 (68.730)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 1.1621 (1.2846)	Prec@1 71.094 (68.785)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 1.2920 (1.2873)	Prec@1 66.406 (68.766)	
Total train loss: 1.2875

Train time: 12.349673986434937
 * Prec@1 61.010 Prec@5 86.730 Loss 1.4766
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.695945262908936

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 1.1377 (1.2713)	Prec@1 72.656 (69.531)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 1.3135 (1.2825)	Prec@1 68.750 (69.126)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 1.2588 (1.2932)	Prec@1 66.406 (68.693)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 1.4268 (1.2880)	Prec@1 59.766 (68.805)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 1.3545 (1.2873)	Prec@1 67.578 (68.740)	
Total train loss: 1.2874

Train time: 12.070216178894043
 * Prec@1 60.760 Prec@5 86.560 Loss 1.4756
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.952163696289062

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 1.4570 (1.3057)	Prec@1 64.453 (68.079)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 1.2334 (1.2898)	Prec@1 69.531 (68.434)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 1.2363 (1.2902)	Prec@1 71.094 (68.523)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 1.3232 (1.2866)	Prec@1 66.406 (68.607)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 1.3057 (1.2875)	Prec@1 66.016 (68.670)	
Total train loss: 1.2876

Train time: 12.282537698745728
 * Prec@1 60.780 Prec@5 86.620 Loss 1.4844
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.6776602268219

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 1.3750 (1.2905)	Prec@1 67.969 (68.970)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 1.2988 (1.2842)	Prec@1 67.188 (69.086)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 1.3438 (1.2846)	Prec@1 68.359 (69.067)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 1.3799 (1.2843)	Prec@1 66.797 (69.028)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 1.3682 (1.2871)	Prec@1 66.016 (68.886)	
Total train loss: 1.2873

Train time: 11.820181369781494
 * Prec@1 60.610 Prec@5 86.680 Loss 1.4844
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.727017879486084

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 1.3975 (1.3036)	Prec@1 64.844 (68.570)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 1.3115 (1.3025)	Prec@1 69.922 (68.460)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 1.2461 (1.2963)	Prec@1 69.922 (68.707)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 1.3662 (1.2891)	Prec@1 68.750 (68.695)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 1.3125 (1.2865)	Prec@1 66.797 (68.784)	
Total train loss: 1.2867

Train time: 12.019479036331177
 * Prec@1 60.840 Prec@5 86.590 Loss 1.4785
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.91357159614563

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 1.1777 (1.2655)	Prec@1 71.875 (69.331)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 1.2451 (1.2733)	Prec@1 66.016 (68.880)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 1.3281 (1.2822)	Prec@1 69.141 (68.683)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 1.2031 (1.2817)	Prec@1 70.703 (68.910)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 1.3516 (1.2865)	Prec@1 66.797 (68.778)	
Total train loss: 1.2866

Train time: 12.215321063995361
 * Prec@1 60.680 Prec@5 86.610 Loss 1.4814
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.098735332489014

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 1.2500 (1.2908)	Prec@1 67.188 (68.850)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 1.3301 (1.2884)	Prec@1 67.578 (68.740)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 1.2754 (1.2816)	Prec@1 66.406 (68.870)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 1.2158 (1.2854)	Prec@1 68.750 (68.727)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 1.3301 (1.2869)	Prec@1 64.453 (68.756)	
Total train loss: 1.2872

Train time: 12.514535188674927
 * Prec@1 60.940 Prec@5 86.570 Loss 1.4824
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.916884422302246

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 1.2705 (1.2893)	Prec@1 67.578 (68.349)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 1.2246 (1.2876)	Prec@1 72.656 (68.620)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 1.2783 (1.2858)	Prec@1 70.703 (68.800)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 1.3818 (1.2842)	Prec@1 69.531 (68.747)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 1.2832 (1.2884)	Prec@1 71.094 (68.732)	
Total train loss: 1.2886

Train time: 11.816030025482178
 * Prec@1 60.910 Prec@5 86.520 Loss 1.4824
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.774576902389526

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 1.3047 (1.2855)	Prec@1 72.656 (68.920)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 1.3154 (1.2840)	Prec@1 68.359 (68.635)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 1.3701 (1.2847)	Prec@1 62.891 (68.820)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 1.3848 (1.2919)	Prec@1 67.578 (68.560)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 1.2637 (1.2880)	Prec@1 69.141 (68.634)	
Total train loss: 1.2879

Train time: 12.17287802696228
 * Prec@1 60.790 Prec@5 86.640 Loss 1.4854
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 14.55264139175415

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 1.2979 (1.2967)	Prec@1 70.703 (68.930)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 1.2119 (1.2927)	Prec@1 71.484 (69.000)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 1.2451 (1.2861)	Prec@1 72.266 (69.017)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 1.2686 (1.2902)	Prec@1 69.922 (68.825)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 1.1445 (1.2870)	Prec@1 73.047 (68.914)	
Total train loss: 1.2872

Train time: 12.528171300888062
 * Prec@1 60.620 Prec@5 86.580 Loss 1.4824
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 16.02676773071289

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 1.3008 (1.3050)	Prec@1 67.578 (68.470)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 1.2891 (1.2907)	Prec@1 64.844 (68.775)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 1.3789 (1.2803)	Prec@1 65.625 (69.044)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 1.4463 (1.2831)	Prec@1 65.625 (68.985)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 1.3525 (1.2868)	Prec@1 69.922 (68.840)	
Total train loss: 1.2868

Train time: 12.715030193328857
 * Prec@1 60.910 Prec@5 86.520 Loss 1.4824
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.111634969711304

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 1.2607 (1.2934)	Prec@1 69.922 (68.379)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 1.3535 (1.2949)	Prec@1 67.578 (68.455)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 1.3594 (1.2893)	Prec@1 69.141 (68.677)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 1.2266 (1.2899)	Prec@1 70.312 (68.647)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 1.2158 (1.2866)	Prec@1 71.094 (68.702)	
Total train loss: 1.2870

Train time: 11.839880466461182
 * Prec@1 60.940 Prec@5 86.680 Loss 1.4756
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 13.79101824760437

