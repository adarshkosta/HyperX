
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 68.950 Prec@5 90.410 Loss 1.1895
Pre-trained Prec@1 with 1 layers frozen: 68.94999694824219 	 Loss: 1.189453125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.5605 (0.5358)	Prec@1 84.375 (84.876)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.6582 (0.5606)	Prec@1 80.078 (84.195)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.5986 (0.5727)	Prec@1 79.688 (83.721)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.7798 (0.5853)	Prec@1 73.828 (83.313)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.6357 (0.5955)	Prec@1 80.469 (83.023)	
Total train loss: 0.5958

Train time: 201.68731474876404
 * Prec@1 66.860 Prec@5 89.540 Loss 1.2490
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 207.04052734375

Epoch: [1][38/196]	LR: 0.01	Loss 0.4302 (0.4907)	Prec@1 88.672 (86.989)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.5029 (0.4975)	Prec@1 87.500 (86.744)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.5435 (0.5062)	Prec@1 85.156 (86.318)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.6855 (0.5133)	Prec@1 80.859 (85.958)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.5801 (0.5182)	Prec@1 83.984 (85.767)	
Total train loss: 0.5186

Train time: 19.262945652008057
 * Prec@1 66.140 Prec@5 89.070 Loss 1.2988
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 23.88542151451111

Epoch: [2][38/196]	LR: 0.01	Loss 0.5015 (0.4143)	Prec@1 87.500 (89.824)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.4724 (0.4263)	Prec@1 90.234 (89.338)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.5039 (0.4415)	Prec@1 88.281 (88.782)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.4954 (0.4477)	Prec@1 86.719 (88.509)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.5020 (0.4569)	Prec@1 88.281 (88.193)	
Total train loss: 0.4570

Train time: 24.049396753311157
 * Prec@1 66.050 Prec@5 88.450 Loss 1.3486
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 29.053412914276123

Epoch: [3][38/196]	LR: 0.01	Loss 0.3718 (0.3768)	Prec@1 90.234 (91.106)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.4834 (0.3863)	Prec@1 84.375 (90.885)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.3369 (0.3972)	Prec@1 92.969 (90.532)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.4216 (0.4016)	Prec@1 90.234 (90.254)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.3721 (0.4091)	Prec@1 92.969 (89.936)	
Total train loss: 0.4095

Train time: 24.620164394378662
 * Prec@1 66.570 Prec@5 88.710 Loss 1.3096
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 30.315460205078125

Epoch: [4][38/196]	LR: 0.01	Loss 0.3525 (0.3480)	Prec@1 92.969 (92.428)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.3535 (0.3517)	Prec@1 90.625 (92.007)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.3218 (0.3539)	Prec@1 91.406 (91.930)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.4663 (0.3625)	Prec@1 87.891 (91.629)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.3665 (0.3685)	Prec@1 91.016 (91.362)	
Total train loss: 0.3686

Train time: 26.496431350708008
 * Prec@1 65.720 Prec@5 88.220 Loss 1.3477
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 32.85360074043274

Epoch: [5][38/196]	LR: 0.01	Loss 0.2622 (0.2989)	Prec@1 96.484 (93.920)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.3174 (0.3083)	Prec@1 92.578 (93.560)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.3362 (0.3122)	Prec@1 92.188 (93.446)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.3296 (0.3203)	Prec@1 91.797 (93.089)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.3894 (0.3271)	Prec@1 90.234 (92.903)	
Total train loss: 0.3273

Train time: 24.002458095550537
 * Prec@1 65.820 Prec@5 87.690 Loss 1.3848
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 29.676518440246582

Epoch: [6][38/196]	LR: 0.01	Loss 0.2578 (0.2704)	Prec@1 96.094 (94.862)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.2971 (0.2798)	Prec@1 94.531 (94.651)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.2820 (0.2836)	Prec@1 96.875 (94.535)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.3047 (0.2885)	Prec@1 94.141 (94.351)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.2844 (0.2962)	Prec@1 94.531 (94.069)	
Total train loss: 0.2967

Train time: 24.806986808776855
 * Prec@1 65.180 Prec@5 87.570 Loss 1.3867
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 29.855515956878662

Epoch: [7][38/196]	LR: 0.01	Loss 0.2595 (0.2450)	Prec@1 95.312 (95.803)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.2615 (0.2496)	Prec@1 95.703 (95.778)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.3018 (0.2541)	Prec@1 93.750 (95.680)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.2805 (0.2601)	Prec@1 95.703 (95.475)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.2939 (0.2667)	Prec@1 94.531 (95.194)	
Total train loss: 0.2670

Train time: 22.540693283081055
 * Prec@1 65.260 Prec@5 87.820 Loss 1.4004
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 26.751959800720215

Epoch: [8][38/196]	LR: 0.001	Loss 0.2241 (0.2106)	Prec@1 97.266 (97.145)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.1638 (0.2014)	Prec@1 98.828 (97.406)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.1704 (0.1997)	Prec@1 97.266 (97.399)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.1726 (0.1968)	Prec@1 98.828 (97.519)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.1951 (0.1937)	Prec@1 99.219 (97.620)	
Total train loss: 0.1940

Train time: 22.76253032684326
 * Prec@1 66.440 Prec@5 88.290 Loss 1.3604
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 29.734849452972412

Epoch: [9][38/196]	LR: 0.001	Loss 0.1908 (0.1791)	Prec@1 98.438 (98.167)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.1420 (0.1762)	Prec@1 99.609 (98.222)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.1838 (0.1761)	Prec@1 99.219 (98.244)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.2126 (0.1764)	Prec@1 96.875 (98.245)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.1621 (0.1762)	Prec@1 98.438 (98.279)	
Total train loss: 0.1765

Train time: 29.70357036590576
 * Prec@1 66.100 Prec@5 88.270 Loss 1.3701
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 35.79425382614136

Epoch: [10][38/196]	LR: 0.001	Loss 0.1508 (0.1696)	Prec@1 99.219 (98.518)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.1758 (0.1682)	Prec@1 98.438 (98.488)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.1960 (0.1684)	Prec@1 98.047 (98.468)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.1915 (0.1690)	Prec@1 98.047 (98.422)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.1595 (0.1685)	Prec@1 98.047 (98.444)	
Total train loss: 0.1685

Train time: 28.02385663986206
 * Prec@1 65.840 Prec@5 88.310 Loss 1.3652
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 35.142380714416504

Epoch: [11][38/196]	LR: 0.001	Loss 0.1475 (0.1610)	Prec@1 98.828 (98.628)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.2017 (0.1613)	Prec@1 96.484 (98.613)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.1199 (0.1598)	Prec@1 99.219 (98.688)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.1731 (0.1619)	Prec@1 98.047 (98.675)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.1652 (0.1634)	Prec@1 96.875 (98.612)	
Total train loss: 0.1635

Train time: 30.132721424102783
 * Prec@1 65.870 Prec@5 88.070 Loss 1.3682
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.329156160354614

Epoch: [12][38/196]	LR: 0.001	Loss 0.1652 (0.1646)	Prec@1 98.828 (98.598)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.1807 (0.1610)	Prec@1 98.438 (98.708)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.1641 (0.1602)	Prec@1 98.438 (98.778)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.1653 (0.1601)	Prec@1 98.438 (98.791)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.1423 (0.1603)	Prec@1 99.219 (98.778)	
Total train loss: 0.1605

Train time: 26.911706924438477
 * Prec@1 65.860 Prec@5 88.190 Loss 1.3740
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 34.34738850593567

Epoch: [13][38/196]	LR: 0.001	Loss 0.1681 (0.1583)	Prec@1 98.828 (98.828)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.1587 (0.1566)	Prec@1 98.438 (98.898)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.1425 (0.1568)	Prec@1 99.219 (98.862)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.1514 (0.1578)	Prec@1 99.609 (98.836)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.1750 (0.1591)	Prec@1 98.828 (98.808)	
Total train loss: 0.1592

Train time: 30.281757354736328
 * Prec@1 65.890 Prec@5 88.000 Loss 1.3779
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.59700417518616

Epoch: [14][38/196]	LR: 0.001	Loss 0.1478 (0.1552)	Prec@1 98.828 (98.948)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.1403 (0.1559)	Prec@1 99.219 (98.903)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.1431 (0.1577)	Prec@1 100.000 (98.791)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.1626 (0.1584)	Prec@1 99.219 (98.788)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.1381 (0.1579)	Prec@1 99.219 (98.812)	
Total train loss: 0.1581

Train time: 29.104300260543823
 * Prec@1 65.880 Prec@5 88.210 Loss 1.3740
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.60572838783264

Epoch: [15][38/196]	LR: 0.001	Loss 0.1346 (0.1509)	Prec@1 98.828 (98.918)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.1511 (0.1515)	Prec@1 99.609 (98.903)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.1466 (0.1531)	Prec@1 98.828 (98.841)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.1372 (0.1531)	Prec@1 99.609 (98.893)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.1421 (0.1541)	Prec@1 99.219 (98.866)	
Total train loss: 0.1543

Train time: 30.82962131500244
 * Prec@1 65.880 Prec@5 88.000 Loss 1.3789
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.70584845542908

Epoch: [16][38/196]	LR: 0.0001	Loss 0.1440 (0.1531)	Prec@1 99.609 (98.938)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.1597 (0.1522)	Prec@1 98.438 (98.923)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.1494 (0.1516)	Prec@1 99.219 (98.972)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.1603 (0.1519)	Prec@1 98.438 (98.971)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.1364 (0.1523)	Prec@1 98.828 (98.946)	
Total train loss: 0.1526

Train time: 29.16042447090149
 * Prec@1 65.740 Prec@5 87.950 Loss 1.3809
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.362602949142456

Epoch: [17][38/196]	LR: 0.0001	Loss 0.1748 (0.1550)	Prec@1 98.438 (98.808)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.1416 (0.1517)	Prec@1 98.828 (98.948)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.1566 (0.1533)	Prec@1 98.828 (98.908)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.1530 (0.1530)	Prec@1 99.219 (98.933)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.1562 (0.1529)	Prec@1 98.828 (98.952)	
Total train loss: 0.1532

Train time: 28.887776374816895
 * Prec@1 65.850 Prec@5 88.080 Loss 1.3750
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 34.25326418876648

Epoch: [18][38/196]	LR: 0.0001	Loss 0.1425 (0.1463)	Prec@1 98.828 (99.209)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.1688 (0.1489)	Prec@1 99.219 (99.028)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.1625 (0.1500)	Prec@1 98.438 (99.008)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.1344 (0.1511)	Prec@1 99.219 (98.963)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.1443 (0.1508)	Prec@1 99.219 (98.992)	
Total train loss: 0.1510

Train time: 28.50309181213379
 * Prec@1 65.900 Prec@5 87.890 Loss 1.3828
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 35.996925592422485

Epoch: [19][38/196]	LR: 0.0001	Loss 0.1639 (0.1540)	Prec@1 97.656 (98.808)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.1469 (0.1525)	Prec@1 99.219 (98.828)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.1824 (0.1536)	Prec@1 98.438 (98.821)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.1498 (0.1530)	Prec@1 98.828 (98.863)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.1542 (0.1520)	Prec@1 99.219 (98.904)	
Total train loss: 0.1521

Train time: 27.77901530265808
 * Prec@1 65.980 Prec@5 88.030 Loss 1.3828
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 33.11371183395386

Epoch: [20][38/196]	LR: 0.0001	Loss 0.1522 (0.1486)	Prec@1 98.828 (98.988)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.1835 (0.1516)	Prec@1 98.828 (98.948)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.1248 (0.1517)	Prec@1 98.828 (98.992)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.1660 (0.1514)	Prec@1 98.438 (99.021)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.1727 (0.1517)	Prec@1 97.656 (98.972)	
Total train loss: 0.1521

Train time: 29.722691535949707
 * Prec@1 65.840 Prec@5 87.950 Loss 1.3779
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.75322604179382

Epoch: [21][38/196]	LR: 0.0001	Loss 0.1346 (0.1517)	Prec@1 99.609 (99.079)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.1505 (0.1526)	Prec@1 98.438 (98.933)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.1500 (0.1541)	Prec@1 99.219 (98.905)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.1655 (0.1527)	Prec@1 98.828 (98.943)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.1741 (0.1534)	Prec@1 98.438 (98.910)	
Total train loss: 0.1536

Train time: 28.357481956481934
 * Prec@1 66.050 Prec@5 88.080 Loss 1.3760
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 34.15178084373474

Epoch: [22][38/196]	LR: 0.0001	Loss 0.1384 (0.1502)	Prec@1 98.828 (99.008)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.1525 (0.1524)	Prec@1 99.219 (99.028)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.1494 (0.1526)	Prec@1 99.219 (98.975)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.1455 (0.1522)	Prec@1 99.609 (98.983)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.1356 (0.1513)	Prec@1 99.219 (98.982)	
Total train loss: 0.1515

Train time: 29.954981088638306
 * Prec@1 66.110 Prec@5 88.100 Loss 1.3740
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 37.20625448226929

Epoch: [23][38/196]	LR: 0.0001	Loss 0.1715 (0.1516)	Prec@1 98.828 (98.978)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.1499 (0.1533)	Prec@1 99.219 (98.918)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.1342 (0.1538)	Prec@1 100.000 (98.942)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.1556 (0.1522)	Prec@1 99.609 (98.976)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.1628 (0.1522)	Prec@1 98.047 (98.952)	
Total train loss: 0.1524

Train time: 27.993577003479004
 * Prec@1 65.970 Prec@5 88.050 Loss 1.3760
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 33.981743812561035

Epoch: [24][38/196]	LR: 1e-05	Loss 0.1440 (0.1560)	Prec@1 99.219 (98.858)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.1523 (0.1538)	Prec@1 98.828 (98.883)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.1481 (0.1535)	Prec@1 99.219 (98.912)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.1804 (0.1537)	Prec@1 98.438 (98.886)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.1459 (0.1525)	Prec@1 99.219 (98.898)	
Total train loss: 0.1526

Train time: 29.150819540023804
 * Prec@1 65.960 Prec@5 88.040 Loss 1.3770
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.59682273864746

Epoch: [25][38/196]	LR: 1e-05	Loss 0.1289 (0.1543)	Prec@1 98.828 (98.878)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.1537 (0.1525)	Prec@1 96.875 (98.908)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.1288 (0.1517)	Prec@1 99.219 (98.952)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.1300 (0.1520)	Prec@1 99.609 (98.913)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.1351 (0.1517)	Prec@1 99.219 (98.944)	
Total train loss: 0.1518

Train time: 28.959158658981323
 * Prec@1 66.070 Prec@5 87.980 Loss 1.3750
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 34.718796730041504

Epoch: [26][38/196]	LR: 1e-05	Loss 0.1768 (0.1513)	Prec@1 98.438 (98.948)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.1522 (0.1526)	Prec@1 99.609 (98.983)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.1672 (0.1521)	Prec@1 98.438 (98.972)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.1479 (0.1538)	Prec@1 100.000 (98.921)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.1414 (0.1528)	Prec@1 98.828 (98.960)	
Total train loss: 0.1529

Train time: 29.728729724884033
 * Prec@1 66.050 Prec@5 88.160 Loss 1.3740
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 36.91405129432678

Epoch: [27][38/196]	LR: 1e-05	Loss 0.1509 (0.1550)	Prec@1 98.047 (98.748)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.1331 (0.1542)	Prec@1 99.609 (98.828)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.1394 (0.1525)	Prec@1 99.219 (98.948)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.1143 (0.1512)	Prec@1 99.609 (99.003)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.1853 (0.1516)	Prec@1 97.656 (98.966)	
Total train loss: 0.1518

Train time: 23.063903331756592
 * Prec@1 66.000 Prec@5 87.940 Loss 1.3809
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 27.1454017162323

Epoch: [28][38/196]	LR: 1e-05	Loss 0.1637 (0.1534)	Prec@1 100.000 (99.149)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.1599 (0.1521)	Prec@1 98.828 (99.094)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.1431 (0.1523)	Prec@1 99.219 (99.058)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.1439 (0.1515)	Prec@1 99.219 (99.061)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.2212 (0.1516)	Prec@1 96.875 (99.044)	
Total train loss: 0.1517

Train time: 20.860694408416748
 * Prec@1 65.930 Prec@5 87.970 Loss 1.3760
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 25.916860818862915

Epoch: [29][38/196]	LR: 1e-05	Loss 0.1321 (0.1528)	Prec@1 99.609 (99.008)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.1700 (0.1533)	Prec@1 99.219 (98.963)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.1444 (0.1537)	Prec@1 98.828 (98.948)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.1614 (0.1517)	Prec@1 98.828 (98.976)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.1554 (0.1513)	Prec@1 99.219 (98.976)	
Total train loss: 0.1516

Train time: 21.34120774269104
 * Prec@1 65.900 Prec@5 87.880 Loss 1.3828
Best acc: 66.860
--------------------------------------------------------------------------------
Test time: 26.35068655014038


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 68.880 Prec@5 90.220 Loss 1.2041
Pre-trained Prec@1 with 3 layers frozen: 68.87999725341797 	 Loss: 1.2041015625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.7026 (0.5332)	Prec@1 76.953 (84.465)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.6060 (0.5605)	Prec@1 85.547 (83.939)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.5723 (0.5779)	Prec@1 85.938 (83.413)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.5869 (0.5869)	Prec@1 86.328 (83.143)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.6279 (0.5993)	Prec@1 81.641 (82.855)	
Total train loss: 0.5999

Train time: 130.46707439422607
 * Prec@1 66.960 Prec@5 89.570 Loss 1.2617
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 135.28307175636292

Epoch: [1][38/196]	LR: 0.01	Loss 0.4429 (0.4709)	Prec@1 89.453 (87.951)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.4087 (0.4906)	Prec@1 89.844 (87.059)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.5796 (0.5022)	Prec@1 83.203 (86.532)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.5762 (0.5147)	Prec@1 83.984 (86.033)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.6470 (0.5220)	Prec@1 80.859 (85.709)	
Total train loss: 0.5224

Train time: 24.100506067276
 * Prec@1 66.860 Prec@5 89.290 Loss 1.2783
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.170233011245728

Epoch: [2][38/196]	LR: 0.01	Loss 0.4841 (0.4342)	Prec@1 83.594 (89.052)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.4314 (0.4340)	Prec@1 87.500 (88.847)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.4839 (0.4396)	Prec@1 88.281 (88.739)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.5986 (0.4485)	Prec@1 83.594 (88.336)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.4836 (0.4585)	Prec@1 87.109 (87.995)	
Total train loss: 0.4588

Train time: 20.810594081878662
 * Prec@1 66.460 Prec@5 88.850 Loss 1.3076
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.418328046798706

Epoch: [3][38/196]	LR: 0.01	Loss 0.3708 (0.3768)	Prec@1 91.797 (91.336)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.4036 (0.3849)	Prec@1 90.234 (90.885)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.4604 (0.3956)	Prec@1 90.234 (90.391)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.4585 (0.4041)	Prec@1 87.109 (90.142)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.4175 (0.4090)	Prec@1 90.234 (89.910)	
Total train loss: 0.4093

Train time: 20.166741609573364
 * Prec@1 65.710 Prec@5 88.780 Loss 1.3340
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.299410581588745

Epoch: [4][38/196]	LR: 0.01	Loss 0.3738 (0.3304)	Prec@1 91.016 (92.879)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.3826 (0.3357)	Prec@1 89.453 (92.688)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.3547 (0.3439)	Prec@1 91.406 (92.341)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.3206 (0.3531)	Prec@1 93.359 (92.015)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.4497 (0.3631)	Prec@1 88.281 (91.643)	
Total train loss: 0.3634

Train time: 22.235604524612427
 * Prec@1 65.980 Prec@5 88.610 Loss 1.3350
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.202531099319458

Epoch: [5][38/196]	LR: 0.01	Loss 0.2803 (0.3096)	Prec@1 94.531 (93.480)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.3142 (0.3111)	Prec@1 94.141 (93.515)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.3689 (0.3159)	Prec@1 93.359 (93.276)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.4138 (0.3191)	Prec@1 88.672 (93.242)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.2988 (0.3235)	Prec@1 94.531 (93.097)	
Total train loss: 0.3237

Train time: 21.430025815963745
 * Prec@1 65.550 Prec@5 88.150 Loss 1.3779
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.851106882095337

Epoch: [6][38/196]	LR: 0.01	Loss 0.2422 (0.2733)	Prec@1 97.266 (95.152)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.2734 (0.2747)	Prec@1 95.312 (95.037)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.2910 (0.2814)	Prec@1 93.359 (94.712)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.3364 (0.2902)	Prec@1 91.406 (94.323)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.3438 (0.2968)	Prec@1 91.016 (94.117)	
Total train loss: 0.2971

Train time: 20.032532215118408
 * Prec@1 65.490 Prec@5 87.820 Loss 1.3818
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.021918773651123

Epoch: [7][38/196]	LR: 0.01	Loss 0.2546 (0.2505)	Prec@1 94.922 (95.763)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.3044 (0.2522)	Prec@1 93.359 (95.858)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.2952 (0.2570)	Prec@1 96.484 (95.633)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.3462 (0.2619)	Prec@1 92.578 (95.428)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.2478 (0.2673)	Prec@1 96.094 (95.182)	
Total train loss: 0.2675

Train time: 22.396745681762695
 * Prec@1 65.090 Prec@5 87.570 Loss 1.4102
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.03194570541382

Epoch: [8][38/196]	LR: 0.001	Loss 0.2095 (0.2041)	Prec@1 96.484 (97.336)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.1810 (0.1991)	Prec@1 98.438 (97.406)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.1722 (0.1950)	Prec@1 98.047 (97.569)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.1797 (0.1927)	Prec@1 99.609 (97.679)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.1915 (0.1912)	Prec@1 97.656 (97.712)	
Total train loss: 0.1918

Train time: 21.231640577316284
 * Prec@1 66.030 Prec@5 87.970 Loss 1.3740
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.612833738327026

Epoch: [9][38/196]	LR: 0.001	Loss 0.1257 (0.1753)	Prec@1 100.000 (98.347)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.2042 (0.1757)	Prec@1 97.266 (98.242)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.1517 (0.1749)	Prec@1 98.047 (98.244)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.1725 (0.1737)	Prec@1 98.438 (98.297)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.1791 (0.1752)	Prec@1 98.047 (98.275)	
Total train loss: 0.1754

Train time: 21.36460828781128
 * Prec@1 66.040 Prec@5 88.050 Loss 1.3799
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.411226511001587

Epoch: [10][38/196]	LR: 0.001	Loss 0.1667 (0.1688)	Prec@1 98.438 (98.327)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.1501 (0.1666)	Prec@1 99.219 (98.432)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.1521 (0.1670)	Prec@1 98.828 (98.444)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.1509 (0.1677)	Prec@1 100.000 (98.475)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.2000 (0.1689)	Prec@1 98.047 (98.480)	
Total train loss: 0.1692

Train time: 22.592594623565674
 * Prec@1 66.010 Prec@5 88.020 Loss 1.3750
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.702150583267212

Epoch: [11][38/196]	LR: 0.001	Loss 0.1685 (0.1600)	Prec@1 98.828 (98.638)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.1667 (0.1615)	Prec@1 98.828 (98.628)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.1628 (0.1625)	Prec@1 98.047 (98.614)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.1658 (0.1637)	Prec@1 98.047 (98.558)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.1537 (0.1641)	Prec@1 98.828 (98.564)	
Total train loss: 0.1643

Train time: 21.65679693222046
 * Prec@1 65.930 Prec@5 87.900 Loss 1.3867
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.18922257423401

Epoch: [12][38/196]	LR: 0.001	Loss 0.1761 (0.1604)	Prec@1 99.609 (98.688)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.1877 (0.1629)	Prec@1 98.047 (98.628)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.1514 (0.1624)	Prec@1 98.438 (98.688)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.1528 (0.1624)	Prec@1 98.438 (98.690)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.1549 (0.1626)	Prec@1 99.219 (98.666)	
Total train loss: 0.1627

Train time: 22.392805337905884
 * Prec@1 65.990 Prec@5 87.860 Loss 1.3887
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.668984174728394

Epoch: [13][38/196]	LR: 0.001	Loss 0.1266 (0.1530)	Prec@1 99.609 (98.918)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.1573 (0.1575)	Prec@1 98.438 (98.753)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.1517 (0.1585)	Prec@1 99.219 (98.751)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.1599 (0.1587)	Prec@1 98.047 (98.738)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.1212 (0.1594)	Prec@1 99.609 (98.738)	
Total train loss: 0.1597

Train time: 21.538692235946655
 * Prec@1 65.850 Prec@5 87.880 Loss 1.3887
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.951903820037842

Epoch: [14][38/196]	LR: 0.001	Loss 0.1687 (0.1553)	Prec@1 99.609 (98.958)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.1545 (0.1548)	Prec@1 98.828 (98.893)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.1525 (0.1567)	Prec@1 99.609 (98.862)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.1920 (0.1565)	Prec@1 96.484 (98.798)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.1732 (0.1568)	Prec@1 97.656 (98.796)	
Total train loss: 0.1570

Train time: 21.34963822364807
 * Prec@1 65.760 Prec@5 87.830 Loss 1.3896
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.395602464675903

Epoch: [15][38/196]	LR: 0.001	Loss 0.1360 (0.1583)	Prec@1 99.219 (98.878)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.1584 (0.1557)	Prec@1 98.438 (98.878)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.1729 (0.1562)	Prec@1 98.828 (98.848)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.1432 (0.1563)	Prec@1 99.609 (98.891)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.1570 (0.1565)	Prec@1 98.047 (98.864)	
Total train loss: 0.1566

Train time: 23.556968450546265
 * Prec@1 66.270 Prec@5 87.890 Loss 1.3867
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 28.943077087402344

Epoch: [16][38/196]	LR: 0.0001	Loss 0.1443 (0.1538)	Prec@1 99.219 (98.878)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.1405 (0.1538)	Prec@1 98.828 (98.898)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.1482 (0.1539)	Prec@1 100.000 (98.918)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.1289 (0.1532)	Prec@1 99.609 (98.961)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.1558 (0.1534)	Prec@1 99.219 (98.974)	
Total train loss: 0.1535

Train time: 21.87680149078369
 * Prec@1 65.850 Prec@5 87.850 Loss 1.3877
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.42973756790161

Epoch: [17][38/196]	LR: 0.0001	Loss 0.1306 (0.1527)	Prec@1 99.609 (99.058)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.1505 (0.1532)	Prec@1 98.438 (98.983)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.1292 (0.1522)	Prec@1 98.828 (98.965)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.1630 (0.1532)	Prec@1 97.656 (98.941)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.1244 (0.1516)	Prec@1 99.609 (99.008)	
Total train loss: 0.1517

Train time: 21.145347833633423
 * Prec@1 65.820 Prec@5 87.750 Loss 1.3945
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.347362756729126

Epoch: [18][38/196]	LR: 0.0001	Loss 0.1754 (0.1528)	Prec@1 99.219 (99.028)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.1754 (0.1554)	Prec@1 98.438 (98.948)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.1411 (0.1531)	Prec@1 99.609 (98.985)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.1704 (0.1525)	Prec@1 99.219 (99.016)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.1694 (0.1528)	Prec@1 98.828 (99.000)	
Total train loss: 0.1530

Train time: 21.432738304138184
 * Prec@1 65.850 Prec@5 87.840 Loss 1.3965
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.037708044052124

Epoch: [19][38/196]	LR: 0.0001	Loss 0.1349 (0.1499)	Prec@1 99.219 (99.028)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.1572 (0.1507)	Prec@1 98.828 (98.968)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.1434 (0.1510)	Prec@1 100.000 (98.982)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.1573 (0.1516)	Prec@1 98.828 (98.981)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.1440 (0.1512)	Prec@1 98.828 (98.984)	
Total train loss: 0.1514

Train time: 22.427769899368286
 * Prec@1 65.970 Prec@5 87.910 Loss 1.3965
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.38070011138916

Epoch: [20][38/196]	LR: 0.0001	Loss 0.1652 (0.1489)	Prec@1 98.438 (99.038)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.1588 (0.1502)	Prec@1 99.609 (99.058)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.1382 (0.1507)	Prec@1 98.828 (98.995)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.1427 (0.1535)	Prec@1 98.828 (98.911)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.1420 (0.1529)	Prec@1 99.219 (98.926)	
Total train loss: 0.1530

Train time: 21.862252235412598
 * Prec@1 65.900 Prec@5 88.000 Loss 1.3877
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.80185317993164

Epoch: [21][38/196]	LR: 0.0001	Loss 0.1825 (0.1499)	Prec@1 97.656 (99.069)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.1366 (0.1505)	Prec@1 100.000 (99.058)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.1497 (0.1511)	Prec@1 98.828 (99.045)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.1224 (0.1506)	Prec@1 99.219 (99.043)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.1624 (0.1511)	Prec@1 98.828 (99.036)	
Total train loss: 0.1514

Train time: 22.358603477478027
 * Prec@1 65.730 Prec@5 87.900 Loss 1.3926
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 27.182528018951416

Epoch: [22][38/196]	LR: 0.0001	Loss 0.1566 (0.1517)	Prec@1 99.609 (98.968)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.1572 (0.1541)	Prec@1 98.047 (98.948)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.1520 (0.1524)	Prec@1 98.828 (99.005)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.1812 (0.1531)	Prec@1 98.828 (98.976)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.1504 (0.1527)	Prec@1 98.438 (98.982)	
Total train loss: 0.1530

Train time: 21.64416766166687
 * Prec@1 65.870 Prec@5 87.890 Loss 1.3887
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.387720823287964

Epoch: [23][38/196]	LR: 0.0001	Loss 0.1389 (0.1499)	Prec@1 98.828 (98.938)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.1385 (0.1523)	Prec@1 100.000 (98.938)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.1315 (0.1513)	Prec@1 98.828 (98.982)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.1785 (0.1514)	Prec@1 98.828 (98.998)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.1373 (0.1517)	Prec@1 98.047 (98.996)	
Total train loss: 0.1519

Train time: 21.477890968322754
 * Prec@1 65.950 Prec@5 88.000 Loss 1.3818
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.792211771011353

Epoch: [24][38/196]	LR: 1e-05	Loss 0.1520 (0.1481)	Prec@1 98.828 (99.109)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.1492 (0.1498)	Prec@1 98.438 (99.038)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.1954 (0.1515)	Prec@1 98.438 (99.018)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.1670 (0.1505)	Prec@1 99.609 (99.028)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.1317 (0.1511)	Prec@1 100.000 (99.004)	
Total train loss: 0.1512

Train time: 21.07353162765503
 * Prec@1 65.860 Prec@5 87.830 Loss 1.3936
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.63774299621582

Epoch: [25][38/196]	LR: 1e-05	Loss 0.1537 (0.1505)	Prec@1 98.438 (99.099)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.1824 (0.1519)	Prec@1 98.828 (99.003)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.1489 (0.1522)	Prec@1 99.219 (98.918)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.1781 (0.1530)	Prec@1 98.828 (98.916)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.1450 (0.1527)	Prec@1 100.000 (98.916)	
Total train loss: 0.1530

Train time: 21.812177896499634
 * Prec@1 65.790 Prec@5 87.900 Loss 1.3867
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.461819648742676

Epoch: [26][38/196]	LR: 1e-05	Loss 0.1674 (0.1492)	Prec@1 98.438 (98.828)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.1610 (0.1506)	Prec@1 99.609 (98.808)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.1406 (0.1518)	Prec@1 99.219 (98.858)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.1641 (0.1512)	Prec@1 98.438 (98.886)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.1797 (0.1518)	Prec@1 99.219 (98.920)	
Total train loss: 0.1520

Train time: 21.546314239501953
 * Prec@1 65.820 Prec@5 87.770 Loss 1.3965
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.922746419906616

Epoch: [27][38/196]	LR: 1e-05	Loss 0.1281 (0.1500)	Prec@1 99.609 (98.948)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.1620 (0.1548)	Prec@1 98.828 (98.793)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.1169 (0.1523)	Prec@1 99.219 (98.872)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.1475 (0.1521)	Prec@1 98.828 (98.871)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.1571 (0.1529)	Prec@1 98.828 (98.884)	
Total train loss: 0.1530

Train time: 21.82548451423645
 * Prec@1 65.780 Prec@5 87.720 Loss 1.3867
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.75330638885498

Epoch: [28][38/196]	LR: 1e-05	Loss 0.1332 (0.1541)	Prec@1 99.609 (98.938)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.1561 (0.1540)	Prec@1 98.828 (98.933)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.1517 (0.1530)	Prec@1 98.438 (98.962)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.1429 (0.1523)	Prec@1 99.219 (98.961)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.1381 (0.1520)	Prec@1 98.828 (98.986)	
Total train loss: 0.1524

Train time: 21.369853258132935
 * Prec@1 65.710 Prec@5 87.940 Loss 1.3857
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 26.12644100189209

Epoch: [29][38/196]	LR: 1e-05	Loss 0.1713 (0.1546)	Prec@1 99.609 (98.898)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.1356 (0.1516)	Prec@1 99.609 (98.953)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.1416 (0.1518)	Prec@1 99.219 (98.968)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.1432 (0.1517)	Prec@1 100.000 (98.933)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.1884 (0.1524)	Prec@1 98.047 (98.906)	
Total train loss: 0.1526

Train time: 20.921301126480103
 * Prec@1 65.670 Prec@5 87.730 Loss 1.3916
Best acc: 66.960
--------------------------------------------------------------------------------
Test time: 25.02888011932373

