
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 49.780 Prec@5 90.690 Loss 2.4121
Pre-trained Prec@1 with 11 layers frozen: 49.779998779296875 	 Loss: 2.412109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.1992 (0.3136)	Prec@1 92.969 (89.704)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.2411 (0.2838)	Prec@1 91.016 (90.575)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2729 (0.2713)	Prec@1 89.453 (90.855)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.2245 (0.2657)	Prec@1 89.844 (91.008)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.1902 (0.2643)	Prec@1 92.188 (91.004)	
Total train loss: 0.2644

Train time: 1565.9709692001343
 * Prec@1 82.550 Prec@5 99.020 Loss 0.5425
Best acc: 82.550
--------------------------------------------------------------------------------
Test time: 1577.251821756363

Epoch: [1][38/196]	LR: 0.1	Loss 0.1776 (0.2164)	Prec@1 94.141 (92.177)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.2351 (0.2294)	Prec@1 92.578 (91.842)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.2340 (0.2351)	Prec@1 92.969 (91.700)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.3206 (0.2409)	Prec@1 90.234 (91.587)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.3347 (0.2465)	Prec@1 88.281 (91.424)	
Total train loss: 0.2464

Train time: 22.81970763206482
 * Prec@1 83.680 Prec@5 99.020 Loss 0.4980
Best acc: 83.680
--------------------------------------------------------------------------------
Test time: 27.12394118309021

Epoch: [2][38/196]	LR: 0.1	Loss 0.2847 (0.2610)	Prec@1 88.281 (90.625)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.2944 (0.2672)	Prec@1 88.672 (90.500)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.2742 (0.2615)	Prec@1 87.109 (90.779)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.2676 (0.2706)	Prec@1 91.797 (90.512)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.2810 (0.2768)	Prec@1 90.625 (90.337)	
Total train loss: 0.2767

Train time: 20.80075430870056
 * Prec@1 58.820 Prec@5 91.750 Loss 1.5332
Best acc: 83.680
--------------------------------------------------------------------------------
Test time: 24.704188585281372

Epoch: [3][38/196]	LR: 0.1	Loss 0.2996 (0.2655)	Prec@1 88.281 (90.725)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.2905 (0.2745)	Prec@1 91.016 (90.239)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.3411 (0.2928)	Prec@1 84.766 (89.507)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.3547 (0.3040)	Prec@1 85.938 (89.115)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.3137 (0.3074)	Prec@1 88.281 (89.083)	
Total train loss: 0.3074

Train time: 19.086348056793213
 * Prec@1 78.010 Prec@5 97.870 Loss 0.7539
Best acc: 83.680
--------------------------------------------------------------------------------
Test time: 22.232173204421997

Epoch: [4][38/196]	LR: 0.1	Loss 0.3293 (0.2800)	Prec@1 89.062 (90.184)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.2284 (0.2822)	Prec@1 91.406 (89.999)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.2820 (0.2876)	Prec@1 91.797 (89.887)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.3337 (0.2861)	Prec@1 85.156 (89.951)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.1946 (0.2822)	Prec@1 91.797 (90.058)	
Total train loss: 0.2820

Train time: 20.150418281555176
 * Prec@1 84.280 Prec@5 98.990 Loss 0.5078
Best acc: 84.280
--------------------------------------------------------------------------------
Test time: 24.09632110595703

Epoch: [5][38/196]	LR: 0.1	Loss 0.3569 (0.2314)	Prec@1 86.328 (91.767)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.3892 (0.2511)	Prec@1 87.109 (91.126)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2510 (0.2591)	Prec@1 91.406 (90.935)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.2155 (0.2544)	Prec@1 92.969 (91.103)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.2421 (0.2522)	Prec@1 91.406 (91.136)	
Total train loss: 0.2521

Train time: 20.06933331489563
 * Prec@1 79.290 Prec@5 98.710 Loss 0.6758
Best acc: 84.280
--------------------------------------------------------------------------------
Test time: 23.477566957473755

Epoch: [6][38/196]	LR: 0.1	Loss 0.2394 (0.2392)	Prec@1 90.625 (91.617)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.2235 (0.2597)	Prec@1 92.578 (90.735)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.3108 (0.2676)	Prec@1 89.453 (90.532)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.2202 (0.2669)	Prec@1 93.750 (90.565)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2336 (0.2641)	Prec@1 90.625 (90.671)	
Total train loss: 0.2643

Train time: 18.915189743041992
 * Prec@1 74.590 Prec@5 98.040 Loss 0.8945
Best acc: 84.280
--------------------------------------------------------------------------------
Test time: 23.057153463363647

Epoch: [7][38/196]	LR: 0.1	Loss 0.2377 (0.2235)	Prec@1 89.453 (91.937)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.2343 (0.2167)	Prec@1 91.016 (92.243)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2096 (0.2156)	Prec@1 92.578 (92.321)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.2856 (0.2160)	Prec@1 91.016 (92.365)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.2357 (0.2210)	Prec@1 91.016 (92.228)	
Total train loss: 0.2210

Train time: 18.8729088306427
 * Prec@1 24.290 Prec@5 72.800 Loss inf
Best acc: 84.280
--------------------------------------------------------------------------------
Test time: 22.39258098602295

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2174 (0.2259)	Prec@1 91.406 (92.308)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2196 (0.2184)	Prec@1 92.188 (92.643)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2310 (0.2136)	Prec@1 93.750 (92.832)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2484 (0.2082)	Prec@1 91.797 (92.989)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2163 (0.2071)	Prec@1 92.578 (93.013)	
Total train loss: 0.2070

Train time: 20.953898191452026
 * Prec@1 87.060 Prec@5 99.470 Loss 0.4104
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 24.740363597869873

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2288 (0.1954)	Prec@1 92.578 (93.269)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1797 (0.1934)	Prec@1 94.141 (93.279)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1477 (0.1883)	Prec@1 96.875 (93.596)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.1511 (0.1880)	Prec@1 94.922 (93.585)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2191 (0.1877)	Prec@1 92.969 (93.602)	
Total train loss: 0.1877

Train time: 19.064549922943115
 * Prec@1 86.870 Prec@5 99.530 Loss 0.4175
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.464123964309692

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1541 (0.1878)	Prec@1 94.141 (93.590)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1698 (0.1823)	Prec@1 95.312 (93.715)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1899 (0.1814)	Prec@1 95.703 (93.743)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.1543 (0.1843)	Prec@1 95.703 (93.642)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2168 (0.1851)	Prec@1 92.578 (93.616)	
Total train loss: 0.1851

Train time: 19.76891827583313
 * Prec@1 86.860 Prec@5 99.570 Loss 0.4121
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.377163410186768

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1909 (0.1776)	Prec@1 93.359 (94.060)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1846 (0.1821)	Prec@1 94.531 (93.740)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1617 (0.1832)	Prec@1 93.359 (93.687)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1968 (0.1840)	Prec@1 93.750 (93.682)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2303 (0.1845)	Prec@1 91.406 (93.666)	
Total train loss: 0.1844

Train time: 20.13034725189209
 * Prec@1 86.760 Prec@5 99.500 Loss 0.4211
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.94494891166687

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2443 (0.1876)	Prec@1 90.625 (93.379)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1382 (0.1851)	Prec@1 96.094 (93.535)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1428 (0.1898)	Prec@1 95.703 (93.353)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1752 (0.1894)	Prec@1 92.578 (93.434)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1733 (0.1893)	Prec@1 94.531 (93.460)	
Total train loss: 0.1894

Train time: 18.8236882686615
 * Prec@1 86.710 Prec@5 99.520 Loss 0.4243
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.04380989074707

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1497 (0.1937)	Prec@1 94.141 (93.279)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2109 (0.1905)	Prec@1 92.188 (93.475)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1902 (0.1937)	Prec@1 94.922 (93.279)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1986 (0.1933)	Prec@1 91.797 (93.282)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1699 (0.1951)	Prec@1 94.531 (93.201)	
Total train loss: 0.1953

Train time: 19.3982675075531
 * Prec@1 86.350 Prec@5 99.420 Loss 0.4341
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.008378982543945

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1555 (0.1946)	Prec@1 94.141 (93.169)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1670 (0.1949)	Prec@1 93.750 (93.169)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1793 (0.1934)	Prec@1 94.922 (93.286)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2115 (0.1939)	Prec@1 94.531 (93.274)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2020 (0.1948)	Prec@1 93.750 (93.271)	
Total train loss: 0.1949

Train time: 18.377336263656616
 * Prec@1 86.560 Prec@5 99.440 Loss 0.4309
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.339507341384888

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1880 (0.1896)	Prec@1 93.750 (93.650)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1687 (0.1945)	Prec@1 94.922 (93.334)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1580 (0.1947)	Prec@1 94.922 (93.369)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2435 (0.1979)	Prec@1 91.797 (93.167)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1848 (0.1998)	Prec@1 93.750 (93.041)	
Total train loss: 0.1999

Train time: 18.687753200531006
 * Prec@1 86.660 Prec@5 99.430 Loss 0.4285
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.17235279083252

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1675 (0.1875)	Prec@1 95.312 (93.209)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1753 (0.1934)	Prec@1 94.531 (93.114)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1976 (0.1985)	Prec@1 92.578 (92.919)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2440 (0.2007)	Prec@1 92.188 (92.909)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1826 (0.2011)	Prec@1 92.188 (92.949)	
Total train loss: 0.2009

Train time: 19.449471473693848
 * Prec@1 86.630 Prec@5 99.430 Loss 0.4336
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.538610219955444

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1459 (0.2045)	Prec@1 94.531 (92.839)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2065 (0.2022)	Prec@1 92.188 (92.944)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1844 (0.2010)	Prec@1 94.141 (92.932)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2010 (0.2001)	Prec@1 94.141 (92.996)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2006 (0.1996)	Prec@1 93.359 (93.001)	
Total train loss: 0.1996

Train time: 19.598016262054443
 * Prec@1 86.640 Prec@5 99.420 Loss 0.4360
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.65157699584961

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1826 (0.2000)	Prec@1 95.703 (92.989)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.2156 (0.1969)	Prec@1 91.406 (93.129)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1779 (0.1963)	Prec@1 94.141 (93.219)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1892 (0.1988)	Prec@1 94.922 (93.114)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1604 (0.1990)	Prec@1 94.141 (93.077)	
Total train loss: 0.1990

Train time: 19.030681848526
 * Prec@1 86.530 Prec@5 99.430 Loss 0.4353
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.047404766082764

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2754 (0.2065)	Prec@1 89.453 (92.889)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1819 (0.2049)	Prec@1 92.578 (92.969)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.2004 (0.2023)	Prec@1 93.359 (93.046)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1322 (0.2001)	Prec@1 95.703 (93.116)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.2496 (0.1992)	Prec@1 91.016 (93.117)	
Total train loss: 0.1992

Train time: 19.923660039901733
 * Prec@1 86.550 Prec@5 99.410 Loss 0.4329
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.801231861114502

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1871 (0.1958)	Prec@1 94.141 (93.289)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1897 (0.1958)	Prec@1 94.922 (93.379)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1851 (0.1970)	Prec@1 93.359 (93.186)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2107 (0.1977)	Prec@1 92.188 (93.137)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1898 (0.2004)	Prec@1 93.359 (93.005)	
Total train loss: 0.2005

Train time: 18.678486108779907
 * Prec@1 86.590 Prec@5 99.410 Loss 0.4363
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.143388032913208

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2316 (0.2032)	Prec@1 93.359 (92.979)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2095 (0.2015)	Prec@1 93.359 (93.014)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1877 (0.2010)	Prec@1 93.750 (93.076)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2622 (0.2015)	Prec@1 88.281 (92.996)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1793 (0.2003)	Prec@1 94.141 (93.057)	
Total train loss: 0.2004

Train time: 19.342507362365723
 * Prec@1 86.510 Prec@5 99.400 Loss 0.4329
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.597254514694214

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1613 (0.2119)	Prec@1 94.922 (92.167)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2236 (0.2059)	Prec@1 92.578 (92.678)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2607 (0.2016)	Prec@1 89.453 (92.882)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1904 (0.2004)	Prec@1 94.141 (92.994)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2144 (0.2005)	Prec@1 92.578 (93.063)	
Total train loss: 0.2006

Train time: 19.28796148300171
 * Prec@1 86.440 Prec@5 99.440 Loss 0.4348
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.647063970565796

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.2179 (0.1972)	Prec@1 92.578 (93.179)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.2595 (0.1962)	Prec@1 89.453 (93.209)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2145 (0.1999)	Prec@1 92.969 (92.962)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2240 (0.2002)	Prec@1 91.016 (92.984)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2139 (0.2008)	Prec@1 90.625 (92.963)	
Total train loss: 0.2008

Train time: 20.516377925872803
 * Prec@1 86.560 Prec@5 99.450 Loss 0.4368
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.703641653060913

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.2053 (0.2011)	Prec@1 89.844 (93.099)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1514 (0.1996)	Prec@1 93.750 (93.144)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1910 (0.1966)	Prec@1 94.922 (93.199)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2301 (0.2000)	Prec@1 91.406 (93.056)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.2534 (0.2002)	Prec@1 90.625 (92.987)	
Total train loss: 0.2002

Train time: 18.84952735900879
 * Prec@1 86.630 Prec@5 99.430 Loss 0.4353
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.810492992401123

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1765 (0.1933)	Prec@1 94.531 (93.429)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2211 (0.1970)	Prec@1 92.969 (93.179)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2488 (0.1973)	Prec@1 89.844 (93.192)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2426 (0.1994)	Prec@1 91.797 (93.099)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.2029 (0.1992)	Prec@1 91.016 (93.069)	
Total train loss: 0.1993

Train time: 19.654770851135254
 * Prec@1 86.600 Prec@5 99.420 Loss 0.4343
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.989079475402832

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.2505 (0.1964)	Prec@1 89.062 (92.959)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.2625 (0.1993)	Prec@1 90.234 (93.019)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2915 (0.1983)	Prec@1 87.109 (93.079)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1951 (0.1983)	Prec@1 93.750 (93.111)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.2140 (0.2002)	Prec@1 91.016 (93.021)	
Total train loss: 0.2003

Train time: 19.77838158607483
 * Prec@1 86.520 Prec@5 99.420 Loss 0.4360
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.998197317123413

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.2507 (0.2072)	Prec@1 90.234 (92.808)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1746 (0.2020)	Prec@1 94.141 (92.984)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1891 (0.2015)	Prec@1 91.797 (92.965)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2129 (0.1998)	Prec@1 92.188 (93.091)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1298 (0.1996)	Prec@1 96.094 (93.055)	
Total train loss: 0.1997

Train time: 18.659207105636597
 * Prec@1 86.540 Prec@5 99.450 Loss 0.4348
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.61089539527893

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1985 (0.2012)	Prec@1 92.969 (92.869)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1663 (0.2023)	Prec@1 94.531 (92.884)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2126 (0.2038)	Prec@1 92.578 (92.805)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1610 (0.2023)	Prec@1 93.359 (92.924)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1747 (0.2009)	Prec@1 95.312 (92.955)	
Total train loss: 0.2009

Train time: 19.29775357246399
 * Prec@1 86.540 Prec@5 99.420 Loss 0.4355
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.444620370864868

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2036 (0.2037)	Prec@1 93.359 (93.089)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2318 (0.2036)	Prec@1 90.625 (93.084)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2109 (0.2028)	Prec@1 91.016 (93.019)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2463 (0.2015)	Prec@1 92.188 (93.016)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1350 (0.2005)	Prec@1 96.875 (93.075)	
Total train loss: 0.2007

Train time: 19.67424178123474
 * Prec@1 86.480 Prec@5 99.410 Loss 0.4360
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.15955090522766

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.2118 (0.1906)	Prec@1 92.188 (93.389)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.2432 (0.1981)	Prec@1 92.578 (93.214)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.2122 (0.1994)	Prec@1 92.188 (93.159)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.1870 (0.2008)	Prec@1 93.359 (93.106)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.1957 (0.1997)	Prec@1 92.578 (93.075)	
Total train loss: 0.1999

Train time: 20.36473298072815
 * Prec@1 86.470 Prec@5 99.420 Loss 0.4348
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 24.593092679977417

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.2260 (0.1963)	Prec@1 91.016 (93.249)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.2377 (0.1990)	Prec@1 90.234 (93.119)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.2010 (0.1989)	Prec@1 93.750 (93.116)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.1720 (0.2006)	Prec@1 94.141 (93.019)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.1949 (0.2004)	Prec@1 92.578 (93.019)	
Total train loss: 0.2004

Train time: 19.7977135181427
 * Prec@1 86.520 Prec@5 99.450 Loss 0.4331
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.274927139282227

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.1737 (0.1995)	Prec@1 95.312 (93.129)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.2302 (0.2013)	Prec@1 91.016 (93.064)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.2581 (0.2008)	Prec@1 91.797 (93.039)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1737 (0.2034)	Prec@1 94.141 (92.909)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.1737 (0.2007)	Prec@1 94.141 (93.071)	
Total train loss: 0.2010

Train time: 19.632531881332397
 * Prec@1 86.470 Prec@5 99.410 Loss 0.4353
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.972224950790405

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.2600 (0.2039)	Prec@1 88.672 (92.969)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.1455 (0.2012)	Prec@1 95.703 (93.014)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.2219 (0.1996)	Prec@1 92.188 (93.022)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.2170 (0.1991)	Prec@1 91.797 (93.056)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1252 (0.1982)	Prec@1 96.484 (93.109)	
Total train loss: 0.1981

Train time: 20.25977635383606
 * Prec@1 86.390 Prec@5 99.430 Loss 0.4343
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 24.37739086151123

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.2568 (0.1996)	Prec@1 92.188 (93.279)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.1542 (0.1971)	Prec@1 94.922 (93.239)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.1866 (0.1988)	Prec@1 93.750 (93.189)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.1880 (0.1988)	Prec@1 93.359 (93.124)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.2047 (0.1994)	Prec@1 92.578 (93.083)	
Total train loss: 0.1995

Train time: 19.70629596710205
 * Prec@1 86.580 Prec@5 99.410 Loss 0.4329
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.32284164428711

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.1543 (0.1984)	Prec@1 95.312 (93.069)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.2424 (0.2064)	Prec@1 92.188 (92.829)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.2688 (0.2043)	Prec@1 91.797 (92.982)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1716 (0.2023)	Prec@1 93.750 (92.984)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1981 (0.2011)	Prec@1 91.406 (93.027)	
Total train loss: 0.2011

Train time: 20.318257331848145
 * Prec@1 86.600 Prec@5 99.380 Loss 0.4341
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.608397960662842

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.1653 (0.1901)	Prec@1 92.969 (93.369)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.2649 (0.1936)	Prec@1 90.625 (93.229)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1990 (0.1962)	Prec@1 92.969 (93.126)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.1792 (0.1968)	Prec@1 94.141 (93.164)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.1930 (0.1991)	Prec@1 93.750 (93.065)	
Total train loss: 0.1991

Train time: 18.555869817733765
 * Prec@1 86.600 Prec@5 99.440 Loss 0.4341
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.674182891845703

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.1652 (0.1960)	Prec@1 94.141 (93.329)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.2384 (0.2013)	Prec@1 92.188 (92.994)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.1896 (0.2014)	Prec@1 92.188 (92.949)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1733 (0.2016)	Prec@1 93.359 (92.926)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.2184 (0.2018)	Prec@1 94.531 (92.919)	
Total train loss: 0.2018

Train time: 19.768261909484863
 * Prec@1 86.570 Prec@5 99.410 Loss 0.4341
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 23.085480213165283

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.1654 (0.1946)	Prec@1 94.141 (93.189)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1965 (0.1972)	Prec@1 90.625 (92.999)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.2457 (0.2009)	Prec@1 91.016 (92.875)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1957 (0.1986)	Prec@1 94.141 (92.954)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.2208 (0.2008)	Prec@1 92.188 (92.939)	
Total train loss: 0.2008

Train time: 19.469143390655518
 * Prec@1 86.420 Prec@5 99.430 Loss 0.4348
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 22.72416639328003

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.1873 (0.2103)	Prec@1 94.531 (92.628)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.1982 (0.2053)	Prec@1 92.578 (92.849)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.1985 (0.2040)	Prec@1 92.188 (92.909)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.2170 (0.2017)	Prec@1 92.969 (92.951)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1731 (0.2004)	Prec@1 96.094 (93.001)	
Total train loss: 0.2007

Train time: 20.136451482772827
 * Prec@1 86.600 Prec@5 99.450 Loss 0.4343
Best acc: 87.060
--------------------------------------------------------------------------------
Test time: 24.023152828216553


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 44.770 Prec@5 88.530 Loss 2.7383
Pre-trained Prec@1 with 13 layers frozen: 44.77000045776367 	 Loss: 2.73828125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.3354 (0.3677)	Prec@1 85.938 (88.271)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.2910 (0.3352)	Prec@1 89.453 (88.862)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2917 (0.3159)	Prec@1 89.844 (89.353)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.3296 (0.3108)	Prec@1 85.938 (89.423)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.2465 (0.3046)	Prec@1 91.016 (89.587)	
Total train loss: 0.3044

Train time: 249.08437395095825
 * Prec@1 83.340 Prec@5 98.860 Loss 0.4939
Best acc: 83.340
--------------------------------------------------------------------------------
Test time: 256.8998975753784

Epoch: [1][38/196]	LR: 0.1	Loss 0.2856 (0.2540)	Prec@1 89.844 (90.976)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.3542 (0.2589)	Prec@1 89.062 (90.925)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.3535 (0.2614)	Prec@1 88.281 (90.785)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.3044 (0.2608)	Prec@1 89.453 (90.820)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.3396 (0.2626)	Prec@1 89.844 (90.761)	
Total train loss: 0.2626

Train time: 21.266271352767944
 * Prec@1 82.020 Prec@5 98.920 Loss 0.5527
Best acc: 83.340
--------------------------------------------------------------------------------
Test time: 24.8053297996521

Epoch: [2][38/196]	LR: 0.1	Loss 0.2900 (0.2666)	Prec@1 91.797 (90.445)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.2773 (0.2763)	Prec@1 90.234 (90.320)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.3118 (0.2721)	Prec@1 87.500 (90.518)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.2335 (0.2720)	Prec@1 90.625 (90.420)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.2474 (0.2752)	Prec@1 91.016 (90.403)	
Total train loss: 0.2753

Train time: 18.63459086418152
 * Prec@1 85.200 Prec@5 99.300 Loss 0.4448
Best acc: 85.200
--------------------------------------------------------------------------------
Test time: 21.560288429260254

Epoch: [3][38/196]	LR: 0.1	Loss 0.2810 (0.2771)	Prec@1 90.234 (90.094)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.2500 (0.2855)	Prec@1 90.234 (90.009)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.3206 (0.2833)	Prec@1 88.281 (90.061)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.3052 (0.2840)	Prec@1 86.719 (90.034)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.3037 (0.2910)	Prec@1 91.016 (89.806)	
Total train loss: 0.2910

Train time: 17.041852951049805
 * Prec@1 84.220 Prec@5 99.400 Loss 0.4617
Best acc: 85.200
--------------------------------------------------------------------------------
Test time: 20.848528385162354

Epoch: [4][38/196]	LR: 0.1	Loss 0.3240 (0.2916)	Prec@1 88.672 (89.784)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.2715 (0.2883)	Prec@1 91.016 (89.829)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.3555 (0.2849)	Prec@1 87.500 (90.001)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.3110 (0.2843)	Prec@1 88.672 (90.009)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.3218 (0.2828)	Prec@1 86.719 (90.042)	
Total train loss: 0.2830

Train time: 19.532341480255127
 * Prec@1 84.120 Prec@5 99.070 Loss 0.4883
Best acc: 85.200
--------------------------------------------------------------------------------
Test time: 22.735171794891357

Epoch: [5][38/196]	LR: 0.1	Loss 0.2585 (0.2627)	Prec@1 90.234 (90.665)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.2426 (0.2676)	Prec@1 92.188 (90.565)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2485 (0.2691)	Prec@1 90.234 (90.578)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.2747 (0.2719)	Prec@1 91.016 (90.440)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.3047 (0.2719)	Prec@1 90.234 (90.409)	
Total train loss: 0.2720

Train time: 19.106478214263916
 * Prec@1 61.890 Prec@5 91.980 Loss 1.2148
Best acc: 85.200
--------------------------------------------------------------------------------
Test time: 22.849600076675415

Epoch: [6][38/196]	LR: 0.1	Loss 0.2769 (0.2783)	Prec@1 89.844 (90.024)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.4023 (0.2787)	Prec@1 87.500 (90.204)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.2915 (0.2724)	Prec@1 90.625 (90.465)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.1653 (0.2672)	Prec@1 94.531 (90.648)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2727 (0.2645)	Prec@1 89.453 (90.715)	
Total train loss: 0.2644

Train time: 19.331990480422974
 * Prec@1 85.870 Prec@5 99.220 Loss 0.4143
Best acc: 85.870
--------------------------------------------------------------------------------
Test time: 23.27362847328186

Epoch: [7][38/196]	LR: 0.1	Loss 0.2151 (0.2159)	Prec@1 92.969 (92.468)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.1877 (0.2125)	Prec@1 92.188 (92.513)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.1608 (0.2108)	Prec@1 96.094 (92.565)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.1788 (0.2146)	Prec@1 93.359 (92.458)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.2325 (0.2193)	Prec@1 92.578 (92.242)	
Total train loss: 0.2193

Train time: 18.5376079082489
 * Prec@1 77.000 Prec@5 98.020 Loss 0.6851
Best acc: 85.870
--------------------------------------------------------------------------------
Test time: 22.04972553253174

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.2062 (0.2384)	Prec@1 93.750 (91.997)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2372 (0.2331)	Prec@1 92.969 (92.132)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2793 (0.2260)	Prec@1 89.844 (92.311)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2318 (0.2248)	Prec@1 91.797 (92.353)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2443 (0.2229)	Prec@1 89.453 (92.436)	
Total train loss: 0.2230

Train time: 19.75562310218811
 * Prec@1 86.910 Prec@5 99.500 Loss 0.4087
Best acc: 86.910
--------------------------------------------------------------------------------
Test time: 23.030997276306152

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1924 (0.2098)	Prec@1 93.750 (92.929)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2588 (0.2120)	Prec@1 92.188 (92.788)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.1562 (0.2115)	Prec@1 95.312 (92.755)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2252 (0.2123)	Prec@1 92.969 (92.678)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2493 (0.2101)	Prec@1 91.016 (92.782)	
Total train loss: 0.2102

Train time: 19.86013889312744
 * Prec@1 86.990 Prec@5 99.530 Loss 0.4058
Best acc: 86.990
--------------------------------------------------------------------------------
Test time: 24.032823085784912

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2527 (0.2106)	Prec@1 90.625 (92.698)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2673 (0.2077)	Prec@1 90.625 (92.763)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1544 (0.2059)	Prec@1 94.531 (92.885)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2458 (0.2054)	Prec@1 91.406 (92.914)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2134 (0.2080)	Prec@1 91.797 (92.800)	
Total train loss: 0.2081

Train time: 20.304011583328247
 * Prec@1 86.980 Prec@5 99.460 Loss 0.4048
Best acc: 86.990
--------------------------------------------------------------------------------
Test time: 23.837982416152954

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1598 (0.2022)	Prec@1 94.141 (92.969)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1610 (0.2053)	Prec@1 93.750 (92.849)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2487 (0.2064)	Prec@1 91.406 (92.889)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2031 (0.2066)	Prec@1 93.359 (92.899)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1493 (0.2059)	Prec@1 95.703 (92.909)	
Total train loss: 0.2062

Train time: 20.436519145965576
 * Prec@1 87.020 Prec@5 99.500 Loss 0.4043
Best acc: 87.020
--------------------------------------------------------------------------------
Test time: 24.389352798461914

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2908 (0.2100)	Prec@1 89.062 (92.288)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2136 (0.2065)	Prec@1 92.578 (92.648)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2225 (0.2067)	Prec@1 91.016 (92.788)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1888 (0.2066)	Prec@1 94.141 (92.829)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1627 (0.2064)	Prec@1 96.094 (92.887)	
Total train loss: 0.2064

Train time: 17.95093584060669
 * Prec@1 87.020 Prec@5 99.480 Loss 0.4050
Best acc: 87.020
--------------------------------------------------------------------------------
Test time: 21.686668395996094

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1649 (0.2022)	Prec@1 94.531 (93.129)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1874 (0.2068)	Prec@1 93.359 (92.874)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1858 (0.2054)	Prec@1 94.531 (92.945)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1871 (0.2046)	Prec@1 94.141 (92.911)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2610 (0.2053)	Prec@1 91.797 (92.895)	
Total train loss: 0.2054

Train time: 20.117559671401978
 * Prec@1 86.980 Prec@5 99.450 Loss 0.4084
Best acc: 87.020
--------------------------------------------------------------------------------
Test time: 23.44069004058838

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.2095 (0.1931)	Prec@1 91.016 (93.520)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1716 (0.1987)	Prec@1 94.922 (93.244)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1763 (0.2032)	Prec@1 95.703 (93.036)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2013 (0.2051)	Prec@1 93.750 (93.011)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1877 (0.2046)	Prec@1 92.969 (93.039)	
Total train loss: 0.2047

Train time: 20.733609914779663
 * Prec@1 86.950 Prec@5 99.450 Loss 0.4050
Best acc: 87.020
--------------------------------------------------------------------------------
Test time: 24.671717882156372

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2001 (0.2028)	Prec@1 93.359 (92.919)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1998 (0.2036)	Prec@1 92.969 (92.899)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2311 (0.2024)	Prec@1 92.578 (93.026)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2764 (0.2026)	Prec@1 88.281 (92.991)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1752 (0.2034)	Prec@1 92.969 (92.939)	
Total train loss: 0.2035

Train time: 19.551860094070435
 * Prec@1 87.040 Prec@5 99.490 Loss 0.4045
Best acc: 87.040
--------------------------------------------------------------------------------
Test time: 23.32209825515747

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1768 (0.1970)	Prec@1 94.531 (93.349)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1593 (0.1972)	Prec@1 95.312 (93.269)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2084 (0.1990)	Prec@1 92.188 (93.222)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2452 (0.2005)	Prec@1 91.016 (93.111)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2090 (0.2009)	Prec@1 91.797 (93.079)	
Total train loss: 0.2008

Train time: 19.810734272003174
 * Prec@1 87.080 Prec@5 99.440 Loss 0.4050
Best acc: 87.080
--------------------------------------------------------------------------------
Test time: 23.057523727416992

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2200 (0.2099)	Prec@1 92.969 (92.588)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1510 (0.2089)	Prec@1 94.922 (92.904)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.2440 (0.2042)	Prec@1 91.016 (93.022)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2064 (0.2045)	Prec@1 92.188 (92.974)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2026 (0.2026)	Prec@1 94.141 (93.031)	
Total train loss: 0.2026

Train time: 20.07536768913269
 * Prec@1 87.080 Prec@5 99.480 Loss 0.4043
Best acc: 87.080
--------------------------------------------------------------------------------
Test time: 23.856818437576294

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1862 (0.2066)	Prec@1 93.750 (92.558)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1835 (0.2046)	Prec@1 92.578 (92.859)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1719 (0.2008)	Prec@1 94.141 (93.015)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2190 (0.2032)	Prec@1 94.531 (92.939)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.2141 (0.2020)	Prec@1 91.406 (93.037)	
Total train loss: 0.2020

Train time: 19.67248225212097
 * Prec@1 87.080 Prec@5 99.440 Loss 0.4062
Best acc: 87.080
--------------------------------------------------------------------------------
Test time: 23.21560311317444

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2153 (0.1972)	Prec@1 91.797 (93.189)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2151 (0.2017)	Prec@1 92.969 (92.974)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.2002 (0.1998)	Prec@1 92.578 (93.086)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2891 (0.2018)	Prec@1 89.453 (93.069)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1835 (0.2018)	Prec@1 94.141 (93.023)	
Total train loss: 0.2019

Train time: 18.78252363204956
 * Prec@1 87.110 Prec@5 99.480 Loss 0.4060
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.067511320114136

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1863 (0.2054)	Prec@1 93.359 (92.979)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1859 (0.2030)	Prec@1 93.359 (93.014)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2010 (0.2024)	Prec@1 92.969 (93.019)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1971 (0.2017)	Prec@1 94.141 (93.059)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1615 (0.2006)	Prec@1 93.750 (93.125)	
Total train loss: 0.2008

Train time: 18.96576690673828
 * Prec@1 87.040 Prec@5 99.470 Loss 0.4043
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.839780569076538

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2025 (0.2033)	Prec@1 92.578 (92.819)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2145 (0.2023)	Prec@1 91.797 (93.004)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2042 (0.2034)	Prec@1 92.969 (92.869)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1676 (0.2009)	Prec@1 94.531 (93.004)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.2422 (0.2018)	Prec@1 91.406 (92.949)	
Total train loss: 0.2020

Train time: 18.98059868812561
 * Prec@1 87.060 Prec@5 99.450 Loss 0.4062
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.11783766746521

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2339 (0.1999)	Prec@1 93.750 (93.169)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2041 (0.1990)	Prec@1 93.750 (93.294)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1888 (0.1998)	Prec@1 93.359 (93.259)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2229 (0.2012)	Prec@1 92.969 (93.091)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2306 (0.2007)	Prec@1 92.188 (93.117)	
Total train loss: 0.2010

Train time: 18.44341731071472
 * Prec@1 87.110 Prec@5 99.440 Loss 0.4072
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 21.588882446289062

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1752 (0.1989)	Prec@1 94.141 (93.119)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.2396 (0.1975)	Prec@1 89.844 (93.229)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2272 (0.1997)	Prec@1 91.016 (93.189)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2131 (0.2013)	Prec@1 94.141 (93.111)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.3547 (0.2027)	Prec@1 87.500 (93.043)	
Total train loss: 0.2026

Train time: 18.457212209701538
 * Prec@1 87.110 Prec@5 99.470 Loss 0.4055
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.355899810791016

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.2130 (0.1950)	Prec@1 92.188 (93.369)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1511 (0.1977)	Prec@1 95.703 (93.294)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1864 (0.2036)	Prec@1 92.969 (93.069)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2417 (0.2020)	Prec@1 92.188 (93.094)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.2051 (0.2021)	Prec@1 93.359 (93.039)	
Total train loss: 0.2023

Train time: 19.05294108390808
 * Prec@1 87.080 Prec@5 99.480 Loss 0.4062
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.888379335403442

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1951 (0.2016)	Prec@1 92.969 (93.209)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1741 (0.2017)	Prec@1 94.531 (93.109)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1442 (0.2058)	Prec@1 94.531 (92.862)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2113 (0.2038)	Prec@1 93.359 (92.974)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1866 (0.2010)	Prec@1 93.359 (93.051)	
Total train loss: 0.2012

Train time: 18.956387758255005
 * Prec@1 86.960 Prec@5 99.480 Loss 0.4058
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.124961614608765

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.2085 (0.2027)	Prec@1 91.406 (92.949)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1978 (0.2060)	Prec@1 93.359 (92.803)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2505 (0.2010)	Prec@1 90.234 (93.036)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1871 (0.2037)	Prec@1 95.312 (92.966)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1748 (0.2032)	Prec@1 93.359 (92.983)	
Total train loss: 0.2032

Train time: 19.8830087184906
 * Prec@1 87.020 Prec@5 99.470 Loss 0.4055
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.749987363815308

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1498 (0.1954)	Prec@1 95.703 (93.279)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.2064 (0.1989)	Prec@1 93.359 (93.144)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2108 (0.2002)	Prec@1 93.359 (93.122)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2219 (0.2009)	Prec@1 92.969 (93.096)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2319 (0.2023)	Prec@1 91.797 (93.067)	
Total train loss: 0.2023

Train time: 18.737287044525146
 * Prec@1 86.880 Prec@5 99.450 Loss 0.4082
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.572694540023804

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1860 (0.2015)	Prec@1 93.359 (93.189)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2351 (0.2047)	Prec@1 91.016 (93.009)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1410 (0.2045)	Prec@1 96.094 (92.995)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2659 (0.2044)	Prec@1 93.359 (92.946)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.2437 (0.2036)	Prec@1 91.406 (92.995)	
Total train loss: 0.2036

Train time: 18.469852447509766
 * Prec@1 87.050 Prec@5 99.460 Loss 0.4062
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 21.695414543151855

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2302 (0.1984)	Prec@1 90.234 (93.379)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1687 (0.2004)	Prec@1 94.141 (93.224)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2039 (0.2016)	Prec@1 92.578 (93.092)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2192 (0.2008)	Prec@1 92.578 (93.169)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1981 (0.2019)	Prec@1 89.844 (93.143)	
Total train loss: 0.2018

Train time: 18.98831868171692
 * Prec@1 87.000 Prec@5 99.480 Loss 0.4043
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.018341302871704

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.1689 (0.1956)	Prec@1 94.141 (93.029)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.2037 (0.1976)	Prec@1 91.406 (93.074)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.2673 (0.1984)	Prec@1 92.578 (93.059)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.2808 (0.2003)	Prec@1 90.625 (93.086)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2322 (0.2021)	Prec@1 92.188 (93.023)	
Total train loss: 0.2023

Train time: 19.997632265090942
 * Prec@1 87.070 Prec@5 99.480 Loss 0.4065
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.74381732940674

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.2445 (0.1999)	Prec@1 90.234 (92.628)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.2152 (0.2004)	Prec@1 91.016 (92.944)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.1862 (0.2011)	Prec@1 93.750 (92.942)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.2004 (0.2015)	Prec@1 92.578 (92.996)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.2720 (0.2032)	Prec@1 88.672 (92.957)	
Total train loss: 0.2034

Train time: 20.028005123138428
 * Prec@1 86.940 Prec@5 99.460 Loss 0.4072
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.150798559188843

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.2263 (0.1952)	Prec@1 92.578 (93.440)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.1577 (0.1949)	Prec@1 95.312 (93.445)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.1919 (0.1973)	Prec@1 93.750 (93.326)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.1826 (0.2023)	Prec@1 94.922 (93.127)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.2404 (0.2008)	Prec@1 91.016 (93.107)	
Total train loss: 0.2007

Train time: 19.4646213054657
 * Prec@1 86.960 Prec@5 99.470 Loss 0.4043
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.386852025985718

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.1674 (0.2010)	Prec@1 93.750 (92.939)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.2346 (0.2048)	Prec@1 93.359 (92.954)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.1577 (0.2040)	Prec@1 95.312 (93.069)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.1990 (0.2033)	Prec@1 93.750 (93.084)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.1731 (0.2031)	Prec@1 93.750 (93.099)	
Total train loss: 0.2031

Train time: 18.209016799926758
 * Prec@1 86.950 Prec@5 99.490 Loss 0.4072
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 21.961278915405273

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.2096 (0.2070)	Prec@1 91.406 (92.698)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.1992 (0.1999)	Prec@1 93.359 (93.059)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.2214 (0.1996)	Prec@1 93.359 (93.152)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.1566 (0.2007)	Prec@1 94.531 (93.124)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.1776 (0.2016)	Prec@1 91.406 (93.091)	
Total train loss: 0.2016

Train time: 19.612215757369995
 * Prec@1 87.090 Prec@5 99.460 Loss 0.4043
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.936991691589355

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.2812 (0.2051)	Prec@1 89.062 (93.059)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.2273 (0.2020)	Prec@1 92.969 (93.189)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.2249 (0.2037)	Prec@1 92.578 (93.069)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.1871 (0.2053)	Prec@1 94.531 (92.981)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.1868 (0.2021)	Prec@1 93.359 (93.043)	
Total train loss: 0.2021

Train time: 19.09550952911377
 * Prec@1 86.950 Prec@5 99.480 Loss 0.4055
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.710853338241577

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.2047 (0.1986)	Prec@1 92.578 (93.309)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.1520 (0.1976)	Prec@1 94.531 (93.294)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.1620 (0.2013)	Prec@1 94.141 (93.092)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.2507 (0.2017)	Prec@1 91.406 (93.086)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.2258 (0.2022)	Prec@1 92.969 (93.013)	
Total train loss: 0.2023

Train time: 19.894750833511353
 * Prec@1 86.940 Prec@5 99.470 Loss 0.4072
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 23.790798664093018

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.2155 (0.2037)	Prec@1 92.578 (92.939)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.2157 (0.2024)	Prec@1 93.750 (93.064)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.2191 (0.2020)	Prec@1 92.578 (93.079)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.1196 (0.2025)	Prec@1 96.094 (92.976)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.1953 (0.2020)	Prec@1 94.531 (93.059)	
Total train loss: 0.2023

Train time: 19.3313045501709
 * Prec@1 87.060 Prec@5 99.480 Loss 0.4065
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.22021174430847

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.2479 (0.2021)	Prec@1 90.625 (93.019)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.1659 (0.1999)	Prec@1 93.750 (93.064)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.1727 (0.2016)	Prec@1 95.703 (93.046)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.1809 (0.2018)	Prec@1 92.969 (93.009)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.2467 (0.2027)	Prec@1 92.188 (92.977)	
Total train loss: 0.2028

Train time: 18.68637228012085
 * Prec@1 87.020 Prec@5 99.450 Loss 0.4055
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.26266098022461

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.1750 (0.2063)	Prec@1 93.750 (92.959)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.2258 (0.1995)	Prec@1 93.750 (93.254)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.2200 (0.2011)	Prec@1 91.797 (93.129)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.2617 (0.2016)	Prec@1 91.406 (93.109)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.1521 (0.2023)	Prec@1 96.094 (93.039)	
Total train loss: 0.2024

Train time: 19.287139654159546
 * Prec@1 86.910 Prec@5 99.500 Loss 0.4065
Best acc: 87.110
--------------------------------------------------------------------------------
Test time: 22.938118934631348


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 35.250 Prec@5 85.490 Loss 3.0469
Pre-trained Prec@1 with 15 layers frozen: 35.25 	 Loss: 3.046875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.5542 (0.7828)	Prec@1 81.250 (76.803)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.3816 (0.6673)	Prec@1 85.547 (78.916)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.4297 (0.6132)	Prec@1 85.547 (80.081)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.4365 (0.5800)	Prec@1 83.203 (80.929)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.5630 (0.5666)	Prec@1 81.641 (81.230)	
Total train loss: 0.5665

Train time: 457.4850616455078
 * Prec@1 80.560 Prec@5 98.770 Loss 0.5767
Best acc: 80.560
--------------------------------------------------------------------------------
Test time: 461.29258847236633

Epoch: [1][38/196]	LR: 0.1	Loss 0.4810 (0.4506)	Prec@1 83.984 (84.545)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.4795 (0.4450)	Prec@1 80.859 (84.756)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.4563 (0.4420)	Prec@1 83.594 (84.699)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.4614 (0.4396)	Prec@1 81.641 (84.813)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.3801 (0.4402)	Prec@1 85.156 (84.728)	
Total train loss: 0.4402

Train time: 18.9269597530365
 * Prec@1 65.600 Prec@5 94.930 Loss 1.0596
Best acc: 80.560
--------------------------------------------------------------------------------
Test time: 22.71867847442627

Epoch: [2][38/196]	LR: 0.1	Loss 0.4453 (0.4031)	Prec@1 84.375 (85.917)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.4246 (0.4114)	Prec@1 83.594 (85.712)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.3652 (0.4150)	Prec@1 87.109 (85.524)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.4302 (0.4200)	Prec@1 87.109 (85.452)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.4604 (0.4265)	Prec@1 83.984 (85.254)	
Total train loss: 0.4266

Train time: 18.93851900100708
 * Prec@1 79.850 Prec@5 98.710 Loss 0.6040
Best acc: 80.560
--------------------------------------------------------------------------------
Test time: 22.426809072494507

Epoch: [3][38/196]	LR: 0.1	Loss 0.3860 (0.4164)	Prec@1 86.328 (85.637)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.3931 (0.4279)	Prec@1 85.156 (85.191)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.3992 (0.4261)	Prec@1 84.375 (85.263)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4617 (0.4206)	Prec@1 84.375 (85.482)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.5220 (0.4254)	Prec@1 80.469 (85.204)	
Total train loss: 0.4256

Train time: 16.724400520324707
 * Prec@1 79.920 Prec@5 98.350 Loss 0.7056
Best acc: 80.560
--------------------------------------------------------------------------------
Test time: 21.083486795425415

Epoch: [4][38/196]	LR: 0.1	Loss 0.3960 (0.3950)	Prec@1 89.453 (86.358)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.3350 (0.3938)	Prec@1 88.672 (86.478)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.4177 (0.3948)	Prec@1 85.156 (86.362)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4299 (0.3924)	Prec@1 84.375 (86.468)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.2686 (0.3916)	Prec@1 91.797 (86.472)	
Total train loss: 0.3916

Train time: 17.31940507888794
 * Prec@1 82.130 Prec@5 98.730 Loss 0.5679
Best acc: 82.130
--------------------------------------------------------------------------------
Test time: 20.49817728996277

Epoch: [5][38/196]	LR: 0.1	Loss 0.3596 (0.3940)	Prec@1 86.719 (86.789)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.4875 (0.4035)	Prec@1 80.859 (86.198)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.4502 (0.4088)	Prec@1 83.984 (85.864)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.3269 (0.4045)	Prec@1 89.844 (86.010)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.4385 (0.4042)	Prec@1 87.109 (85.974)	
Total train loss: 0.4042

Train time: 17.591978073120117
 * Prec@1 81.470 Prec@5 98.850 Loss 0.6328
Best acc: 82.130
--------------------------------------------------------------------------------
Test time: 21.67137908935547

Epoch: [6][38/196]	LR: 0.1	Loss 0.4087 (0.3956)	Prec@1 83.984 (86.468)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.3176 (0.3888)	Prec@1 89.062 (86.699)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.3638 (0.3901)	Prec@1 89.453 (86.575)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.4690 (0.3894)	Prec@1 85.547 (86.579)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.4514 (0.3940)	Prec@1 83.594 (86.356)	
Total train loss: 0.3939

Train time: 17.069467067718506
 * Prec@1 58.850 Prec@5 97.090 Loss 2.0938
Best acc: 82.130
--------------------------------------------------------------------------------
Test time: 21.70523166656494

Epoch: [7][38/196]	LR: 0.1	Loss 0.3997 (0.3992)	Prec@1 85.938 (86.008)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.3235 (0.3825)	Prec@1 87.891 (86.829)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2437 (0.3807)	Prec@1 90.625 (86.849)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.4722 (0.3796)	Prec@1 86.328 (86.897)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.3196 (0.3796)	Prec@1 88.281 (86.921)	
Total train loss: 0.3800

Train time: 17.22090458869934
 * Prec@1 82.340 Prec@5 98.950 Loss 0.5225
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 20.916959047317505

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3672 (0.3713)	Prec@1 88.281 (87.069)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3115 (0.3539)	Prec@1 88.672 (87.480)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3608 (0.3517)	Prec@1 88.281 (87.567)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3193 (0.3472)	Prec@1 89.453 (87.778)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.3960 (0.3467)	Prec@1 87.500 (87.867)	
Total train loss: 0.3465

Train time: 17.06550121307373
 * Prec@1 84.180 Prec@5 99.080 Loss 0.4849
Best acc: 84.180
--------------------------------------------------------------------------------
Test time: 20.476165533065796

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.3325 (0.3331)	Prec@1 87.891 (88.752)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3867 (0.3307)	Prec@1 87.500 (88.702)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.3486 (0.3375)	Prec@1 86.328 (88.378)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3274 (0.3386)	Prec@1 90.625 (88.304)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.3191 (0.3371)	Prec@1 89.062 (88.301)	
Total train loss: 0.3373

Train time: 17.728022575378418
 * Prec@1 84.430 Prec@5 99.140 Loss 0.4888
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.12077021598816

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.4302 (0.3315)	Prec@1 84.375 (88.391)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3254 (0.3326)	Prec@1 88.281 (88.381)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.3884 (0.3357)	Prec@1 88.281 (88.218)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2915 (0.3340)	Prec@1 92.188 (88.321)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.3464 (0.3378)	Prec@1 88.672 (88.257)	
Total train loss: 0.3377

Train time: 17.92738437652588
 * Prec@1 84.070 Prec@5 99.110 Loss 0.4973
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.269615650177002

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.3545 (0.3371)	Prec@1 87.500 (88.241)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.3210 (0.3365)	Prec@1 87.891 (88.076)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2686 (0.3365)	Prec@1 91.797 (88.111)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.3491 (0.3340)	Prec@1 88.281 (88.211)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.3196 (0.3352)	Prec@1 88.281 (88.205)	
Total train loss: 0.3350

Train time: 18.10513401031494
 * Prec@1 84.290 Prec@5 99.100 Loss 0.4995
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.934176683425903

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2830 (0.3319)	Prec@1 90.625 (88.552)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.3523 (0.3266)	Prec@1 87.891 (88.477)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.3208 (0.3289)	Prec@1 89.062 (88.421)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.4087 (0.3332)	Prec@1 83.984 (88.316)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2615 (0.3351)	Prec@1 91.797 (88.313)	
Total train loss: 0.3350

Train time: 17.463727712631226
 * Prec@1 84.080 Prec@5 99.100 Loss 0.4961
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.403793811798096

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.3091 (0.3335)	Prec@1 91.406 (88.191)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.4194 (0.3392)	Prec@1 83.594 (88.216)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.3757 (0.3405)	Prec@1 89.453 (88.221)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.3079 (0.3370)	Prec@1 89.062 (88.291)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.3784 (0.3366)	Prec@1 85.938 (88.301)	
Total train loss: 0.3365

Train time: 17.1433584690094
 * Prec@1 84.180 Prec@5 99.090 Loss 0.4968
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.1888427734375

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.3152 (0.3353)	Prec@1 89.062 (88.321)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2764 (0.3335)	Prec@1 89.844 (88.301)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2874 (0.3300)	Prec@1 90.625 (88.428)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3716 (0.3369)	Prec@1 89.062 (88.174)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2642 (0.3380)	Prec@1 89.844 (88.175)	
Total train loss: 0.3379

Train time: 17.243688344955444
 * Prec@1 84.150 Prec@5 99.090 Loss 0.5054
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.634648323059082

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.3552 (0.3371)	Prec@1 87.500 (88.311)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.3330 (0.3407)	Prec@1 87.891 (88.151)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.3230 (0.3348)	Prec@1 89.844 (88.285)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.3420 (0.3378)	Prec@1 85.938 (88.181)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.3315 (0.3386)	Prec@1 87.891 (88.221)	
Total train loss: 0.3388

Train time: 16.315534353256226
 * Prec@1 84.330 Prec@5 99.080 Loss 0.4963
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.694753885269165

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.3181 (0.3425)	Prec@1 92.188 (88.201)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.3469 (0.3405)	Prec@1 87.109 (88.136)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2842 (0.3423)	Prec@1 89.844 (88.128)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.3384 (0.3416)	Prec@1 86.328 (88.116)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2603 (0.3380)	Prec@1 91.016 (88.217)	
Total train loss: 0.3381

Train time: 17.468531370162964
 * Prec@1 84.360 Prec@5 99.070 Loss 0.4995
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.948049545288086

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.3159 (0.3443)	Prec@1 91.406 (88.021)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.3433 (0.3426)	Prec@1 85.938 (88.166)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.3411 (0.3414)	Prec@1 89.062 (88.118)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.3423 (0.3382)	Prec@1 88.281 (88.214)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.3413 (0.3383)	Prec@1 87.500 (88.233)	
Total train loss: 0.3382

Train time: 18.456957578659058
 * Prec@1 84.130 Prec@5 99.110 Loss 0.5020
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.70679259300232

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.3523 (0.3337)	Prec@1 87.500 (88.592)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.4189 (0.3449)	Prec@1 83.594 (88.096)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.4023 (0.3440)	Prec@1 87.109 (87.977)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.3467 (0.3377)	Prec@1 89.062 (88.221)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.3704 (0.3364)	Prec@1 87.891 (88.267)	
Total train loss: 0.3365

Train time: 19.028002500534058
 * Prec@1 84.170 Prec@5 99.090 Loss 0.5059
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 23.23535418510437

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.3145 (0.3276)	Prec@1 88.672 (88.632)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2488 (0.3306)	Prec@1 91.016 (88.401)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.3223 (0.3316)	Prec@1 88.672 (88.482)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.3833 (0.3355)	Prec@1 87.891 (88.299)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.3074 (0.3363)	Prec@1 89.062 (88.291)	
Total train loss: 0.3365

Train time: 18.71002721786499
 * Prec@1 84.280 Prec@5 99.100 Loss 0.5024
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.747847080230713

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2484 (0.3478)	Prec@1 91.797 (88.031)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.3826 (0.3325)	Prec@1 85.547 (88.537)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.3142 (0.3307)	Prec@1 89.844 (88.528)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.3752 (0.3350)	Prec@1 86.328 (88.379)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.3247 (0.3350)	Prec@1 91.016 (88.381)	
Total train loss: 0.3352

Train time: 17.976142644882202
 * Prec@1 84.260 Prec@5 99.120 Loss 0.5034
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.03847575187683

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.4053 (0.3330)	Prec@1 82.812 (88.431)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.3323 (0.3305)	Prec@1 89.062 (88.627)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.3350 (0.3360)	Prec@1 87.891 (88.502)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2776 (0.3375)	Prec@1 89.844 (88.364)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.3542 (0.3365)	Prec@1 87.109 (88.413)	
Total train loss: 0.3369

Train time: 17.76118230819702
 * Prec@1 84.230 Prec@5 99.060 Loss 0.5059
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.238567113876343

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2905 (0.3381)	Prec@1 91.016 (88.321)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.3711 (0.3424)	Prec@1 86.719 (88.021)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.3359 (0.3399)	Prec@1 89.062 (88.201)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.3086 (0.3359)	Prec@1 88.281 (88.291)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.3572 (0.3361)	Prec@1 87.109 (88.275)	
Total train loss: 0.3362

Train time: 17.845624923706055
 * Prec@1 84.160 Prec@5 99.080 Loss 0.5063
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.18423628807068

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.3240 (0.3512)	Prec@1 87.500 (87.710)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.3689 (0.3471)	Prec@1 85.938 (87.826)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.3423 (0.3380)	Prec@1 87.891 (88.111)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.3220 (0.3363)	Prec@1 89.062 (88.144)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.3472 (0.3373)	Prec@1 85.938 (88.167)	
Total train loss: 0.3374

Train time: 17.15872883796692
 * Prec@1 84.240 Prec@5 99.080 Loss 0.5044
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.998079299926758

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.3386 (0.3417)	Prec@1 88.281 (87.810)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.3115 (0.3353)	Prec@1 89.844 (88.241)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2898 (0.3378)	Prec@1 89.844 (88.178)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.3662 (0.3363)	Prec@1 86.719 (88.249)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.2328 (0.3361)	Prec@1 94.141 (88.243)	
Total train loss: 0.3364

Train time: 17.138942003250122
 * Prec@1 84.140 Prec@5 99.080 Loss 0.5010
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.26564121246338

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.3538 (0.3404)	Prec@1 88.281 (88.051)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.3447 (0.3332)	Prec@1 89.453 (88.512)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.3379 (0.3357)	Prec@1 88.281 (88.328)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2656 (0.3345)	Prec@1 90.625 (88.321)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.3574 (0.3357)	Prec@1 89.453 (88.263)	
Total train loss: 0.3355

Train time: 16.559881925582886
 * Prec@1 84.160 Prec@5 99.070 Loss 0.5020
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.688043117523193

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.3347 (0.3431)	Prec@1 89.453 (88.281)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.3271 (0.3391)	Prec@1 88.281 (88.331)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.3987 (0.3376)	Prec@1 86.719 (88.391)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.3511 (0.3353)	Prec@1 87.500 (88.404)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.3545 (0.3366)	Prec@1 88.672 (88.293)	
Total train loss: 0.3370

Train time: 18.272531747817993
 * Prec@1 84.310 Prec@5 99.100 Loss 0.5044
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.684640169143677

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.3591 (0.3461)	Prec@1 88.672 (88.311)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.2333 (0.3372)	Prec@1 92.188 (88.376)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.3054 (0.3369)	Prec@1 88.672 (88.338)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.3096 (0.3369)	Prec@1 90.234 (88.319)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2864 (0.3352)	Prec@1 89.453 (88.315)	
Total train loss: 0.3350

Train time: 17.138798236846924
 * Prec@1 84.210 Prec@5 99.050 Loss 0.5010
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.380655765533447

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.3540 (0.3412)	Prec@1 87.500 (88.151)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.3398 (0.3352)	Prec@1 88.672 (88.482)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.4177 (0.3334)	Prec@1 85.156 (88.418)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.3174 (0.3347)	Prec@1 89.062 (88.354)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.3518 (0.3357)	Prec@1 87.891 (88.295)	
Total train loss: 0.3355

Train time: 16.648329973220825
 * Prec@1 84.360 Prec@5 99.080 Loss 0.5020
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 19.77402353286743

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.3306 (0.3377)	Prec@1 90.234 (88.241)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2495 (0.3351)	Prec@1 91.406 (88.281)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.3240 (0.3324)	Prec@1 89.453 (88.345)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.3359 (0.3344)	Prec@1 86.719 (88.301)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.4297 (0.3356)	Prec@1 85.547 (88.301)	
Total train loss: 0.3359

Train time: 16.943052768707275
 * Prec@1 84.350 Prec@5 99.100 Loss 0.5034
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.728959798812866

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.3542 (0.3544)	Prec@1 86.719 (87.450)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.3105 (0.3423)	Prec@1 90.234 (87.971)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.4497 (0.3406)	Prec@1 85.547 (88.064)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.2983 (0.3375)	Prec@1 89.062 (88.186)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.2754 (0.3362)	Prec@1 92.578 (88.235)	
Total train loss: 0.3364

Train time: 18.62901782989502
 * Prec@1 84.380 Prec@5 99.070 Loss 0.5039
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 23.033780336380005

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.2805 (0.3334)	Prec@1 91.406 (88.732)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.3489 (0.3334)	Prec@1 86.328 (88.492)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.2800 (0.3324)	Prec@1 88.672 (88.615)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.3643 (0.3337)	Prec@1 86.328 (88.436)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.3083 (0.3368)	Prec@1 90.625 (88.375)	
Total train loss: 0.3368

Train time: 17.247485399246216
 * Prec@1 84.320 Prec@5 99.040 Loss 0.5034
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.075137853622437

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.2964 (0.3435)	Prec@1 90.625 (88.001)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.3164 (0.3447)	Prec@1 88.672 (87.821)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.4392 (0.3369)	Prec@1 83.984 (88.131)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.4021 (0.3362)	Prec@1 85.938 (88.209)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.2793 (0.3353)	Prec@1 90.234 (88.227)	
Total train loss: 0.3355

Train time: 17.738765716552734
 * Prec@1 84.280 Prec@5 99.130 Loss 0.5005
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.892812728881836

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.3252 (0.3307)	Prec@1 87.109 (88.602)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.3025 (0.3300)	Prec@1 91.016 (88.602)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.3064 (0.3351)	Prec@1 89.453 (88.428)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.3618 (0.3360)	Prec@1 87.891 (88.371)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.3186 (0.3353)	Prec@1 87.500 (88.321)	
Total train loss: 0.3353

Train time: 17.273510694503784
 * Prec@1 84.210 Prec@5 99.070 Loss 0.5063
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.1562716960907

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.3572 (0.3350)	Prec@1 89.453 (88.712)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.2839 (0.3347)	Prec@1 91.797 (88.557)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.2742 (0.3384)	Prec@1 89.062 (88.208)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.3887 (0.3385)	Prec@1 88.281 (88.194)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.3127 (0.3354)	Prec@1 87.500 (88.281)	
Total train loss: 0.3358

Train time: 17.27784824371338
 * Prec@1 84.140 Prec@5 99.060 Loss 0.5044
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.72810411453247

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.2942 (0.3488)	Prec@1 87.109 (87.861)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.3660 (0.3370)	Prec@1 88.281 (88.376)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.2939 (0.3370)	Prec@1 89.844 (88.275)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.3936 (0.3370)	Prec@1 85.547 (88.204)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.2805 (0.3357)	Prec@1 90.234 (88.221)	
Total train loss: 0.3357

Train time: 17.84201955795288
 * Prec@1 84.150 Prec@5 99.090 Loss 0.5010
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 21.68015193939209

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.3413 (0.3425)	Prec@1 88.672 (87.911)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.3335 (0.3352)	Prec@1 86.719 (88.201)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.2954 (0.3342)	Prec@1 88.281 (88.285)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.4612 (0.3367)	Prec@1 86.719 (88.249)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.3433 (0.3370)	Prec@1 88.281 (88.279)	
Total train loss: 0.3371

Train time: 16.752901315689087
 * Prec@1 84.150 Prec@5 99.090 Loss 0.5059
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 20.4504132270813

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.4209 (0.3348)	Prec@1 83.984 (88.381)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.4336 (0.3367)	Prec@1 85.156 (88.181)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.4590 (0.3382)	Prec@1 85.156 (88.251)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.4026 (0.3369)	Prec@1 87.109 (88.194)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.3916 (0.3367)	Prec@1 86.328 (88.251)	
Total train loss: 0.3370

Train time: 17.063618421554565
 * Prec@1 84.200 Prec@5 99.070 Loss 0.5034
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.050331115722656

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.3669 (0.3240)	Prec@1 87.109 (88.882)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.2793 (0.3281)	Prec@1 90.625 (88.767)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.3228 (0.3339)	Prec@1 87.500 (88.538)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.3179 (0.3338)	Prec@1 88.672 (88.401)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.3662 (0.3363)	Prec@1 89.844 (88.331)	
Total train loss: 0.3362

Train time: 18.000073671340942
 * Prec@1 84.260 Prec@5 99.080 Loss 0.5054
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 22.775131464004517

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.3552 (0.3353)	Prec@1 89.062 (88.492)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.3611 (0.3444)	Prec@1 83.984 (88.031)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.3826 (0.3426)	Prec@1 87.109 (88.094)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.3401 (0.3418)	Prec@1 85.156 (88.033)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.2795 (0.3380)	Prec@1 89.062 (88.179)	
Total train loss: 0.3381

Train time: 11.097167015075684
 * Prec@1 84.270 Prec@5 99.100 Loss 0.5020
Best acc: 84.430
--------------------------------------------------------------------------------
Test time: 13.734257698059082


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 34.790 Prec@5 85.020 Loss 3.0625
Pre-trained Prec@1 with 17 layers frozen: 34.790000915527344 	 Loss: 3.0625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.7095 (0.9191)	Prec@1 75.781 (72.346)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.5649 (0.7865)	Prec@1 82.031 (75.250)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.5630 (0.7363)	Prec@1 80.078 (76.362)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.5654 (0.7053)	Prec@1 81.250 (77.058)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.6284 (0.6852)	Prec@1 78.906 (77.540)	
Total train loss: 0.6849

Train time: 205.91404366493225
 * Prec@1 79.340 Prec@5 98.650 Loss 0.6362
Best acc: 79.340
--------------------------------------------------------------------------------
Test time: 212.41213297843933

Epoch: [1][38/196]	LR: 0.1	Loss 0.4844 (0.5762)	Prec@1 82.031 (80.128)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.5649 (0.5767)	Prec@1 80.469 (80.228)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.5923 (0.5727)	Prec@1 81.250 (80.479)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.5747 (0.5732)	Prec@1 80.469 (80.486)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.6494 (0.5705)	Prec@1 76.953 (80.565)	
Total train loss: 0.5705

Train time: 21.9996919631958
 * Prec@1 79.600 Prec@5 98.790 Loss 0.5869
Best acc: 79.600
--------------------------------------------------------------------------------
Test time: 26.223769664764404

Epoch: [2][38/196]	LR: 0.1	Loss 0.5137 (0.5553)	Prec@1 83.203 (81.040)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.4556 (0.5431)	Prec@1 84.766 (81.450)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.6084 (0.5403)	Prec@1 80.078 (81.327)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.5234 (0.5395)	Prec@1 80.859 (81.350)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.5908 (0.5402)	Prec@1 82.422 (81.318)	
Total train loss: 0.5402

Train time: 17.668347597122192
 * Prec@1 77.630 Prec@5 98.660 Loss 0.6577
Best acc: 79.600
--------------------------------------------------------------------------------
Test time: 20.90239405632019

Epoch: [3][38/196]	LR: 0.1	Loss 0.5488 (0.5278)	Prec@1 80.078 (81.751)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.6064 (0.5267)	Prec@1 77.344 (82.016)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.6050 (0.5282)	Prec@1 77.344 (81.941)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4541 (0.5271)	Prec@1 83.594 (81.901)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.5400 (0.5306)	Prec@1 81.250 (81.781)	
Total train loss: 0.5307

Train time: 15.75745701789856
 * Prec@1 80.180 Prec@5 98.610 Loss 0.6230
Best acc: 80.180
--------------------------------------------------------------------------------
Test time: 19.79972195625305

Epoch: [4][38/196]	LR: 0.1	Loss 0.6011 (0.5241)	Prec@1 79.297 (82.081)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.5068 (0.5304)	Prec@1 81.641 (81.596)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.5806 (0.5254)	Prec@1 79.297 (81.798)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.4946 (0.5273)	Prec@1 83.984 (81.781)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.5732 (0.5303)	Prec@1 77.734 (81.631)	
Total train loss: 0.5303

Train time: 17.819936513900757
 * Prec@1 77.770 Prec@5 98.410 Loss 0.6499
Best acc: 80.180
--------------------------------------------------------------------------------
Test time: 21.261441230773926

Epoch: [5][38/196]	LR: 0.1	Loss 0.4863 (0.5371)	Prec@1 83.203 (81.701)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6284 (0.5303)	Prec@1 76.953 (81.696)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.5371 (0.5360)	Prec@1 83.594 (81.574)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.5264 (0.5390)	Prec@1 79.688 (81.450)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.5249 (0.5341)	Prec@1 83.594 (81.617)	
Total train loss: 0.5341

Train time: 17.190131902694702
 * Prec@1 80.100 Prec@5 98.530 Loss 0.6479
Best acc: 80.180
--------------------------------------------------------------------------------
Test time: 21.058079719543457

Epoch: [6][38/196]	LR: 0.1	Loss 0.4893 (0.5092)	Prec@1 82.812 (82.802)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5801 (0.5068)	Prec@1 80.078 (82.577)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.4937 (0.5130)	Prec@1 82.812 (82.232)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.5151 (0.5093)	Prec@1 82.031 (82.347)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.5278 (0.5087)	Prec@1 82.031 (82.348)	
Total train loss: 0.5087

Train time: 18.089006662368774
 * Prec@1 81.680 Prec@5 98.810 Loss 0.5596
Best acc: 81.680
--------------------------------------------------------------------------------
Test time: 21.887246131896973

Epoch: [7][38/196]	LR: 0.1	Loss 0.4338 (0.4959)	Prec@1 82.031 (82.422)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4109 (0.4989)	Prec@1 84.375 (82.402)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.4795 (0.4995)	Prec@1 84.375 (82.449)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.4365 (0.4991)	Prec@1 83.984 (82.502)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.5903 (0.4983)	Prec@1 83.594 (82.562)	
Total train loss: 0.4981

Train time: 18.074615001678467
 * Prec@1 80.760 Prec@5 98.530 Loss 0.5986
Best acc: 81.680
--------------------------------------------------------------------------------
Test time: 22.42292881011963

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.4775 (0.4921)	Prec@1 84.375 (82.863)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.4114 (0.4827)	Prec@1 85.156 (83.188)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.5176 (0.4809)	Prec@1 84.375 (83.390)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.4927 (0.4802)	Prec@1 82.422 (83.388)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4961 (0.4794)	Prec@1 82.422 (83.397)	
Total train loss: 0.4794

Train time: 17.997631549835205
 * Prec@1 82.080 Prec@5 98.870 Loss 0.5449
Best acc: 82.080
--------------------------------------------------------------------------------
Test time: 21.23667812347412

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.5269 (0.4744)	Prec@1 80.469 (83.444)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.5122 (0.4767)	Prec@1 80.469 (83.609)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.4375 (0.4753)	Prec@1 84.766 (83.594)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.4470 (0.4753)	Prec@1 86.719 (83.569)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.6240 (0.4801)	Prec@1 78.516 (83.381)	
Total train loss: 0.4802

Train time: 17.01520800590515
 * Prec@1 82.130 Prec@5 98.850 Loss 0.5518
Best acc: 82.130
--------------------------------------------------------------------------------
Test time: 21.531449794769287

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.5684 (0.4847)	Prec@1 79.688 (82.953)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.5444 (0.4848)	Prec@1 83.203 (83.118)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.4377 (0.4807)	Prec@1 86.328 (83.247)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.5693 (0.4801)	Prec@1 80.859 (83.353)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5562 (0.4795)	Prec@1 80.078 (83.385)	
Total train loss: 0.4795

Train time: 18.043419361114502
 * Prec@1 82.260 Prec@5 98.900 Loss 0.5547
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.679641485214233

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.4890 (0.4612)	Prec@1 80.469 (83.754)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.4597 (0.4732)	Prec@1 82.812 (83.418)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.4763 (0.4781)	Prec@1 83.203 (83.260)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.4954 (0.4806)	Prec@1 81.641 (83.178)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.4690 (0.4803)	Prec@1 82.812 (83.273)	
Total train loss: 0.4806

Train time: 17.83554458618164
 * Prec@1 82.200 Prec@5 98.910 Loss 0.5527
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.646132707595825

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.4417 (0.4784)	Prec@1 84.375 (83.373)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5566 (0.4852)	Prec@1 80.469 (83.163)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5942 (0.4877)	Prec@1 79.297 (83.066)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.4785 (0.4934)	Prec@1 83.594 (82.830)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5312 (0.4913)	Prec@1 81.641 (82.903)	
Total train loss: 0.4914

Train time: 17.11761713027954
 * Prec@1 81.790 Prec@5 98.870 Loss 0.5596
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.2969229221344

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.4890 (0.5058)	Prec@1 82.031 (82.372)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5176 (0.5133)	Prec@1 80.078 (82.181)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.4148 (0.5060)	Prec@1 85.547 (82.425)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.5962 (0.5065)	Prec@1 78.516 (82.467)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.4724 (0.5073)	Prec@1 83.203 (82.414)	
Total train loss: 0.5073

Train time: 18.624924421310425
 * Prec@1 81.400 Prec@5 98.900 Loss 0.5630
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 22.557127237319946

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.4138 (0.5278)	Prec@1 88.672 (82.041)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.5229 (0.5288)	Prec@1 82.031 (81.796)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.4946 (0.5278)	Prec@1 83.203 (81.814)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.7285 (0.5323)	Prec@1 72.656 (81.623)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.5664 (0.5304)	Prec@1 81.250 (81.767)	
Total train loss: 0.5305

Train time: 18.455789804458618
 * Prec@1 81.000 Prec@5 98.830 Loss 0.5718
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.997232675552368

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.5391 (0.5433)	Prec@1 80.859 (81.480)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.4805 (0.5368)	Prec@1 80.859 (81.591)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.6016 (0.5364)	Prec@1 81.641 (81.514)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.5566 (0.5400)	Prec@1 81.250 (81.500)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.5303 (0.5387)	Prec@1 80.078 (81.516)	
Total train loss: 0.5388

Train time: 18.20461130142212
 * Prec@1 80.940 Prec@5 98.830 Loss 0.5684
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 22.465590238571167

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.4656 (0.5353)	Prec@1 82.422 (81.721)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.4741 (0.5389)	Prec@1 83.203 (81.490)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.5830 (0.5366)	Prec@1 83.203 (81.584)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.4707 (0.5366)	Prec@1 86.328 (81.658)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.5654 (0.5378)	Prec@1 80.859 (81.617)	
Total train loss: 0.5379

Train time: 17.35809350013733
 * Prec@1 81.010 Prec@5 98.820 Loss 0.5698
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.829342126846313

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.5376 (0.5294)	Prec@1 80.078 (82.071)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.5933 (0.5312)	Prec@1 79.297 (81.901)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.5420 (0.5357)	Prec@1 81.641 (81.654)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.4822 (0.5357)	Prec@1 84.766 (81.636)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.5522 (0.5378)	Prec@1 81.641 (81.514)	
Total train loss: 0.5378

Train time: 17.67536163330078
 * Prec@1 80.930 Prec@5 98.850 Loss 0.5654
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.417157888412476

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.4817 (0.5345)	Prec@1 85.156 (81.781)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.5581 (0.5416)	Prec@1 80.469 (81.430)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.5278 (0.5371)	Prec@1 84.375 (81.681)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5830 (0.5388)	Prec@1 76.953 (81.473)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.4343 (0.5374)	Prec@1 87.109 (81.530)	
Total train loss: 0.5375

Train time: 21.89513635635376
 * Prec@1 80.940 Prec@5 98.780 Loss 0.5679
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 26.21280598640442

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.5254 (0.5297)	Prec@1 83.594 (82.292)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.6011 (0.5339)	Prec@1 75.781 (81.841)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.6704 (0.5322)	Prec@1 76.562 (81.808)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.5801 (0.5384)	Prec@1 82.422 (81.608)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.5049 (0.5364)	Prec@1 83.594 (81.609)	
Total train loss: 0.5362

Train time: 19.018357753753662
 * Prec@1 81.030 Prec@5 98.830 Loss 0.5654
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 23.091548681259155

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.5952 (0.5374)	Prec@1 79.688 (81.601)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.5322 (0.5331)	Prec@1 82.031 (81.666)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.5815 (0.5361)	Prec@1 81.250 (81.604)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.5098 (0.5370)	Prec@1 80.469 (81.598)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.4189 (0.5357)	Prec@1 85.938 (81.645)	
Total train loss: 0.5359

Train time: 17.58415150642395
 * Prec@1 80.980 Prec@5 98.810 Loss 0.5698
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 22.69469428062439

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.4443 (0.5470)	Prec@1 82.812 (81.010)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.6226 (0.5404)	Prec@1 75.391 (81.360)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.4670 (0.5382)	Prec@1 85.938 (81.527)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.5400 (0.5398)	Prec@1 79.297 (81.503)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.5757 (0.5373)	Prec@1 80.078 (81.593)	
Total train loss: 0.5371

Train time: 18.385782480239868
 * Prec@1 81.040 Prec@5 98.840 Loss 0.5654
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 22.4381844997406

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.5317 (0.5371)	Prec@1 82.812 (81.641)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.5991 (0.5397)	Prec@1 79.688 (81.550)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5752 (0.5410)	Prec@1 80.469 (81.537)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.5298 (0.5390)	Prec@1 80.469 (81.523)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.4595 (0.5377)	Prec@1 83.984 (81.597)	
Total train loss: 0.5377

Train time: 18.863648653030396
 * Prec@1 81.120 Prec@5 98.810 Loss 0.5718
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 22.645843744277954

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.4678 (0.5217)	Prec@1 83.594 (82.342)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.6270 (0.5278)	Prec@1 80.078 (82.131)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.4919 (0.5307)	Prec@1 79.688 (81.874)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.5176 (0.5333)	Prec@1 81.250 (81.736)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.5537 (0.5357)	Prec@1 81.250 (81.605)	
Total train loss: 0.5363

Train time: 18.73041009902954
 * Prec@1 81.000 Prec@5 98.870 Loss 0.5684
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.940603971481323

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.5781 (0.5303)	Prec@1 80.859 (81.621)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.6587 (0.5329)	Prec@1 77.344 (81.581)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.4924 (0.5352)	Prec@1 83.984 (81.644)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.5415 (0.5376)	Prec@1 82.812 (81.533)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.5625 (0.5367)	Prec@1 81.250 (81.613)	
Total train loss: 0.5367

Train time: 16.64848518371582
 * Prec@1 81.020 Prec@5 98.840 Loss 0.5659
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.642569303512573

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.5566 (0.5297)	Prec@1 80.469 (81.981)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.5664 (0.5409)	Prec@1 78.125 (81.621)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.5659 (0.5366)	Prec@1 81.641 (81.674)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.5991 (0.5368)	Prec@1 79.688 (81.636)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5488 (0.5358)	Prec@1 77.344 (81.627)	
Total train loss: 0.5360

Train time: 17.709424257278442
 * Prec@1 80.890 Prec@5 98.780 Loss 0.5737
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.53099226951599

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.4390 (0.5361)	Prec@1 87.891 (81.340)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.4490 (0.5347)	Prec@1 83.984 (81.485)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.6094 (0.5325)	Prec@1 77.734 (81.727)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.6240 (0.5356)	Prec@1 78.125 (81.525)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.5996 (0.5368)	Prec@1 81.250 (81.536)	
Total train loss: 0.5368

Train time: 17.318586826324463
 * Prec@1 80.900 Prec@5 98.840 Loss 0.5718
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.546894550323486

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.5767 (0.5425)	Prec@1 80.469 (81.691)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.4832 (0.5402)	Prec@1 83.203 (81.726)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.4753 (0.5361)	Prec@1 82.812 (81.831)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.5430 (0.5360)	Prec@1 80.859 (81.781)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.4656 (0.5346)	Prec@1 84.375 (81.737)	
Total train loss: 0.5348

Train time: 16.653754472732544
 * Prec@1 81.030 Prec@5 98.830 Loss 0.5698
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.508493185043335

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.5752 (0.5394)	Prec@1 79.688 (81.160)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.5381 (0.5386)	Prec@1 82.031 (81.460)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.6245 (0.5377)	Prec@1 76.172 (81.554)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.5469 (0.5377)	Prec@1 81.641 (81.591)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.4546 (0.5368)	Prec@1 83.594 (81.607)	
Total train loss: 0.5367

Train time: 17.753955602645874
 * Prec@1 81.130 Prec@5 98.810 Loss 0.5669
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.560206651687622

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.6245 (0.5317)	Prec@1 81.641 (81.601)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.4944 (0.5311)	Prec@1 84.375 (81.776)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.6289 (0.5327)	Prec@1 79.688 (81.657)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.5830 (0.5365)	Prec@1 80.469 (81.505)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.5400 (0.5373)	Prec@1 83.203 (81.585)	
Total train loss: 0.5373

Train time: 17.280250549316406
 * Prec@1 81.030 Prec@5 98.820 Loss 0.5693
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.741647243499756

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.6025 (0.5400)	Prec@1 79.297 (81.230)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.5923 (0.5368)	Prec@1 81.641 (81.365)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.4583 (0.5363)	Prec@1 87.109 (81.427)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.6157 (0.5391)	Prec@1 76.172 (81.328)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.4800 (0.5348)	Prec@1 83.984 (81.565)	
Total train loss: 0.5348

Train time: 17.731000900268555
 * Prec@1 80.900 Prec@5 98.830 Loss 0.5674
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.506763458251953

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.5430 (0.5496)	Prec@1 79.297 (81.230)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.5518 (0.5440)	Prec@1 80.469 (81.365)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.4885 (0.5412)	Prec@1 84.375 (81.384)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.5396 (0.5356)	Prec@1 80.078 (81.571)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.4832 (0.5379)	Prec@1 84.766 (81.474)	
Total train loss: 0.5380

Train time: 16.988736629486084
 * Prec@1 81.030 Prec@5 98.830 Loss 0.5674
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.851033926010132

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.4614 (0.5295)	Prec@1 83.594 (81.540)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.5815 (0.5340)	Prec@1 80.078 (81.666)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.5361 (0.5357)	Prec@1 82.422 (81.607)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.4675 (0.5359)	Prec@1 84.375 (81.555)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.5049 (0.5353)	Prec@1 82.812 (81.589)	
Total train loss: 0.5354

Train time: 18.41484832763672
 * Prec@1 81.100 Prec@5 98.820 Loss 0.5635
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.535097360610962

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.5874 (0.5387)	Prec@1 80.859 (81.751)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.5190 (0.5416)	Prec@1 80.859 (81.345)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.5903 (0.5389)	Prec@1 78.906 (81.500)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.4805 (0.5358)	Prec@1 83.594 (81.583)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.4377 (0.5373)	Prec@1 84.766 (81.510)	
Total train loss: 0.5378

Train time: 19.765175580978394
 * Prec@1 81.030 Prec@5 98.810 Loss 0.5664
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 23.5480375289917

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.6396 (0.5535)	Prec@1 76.953 (80.799)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.5249 (0.5461)	Prec@1 82.031 (81.240)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.4646 (0.5422)	Prec@1 83.594 (81.564)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.5063 (0.5374)	Prec@1 80.859 (81.573)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.5337 (0.5371)	Prec@1 78.906 (81.554)	
Total train loss: 0.5371

Train time: 17.608319520950317
 * Prec@1 81.020 Prec@5 98.850 Loss 0.5669
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.457797527313232

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.5674 (0.5330)	Prec@1 78.906 (81.480)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.4563 (0.5392)	Prec@1 85.938 (81.340)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.5479 (0.5363)	Prec@1 81.250 (81.390)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.4929 (0.5382)	Prec@1 84.375 (81.455)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.5444 (0.5358)	Prec@1 80.859 (81.571)	
Total train loss: 0.5362

Train time: 16.674604654312134
 * Prec@1 80.990 Prec@5 98.830 Loss 0.5669
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 19.777734994888306

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.5312 (0.5263)	Prec@1 80.469 (81.731)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.4553 (0.5338)	Prec@1 84.375 (81.631)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.5620 (0.5322)	Prec@1 80.078 (81.788)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.5088 (0.5351)	Prec@1 82.031 (81.658)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.5591 (0.5369)	Prec@1 80.859 (81.569)	
Total train loss: 0.5369

Train time: 18.326011180877686
 * Prec@1 81.160 Prec@5 98.810 Loss 0.5669
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.97561025619507

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.5312 (0.5520)	Prec@1 82.031 (80.889)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.4497 (0.5413)	Prec@1 80.859 (81.515)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.5679 (0.5361)	Prec@1 78.906 (81.641)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.4688 (0.5375)	Prec@1 84.766 (81.631)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.4844 (0.5351)	Prec@1 85.938 (81.745)	
Total train loss: 0.5353

Train time: 18.122017860412598
 * Prec@1 81.010 Prec@5 98.800 Loss 0.5674
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 21.95474100112915

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.5264 (0.5263)	Prec@1 82.812 (82.412)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.4316 (0.5368)	Prec@1 84.766 (81.686)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.5381 (0.5353)	Prec@1 82.031 (81.761)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.5625 (0.5377)	Prec@1 81.641 (81.548)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.5454 (0.5369)	Prec@1 79.688 (81.593)	
Total train loss: 0.5370

Train time: 17.73414397239685
 * Prec@1 80.820 Prec@5 98.810 Loss 0.5786
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 20.727646350860596

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.4963 (0.5453)	Prec@1 82.422 (81.150)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.6079 (0.5502)	Prec@1 79.688 (81.090)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.4272 (0.5418)	Prec@1 87.891 (81.400)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.4490 (0.5390)	Prec@1 86.328 (81.593)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.5352 (0.5368)	Prec@1 81.641 (81.645)	
Total train loss: 0.5368

Train time: 16.452497959136963
 * Prec@1 80.970 Prec@5 98.890 Loss 0.5669
Best acc: 82.260
--------------------------------------------------------------------------------
Test time: 19.193029642105103


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: sram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 31.840 Prec@5 85.120 Loss 3.1191
Pre-trained Prec@1 with 19 layers frozen: 31.839998245239258 	 Loss: 3.119140625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.8037 (1.1220)	Prec@1 76.953 (68.660)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.8501 (0.9866)	Prec@1 68.750 (69.877)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.7988 (0.9242)	Prec@1 71.094 (71.027)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.7017 (0.8933)	Prec@1 76.562 (71.565)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.9072 (0.8746)	Prec@1 71.094 (71.813)	
Total train loss: 0.8748

Train time: 239.30496644973755
 * Prec@1 74.820 Prec@5 97.640 Loss 0.7734
Best acc: 74.820
--------------------------------------------------------------------------------
Test time: 264.18876242637634

Epoch: [1][38/196]	LR: 0.1	Loss 0.7603 (0.7472)	Prec@1 71.875 (74.740)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.7729 (0.7702)	Prec@1 73.438 (73.943)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.7861 (0.7716)	Prec@1 75.391 (73.925)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.8994 (0.7796)	Prec@1 71.875 (73.640)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.7593 (0.7796)	Prec@1 73.828 (73.674)	
Total train loss: 0.7803

Train time: 18.55929923057556
 * Prec@1 75.140 Prec@5 97.920 Loss 0.7632
Best acc: 75.140
--------------------------------------------------------------------------------
Test time: 21.183155059814453

Epoch: [2][38/196]	LR: 0.1	Loss 0.7300 (0.7763)	Prec@1 73.438 (73.918)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.7573 (0.7660)	Prec@1 73.828 (74.094)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.8154 (0.7685)	Prec@1 70.703 (74.052)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.5879 (0.7699)	Prec@1 80.078 (74.001)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.8140 (0.7707)	Prec@1 73.828 (74.008)	
Total train loss: 0.7712

Train time: 12.02582311630249
 * Prec@1 75.440 Prec@5 97.780 Loss 0.7603
Best acc: 75.440
--------------------------------------------------------------------------------
Test time: 14.347574710845947

Epoch: [3][38/196]	LR: 0.1	Loss 0.8169 (0.7788)	Prec@1 73.438 (73.778)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.7588 (0.7726)	Prec@1 73.438 (74.048)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8774 (0.7720)	Prec@1 71.875 (74.079)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.7217 (0.7719)	Prec@1 75.000 (73.993)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.8643 (0.7712)	Prec@1 70.703 (74.058)	
Total train loss: 0.7711

Train time: 11.204127788543701
 * Prec@1 75.300 Prec@5 97.840 Loss 0.7490
Best acc: 75.440
--------------------------------------------------------------------------------
Test time: 14.395084857940674

Epoch: [4][38/196]	LR: 0.1	Loss 0.7876 (0.7783)	Prec@1 69.922 (73.317)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7520 (0.7773)	Prec@1 76.562 (73.678)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7900 (0.7700)	Prec@1 76.172 (73.955)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.7368 (0.7702)	Prec@1 74.609 (73.946)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.7896 (0.7648)	Prec@1 75.000 (74.165)	
Total train loss: 0.7651

Train time: 12.382637739181519
 * Prec@1 75.500 Prec@5 97.890 Loss 0.7524
Best acc: 75.500
--------------------------------------------------------------------------------
Test time: 14.967716455459595

Epoch: [5][38/196]	LR: 0.1	Loss 0.8779 (0.7676)	Prec@1 69.922 (73.838)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.7354 (0.7700)	Prec@1 76.562 (73.728)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7544 (0.7669)	Prec@1 76.562 (73.918)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.8076 (0.7618)	Prec@1 72.266 (74.236)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.7544 (0.7635)	Prec@1 74.219 (74.153)	
Total train loss: 0.7637

Train time: 12.115242958068848
 * Prec@1 75.450 Prec@5 97.960 Loss 0.7510
Best acc: 75.500
--------------------------------------------------------------------------------
Test time: 15.051851749420166

Epoch: [6][38/196]	LR: 0.1	Loss 0.8433 (0.7480)	Prec@1 70.703 (74.589)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.7026 (0.7657)	Prec@1 76.172 (73.953)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.8018 (0.7645)	Prec@1 73.438 (73.975)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.8354 (0.7601)	Prec@1 72.266 (74.196)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7666 (0.7644)	Prec@1 76.172 (74.143)	
Total train loss: 0.7644

Train time: 13.133109092712402
 * Prec@1 75.170 Prec@5 97.910 Loss 0.7515
Best acc: 75.500
--------------------------------------------------------------------------------
Test time: 15.506110191345215

Epoch: [7][38/196]	LR: 0.1	Loss 0.6821 (0.7549)	Prec@1 78.125 (74.700)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.7344 (0.7511)	Prec@1 74.609 (74.820)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.8281 (0.7578)	Prec@1 69.141 (74.586)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.7651 (0.7625)	Prec@1 73.047 (74.382)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.7437 (0.7604)	Prec@1 75.000 (74.429)	
Total train loss: 0.7602

Train time: 11.664986848831177
 * Prec@1 75.480 Prec@5 97.880 Loss 0.7500
Best acc: 75.500
--------------------------------------------------------------------------------
Test time: 14.872438430786133

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.7183 (0.7488)	Prec@1 76.172 (74.940)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.6772 (0.7472)	Prec@1 75.391 (74.815)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.7031 (0.7467)	Prec@1 74.609 (74.669)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.7251 (0.7519)	Prec@1 75.000 (74.552)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.8169 (0.7562)	Prec@1 72.266 (74.457)	
Total train loss: 0.7563

Train time: 12.043317794799805
 * Prec@1 75.510 Prec@5 97.920 Loss 0.7480
Best acc: 75.510
--------------------------------------------------------------------------------
Test time: 14.742382764816284

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.6670 (0.7404)	Prec@1 76.172 (74.850)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.8853 (0.7416)	Prec@1 68.359 (74.865)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.8320 (0.7504)	Prec@1 71.484 (74.586)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.8018 (0.7557)	Prec@1 75.000 (74.387)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.8428 (0.7532)	Prec@1 70.703 (74.443)	
Total train loss: 0.7531

Train time: 11.775168657302856
 * Prec@1 75.840 Prec@5 97.860 Loss 0.7446
Best acc: 75.840
--------------------------------------------------------------------------------
Test time: 14.724730491638184

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.7114 (0.7557)	Prec@1 75.781 (74.249)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.7441 (0.7547)	Prec@1 73.047 (74.304)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.9634 (0.7533)	Prec@1 69.141 (74.352)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.7603 (0.7510)	Prec@1 73.047 (74.544)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.8599 (0.7527)	Prec@1 73.438 (74.543)	
Total train loss: 0.7528

Train time: 13.005505084991455
 * Prec@1 75.790 Prec@5 97.900 Loss 0.7466
Best acc: 75.840
--------------------------------------------------------------------------------
Test time: 15.37401008605957

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.7715 (0.7415)	Prec@1 73.047 (74.519)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.7568 (0.7455)	Prec@1 74.609 (74.494)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.7661 (0.7454)	Prec@1 76.172 (74.633)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.9409 (0.7516)	Prec@1 68.750 (74.514)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.9419 (0.7531)	Prec@1 69.141 (74.481)	
Total train loss: 0.7531

Train time: 12.514030456542969
 * Prec@1 75.750 Prec@5 97.890 Loss 0.7476
Best acc: 75.840
--------------------------------------------------------------------------------
Test time: 15.660502195358276

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.7969 (0.7421)	Prec@1 74.219 (74.279)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.6904 (0.7474)	Prec@1 75.781 (74.259)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.9331 (0.7506)	Prec@1 67.578 (74.416)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.7700 (0.7534)	Prec@1 76.562 (74.464)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.7261 (0.7520)	Prec@1 73.438 (74.481)	
Total train loss: 0.7519

Train time: 11.80792236328125
 * Prec@1 75.700 Prec@5 97.930 Loss 0.7446
Best acc: 75.840
--------------------------------------------------------------------------------
Test time: 14.531608581542969

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.6689 (0.7452)	Prec@1 76.172 (74.319)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.8438 (0.7502)	Prec@1 71.875 (74.194)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.7139 (0.7489)	Prec@1 75.781 (74.429)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.7236 (0.7557)	Prec@1 75.781 (74.311)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.6299 (0.7519)	Prec@1 76.953 (74.395)	
Total train loss: 0.7522

Train time: 11.72186541557312
 * Prec@1 75.850 Prec@5 97.920 Loss 0.7461
Best acc: 75.850
--------------------------------------------------------------------------------
Test time: 14.713983058929443

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.8599 (0.7558)	Prec@1 71.875 (74.479)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.7764 (0.7434)	Prec@1 70.703 (74.860)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.7417 (0.7476)	Prec@1 76.562 (74.673)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.7983 (0.7501)	Prec@1 72.656 (74.534)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.8081 (0.7517)	Prec@1 73.828 (74.479)	
Total train loss: 0.7518

Train time: 12.526434659957886
 * Prec@1 75.860 Prec@5 97.900 Loss 0.7490
Best acc: 75.860
--------------------------------------------------------------------------------
Test time: 14.841714859008789

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.6367 (0.7406)	Prec@1 79.688 (74.760)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.8120 (0.7486)	Prec@1 73.047 (74.494)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.7378 (0.7532)	Prec@1 73.438 (74.272)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.7437 (0.7533)	Prec@1 76.172 (74.394)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.8252 (0.7536)	Prec@1 69.531 (74.421)	
Total train loss: 0.7536

Train time: 12.826576709747314
 * Prec@1 75.790 Prec@5 97.890 Loss 0.7461
Best acc: 75.860
--------------------------------------------------------------------------------
Test time: 15.860612392425537

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.6963 (0.7588)	Prec@1 75.391 (74.008)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.7651 (0.7530)	Prec@1 78.125 (74.469)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.7314 (0.7546)	Prec@1 76.953 (74.526)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.7441 (0.7543)	Prec@1 75.391 (74.409)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.6890 (0.7526)	Prec@1 76.172 (74.477)	
Total train loss: 0.7528

Train time: 11.904693841934204
 * Prec@1 75.800 Prec@5 97.910 Loss 0.7466
Best acc: 75.860
--------------------------------------------------------------------------------
Test time: 14.631394863128662

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.6709 (0.7693)	Prec@1 78.125 (73.748)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.7617 (0.7572)	Prec@1 73.828 (74.129)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.6265 (0.7577)	Prec@1 75.781 (74.229)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.8589 (0.7555)	Prec@1 75.781 (74.286)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.7739 (0.7531)	Prec@1 75.781 (74.447)	
Total train loss: 0.7531

Train time: 11.704399347305298
 * Prec@1 75.870 Prec@5 97.900 Loss 0.7441
Best acc: 75.870
--------------------------------------------------------------------------------
Test time: 14.790486574172974

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.7446 (0.7419)	Prec@1 76.953 (74.569)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.6694 (0.7386)	Prec@1 80.078 (74.775)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.7021 (0.7452)	Prec@1 76.562 (74.586)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.7705 (0.7534)	Prec@1 73.438 (74.344)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.8042 (0.7519)	Prec@1 71.094 (74.503)	
Total train loss: 0.7519

Train time: 11.673548460006714
 * Prec@1 75.890 Prec@5 97.910 Loss 0.7466
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.248892545700073

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.8330 (0.7460)	Prec@1 71.094 (74.700)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.7837 (0.7546)	Prec@1 72.656 (74.604)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.9180 (0.7576)	Prec@1 67.969 (74.366)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.7231 (0.7528)	Prec@1 73.828 (74.467)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.8223 (0.7535)	Prec@1 68.750 (74.537)	
Total train loss: 0.7534

Train time: 12.60208797454834
 * Prec@1 75.890 Prec@5 97.890 Loss 0.7441
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.446971416473389

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.6606 (0.7510)	Prec@1 78.125 (74.820)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.7295 (0.7472)	Prec@1 73.828 (74.494)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.7510 (0.7491)	Prec@1 75.000 (74.446)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.8232 (0.7507)	Prec@1 70.703 (74.392)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.8662 (0.7523)	Prec@1 73.047 (74.413)	
Total train loss: 0.7524

Train time: 12.283493757247925
 * Prec@1 75.730 Prec@5 97.930 Loss 0.7446
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.819443941116333

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.8838 (0.7584)	Prec@1 69.922 (74.269)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.7969 (0.7531)	Prec@1 70.312 (74.294)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.6807 (0.7462)	Prec@1 76.562 (74.569)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.7695 (0.7508)	Prec@1 73.828 (74.567)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.8193 (0.7514)	Prec@1 69.922 (74.541)	
Total train loss: 0.7513

Train time: 11.848581314086914
 * Prec@1 75.890 Prec@5 97.910 Loss 0.7466
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.139768838882446

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.7437 (0.7554)	Prec@1 77.344 (74.399)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.7856 (0.7568)	Prec@1 73.438 (74.614)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.6958 (0.7528)	Prec@1 75.000 (74.760)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.7598 (0.7514)	Prec@1 72.656 (74.637)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.8716 (0.7544)	Prec@1 71.484 (74.549)	
Total train loss: 0.7547

Train time: 11.722436428070068
 * Prec@1 75.720 Prec@5 97.900 Loss 0.7461
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.308718204498291

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.6699 (0.7322)	Prec@1 78.516 (75.050)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.7310 (0.7395)	Prec@1 75.781 (74.579)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.6562 (0.7464)	Prec@1 77.734 (74.452)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.8701 (0.7469)	Prec@1 69.922 (74.547)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.8628 (0.7525)	Prec@1 71.094 (74.433)	
Total train loss: 0.7521

Train time: 12.334580659866333
 * Prec@1 75.560 Prec@5 97.860 Loss 0.7476
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.19666337966919

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.6709 (0.7669)	Prec@1 76.953 (74.309)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.6362 (0.7546)	Prec@1 77.734 (74.825)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.7632 (0.7559)	Prec@1 73.438 (74.539)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.7832 (0.7573)	Prec@1 71.484 (74.507)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.7026 (0.7538)	Prec@1 75.391 (74.559)	
Total train loss: 0.7538

Train time: 12.44486927986145
 * Prec@1 75.800 Prec@5 97.880 Loss 0.7466
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.73636794090271

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.6973 (0.7363)	Prec@1 73.828 (75.331)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.8115 (0.7432)	Prec@1 74.219 (74.965)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.6094 (0.7482)	Prec@1 78.906 (74.730)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.6670 (0.7487)	Prec@1 77.344 (74.662)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.7310 (0.7518)	Prec@1 74.219 (74.497)	
Total train loss: 0.7517

Train time: 12.142191648483276
 * Prec@1 75.870 Prec@5 97.850 Loss 0.7485
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.196284294128418

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.8242 (0.7502)	Prec@1 73.828 (74.099)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.7598 (0.7557)	Prec@1 75.000 (74.424)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.7695 (0.7532)	Prec@1 71.875 (74.499)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.7129 (0.7482)	Prec@1 75.000 (74.667)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.7505 (0.7527)	Prec@1 73.438 (74.511)	
Total train loss: 0.7524

Train time: 11.736347675323486
 * Prec@1 75.820 Prec@5 97.910 Loss 0.7456
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.497029781341553

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.8022 (0.7456)	Prec@1 73.828 (74.669)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.8901 (0.7500)	Prec@1 69.922 (74.484)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.8574 (0.7548)	Prec@1 67.969 (74.456)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.6099 (0.7524)	Prec@1 79.688 (74.512)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.7100 (0.7520)	Prec@1 79.297 (74.497)	
Total train loss: 0.7521

Train time: 12.32692837715149
 * Prec@1 75.880 Prec@5 97.940 Loss 0.7461
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.417251110076904

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.7373 (0.7522)	Prec@1 76.953 (74.529)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.7280 (0.7554)	Prec@1 73.828 (74.219)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.7920 (0.7566)	Prec@1 71.875 (74.282)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.6699 (0.7535)	Prec@1 75.391 (74.319)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.6274 (0.7521)	Prec@1 78.516 (74.529)	
Total train loss: 0.7522

Train time: 12.17361068725586
 * Prec@1 75.880 Prec@5 97.920 Loss 0.7427
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.496991157531738

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.7163 (0.7528)	Prec@1 76.172 (74.319)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.6396 (0.7491)	Prec@1 79.297 (74.334)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.7349 (0.7497)	Prec@1 74.219 (74.392)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.7661 (0.7524)	Prec@1 73.828 (74.339)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.8467 (0.7516)	Prec@1 71.484 (74.413)	
Total train loss: 0.7514

Train time: 12.497357845306396
 * Prec@1 75.850 Prec@5 97.880 Loss 0.7446
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.34411907196045

Epoch: [30][38/196]	LR: 0.00010000000000000003	Loss 0.8643 (0.7472)	Prec@1 69.531 (74.179)	
Epoch: [30][77/196]	LR: 0.00010000000000000003	Loss 0.7915 (0.7559)	Prec@1 70.703 (74.184)	
Epoch: [30][116/196]	LR: 0.00010000000000000003	Loss 0.7524 (0.7553)	Prec@1 72.656 (74.189)	
Epoch: [30][155/196]	LR: 0.00010000000000000003	Loss 0.7808 (0.7534)	Prec@1 75.781 (74.309)	
Epoch: [30][194/196]	LR: 0.00010000000000000003	Loss 0.8008 (0.7536)	Prec@1 72.656 (74.365)	
Total train loss: 0.7536

Train time: 11.894820928573608
 * Prec@1 75.770 Prec@5 97.960 Loss 0.7437
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.761137247085571

Epoch: [31][38/196]	LR: 0.00010000000000000003	Loss 0.6748 (0.7585)	Prec@1 81.250 (74.940)	
Epoch: [31][77/196]	LR: 0.00010000000000000003	Loss 0.7495 (0.7584)	Prec@1 76.172 (74.674)	
Epoch: [31][116/196]	LR: 0.00010000000000000003	Loss 0.7168 (0.7570)	Prec@1 72.266 (74.459)	
Epoch: [31][155/196]	LR: 0.00010000000000000003	Loss 0.7476 (0.7551)	Prec@1 73.828 (74.567)	
Epoch: [31][194/196]	LR: 0.00010000000000000003	Loss 0.5981 (0.7526)	Prec@1 81.641 (74.549)	
Total train loss: 0.7529

Train time: 12.157611608505249
 * Prec@1 75.670 Prec@5 97.890 Loss 0.7466
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.398739099502563

Epoch: [32][38/196]	LR: 1.0000000000000004e-05	Loss 0.6636 (0.7565)	Prec@1 77.734 (73.988)	
Epoch: [32][77/196]	LR: 1.0000000000000004e-05	Loss 0.7539 (0.7563)	Prec@1 73.828 (74.159)	
Epoch: [32][116/196]	LR: 1.0000000000000004e-05	Loss 0.6162 (0.7516)	Prec@1 78.125 (74.469)	
Epoch: [32][155/196]	LR: 1.0000000000000004e-05	Loss 0.7217 (0.7521)	Prec@1 75.781 (74.577)	
Epoch: [32][194/196]	LR: 1.0000000000000004e-05	Loss 0.7007 (0.7523)	Prec@1 78.516 (74.565)	
Total train loss: 0.7525

Train time: 12.221877574920654
 * Prec@1 75.880 Prec@5 97.880 Loss 0.7437
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.796612977981567

Epoch: [33][38/196]	LR: 1.0000000000000004e-05	Loss 0.7524 (0.7664)	Prec@1 77.344 (74.119)	
Epoch: [33][77/196]	LR: 1.0000000000000004e-05	Loss 0.8096 (0.7515)	Prec@1 72.266 (74.564)	
Epoch: [33][116/196]	LR: 1.0000000000000004e-05	Loss 0.8525 (0.7482)	Prec@1 71.875 (74.616)	
Epoch: [33][155/196]	LR: 1.0000000000000004e-05	Loss 0.7212 (0.7479)	Prec@1 73.828 (74.549)	
Epoch: [33][194/196]	LR: 1.0000000000000004e-05	Loss 0.8301 (0.7523)	Prec@1 76.172 (74.499)	
Total train loss: 0.7521

Train time: 12.373685359954834
 * Prec@1 75.750 Prec@5 97.890 Loss 0.7480
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.266221523284912

Epoch: [34][38/196]	LR: 1.0000000000000004e-05	Loss 0.7603 (0.7503)	Prec@1 73.438 (74.399)	
Epoch: [34][77/196]	LR: 1.0000000000000004e-05	Loss 0.7583 (0.7543)	Prec@1 76.172 (74.459)	
Epoch: [34][116/196]	LR: 1.0000000000000004e-05	Loss 0.7021 (0.7519)	Prec@1 72.656 (74.559)	
Epoch: [34][155/196]	LR: 1.0000000000000004e-05	Loss 0.8354 (0.7590)	Prec@1 71.875 (74.296)	
Epoch: [34][194/196]	LR: 1.0000000000000004e-05	Loss 0.7432 (0.7534)	Prec@1 75.391 (74.411)	
Total train loss: 0.7532

Train time: 12.286699533462524
 * Prec@1 75.780 Prec@5 97.880 Loss 0.7446
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.146582841873169

Epoch: [35][38/196]	LR: 1.0000000000000004e-05	Loss 0.7568 (0.7439)	Prec@1 73.047 (74.619)	
Epoch: [35][77/196]	LR: 1.0000000000000004e-05	Loss 0.8193 (0.7557)	Prec@1 73.047 (74.444)	
Epoch: [35][116/196]	LR: 1.0000000000000004e-05	Loss 0.7158 (0.7569)	Prec@1 75.000 (74.269)	
Epoch: [35][155/196]	LR: 1.0000000000000004e-05	Loss 0.8315 (0.7519)	Prec@1 72.656 (74.437)	
Epoch: [35][194/196]	LR: 1.0000000000000004e-05	Loss 0.7456 (0.7522)	Prec@1 74.219 (74.439)	
Total train loss: 0.7521

Train time: 12.235930919647217
 * Prec@1 75.790 Prec@5 97.860 Loss 0.7476
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.47835087776184

Epoch: [36][38/196]	LR: 1.0000000000000004e-05	Loss 0.7314 (0.7646)	Prec@1 74.609 (74.249)	
Epoch: [36][77/196]	LR: 1.0000000000000004e-05	Loss 0.6382 (0.7538)	Prec@1 79.297 (74.569)	
Epoch: [36][116/196]	LR: 1.0000000000000004e-05	Loss 0.8154 (0.7539)	Prec@1 72.266 (74.479)	
Epoch: [36][155/196]	LR: 1.0000000000000004e-05	Loss 0.5845 (0.7509)	Prec@1 79.688 (74.637)	
Epoch: [36][194/196]	LR: 1.0000000000000004e-05	Loss 0.7432 (0.7514)	Prec@1 72.656 (74.617)	
Total train loss: 0.7514

Train time: 12.042882680892944
 * Prec@1 75.710 Prec@5 97.870 Loss 0.7466
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 14.584274530410767

Epoch: [37][38/196]	LR: 1.0000000000000004e-05	Loss 0.7236 (0.7457)	Prec@1 75.391 (74.760)	
Epoch: [37][77/196]	LR: 1.0000000000000004e-05	Loss 0.6626 (0.7530)	Prec@1 74.609 (74.359)	
Epoch: [37][116/196]	LR: 1.0000000000000004e-05	Loss 0.6621 (0.7526)	Prec@1 75.781 (74.155)	
Epoch: [37][155/196]	LR: 1.0000000000000004e-05	Loss 0.7349 (0.7527)	Prec@1 74.609 (74.346)	
Epoch: [37][194/196]	LR: 1.0000000000000004e-05	Loss 0.8057 (0.7521)	Prec@1 73.438 (74.365)	
Total train loss: 0.7519

Train time: 12.626828908920288
 * Prec@1 75.830 Prec@5 97.940 Loss 0.7446
Best acc: 75.890
--------------------------------------------------------------------------------
Test time: 15.490843534469604

Epoch: [38][38/196]	LR: 1.0000000000000004e-05	Loss 0.5669 (0.7268)	Prec@1 80.078 (75.230)	
Epoch: [38][77/196]	LR: 1.0000000000000004e-05	Loss 0.7153 (0.7429)	Prec@1 75.000 (74.524)	
Epoch: [38][116/196]	LR: 1.0000000000000004e-05	Loss 0.6719 (0.7470)	Prec@1 76.172 (74.442)	
Epoch: [38][155/196]	LR: 1.0000000000000004e-05	Loss 0.8218 (0.7499)	Prec@1 70.703 (74.484)	
Epoch: [38][194/196]	LR: 1.0000000000000004e-05	Loss 0.8491 (0.7502)	Prec@1 71.484 (74.505)	
Total train loss: 0.7503

Train time: 12.094054222106934
 * Prec@1 75.900 Prec@5 97.930 Loss 0.7466
Best acc: 75.900
--------------------------------------------------------------------------------
Test time: 14.923758029937744

Epoch: [39][38/196]	LR: 1.0000000000000004e-05	Loss 0.7153 (0.7544)	Prec@1 76.172 (74.119)	
Epoch: [39][77/196]	LR: 1.0000000000000004e-05	Loss 0.7344 (0.7546)	Prec@1 73.828 (74.033)	
Epoch: [39][116/196]	LR: 1.0000000000000004e-05	Loss 0.7070 (0.7449)	Prec@1 78.125 (74.503)	
Epoch: [39][155/196]	LR: 1.0000000000000004e-05	Loss 0.8613 (0.7492)	Prec@1 71.484 (74.464)	
Epoch: [39][194/196]	LR: 1.0000000000000004e-05	Loss 0.7314 (0.7526)	Prec@1 76.172 (74.359)	
Total train loss: 0.7525

Train time: 12.100072622299194
 * Prec@1 75.870 Prec@5 97.930 Loss 0.7456
Best acc: 75.900
--------------------------------------------------------------------------------
Test time: 15.255347728729248

