
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 35.040 Prec@5 85.310 Loss 3.3828
Pre-trained Prec@1 with 11 layers frozen: 35.040000915527344 	 Loss: 3.3828125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.3367 (0.4538)	Prec@1 90.234 (87.049)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.4082 (0.4130)	Prec@1 87.109 (87.826)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.4700 (0.3957)	Prec@1 83.594 (88.188)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.2411 (0.3857)	Prec@1 91.797 (88.366)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.3564 (0.3780)	Prec@1 85.547 (88.530)	
Total train loss: 0.3780

Train time: 435.3679485321045
 * Prec@1 85.790 Prec@5 99.340 Loss 0.5493
Best acc: 85.790
--------------------------------------------------------------------------------
Test time: 442.8974254131317

Epoch: [1][38/196]	LR: 0.01	Loss 0.2717 (0.3176)	Prec@1 91.016 (89.694)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.3450 (0.3249)	Prec@1 89.844 (89.448)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.1937 (0.3233)	Prec@1 94.531 (89.580)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.3999 (0.3276)	Prec@1 85.156 (89.431)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.3608 (0.3263)	Prec@1 87.891 (89.451)	
Total train loss: 0.3260

Train time: 25.814282178878784
 * Prec@1 86.100 Prec@5 99.370 Loss 0.5249
Best acc: 86.100
--------------------------------------------------------------------------------
Test time: 32.306267738342285

Epoch: [2][38/196]	LR: 0.01	Loss 0.2937 (0.3115)	Prec@1 89.844 (89.413)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.2542 (0.3130)	Prec@1 90.625 (89.418)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.3025 (0.3092)	Prec@1 90.625 (89.597)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.3540 (0.3142)	Prec@1 88.672 (89.496)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.2986 (0.3147)	Prec@1 89.844 (89.493)	
Total train loss: 0.3146

Train time: 27.630404710769653
 * Prec@1 85.800 Prec@5 99.260 Loss 0.5181
Best acc: 86.100
--------------------------------------------------------------------------------
Test time: 32.67842626571655

Epoch: [3][38/196]	LR: 0.01	Loss 0.3015 (0.2959)	Prec@1 89.844 (89.934)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.4084 (0.3088)	Prec@1 87.891 (89.613)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.3545 (0.3174)	Prec@1 87.891 (89.356)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.2568 (0.3162)	Prec@1 91.406 (89.393)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.3250 (0.3146)	Prec@1 89.844 (89.509)	
Total train loss: 0.3146

Train time: 26.291165828704834
 * Prec@1 86.200 Prec@5 99.270 Loss 0.5078
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 30.1153244972229

Epoch: [4][38/196]	LR: 0.01	Loss 0.3428 (0.3140)	Prec@1 89.844 (89.623)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.2529 (0.3240)	Prec@1 91.406 (89.328)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.3411 (0.3247)	Prec@1 87.891 (89.259)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.3723 (0.3205)	Prec@1 86.328 (89.343)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.2854 (0.3196)	Prec@1 91.797 (89.345)	
Total train loss: 0.3195

Train time: 20.8096981048584
 * Prec@1 85.750 Prec@5 99.270 Loss 0.5093
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.227086305618286

Epoch: [5][38/196]	LR: 0.01	Loss 0.4053 (0.3142)	Prec@1 86.719 (89.313)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.3911 (0.3198)	Prec@1 87.891 (89.083)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.3269 (0.3197)	Prec@1 91.016 (89.153)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.2664 (0.3177)	Prec@1 92.188 (89.293)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.3511 (0.3190)	Prec@1 87.109 (89.267)	
Total train loss: 0.3190

Train time: 20.174756288528442
 * Prec@1 85.670 Prec@5 99.290 Loss 0.5078
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.297649145126343

Epoch: [6][38/196]	LR: 0.01	Loss 0.2693 (0.3167)	Prec@1 88.281 (89.423)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.2649 (0.3204)	Prec@1 89.844 (89.178)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.3945 (0.3233)	Prec@1 85.156 (89.056)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.3289 (0.3241)	Prec@1 89.844 (88.952)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.3147 (0.3224)	Prec@1 89.453 (88.984)	
Total train loss: 0.3225

Train time: 19.677486181259155
 * Prec@1 85.580 Prec@5 99.240 Loss 0.5068
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.807144165039062

Epoch: [7][38/196]	LR: 0.01	Loss 0.3333 (0.3309)	Prec@1 91.406 (88.602)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.4062 (0.3195)	Prec@1 87.500 (89.022)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.3147 (0.3171)	Prec@1 87.500 (89.129)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.4910 (0.3194)	Prec@1 84.375 (89.078)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.3279 (0.3212)	Prec@1 89.844 (89.040)	
Total train loss: 0.3212

Train time: 19.401474475860596
 * Prec@1 85.560 Prec@5 99.310 Loss 0.4988
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.6507785320282

Epoch: [8][38/196]	LR: 0.001	Loss 0.2356 (0.3102)	Prec@1 91.406 (89.453)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.2952 (0.3141)	Prec@1 88.672 (89.343)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.2742 (0.3104)	Prec@1 92.578 (89.530)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.3496 (0.3128)	Prec@1 87.500 (89.433)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.3655 (0.3157)	Prec@1 88.281 (89.379)	
Total train loss: 0.3157

Train time: 19.419363498687744
 * Prec@1 85.540 Prec@5 99.290 Loss 0.5039
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.611279487609863

Epoch: [9][38/196]	LR: 0.001	Loss 0.3530 (0.3273)	Prec@1 88.672 (88.832)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.2859 (0.3255)	Prec@1 90.625 (88.972)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.3025 (0.3187)	Prec@1 90.234 (89.213)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.3254 (0.3192)	Prec@1 90.234 (89.203)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.4219 (0.3182)	Prec@1 87.109 (89.255)	
Total train loss: 0.3183

Train time: 19.222636222839355
 * Prec@1 85.610 Prec@5 99.320 Loss 0.5049
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.792059183120728

Epoch: [10][38/196]	LR: 0.001	Loss 0.3596 (0.3235)	Prec@1 90.625 (89.103)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.3123 (0.3121)	Prec@1 89.844 (89.413)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.2308 (0.3122)	Prec@1 93.750 (89.336)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.2988 (0.3153)	Prec@1 88.672 (89.330)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.3682 (0.3135)	Prec@1 87.500 (89.345)	
Total train loss: 0.3134

Train time: 19.65581750869751
 * Prec@1 85.520 Prec@5 99.250 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.073817014694214

Epoch: [11][38/196]	LR: 0.001	Loss 0.3694 (0.3118)	Prec@1 88.672 (89.433)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.2998 (0.3135)	Prec@1 90.625 (89.408)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.1803 (0.3131)	Prec@1 93.359 (89.363)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.3721 (0.3148)	Prec@1 87.500 (89.248)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.3975 (0.3151)	Prec@1 87.109 (89.249)	
Total train loss: 0.3151

Train time: 19.707064628601074
 * Prec@1 85.570 Prec@5 99.260 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.58186674118042

Epoch: [12][38/196]	LR: 0.001	Loss 0.3718 (0.3166)	Prec@1 87.891 (89.413)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.2260 (0.3083)	Prec@1 91.016 (89.573)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.3438 (0.3152)	Prec@1 88.281 (89.436)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.3101 (0.3172)	Prec@1 91.797 (89.358)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.3806 (0.3144)	Prec@1 88.672 (89.381)	
Total train loss: 0.3144

Train time: 18.985433340072632
 * Prec@1 85.440 Prec@5 99.330 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.262463331222534

Epoch: [13][38/196]	LR: 0.001	Loss 0.2201 (0.3280)	Prec@1 91.406 (89.143)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.3579 (0.3233)	Prec@1 89.062 (89.348)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.2781 (0.3195)	Prec@1 91.797 (89.410)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.3323 (0.3163)	Prec@1 87.891 (89.391)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.2817 (0.3157)	Prec@1 90.234 (89.371)	
Total train loss: 0.3157

Train time: 19.650047302246094
 * Prec@1 85.480 Prec@5 99.260 Loss 0.5020
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.452103853225708

Epoch: [14][38/196]	LR: 0.001	Loss 0.2380 (0.3169)	Prec@1 91.406 (89.363)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.2898 (0.3171)	Prec@1 89.453 (89.203)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.2559 (0.3157)	Prec@1 90.234 (89.293)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.2725 (0.3127)	Prec@1 89.453 (89.378)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.2979 (0.3147)	Prec@1 90.234 (89.297)	
Total train loss: 0.3148

Train time: 22.31263041496277
 * Prec@1 85.480 Prec@5 99.310 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 26.138890981674194

Epoch: [15][38/196]	LR: 0.001	Loss 0.3740 (0.3101)	Prec@1 87.109 (89.393)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.3059 (0.3148)	Prec@1 89.062 (89.047)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.2859 (0.3145)	Prec@1 91.406 (89.193)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.3484 (0.3153)	Prec@1 90.234 (89.268)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.2754 (0.3152)	Prec@1 90.625 (89.273)	
Total train loss: 0.3152

Train time: 19.328616619110107
 * Prec@1 85.430 Prec@5 99.270 Loss 0.5000
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.16231870651245

Epoch: [16][38/196]	LR: 0.0001	Loss 0.2629 (0.2954)	Prec@1 91.406 (89.984)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.2410 (0.3056)	Prec@1 92.188 (89.678)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.3083 (0.3142)	Prec@1 90.234 (89.336)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.3635 (0.3149)	Prec@1 89.453 (89.318)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.3279 (0.3146)	Prec@1 89.062 (89.277)	
Total train loss: 0.3143

Train time: 17.623374462127686
 * Prec@1 85.610 Prec@5 99.320 Loss 0.4956
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.49963116645813

Epoch: [17][38/196]	LR: 0.0001	Loss 0.2369 (0.3141)	Prec@1 92.578 (89.343)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.3049 (0.3159)	Prec@1 89.062 (89.243)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.2886 (0.3193)	Prec@1 89.062 (89.126)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.2620 (0.3151)	Prec@1 92.578 (89.298)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.3206 (0.3147)	Prec@1 87.891 (89.255)	
Total train loss: 0.3145

Train time: 18.213327407836914
 * Prec@1 85.470 Prec@5 99.340 Loss 0.5029
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.085083484649658

Epoch: [18][38/196]	LR: 0.0001	Loss 0.2896 (0.3206)	Prec@1 91.016 (89.273)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.2974 (0.3150)	Prec@1 93.359 (89.478)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.2593 (0.3129)	Prec@1 92.578 (89.470)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.3118 (0.3137)	Prec@1 91.406 (89.376)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.2654 (0.3130)	Prec@1 89.453 (89.357)	
Total train loss: 0.3128

Train time: 19.101709365844727
 * Prec@1 85.580 Prec@5 99.280 Loss 0.5000
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.801136255264282

Epoch: [19][38/196]	LR: 0.0001	Loss 0.4351 (0.3115)	Prec@1 85.938 (89.323)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.3018 (0.3205)	Prec@1 91.016 (89.188)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.2891 (0.3156)	Prec@1 87.109 (89.406)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.2201 (0.3142)	Prec@1 92.578 (89.411)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.3259 (0.3128)	Prec@1 87.891 (89.421)	
Total train loss: 0.3127

Train time: 19.10612440109253
 * Prec@1 85.420 Prec@5 99.270 Loss 0.4993
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.236812353134155

Epoch: [20][38/196]	LR: 0.0001	Loss 0.3064 (0.3094)	Prec@1 88.281 (89.413)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.2791 (0.3115)	Prec@1 90.625 (89.433)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.2932 (0.3111)	Prec@1 87.891 (89.383)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.3364 (0.3122)	Prec@1 90.625 (89.353)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.3069 (0.3145)	Prec@1 91.406 (89.307)	
Total train loss: 0.3146

Train time: 19.71674633026123
 * Prec@1 85.530 Prec@5 99.300 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.52384066581726

Epoch: [21][38/196]	LR: 0.0001	Loss 0.3438 (0.3170)	Prec@1 88.672 (89.353)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.3054 (0.3159)	Prec@1 90.234 (89.463)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.3127 (0.3164)	Prec@1 88.672 (89.396)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.4006 (0.3154)	Prec@1 85.547 (89.388)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.2306 (0.3135)	Prec@1 92.578 (89.353)	
Total train loss: 0.3136

Train time: 19.190994262695312
 * Prec@1 85.610 Prec@5 99.300 Loss 0.5000
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.310632705688477

Epoch: [22][38/196]	LR: 0.0001	Loss 0.2266 (0.3254)	Prec@1 92.578 (89.062)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.3411 (0.3171)	Prec@1 88.281 (89.083)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.4243 (0.3133)	Prec@1 85.156 (89.166)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.2686 (0.3135)	Prec@1 91.016 (89.233)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.3401 (0.3153)	Prec@1 91.016 (89.265)	
Total train loss: 0.3153

Train time: 19.52405595779419
 * Prec@1 85.480 Prec@5 99.320 Loss 0.5020
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 25.41180157661438

Epoch: [23][38/196]	LR: 0.0001	Loss 0.3525 (0.3153)	Prec@1 89.062 (89.353)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.3232 (0.3133)	Prec@1 89.062 (89.578)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.3069 (0.3145)	Prec@1 89.844 (89.413)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.3259 (0.3134)	Prec@1 87.891 (89.463)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.3711 (0.3157)	Prec@1 85.156 (89.397)	
Total train loss: 0.3154

Train time: 18.85307788848877
 * Prec@1 85.490 Prec@5 99.280 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.658499717712402

Epoch: [24][38/196]	LR: 1e-05	Loss 0.3069 (0.3098)	Prec@1 90.234 (89.353)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.2800 (0.3097)	Prec@1 91.797 (89.583)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.3989 (0.3080)	Prec@1 87.109 (89.643)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.3474 (0.3113)	Prec@1 87.891 (89.436)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.3831 (0.3136)	Prec@1 87.891 (89.385)	
Total train loss: 0.3139

Train time: 19.34560513496399
 * Prec@1 85.480 Prec@5 99.250 Loss 0.5029
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.206920385360718

Epoch: [25][38/196]	LR: 1e-05	Loss 0.3408 (0.3115)	Prec@1 88.281 (89.093)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.3125 (0.3186)	Prec@1 90.625 (88.997)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.4160 (0.3134)	Prec@1 85.938 (89.266)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.4019 (0.3139)	Prec@1 85.156 (89.278)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.2910 (0.3132)	Prec@1 89.062 (89.345)	
Total train loss: 0.3133

Train time: 20.66005825996399
 * Prec@1 85.570 Prec@5 99.270 Loss 0.5034
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 27.38189458847046

Epoch: [26][38/196]	LR: 1e-05	Loss 0.3762 (0.3063)	Prec@1 85.938 (89.563)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.3062 (0.3146)	Prec@1 87.500 (89.358)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.4607 (0.3134)	Prec@1 83.203 (89.330)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.2866 (0.3136)	Prec@1 91.016 (89.295)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.3713 (0.3155)	Prec@1 89.062 (89.199)	
Total train loss: 0.3157

Train time: 19.506412267684937
 * Prec@1 85.520 Prec@5 99.280 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.367782592773438

Epoch: [27][38/196]	LR: 1e-05	Loss 0.3901 (0.3141)	Prec@1 87.891 (89.343)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.3113 (0.3176)	Prec@1 90.625 (89.243)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.3096 (0.3154)	Prec@1 87.891 (89.273)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.3325 (0.3125)	Prec@1 89.062 (89.335)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.2443 (0.3136)	Prec@1 91.406 (89.317)	
Total train loss: 0.3137

Train time: 19.699581384658813
 * Prec@1 85.700 Prec@5 99.280 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.94412088394165

Epoch: [28][38/196]	LR: 1e-05	Loss 0.3088 (0.3031)	Prec@1 87.109 (89.633)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.2703 (0.3121)	Prec@1 91.016 (89.348)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.3423 (0.3156)	Prec@1 88.281 (89.236)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.1997 (0.3146)	Prec@1 92.188 (89.205)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.3989 (0.3151)	Prec@1 88.281 (89.213)	
Total train loss: 0.3151

Train time: 19.651907444000244
 * Prec@1 85.470 Prec@5 99.280 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.177104234695435

Epoch: [29][38/196]	LR: 1e-05	Loss 0.3599 (0.3199)	Prec@1 89.062 (89.263)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.2732 (0.3116)	Prec@1 91.406 (89.338)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.3694 (0.3123)	Prec@1 89.453 (89.323)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.3279 (0.3117)	Prec@1 88.672 (89.315)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.1682 (0.3130)	Prec@1 94.531 (89.309)	
Total train loss: 0.3130

Train time: 19.673553705215454
 * Prec@1 85.470 Prec@5 99.310 Loss 0.5054
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.509339332580566

Epoch: [30][38/196]	LR: 1e-05	Loss 0.4255 (0.3057)	Prec@1 85.938 (89.854)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.3652 (0.3088)	Prec@1 87.500 (89.668)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.3262 (0.3126)	Prec@1 89.453 (89.420)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.4167 (0.3150)	Prec@1 87.891 (89.295)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.2827 (0.3137)	Prec@1 90.234 (89.351)	
Total train loss: 0.3138

Train time: 19.87592601776123
 * Prec@1 85.540 Prec@5 99.260 Loss 0.5005
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 23.669991731643677

Epoch: [31][38/196]	LR: 1e-05	Loss 0.3264 (0.3117)	Prec@1 88.281 (89.363)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.4438 (0.3192)	Prec@1 86.719 (89.313)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.3611 (0.3142)	Prec@1 89.453 (89.410)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.3037 (0.3169)	Prec@1 91.016 (89.401)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.2649 (0.3151)	Prec@1 91.406 (89.361)	
Total train loss: 0.3149

Train time: 20.372488498687744
 * Prec@1 85.580 Prec@5 99.310 Loss 0.4973
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.14446711540222

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.2859 (0.3086)	Prec@1 88.281 (89.553)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.3389 (0.3122)	Prec@1 88.672 (89.428)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.4253 (0.3111)	Prec@1 83.984 (89.423)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.2754 (0.3171)	Prec@1 89.062 (89.250)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.2620 (0.3144)	Prec@1 90.234 (89.331)	
Total train loss: 0.3147

Train time: 19.39033794403076
 * Prec@1 85.580 Prec@5 99.310 Loss 0.5054
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.576385498046875

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.3936 (0.3078)	Prec@1 88.672 (89.523)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.2861 (0.3109)	Prec@1 89.844 (89.433)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.3152 (0.3114)	Prec@1 89.453 (89.360)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.3547 (0.3133)	Prec@1 87.891 (89.265)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.3276 (0.3130)	Prec@1 89.062 (89.311)	
Total train loss: 0.3131

Train time: 19.837079286575317
 * Prec@1 85.580 Prec@5 99.240 Loss 0.4983
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.185675382614136

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.2979 (0.3108)	Prec@1 88.281 (89.273)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.2981 (0.3157)	Prec@1 90.625 (89.183)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.3298 (0.3133)	Prec@1 90.234 (89.320)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.2170 (0.3123)	Prec@1 92.969 (89.403)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.3396 (0.3143)	Prec@1 88.672 (89.355)	
Total train loss: 0.3140

Train time: 18.766596794128418
 * Prec@1 85.580 Prec@5 99.260 Loss 0.5000
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.79843497276306

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.2494 (0.3155)	Prec@1 92.188 (89.223)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.4075 (0.3186)	Prec@1 87.500 (89.153)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.3274 (0.3142)	Prec@1 87.109 (89.203)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.2444 (0.3170)	Prec@1 92.188 (89.218)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.3477 (0.3146)	Prec@1 87.500 (89.221)	
Total train loss: 0.3147

Train time: 20.801867723464966
 * Prec@1 85.460 Prec@5 99.310 Loss 0.5015
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.470720767974854

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.3218 (0.3123)	Prec@1 89.453 (89.233)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.2479 (0.3093)	Prec@1 93.359 (89.323)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.3477 (0.3112)	Prec@1 88.281 (89.306)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.3594 (0.3105)	Prec@1 90.625 (89.328)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.3015 (0.3111)	Prec@1 89.844 (89.361)	
Total train loss: 0.3111

Train time: 19.70660948753357
 * Prec@1 85.550 Prec@5 99.320 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.091795444488525

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.3245 (0.3105)	Prec@1 88.672 (89.193)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.3486 (0.3111)	Prec@1 88.281 (89.198)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.3103 (0.3135)	Prec@1 89.453 (89.209)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.2668 (0.3121)	Prec@1 91.797 (89.245)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.3149 (0.3141)	Prec@1 90.625 (89.213)	
Total train loss: 0.3142

Train time: 18.42248272895813
 * Prec@1 85.480 Prec@5 99.250 Loss 0.5010
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 22.476531982421875

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.2456 (0.3008)	Prec@1 90.234 (90.044)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.2896 (0.3039)	Prec@1 88.672 (89.678)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.3818 (0.3099)	Prec@1 87.500 (89.523)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.3464 (0.3108)	Prec@1 90.625 (89.418)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.3997 (0.3129)	Prec@1 87.891 (89.381)	
Total train loss: 0.3128

Train time: 19.956488847732544
 * Prec@1 85.520 Prec@5 99.250 Loss 0.5005
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 24.816709518432617

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.2654 (0.3258)	Prec@1 91.016 (88.832)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.2878 (0.3169)	Prec@1 89.844 (89.047)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.2612 (0.3173)	Prec@1 90.234 (88.992)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.3955 (0.3159)	Prec@1 85.938 (89.085)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.2830 (0.3130)	Prec@1 91.406 (89.251)	
Total train loss: 0.3132

Train time: 10.97727370262146
 * Prec@1 85.680 Prec@5 99.280 Loss 0.5005
Best acc: 86.200
--------------------------------------------------------------------------------
Test time: 13.573265552520752


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 29.980 Prec@5 81.840 Loss 3.6895
Pre-trained Prec@1 with 13 layers frozen: 29.979999542236328 	 Loss: 3.689453125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.6440 (0.6122)	Prec@1 82.812 (83.123)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.5449 (0.5774)	Prec@1 84.766 (83.609)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.5352 (0.5582)	Prec@1 82.422 (84.021)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.4829 (0.5408)	Prec@1 84.766 (84.282)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.4062 (0.5309)	Prec@1 87.109 (84.491)	
Total train loss: 0.5308

Train time: 163.15399074554443
 * Prec@1 82.340 Prec@5 98.740 Loss 0.6748
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 168.13770580291748

Epoch: [1][38/196]	LR: 0.01	Loss 0.4558 (0.4890)	Prec@1 82.422 (84.365)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.6436 (0.4858)	Prec@1 80.078 (84.585)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.6421 (0.4961)	Prec@1 83.203 (84.388)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.5078 (0.4997)	Prec@1 84.766 (84.315)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.6753 (0.5071)	Prec@1 79.688 (84.137)	
Total train loss: 0.5070

Train time: 19.9835946559906
 * Prec@1 82.240 Prec@5 98.920 Loss 0.6431
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.15136742591858

Epoch: [2][38/196]	LR: 0.01	Loss 0.5840 (0.5106)	Prec@1 84.766 (83.514)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.5195 (0.5178)	Prec@1 81.641 (83.433)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.5845 (0.5090)	Prec@1 81.641 (83.577)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.5796 (0.5175)	Prec@1 82.031 (83.368)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.5327 (0.5235)	Prec@1 84.766 (83.235)	
Total train loss: 0.5234

Train time: 19.87911868095398
 * Prec@1 82.100 Prec@5 98.890 Loss 0.6318
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.661839485168457

Epoch: [3][38/196]	LR: 0.01	Loss 0.6035 (0.5200)	Prec@1 81.250 (83.323)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.4956 (0.5279)	Prec@1 82.422 (83.033)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.4385 (0.5244)	Prec@1 85.938 (83.237)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.6704 (0.5337)	Prec@1 79.688 (82.975)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.4907 (0.5323)	Prec@1 83.203 (82.911)	
Total train loss: 0.5324

Train time: 19.603267908096313
 * Prec@1 81.220 Prec@5 98.650 Loss 0.6343
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.160025358200073

Epoch: [4][38/196]	LR: 0.01	Loss 0.6143 (0.5345)	Prec@1 79.688 (82.011)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.4822 (0.5376)	Prec@1 83.594 (82.242)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.7095 (0.5340)	Prec@1 79.688 (82.115)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.4739 (0.5296)	Prec@1 82.812 (82.294)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.5293 (0.5274)	Prec@1 80.469 (82.230)	
Total train loss: 0.5275

Train time: 21.061290502548218
 * Prec@1 81.420 Prec@5 98.850 Loss 0.5747
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.163862466812134

Epoch: [5][38/196]	LR: 0.01	Loss 0.4951 (0.4997)	Prec@1 85.156 (82.923)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.5142 (0.5003)	Prec@1 81.641 (82.898)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.4802 (0.5042)	Prec@1 83.203 (82.686)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.5161 (0.5066)	Prec@1 82.031 (82.605)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.5259 (0.5089)	Prec@1 81.641 (82.580)	
Total train loss: 0.5092

Train time: 20.607391595840454
 * Prec@1 81.830 Prec@5 98.900 Loss 0.5532
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.94881534576416

Epoch: [6][38/196]	LR: 0.01	Loss 0.5957 (0.4955)	Prec@1 79.297 (82.933)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.7349 (0.4935)	Prec@1 77.734 (83.053)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.4888 (0.4964)	Prec@1 82.031 (82.919)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.4993 (0.4962)	Prec@1 83.594 (82.948)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.4966 (0.4994)	Prec@1 83.594 (82.837)	
Total train loss: 0.4994

Train time: 20.53897738456726
 * Prec@1 81.550 Prec@5 98.800 Loss 0.5615
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.897207498550415

Epoch: [7][38/196]	LR: 0.01	Loss 0.5742 (0.4818)	Prec@1 83.203 (83.534)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.5298 (0.4917)	Prec@1 80.859 (83.193)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.4556 (0.4914)	Prec@1 87.109 (83.310)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.4854 (0.4916)	Prec@1 81.250 (83.231)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.4897 (0.4909)	Prec@1 80.859 (83.195)	
Total train loss: 0.4909

Train time: 20.968994617462158
 * Prec@1 81.700 Prec@5 98.880 Loss 0.5498
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.069742918014526

Epoch: [8][38/196]	LR: 0.001	Loss 0.4670 (0.4921)	Prec@1 84.375 (83.163)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.5195 (0.4948)	Prec@1 81.641 (83.158)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.5205 (0.4888)	Prec@1 82.422 (83.373)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.5479 (0.4890)	Prec@1 80.078 (83.318)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.6064 (0.4887)	Prec@1 80.078 (83.305)	
Total train loss: 0.4890

Train time: 19.792945623397827
 * Prec@1 82.110 Prec@5 98.840 Loss 0.5425
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.61780071258545

Epoch: [9][38/196]	LR: 0.001	Loss 0.5112 (0.4836)	Prec@1 81.641 (83.604)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.5273 (0.4920)	Prec@1 82.031 (83.183)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.4353 (0.4901)	Prec@1 84.375 (83.257)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.5352 (0.4892)	Prec@1 81.641 (83.276)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.5068 (0.4873)	Prec@1 82.031 (83.343)	
Total train loss: 0.4875

Train time: 19.290552854537964
 * Prec@1 82.180 Prec@5 98.850 Loss 0.5396
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.361832857131958

Epoch: [10][38/196]	LR: 0.001	Loss 0.6187 (0.5132)	Prec@1 80.859 (82.362)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.4834 (0.4952)	Prec@1 84.766 (82.883)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.3730 (0.4901)	Prec@1 87.109 (83.073)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.5229 (0.4865)	Prec@1 80.859 (83.243)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.4612 (0.4865)	Prec@1 82.031 (83.245)	
Total train loss: 0.4865

Train time: 18.602714776992798
 * Prec@1 82.150 Prec@5 98.850 Loss 0.5420
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.294153690338135

Epoch: [11][38/196]	LR: 0.001	Loss 0.4187 (0.4776)	Prec@1 85.938 (83.804)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.4265 (0.4864)	Prec@1 86.719 (83.313)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.5327 (0.4911)	Prec@1 83.984 (83.260)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.4753 (0.4871)	Prec@1 81.250 (83.356)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.3999 (0.4859)	Prec@1 85.938 (83.281)	
Total train loss: 0.4862

Train time: 18.30054020881653
 * Prec@1 81.970 Prec@5 98.880 Loss 0.5415
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.236271858215332

Epoch: [12][38/196]	LR: 0.001	Loss 0.6104 (0.4938)	Prec@1 80.859 (82.913)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.4431 (0.4919)	Prec@1 83.594 (83.153)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.5474 (0.4918)	Prec@1 81.641 (83.110)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.5254 (0.4902)	Prec@1 82.812 (83.163)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.4438 (0.4879)	Prec@1 83.984 (83.265)	
Total train loss: 0.4879

Train time: 17.284127712249756
 * Prec@1 82.080 Prec@5 98.870 Loss 0.5396
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 21.087446212768555

Epoch: [13][38/196]	LR: 0.001	Loss 0.4829 (0.4888)	Prec@1 84.375 (83.343)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.4446 (0.4920)	Prec@1 84.375 (83.158)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.4150 (0.4879)	Prec@1 85.156 (83.243)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.4443 (0.4850)	Prec@1 85.938 (83.316)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.5000 (0.4877)	Prec@1 82.031 (83.261)	
Total train loss: 0.4876

Train time: 19.96595048904419
 * Prec@1 82.100 Prec@5 98.820 Loss 0.5439
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.291720867156982

Epoch: [14][38/196]	LR: 0.001	Loss 0.4673 (0.4760)	Prec@1 83.984 (83.834)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.4871 (0.4821)	Prec@1 83.203 (83.594)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.4985 (0.4888)	Prec@1 83.594 (83.410)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.5405 (0.4882)	Prec@1 82.422 (83.281)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.4763 (0.4855)	Prec@1 82.031 (83.417)	
Total train loss: 0.4856

Train time: 19.605023860931396
 * Prec@1 82.240 Prec@5 98.880 Loss 0.5415
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.445250749588013

Epoch: [15][38/196]	LR: 0.001	Loss 0.4614 (0.4870)	Prec@1 83.203 (83.273)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.4221 (0.4889)	Prec@1 84.766 (83.293)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.5083 (0.4856)	Prec@1 82.422 (83.390)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.5063 (0.4854)	Prec@1 82.422 (83.426)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.4387 (0.4848)	Prec@1 83.594 (83.446)	
Total train loss: 0.4850

Train time: 18.90041732788086
 * Prec@1 82.170 Prec@5 98.840 Loss 0.5400
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.876713514328003

Epoch: [16][38/196]	LR: 0.0001	Loss 0.4341 (0.4929)	Prec@1 85.156 (83.173)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.5278 (0.4879)	Prec@1 82.812 (83.263)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.4600 (0.4883)	Prec@1 83.984 (83.103)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.5991 (0.4847)	Prec@1 80.078 (83.318)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.4243 (0.4855)	Prec@1 86.328 (83.241)	
Total train loss: 0.4856

Train time: 18.484428644180298
 * Prec@1 82.110 Prec@5 98.900 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.21422815322876

Epoch: [17][38/196]	LR: 0.0001	Loss 0.4514 (0.4904)	Prec@1 83.203 (83.313)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.4326 (0.4952)	Prec@1 83.984 (83.053)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.4873 (0.4882)	Prec@1 82.422 (83.237)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.4934 (0.4884)	Prec@1 81.641 (83.196)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.5151 (0.4868)	Prec@1 80.078 (83.241)	
Total train loss: 0.4870

Train time: 18.43398880958557
 * Prec@1 82.180 Prec@5 98.880 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.683822870254517

Epoch: [18][38/196]	LR: 0.0001	Loss 0.5088 (0.4871)	Prec@1 81.641 (83.113)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.4607 (0.4904)	Prec@1 82.422 (82.908)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.4524 (0.4840)	Prec@1 83.203 (83.223)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.4658 (0.4884)	Prec@1 85.156 (83.110)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.4905 (0.4869)	Prec@1 83.984 (83.207)	
Total train loss: 0.4872

Train time: 19.381715059280396
 * Prec@1 82.070 Prec@5 98.870 Loss 0.5430
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.324628829956055

Epoch: [19][38/196]	LR: 0.0001	Loss 0.4636 (0.4770)	Prec@1 85.156 (83.784)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.5322 (0.4847)	Prec@1 81.250 (83.373)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.4958 (0.4854)	Prec@1 82.031 (83.350)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.5898 (0.4885)	Prec@1 81.250 (83.241)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.5088 (0.4858)	Prec@1 84.375 (83.265)	
Total train loss: 0.4859

Train time: 17.854106664657593
 * Prec@1 82.280 Prec@5 98.830 Loss 0.5420
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 21.833532571792603

Epoch: [20][38/196]	LR: 0.0001	Loss 0.4456 (0.4870)	Prec@1 84.766 (83.323)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.4568 (0.4888)	Prec@1 84.766 (83.348)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.5269 (0.4909)	Prec@1 79.297 (83.086)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.4399 (0.4884)	Prec@1 84.375 (83.136)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.5430 (0.4863)	Prec@1 82.812 (83.247)	
Total train loss: 0.4865

Train time: 19.35325813293457
 * Prec@1 82.160 Prec@5 98.890 Loss 0.5439
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.72188711166382

Epoch: [21][38/196]	LR: 0.0001	Loss 0.5078 (0.4761)	Prec@1 84.375 (83.884)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.5952 (0.4824)	Prec@1 81.641 (83.644)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.5586 (0.4889)	Prec@1 83.984 (83.467)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.4690 (0.4865)	Prec@1 85.156 (83.398)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.5234 (0.4878)	Prec@1 81.641 (83.267)	
Total train loss: 0.4879

Train time: 19.12178349494934
 * Prec@1 82.170 Prec@5 98.890 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.97687292098999

Epoch: [22][38/196]	LR: 0.0001	Loss 0.5742 (0.4851)	Prec@1 80.859 (83.554)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.4673 (0.4826)	Prec@1 83.984 (83.624)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.5132 (0.4864)	Prec@1 82.031 (83.407)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.4797 (0.4850)	Prec@1 81.250 (83.361)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.5552 (0.4872)	Prec@1 81.641 (83.267)	
Total train loss: 0.4873

Train time: 19.344562768936157
 * Prec@1 82.070 Prec@5 98.890 Loss 0.5386
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.716342449188232

Epoch: [23][38/196]	LR: 0.0001	Loss 0.4348 (0.4831)	Prec@1 84.766 (83.494)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.5137 (0.4815)	Prec@1 84.375 (83.464)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.5195 (0.4868)	Prec@1 81.641 (83.260)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.4438 (0.4879)	Prec@1 86.719 (83.231)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.6387 (0.4885)	Prec@1 75.781 (83.191)	
Total train loss: 0.4886

Train time: 18.690412044525146
 * Prec@1 82.230 Prec@5 98.860 Loss 0.5415
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.466426849365234

Epoch: [24][38/196]	LR: 1e-05	Loss 0.5938 (0.4843)	Prec@1 80.859 (83.063)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.4358 (0.4829)	Prec@1 85.938 (83.143)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.5264 (0.4868)	Prec@1 82.422 (83.210)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.4341 (0.4852)	Prec@1 85.156 (83.258)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.4824 (0.4859)	Prec@1 84.375 (83.289)	
Total train loss: 0.4861

Train time: 19.209394454956055
 * Prec@1 82.240 Prec@5 98.840 Loss 0.5400
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.9644718170166

Epoch: [25][38/196]	LR: 1e-05	Loss 0.4912 (0.4854)	Prec@1 80.469 (83.113)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.4526 (0.4813)	Prec@1 83.984 (83.388)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.4661 (0.4868)	Prec@1 85.156 (83.186)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.4402 (0.4873)	Prec@1 85.547 (83.226)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.3403 (0.4859)	Prec@1 87.109 (83.299)	
Total train loss: 0.4858

Train time: 18.952431201934814
 * Prec@1 82.180 Prec@5 98.880 Loss 0.5396
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.772037506103516

Epoch: [26][38/196]	LR: 1e-05	Loss 0.5898 (0.4874)	Prec@1 78.906 (83.313)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.5806 (0.4857)	Prec@1 81.641 (83.378)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.6079 (0.4830)	Prec@1 78.906 (83.420)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.4465 (0.4885)	Prec@1 83.594 (83.226)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.3760 (0.4857)	Prec@1 85.938 (83.347)	
Total train loss: 0.4858

Train time: 20.727518320083618
 * Prec@1 82.020 Prec@5 98.890 Loss 0.5391
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.683366298675537

Epoch: [27][38/196]	LR: 1e-05	Loss 0.3638 (0.4909)	Prec@1 89.062 (83.383)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.4280 (0.4825)	Prec@1 86.328 (83.609)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.5552 (0.4838)	Prec@1 80.078 (83.447)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.4929 (0.4849)	Prec@1 84.766 (83.403)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.4944 (0.4861)	Prec@1 83.984 (83.273)	
Total train loss: 0.4861

Train time: 19.027047395706177
 * Prec@1 82.260 Prec@5 98.850 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.04599905014038

Epoch: [28][38/196]	LR: 1e-05	Loss 0.4106 (0.4772)	Prec@1 84.766 (83.514)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.5234 (0.4859)	Prec@1 82.422 (83.198)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.3706 (0.4845)	Prec@1 88.281 (83.440)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.5166 (0.4865)	Prec@1 82.031 (83.388)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.5601 (0.4866)	Prec@1 80.469 (83.339)	
Total train loss: 0.4865

Train time: 18.54366898536682
 * Prec@1 82.170 Prec@5 98.860 Loss 0.5405
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 21.932810068130493

Epoch: [29][38/196]	LR: 1e-05	Loss 0.5640 (0.4843)	Prec@1 80.469 (83.704)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.4570 (0.4784)	Prec@1 83.594 (83.689)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.5312 (0.4810)	Prec@1 82.031 (83.544)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.4937 (0.4824)	Prec@1 83.203 (83.459)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.5449 (0.4864)	Prec@1 82.031 (83.335)	
Total train loss: 0.4865

Train time: 17.877925634384155
 * Prec@1 82.200 Prec@5 98.800 Loss 0.5405
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 21.229861736297607

Epoch: [30][38/196]	LR: 1e-05	Loss 0.4961 (0.4735)	Prec@1 82.031 (83.754)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.4724 (0.4814)	Prec@1 81.250 (83.318)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.6016 (0.4848)	Prec@1 80.469 (83.220)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.5352 (0.4856)	Prec@1 80.078 (83.213)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.6128 (0.4865)	Prec@1 80.859 (83.219)	
Total train loss: 0.4866

Train time: 19.504767894744873
 * Prec@1 82.010 Prec@5 98.850 Loss 0.5420
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.167979955673218

Epoch: [31][38/196]	LR: 1e-05	Loss 0.4534 (0.4891)	Prec@1 84.766 (83.213)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.5605 (0.4871)	Prec@1 80.078 (83.313)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.3894 (0.4854)	Prec@1 88.281 (83.390)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.4797 (0.4847)	Prec@1 81.641 (83.413)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.5308 (0.4862)	Prec@1 83.203 (83.351)	
Total train loss: 0.4863

Train time: 17.88414716720581
 * Prec@1 82.110 Prec@5 98.860 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 21.65246844291687

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.5361 (0.4828)	Prec@1 80.859 (83.133)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.5156 (0.4808)	Prec@1 82.422 (83.338)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.5234 (0.4836)	Prec@1 82.422 (83.327)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.4189 (0.4896)	Prec@1 87.500 (83.128)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.5913 (0.4867)	Prec@1 78.516 (83.241)	
Total train loss: 0.4865

Train time: 19.2734956741333
 * Prec@1 82.070 Prec@5 98.850 Loss 0.5405
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.17568016052246

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.4822 (0.4781)	Prec@1 82.031 (83.474)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.4851 (0.4819)	Prec@1 84.766 (83.318)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.4814 (0.4831)	Prec@1 84.375 (83.357)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.4800 (0.4846)	Prec@1 84.375 (83.368)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.4026 (0.4862)	Prec@1 85.938 (83.229)	
Total train loss: 0.4863

Train time: 19.29934287071228
 * Prec@1 81.980 Prec@5 98.880 Loss 0.5376
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 26.15078043937683

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.5361 (0.4863)	Prec@1 81.250 (83.233)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.4641 (0.4850)	Prec@1 83.203 (83.153)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.4634 (0.4841)	Prec@1 83.203 (83.317)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.5171 (0.4847)	Prec@1 83.203 (83.311)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.4768 (0.4860)	Prec@1 82.422 (83.257)	
Total train loss: 0.4860

Train time: 20.128217220306396
 * Prec@1 82.110 Prec@5 98.860 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 24.011555194854736

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.4832)	Prec@1 80.469 (83.423)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.4641 (0.4818)	Prec@1 84.375 (83.408)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.5425 (0.4878)	Prec@1 80.859 (83.257)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.4736 (0.4899)	Prec@1 84.766 (83.178)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.5415 (0.4862)	Prec@1 80.078 (83.253)	
Total train loss: 0.4862

Train time: 18.385091066360474
 * Prec@1 82.030 Prec@5 98.860 Loss 0.5410
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.14972233772278

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.5518 (0.4833)	Prec@1 80.469 (83.744)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.3706 (0.4800)	Prec@1 85.547 (83.719)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.4097 (0.4845)	Prec@1 86.328 (83.460)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.5996 (0.4859)	Prec@1 78.516 (83.318)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.5415 (0.4871)	Prec@1 81.250 (83.291)	
Total train loss: 0.4871

Train time: 18.831820487976074
 * Prec@1 82.160 Prec@5 98.840 Loss 0.5415
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.113640308380127

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.3806 (0.4870)	Prec@1 87.109 (82.883)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.5156 (0.4929)	Prec@1 80.859 (82.918)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.4221 (0.4906)	Prec@1 83.984 (83.026)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.4128 (0.4886)	Prec@1 84.766 (83.166)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.4067 (0.4852)	Prec@1 86.328 (83.313)	
Total train loss: 0.4854

Train time: 20.756350994110107
 * Prec@1 82.260 Prec@5 98.890 Loss 0.5400
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 25.18441939353943

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.5342 (0.4856)	Prec@1 83.594 (82.923)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.4421 (0.4823)	Prec@1 84.375 (83.418)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.4734 (0.4830)	Prec@1 82.812 (83.333)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.4897 (0.4858)	Prec@1 83.203 (83.283)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.4870)	Prec@1 79.688 (83.213)	
Total train loss: 0.4868

Train time: 19.408286809921265
 * Prec@1 82.190 Prec@5 98.920 Loss 0.5396
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 23.46026873588562

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.4766 (0.4885)	Prec@1 82.812 (83.263)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.3726 (0.4823)	Prec@1 87.500 (83.243)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.5552 (0.4880)	Prec@1 82.422 (83.226)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.5059 (0.4894)	Prec@1 81.641 (83.078)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.4397 (0.4874)	Prec@1 83.594 (83.163)	
Total train loss: 0.4873

Train time: 18.445512533187866
 * Prec@1 82.140 Prec@5 98.940 Loss 0.5386
Best acc: 82.340
--------------------------------------------------------------------------------
Test time: 22.91992735862732


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 20.600 Prec@5 77.920 Loss 3.8691
Pre-trained Prec@1 with 15 layers frozen: 20.600000381469727 	 Loss: 3.869140625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 1.2012 (1.5920)	Prec@1 67.969 (63.502)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.9180 (1.3586)	Prec@1 71.094 (65.530)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.8174 (1.2576)	Prec@1 75.000 (66.216)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.9302 (1.1881)	Prec@1 69.922 (66.649)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.9404 (1.1452)	Prec@1 71.875 (66.965)	
Total train loss: 1.1445

Train time: 103.69466638565063
 * Prec@1 68.730 Prec@5 96.070 Loss 0.9697
Best acc: 68.730
--------------------------------------------------------------------------------
Test time: 108.45289373397827

Epoch: [1][38/196]	LR: 0.01	Loss 0.9780 (0.9326)	Prec@1 68.359 (68.580)	
Epoch: [1][77/196]	LR: 0.01	Loss 1.0205 (0.9359)	Prec@1 66.016 (68.910)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.9614 (0.9481)	Prec@1 66.016 (68.546)	
Epoch: [1][155/196]	LR: 0.01	Loss 1.0186 (0.9496)	Prec@1 64.453 (68.394)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.9712 (0.9565)	Prec@1 65.625 (68.117)	
Total train loss: 0.9566

Train time: 17.94210457801819
 * Prec@1 68.710 Prec@5 97.040 Loss 0.9263
Best acc: 68.730
--------------------------------------------------------------------------------
Test time: 22.4304416179657

Epoch: [2][38/196]	LR: 0.01	Loss 0.9521 (0.9346)	Prec@1 67.969 (68.860)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.8965 (0.9335)	Prec@1 69.531 (68.695)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.8208 (0.9266)	Prec@1 71.484 (68.833)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.8467 (0.9193)	Prec@1 74.609 (69.108)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.9824 (0.9200)	Prec@1 64.844 (68.958)	
Total train loss: 0.9202

Train time: 18.0157790184021
 * Prec@1 70.490 Prec@5 97.140 Loss 0.8726
Best acc: 70.490
--------------------------------------------------------------------------------
Test time: 21.858471632003784

Epoch: [3][38/196]	LR: 0.01	Loss 0.8535 (0.8700)	Prec@1 71.094 (70.333)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.8667 (0.8798)	Prec@1 68.359 (69.922)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.9106 (0.8845)	Prec@1 66.016 (69.798)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.8389 (0.8830)	Prec@1 73.438 (70.022)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.9570 (0.8809)	Prec@1 68.359 (70.028)	
Total train loss: 0.8811

Train time: 18.63402032852173
 * Prec@1 71.060 Prec@5 97.240 Loss 0.8535
Best acc: 71.060
--------------------------------------------------------------------------------
Test time: 25.65074110031128

Epoch: [4][38/196]	LR: 0.01	Loss 0.8843 (0.8607)	Prec@1 71.875 (70.903)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.8667 (0.8570)	Prec@1 73.438 (71.114)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.8745 (0.8557)	Prec@1 67.578 (71.231)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.8862 (0.8586)	Prec@1 69.531 (70.959)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.7300 (0.8536)	Prec@1 76.172 (70.919)	
Total train loss: 0.8535

Train time: 17.813944101333618
 * Prec@1 71.880 Prec@5 97.420 Loss 0.8306
Best acc: 71.880
--------------------------------------------------------------------------------
Test time: 22.141024351119995

Epoch: [5][38/196]	LR: 0.01	Loss 0.7490 (0.8429)	Prec@1 73.828 (71.394)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.8604 (0.8503)	Prec@1 74.219 (71.054)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.8228 (0.8462)	Prec@1 72.656 (71.107)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.8018 (0.8425)	Prec@1 71.875 (71.352)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.8267 (0.8436)	Prec@1 70.312 (71.350)	
Total train loss: 0.8435

Train time: 18.370039701461792
 * Prec@1 71.680 Prec@5 97.410 Loss 0.8345
Best acc: 71.880
--------------------------------------------------------------------------------
Test time: 22.869203567504883

Epoch: [6][38/196]	LR: 0.01	Loss 0.8755 (0.8493)	Prec@1 71.484 (70.593)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.7695 (0.8309)	Prec@1 72.656 (71.509)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.7383 (0.8345)	Prec@1 75.781 (71.645)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.8428 (0.8327)	Prec@1 71.484 (71.745)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.8301 (0.8368)	Prec@1 72.266 (71.663)	
Total train loss: 0.8367

Train time: 17.09428358078003
 * Prec@1 72.230 Prec@5 97.310 Loss 0.8169
Best acc: 72.230
--------------------------------------------------------------------------------
Test time: 23.12234902381897

Epoch: [7][38/196]	LR: 0.01	Loss 0.8130 (0.8497)	Prec@1 69.922 (71.214)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.8159 (0.8276)	Prec@1 73.828 (72.080)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.7808 (0.8276)	Prec@1 71.484 (72.095)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.8716 (0.8268)	Prec@1 69.922 (71.998)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.8364 (0.8246)	Prec@1 71.875 (72.075)	
Total train loss: 0.8248

Train time: 18.48767375946045
 * Prec@1 72.860 Prec@5 97.430 Loss 0.8037
Best acc: 72.860
--------------------------------------------------------------------------------
Test time: 22.942331314086914

Epoch: [8][38/196]	LR: 0.001	Loss 0.8711 (0.8408)	Prec@1 73.828 (71.655)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.8037 (0.8211)	Prec@1 69.531 (72.276)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.7861 (0.8175)	Prec@1 72.266 (72.289)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.8452 (0.8169)	Prec@1 71.484 (72.328)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.9282 (0.8175)	Prec@1 70.312 (72.310)	
Total train loss: 0.8176

Train time: 19.597700595855713
 * Prec@1 72.910 Prec@5 97.510 Loss 0.7993
Best acc: 72.910
--------------------------------------------------------------------------------
Test time: 23.418737173080444

Epoch: [9][38/196]	LR: 0.001	Loss 0.8535 (0.8172)	Prec@1 67.969 (71.875)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.7827 (0.8097)	Prec@1 70.703 (72.451)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.8701 (0.8159)	Prec@1 68.750 (72.392)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.7612 (0.8174)	Prec@1 74.609 (72.423)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.7930 (0.8158)	Prec@1 70.312 (72.454)	
Total train loss: 0.8159

Train time: 16.601207733154297
 * Prec@1 72.850 Prec@5 97.480 Loss 0.7998
Best acc: 72.910
--------------------------------------------------------------------------------
Test time: 20.79711675643921

Epoch: [10][38/196]	LR: 0.001	Loss 0.9370 (0.8146)	Prec@1 68.750 (72.506)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.7388 (0.8077)	Prec@1 74.609 (72.446)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.9307 (0.8160)	Prec@1 66.797 (72.376)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.8369 (0.8147)	Prec@1 67.969 (72.406)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.7910 (0.8157)	Prec@1 73.047 (72.428)	
Total train loss: 0.8153

Train time: 19.58857822418213
 * Prec@1 72.950 Prec@5 97.490 Loss 0.8003
Best acc: 72.950
--------------------------------------------------------------------------------
Test time: 24.742570161819458

Epoch: [11][38/196]	LR: 0.001	Loss 0.7861 (0.8065)	Prec@1 75.781 (73.357)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.7925 (0.8153)	Prec@1 71.875 (72.726)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.7676 (0.8145)	Prec@1 74.609 (72.436)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.7285 (0.8138)	Prec@1 76.953 (72.311)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.8228 (0.8151)	Prec@1 72.266 (72.318)	
Total train loss: 0.8148

Train time: 19.131077527999878
 * Prec@1 72.980 Prec@5 97.530 Loss 0.7974
Best acc: 72.980
--------------------------------------------------------------------------------
Test time: 23.216157913208008

Epoch: [12][38/196]	LR: 0.001	Loss 0.8584 (0.8273)	Prec@1 72.656 (72.326)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.7700 (0.8157)	Prec@1 72.266 (72.201)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.9126 (0.8129)	Prec@1 70.703 (72.423)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.7603 (0.8132)	Prec@1 73.828 (72.451)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.7744 (0.8145)	Prec@1 71.875 (72.438)	
Total train loss: 0.8143

Train time: 18.457857847213745
 * Prec@1 73.020 Prec@5 97.530 Loss 0.7964
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 24.686028718948364

Epoch: [13][38/196]	LR: 0.001	Loss 0.8633 (0.8025)	Prec@1 73.047 (72.526)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.8413 (0.8142)	Prec@1 66.016 (72.446)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.8535 (0.8146)	Prec@1 70.312 (72.229)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.8618 (0.8133)	Prec@1 70.312 (72.351)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.7866 (0.8155)	Prec@1 73.047 (72.302)	
Total train loss: 0.8154

Train time: 17.73792028427124
 * Prec@1 72.860 Prec@5 97.590 Loss 0.7954
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 21.569283723831177

Epoch: [14][38/196]	LR: 0.001	Loss 0.8926 (0.8105)	Prec@1 67.969 (72.386)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.7783 (0.8115)	Prec@1 73.438 (72.396)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.7549 (0.8099)	Prec@1 75.391 (72.503)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.8203 (0.8112)	Prec@1 74.609 (72.574)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.7549 (0.8140)	Prec@1 77.344 (72.496)	
Total train loss: 0.8139

Train time: 19.559406757354736
 * Prec@1 72.950 Prec@5 97.520 Loss 0.7988
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 24.149542808532715

Epoch: [15][38/196]	LR: 0.001	Loss 0.7842 (0.8047)	Prec@1 76.562 (72.516)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.7812 (0.8193)	Prec@1 71.484 (72.241)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.7964 (0.8117)	Prec@1 75.000 (72.569)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.8052 (0.8111)	Prec@1 70.703 (72.526)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.9297 (0.8125)	Prec@1 67.578 (72.428)	
Total train loss: 0.8129

Train time: 19.13798713684082
 * Prec@1 72.990 Prec@5 97.550 Loss 0.7974
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 24.533828735351562

Epoch: [16][38/196]	LR: 0.0001	Loss 0.7856 (0.8229)	Prec@1 76.172 (72.546)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.8457 (0.8225)	Prec@1 72.266 (72.446)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.7944 (0.8189)	Prec@1 72.266 (72.710)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.7578 (0.8148)	Prec@1 73.438 (72.719)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.7729 (0.8129)	Prec@1 74.219 (72.688)	
Total train loss: 0.8130

Train time: 18.74574613571167
 * Prec@1 72.870 Prec@5 97.460 Loss 0.7998
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 22.830848932266235

Epoch: [17][38/196]	LR: 0.0001	Loss 0.9336 (0.8260)	Prec@1 69.141 (72.085)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.8491 (0.8192)	Prec@1 69.922 (72.381)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.8540 (0.8195)	Prec@1 72.266 (72.262)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.7466 (0.8141)	Prec@1 73.828 (72.476)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.7573 (0.8145)	Prec@1 75.781 (72.558)	
Total train loss: 0.8144

Train time: 18.289321422576904
 * Prec@1 72.940 Prec@5 97.550 Loss 0.7954
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 21.809643745422363

Epoch: [18][38/196]	LR: 0.0001	Loss 0.8442 (0.7958)	Prec@1 72.656 (73.327)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.9165 (0.8194)	Prec@1 70.312 (72.346)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.7192 (0.8164)	Prec@1 76.562 (72.443)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.8262 (0.8109)	Prec@1 71.094 (72.609)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.8486 (0.8143)	Prec@1 67.969 (72.456)	
Total train loss: 0.8142

Train time: 17.986148357391357
 * Prec@1 72.980 Prec@5 97.550 Loss 0.8003
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 25.42753291130066

Epoch: [19][38/196]	LR: 0.0001	Loss 0.8267 (0.8095)	Prec@1 72.266 (72.716)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.7256 (0.8093)	Prec@1 78.125 (72.706)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.8833 (0.8070)	Prec@1 72.656 (72.786)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.8096 (0.8115)	Prec@1 69.922 (72.601)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.6987 (0.8132)	Prec@1 77.344 (72.562)	
Total train loss: 0.8133

Train time: 19.140860319137573
 * Prec@1 73.010 Prec@5 97.530 Loss 0.7998
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 22.85293984413147

Epoch: [20][38/196]	LR: 0.0001	Loss 0.7520 (0.8272)	Prec@1 72.266 (72.175)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.7876 (0.8107)	Prec@1 75.000 (72.676)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.7856 (0.8099)	Prec@1 74.219 (72.743)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.8828 (0.8125)	Prec@1 68.359 (72.519)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.7881 (0.8140)	Prec@1 72.656 (72.394)	
Total train loss: 0.8142

Train time: 19.07946252822876
 * Prec@1 72.770 Prec@5 97.380 Loss 0.8018
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 24.012930870056152

Epoch: [21][38/196]	LR: 0.0001	Loss 0.8794 (0.8129)	Prec@1 70.312 (72.817)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.7622 (0.8102)	Prec@1 75.000 (72.801)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.8403 (0.8165)	Prec@1 70.703 (72.459)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.8018 (0.8145)	Prec@1 71.094 (72.461)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.8633 (0.8141)	Prec@1 69.922 (72.452)	
Total train loss: 0.8139

Train time: 18.366499185562134
 * Prec@1 72.900 Prec@5 97.400 Loss 0.8003
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 22.503983974456787

Epoch: [22][38/196]	LR: 0.0001	Loss 0.7617 (0.8018)	Prec@1 74.219 (72.806)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.7710 (0.8153)	Prec@1 73.438 (72.346)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.7690 (0.8159)	Prec@1 73.047 (72.236)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.7441 (0.8149)	Prec@1 73.047 (72.371)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.8403 (0.8128)	Prec@1 74.219 (72.504)	
Total train loss: 0.8127

Train time: 19.110026359558105
 * Prec@1 72.910 Prec@5 97.440 Loss 0.8003
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 22.851842641830444

Epoch: [23][38/196]	LR: 0.0001	Loss 0.7437 (0.8376)	Prec@1 74.219 (71.965)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.8394 (0.8287)	Prec@1 70.703 (72.130)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.8779 (0.8171)	Prec@1 70.312 (72.376)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.7817 (0.8126)	Prec@1 73.438 (72.544)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.8413 (0.8138)	Prec@1 72.656 (72.480)	
Total train loss: 0.8137

Train time: 17.730304479599
 * Prec@1 72.950 Prec@5 97.490 Loss 0.8003
Best acc: 73.020
--------------------------------------------------------------------------------
Test time: 21.444659948349

Epoch: [24][38/196]	LR: 1e-05	Loss 0.8340 (0.8169)	Prec@1 73.047 (72.786)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.7412 (0.8178)	Prec@1 72.266 (72.326)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.8149 (0.8199)	Prec@1 73.047 (72.329)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.8604 (0.8146)	Prec@1 71.484 (72.493)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.8252 (0.8135)	Prec@1 71.094 (72.470)	
Total train loss: 0.8135

Train time: 17.741827964782715
 * Prec@1 73.030 Prec@5 97.530 Loss 0.7954
Best acc: 73.030
--------------------------------------------------------------------------------
Test time: 21.545029640197754

Epoch: [25][38/196]	LR: 1e-05	Loss 0.8574 (0.8093)	Prec@1 71.484 (72.476)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.8560 (0.8088)	Prec@1 72.656 (72.571)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.7866 (0.8144)	Prec@1 75.000 (72.389)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.7031 (0.8115)	Prec@1 73.438 (72.448)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.8740 (0.8134)	Prec@1 70.703 (72.428)	
Total train loss: 0.8131

Train time: 18.625685453414917
 * Prec@1 72.960 Prec@5 97.470 Loss 0.7988
Best acc: 73.030
--------------------------------------------------------------------------------
Test time: 22.122554540634155

Epoch: [26][38/196]	LR: 1e-05	Loss 0.8267 (0.8194)	Prec@1 71.875 (71.885)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.8472 (0.8196)	Prec@1 73.438 (72.281)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.8389 (0.8145)	Prec@1 71.094 (72.426)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.9629 (0.8095)	Prec@1 70.312 (72.594)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.8428 (0.8144)	Prec@1 70.703 (72.428)	
Total train loss: 0.8142

Train time: 18.586777448654175
 * Prec@1 72.980 Prec@5 97.510 Loss 0.7974
Best acc: 73.030
--------------------------------------------------------------------------------
Test time: 22.29808211326599

Epoch: [27][38/196]	LR: 1e-05	Loss 0.8140 (0.8216)	Prec@1 71.484 (72.145)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.7002 (0.8108)	Prec@1 76.562 (72.606)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.8423 (0.8098)	Prec@1 71.484 (72.630)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.7925 (0.8108)	Prec@1 73.438 (72.421)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.7720 (0.8129)	Prec@1 74.219 (72.368)	
Total train loss: 0.8129

Train time: 18.009917736053467
 * Prec@1 73.060 Prec@5 97.490 Loss 0.7993
Best acc: 73.060
--------------------------------------------------------------------------------
Test time: 21.61974859237671

Epoch: [28][38/196]	LR: 1e-05	Loss 0.8057 (0.8137)	Prec@1 73.438 (72.937)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.8384 (0.8068)	Prec@1 68.750 (72.746)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.8379 (0.8104)	Prec@1 73.438 (72.683)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.7769 (0.8108)	Prec@1 73.438 (72.529)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.8555 (0.8135)	Prec@1 71.094 (72.482)	
Total train loss: 0.8133

Train time: 18.278156995773315
 * Prec@1 72.910 Prec@5 97.420 Loss 0.7993
Best acc: 73.060
--------------------------------------------------------------------------------
Test time: 21.595652103424072

Epoch: [29][38/196]	LR: 1e-05	Loss 0.8628 (0.8065)	Prec@1 71.875 (72.977)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.7910 (0.8131)	Prec@1 71.484 (72.486)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.7915 (0.8135)	Prec@1 71.094 (72.413)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.8877 (0.8151)	Prec@1 69.922 (72.298)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.8770 (0.8120)	Prec@1 69.922 (72.432)	
Total train loss: 0.8123

Train time: 17.982813119888306
 * Prec@1 73.070 Prec@5 97.550 Loss 0.7974
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 21.875911235809326

Epoch: [30][38/196]	LR: 1e-05	Loss 0.8364 (0.8156)	Prec@1 70.312 (72.165)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.7622 (0.8112)	Prec@1 71.875 (72.571)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.9253 (0.8192)	Prec@1 69.141 (72.242)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.8350 (0.8159)	Prec@1 69.922 (72.318)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.7573 (0.8125)	Prec@1 73.438 (72.516)	
Total train loss: 0.8127

Train time: 18.10513472557068
 * Prec@1 72.940 Prec@5 97.420 Loss 0.8018
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 21.897167921066284

Epoch: [31][38/196]	LR: 1e-05	Loss 0.6968 (0.8085)	Prec@1 75.391 (72.185)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.8232 (0.8085)	Prec@1 68.750 (72.376)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.7847 (0.8125)	Prec@1 73.047 (72.352)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.8169 (0.8115)	Prec@1 71.484 (72.466)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.7520 (0.8132)	Prec@1 76.953 (72.380)	
Total train loss: 0.8133

Train time: 18.226526737213135
 * Prec@1 72.850 Prec@5 97.420 Loss 0.8032
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 21.564762353897095

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.8389 (0.8302)	Prec@1 72.266 (71.925)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.8330 (0.8288)	Prec@1 69.922 (71.935)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.8843 (0.8233)	Prec@1 73.047 (72.115)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.8711 (0.8208)	Prec@1 67.578 (72.228)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.7910 (0.8138)	Prec@1 71.875 (72.550)	
Total train loss: 0.8140

Train time: 18.04294180870056
 * Prec@1 72.960 Prec@5 97.500 Loss 0.8003
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 22.03258204460144

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.7686 (0.7954)	Prec@1 75.391 (73.097)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.9170 (0.8016)	Prec@1 67.969 (72.917)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.8081 (0.8109)	Prec@1 74.219 (72.586)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.7627 (0.8120)	Prec@1 73.047 (72.488)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.7559 (0.8134)	Prec@1 77.344 (72.402)	
Total train loss: 0.8131

Train time: 17.68814253807068
 * Prec@1 73.030 Prec@5 97.460 Loss 0.7964
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 21.48574995994568

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.8960 (0.8260)	Prec@1 71.875 (72.015)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.7876 (0.8090)	Prec@1 71.875 (72.721)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.7656 (0.8139)	Prec@1 76.172 (72.646)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.8838 (0.8120)	Prec@1 70.312 (72.639)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.8413 (0.8131)	Prec@1 72.656 (72.552)	
Total train loss: 0.8133

Train time: 19.145416736602783
 * Prec@1 72.930 Prec@5 97.400 Loss 0.8018
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 22.545021533966064

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.7783 (0.8345)	Prec@1 74.219 (71.575)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.8950 (0.8151)	Prec@1 70.312 (72.551)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.7524 (0.8120)	Prec@1 75.781 (72.573)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.8550 (0.8134)	Prec@1 71.875 (72.526)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.7168 (0.8117)	Prec@1 76.562 (72.542)	
Total train loss: 0.8116

Train time: 19.284242391586304
 * Prec@1 72.990 Prec@5 97.540 Loss 0.7959
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 23.129624366760254

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.8462 (0.8127)	Prec@1 72.656 (72.857)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.8628 (0.8099)	Prec@1 69.922 (72.656)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.8359 (0.8144)	Prec@1 72.266 (72.579)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.9341 (0.8137)	Prec@1 68.750 (72.554)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.7827 (0.8140)	Prec@1 73.828 (72.592)	
Total train loss: 0.8143

Train time: 16.873688459396362
 * Prec@1 72.980 Prec@5 97.490 Loss 0.7998
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 21.468659162521362

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.8145 (0.8168)	Prec@1 69.922 (72.196)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.8843 (0.8132)	Prec@1 70.703 (72.586)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.8975 (0.8103)	Prec@1 67.188 (72.693)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.8740 (0.8118)	Prec@1 72.266 (72.604)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.8086 (0.8133)	Prec@1 73.047 (72.542)	
Total train loss: 0.8138

Train time: 17.191726684570312
 * Prec@1 73.020 Prec@5 97.460 Loss 0.7979
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 20.779568433761597

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.8125 (0.8001)	Prec@1 70.312 (72.706)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.6968 (0.8061)	Prec@1 76.172 (72.556)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.9102 (0.8088)	Prec@1 69.922 (72.529)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.7598 (0.8102)	Prec@1 74.219 (72.554)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.7896 (0.8129)	Prec@1 73.438 (72.512)	
Total train loss: 0.8133

Train time: 18.859890699386597
 * Prec@1 72.980 Prec@5 97.520 Loss 0.7993
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 22.930103540420532

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.8706 (0.8140)	Prec@1 70.703 (72.897)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.7930 (0.8230)	Prec@1 74.609 (72.431)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.8433 (0.8198)	Prec@1 71.094 (72.406)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.8140 (0.8155)	Prec@1 74.219 (72.509)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.8364 (0.8143)	Prec@1 68.750 (72.522)	
Total train loss: 0.8143

Train time: 19.759926795959473
 * Prec@1 73.030 Prec@5 97.520 Loss 0.7969
Best acc: 73.070
--------------------------------------------------------------------------------
Test time: 22.108502864837646


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 20.140 Prec@5 77.060 Loss 3.8711
Pre-trained Prec@1 with 17 layers frozen: 20.139999389648438 	 Loss: 3.87109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 1.4287 (1.9329)	Prec@1 64.062 (56.460)	
Epoch: [0][77/196]	LR: 0.01	Loss 1.0938 (1.6380)	Prec@1 66.797 (59.811)	
Epoch: [0][116/196]	LR: 0.01	Loss 1.0029 (1.4965)	Prec@1 68.359 (61.415)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.9824 (1.4107)	Prec@1 69.141 (62.387)	
Epoch: [0][194/196]	LR: 0.01	Loss 1.2002 (1.3491)	Prec@1 67.578 (62.949)	
Total train loss: 1.3492

Train time: 130.48644185066223
 * Prec@1 67.330 Prec@5 96.200 Loss 1.1016
Best acc: 67.330
--------------------------------------------------------------------------------
Test time: 135.1177875995636

Epoch: [1][38/196]	LR: 0.01	Loss 0.8857 (1.0661)	Prec@1 68.750 (65.615)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.9873 (1.0668)	Prec@1 68.750 (65.550)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.9443 (1.0519)	Prec@1 70.312 (65.769)	
Epoch: [1][155/196]	LR: 0.01	Loss 1.0068 (1.0522)	Prec@1 67.188 (65.708)	
Epoch: [1][194/196]	LR: 0.01	Loss 1.2061 (1.0477)	Prec@1 62.109 (65.743)	
Total train loss: 1.0477

Train time: 21.044018507003784
 * Prec@1 67.970 Prec@5 96.520 Loss 1.0039
Best acc: 67.970
--------------------------------------------------------------------------------
Test time: 25.532528162002563

Epoch: [2][38/196]	LR: 0.01	Loss 1.0547 (1.0334)	Prec@1 69.922 (65.605)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.9395 (1.0221)	Prec@1 67.578 (66.001)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.0840 (1.0184)	Prec@1 63.672 (66.066)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.9624 (1.0175)	Prec@1 65.625 (65.941)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.9951 (1.0163)	Prec@1 66.016 (65.994)	
Total train loss: 1.0164

Train time: 18.13636612892151
 * Prec@1 68.310 Prec@5 96.590 Loss 0.9761
Best acc: 68.310
--------------------------------------------------------------------------------
Test time: 21.926764965057373

Epoch: [3][38/196]	LR: 0.01	Loss 1.0381 (0.9946)	Prec@1 63.672 (66.526)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.1230 (0.9994)	Prec@1 62.500 (66.541)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.1035 (1.0013)	Prec@1 63.672 (66.506)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.9922 (0.9996)	Prec@1 64.844 (66.526)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.9961 (1.0037)	Prec@1 65.625 (66.424)	
Total train loss: 1.0038

Train time: 16.816802978515625
 * Prec@1 68.140 Prec@5 96.650 Loss 0.9702
Best acc: 68.310
--------------------------------------------------------------------------------
Test time: 20.931649208068848

Epoch: [4][38/196]	LR: 0.01	Loss 1.1074 (0.9986)	Prec@1 62.109 (66.546)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.9629 (1.0011)	Prec@1 69.922 (66.426)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.0791 (0.9924)	Prec@1 61.719 (66.677)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.9756 (0.9947)	Prec@1 64.453 (66.612)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.8979 (0.9927)	Prec@1 69.141 (66.645)	
Total train loss: 0.9925

Train time: 19.04386329650879
 * Prec@1 68.630 Prec@5 96.750 Loss 0.9502
Best acc: 68.630
--------------------------------------------------------------------------------
Test time: 22.942035913467407

Epoch: [5][38/196]	LR: 0.01	Loss 0.8760 (0.9694)	Prec@1 73.438 (67.177)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.0967 (0.9789)	Prec@1 62.500 (66.922)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.8970 (0.9828)	Prec@1 68.750 (66.870)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.0059 (0.9857)	Prec@1 65.625 (66.719)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.9873 (0.9839)	Prec@1 67.578 (66.835)	
Total train loss: 0.9838

Train time: 16.751939296722412
 * Prec@1 68.670 Prec@5 96.790 Loss 0.9473
Best acc: 68.670
--------------------------------------------------------------------------------
Test time: 21.839844703674316

Epoch: [6][38/196]	LR: 0.01	Loss 0.9365 (0.9866)	Prec@1 66.406 (66.817)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.0137 (0.9804)	Prec@1 65.625 (66.967)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.0146 (0.9791)	Prec@1 65.625 (67.021)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.9473 (0.9751)	Prec@1 64.844 (67.152)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.1592 (0.9803)	Prec@1 59.375 (66.985)	
Total train loss: 0.9802

Train time: 16.687979459762573
 * Prec@1 68.510 Prec@5 96.760 Loss 0.9414
Best acc: 68.670
--------------------------------------------------------------------------------
Test time: 20.05730104446411

Epoch: [7][38/196]	LR: 0.01	Loss 0.9521 (0.9813)	Prec@1 65.625 (66.416)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.8735 (0.9818)	Prec@1 69.531 (66.567)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.9932 (0.9796)	Prec@1 68.750 (66.814)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.8408 (0.9778)	Prec@1 69.531 (66.802)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.0801 (0.9771)	Prec@1 64.062 (66.885)	
Total train loss: 0.9769

Train time: 16.50647497177124
 * Prec@1 68.760 Prec@5 96.600 Loss 0.9482
Best acc: 68.760
--------------------------------------------------------------------------------
Test time: 20.64383625984192

Epoch: [8][38/196]	LR: 0.001	Loss 0.9277 (0.9921)	Prec@1 71.875 (67.077)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.8730 (0.9878)	Prec@1 68.359 (66.762)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.0693 (0.9811)	Prec@1 64.062 (67.037)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.0273 (0.9777)	Prec@1 64.062 (66.980)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.9668 (0.9765)	Prec@1 67.188 (67.043)	
Total train loss: 0.9767

Train time: 17.799376249313354
 * Prec@1 68.780 Prec@5 96.630 Loss 0.9385
Best acc: 68.780
--------------------------------------------------------------------------------
Test time: 21.747308254241943

Epoch: [9][38/196]	LR: 0.001	Loss 1.1357 (0.9789)	Prec@1 64.453 (67.057)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.9614 (0.9771)	Prec@1 65.625 (67.203)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.9888 (0.9715)	Prec@1 67.969 (67.338)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.8599 (0.9716)	Prec@1 71.484 (67.253)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.0791 (0.9761)	Prec@1 64.844 (67.127)	
Total train loss: 0.9763

Train time: 16.82433772087097
 * Prec@1 68.770 Prec@5 96.640 Loss 0.9473
Best acc: 68.780
--------------------------------------------------------------------------------
Test time: 20.706974506378174

Epoch: [10][38/196]	LR: 0.001	Loss 1.0830 (0.9805)	Prec@1 63.672 (67.218)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.0967 (0.9823)	Prec@1 62.500 (67.002)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.9077 (0.9823)	Prec@1 67.969 (66.950)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.1084 (0.9814)	Prec@1 63.672 (66.880)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.0898 (0.9775)	Prec@1 63.281 (67.067)	
Total train loss: 0.9777

Train time: 17.787920236587524
 * Prec@1 68.820 Prec@5 96.650 Loss 0.9409
Best acc: 68.820
--------------------------------------------------------------------------------
Test time: 21.249831438064575

Epoch: [11][38/196]	LR: 0.001	Loss 0.9819 (0.9602)	Prec@1 66.797 (66.747)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.0059 (0.9745)	Prec@1 64.062 (66.777)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.8960 (0.9786)	Prec@1 73.047 (66.867)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.9189 (0.9806)	Prec@1 72.656 (66.882)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.9932 (0.9771)	Prec@1 65.625 (66.977)	
Total train loss: 0.9775

Train time: 18.2239670753479
 * Prec@1 68.930 Prec@5 96.630 Loss 0.9448
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 21.596218824386597

Epoch: [12][38/196]	LR: 0.001	Loss 1.0107 (0.9777)	Prec@1 68.750 (67.077)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.0596 (0.9849)	Prec@1 64.453 (66.942)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.9551 (0.9742)	Prec@1 66.797 (67.211)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.9082 (0.9785)	Prec@1 70.703 (67.020)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.9536 (0.9771)	Prec@1 69.141 (67.007)	
Total train loss: 0.9771

Train time: 18.59721326828003
 * Prec@1 68.720 Prec@5 96.670 Loss 0.9463
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 22.11610722541809

Epoch: [13][38/196]	LR: 0.001	Loss 0.9053 (0.9814)	Prec@1 66.797 (66.827)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.0713 (0.9869)	Prec@1 65.234 (66.607)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.9502 (0.9783)	Prec@1 69.141 (66.910)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.9673 (0.9776)	Prec@1 68.359 (66.930)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.8882 (0.9775)	Prec@1 71.094 (67.033)	
Total train loss: 0.9774

Train time: 18.973519563674927
 * Prec@1 68.900 Prec@5 96.680 Loss 0.9399
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 23.10684847831726

Epoch: [14][38/196]	LR: 0.001	Loss 0.8901 (0.9805)	Prec@1 70.312 (67.127)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.9463 (0.9796)	Prec@1 65.625 (67.002)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.8594 (0.9796)	Prec@1 72.656 (66.944)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.2012 (0.9795)	Prec@1 59.375 (66.960)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.9146 (0.9772)	Prec@1 69.141 (67.021)	
Total train loss: 0.9774

Train time: 19.19011116027832
 * Prec@1 68.830 Prec@5 96.690 Loss 0.9409
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 22.698752403259277

Epoch: [15][38/196]	LR: 0.001	Loss 1.0088 (0.9630)	Prec@1 66.406 (67.198)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.9106 (0.9702)	Prec@1 69.922 (67.278)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.0762 (0.9735)	Prec@1 60.547 (67.238)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.8955 (0.9786)	Prec@1 69.141 (67.105)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.9102 (0.9763)	Prec@1 69.141 (67.210)	
Total train loss: 0.9769

Train time: 18.1017165184021
 * Prec@1 68.880 Prec@5 96.690 Loss 0.9434
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 24.89038586616516

Epoch: [16][38/196]	LR: 0.0001	Loss 1.0010 (0.9714)	Prec@1 66.797 (67.348)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.0059 (0.9745)	Prec@1 68.359 (67.002)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.9688 (0.9740)	Prec@1 69.141 (66.991)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.9106 (0.9738)	Prec@1 70.703 (66.960)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.0264 (0.9768)	Prec@1 64.062 (66.855)	
Total train loss: 0.9770

Train time: 18.792524576187134
 * Prec@1 68.880 Prec@5 96.710 Loss 0.9434
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 22.73536252975464

Epoch: [17][38/196]	LR: 0.0001	Loss 0.9058 (0.9818)	Prec@1 68.750 (67.047)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.9907 (0.9745)	Prec@1 67.188 (67.057)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.8745 (0.9726)	Prec@1 72.266 (67.094)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.9395 (0.9753)	Prec@1 64.453 (66.987)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.9873 (0.9769)	Prec@1 69.531 (66.973)	
Total train loss: 0.9766

Train time: 16.491538286209106
 * Prec@1 68.780 Prec@5 96.620 Loss 0.9434
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 20.450881481170654

Epoch: [18][38/196]	LR: 0.0001	Loss 0.9634 (0.9668)	Prec@1 68.750 (67.628)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.9053 (0.9798)	Prec@1 69.531 (66.857)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.0205 (0.9800)	Prec@1 65.625 (66.794)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.0449 (0.9798)	Prec@1 64.453 (66.864)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.9448 (0.9767)	Prec@1 67.188 (66.967)	
Total train loss: 0.9771

Train time: 17.104100704193115
 * Prec@1 68.790 Prec@5 96.660 Loss 0.9434
Best acc: 68.930
--------------------------------------------------------------------------------
Test time: 25.969869375228882

Epoch: [19][38/196]	LR: 0.0001	Loss 0.9746 (0.9732)	Prec@1 66.016 (67.498)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.0078 (0.9766)	Prec@1 66.016 (67.172)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.1113 (0.9711)	Prec@1 60.156 (67.324)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.0098 (0.9784)	Prec@1 64.844 (67.012)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.9819 (0.9768)	Prec@1 65.234 (67.101)	
Total train loss: 0.9768

Train time: 19.218541860580444
 * Prec@1 68.980 Prec@5 96.630 Loss 0.9424
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 22.88193655014038

Epoch: [20][38/196]	LR: 0.0001	Loss 1.1113 (0.9839)	Prec@1 62.891 (66.897)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.9448 (0.9774)	Prec@1 62.891 (66.982)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.1035 (0.9778)	Prec@1 64.453 (67.104)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.9692 (0.9778)	Prec@1 66.016 (67.130)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.8442 (0.9759)	Prec@1 73.828 (67.151)	
Total train loss: 0.9760

Train time: 18.275855779647827
 * Prec@1 68.850 Prec@5 96.660 Loss 0.9390
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 21.956632614135742

Epoch: [21][38/196]	LR: 0.0001	Loss 0.8384 (0.9850)	Prec@1 74.219 (67.177)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.0615 (0.9759)	Prec@1 62.500 (67.298)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.9448 (0.9729)	Prec@1 70.312 (67.321)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.0166 (0.9784)	Prec@1 64.062 (67.170)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.0068 (0.9769)	Prec@1 69.531 (67.123)	
Total train loss: 0.9765

Train time: 18.241395950317383
 * Prec@1 68.860 Prec@5 96.690 Loss 0.9409
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 22.943087100982666

Epoch: [22][38/196]	LR: 0.0001	Loss 0.9897 (0.9732)	Prec@1 65.625 (67.067)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.0615 (0.9780)	Prec@1 65.625 (66.957)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.0498 (0.9807)	Prec@1 64.453 (66.787)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.9746 (0.9778)	Prec@1 65.234 (66.885)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.9214 (0.9771)	Prec@1 70.312 (66.925)	
Total train loss: 0.9773

Train time: 18.232550859451294
 * Prec@1 68.770 Prec@5 96.690 Loss 0.9463
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 22.446333646774292

Epoch: [23][38/196]	LR: 0.0001	Loss 0.9209 (0.9627)	Prec@1 71.484 (67.047)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.1133 (0.9694)	Prec@1 66.016 (67.358)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.9810 (0.9676)	Prec@1 67.578 (67.398)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.9219 (0.9735)	Prec@1 69.922 (67.233)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.9292 (0.9762)	Prec@1 67.578 (67.137)	
Total train loss: 0.9771

Train time: 16.5779812335968
 * Prec@1 68.690 Prec@5 96.640 Loss 0.9463
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 20.32042932510376

Epoch: [24][38/196]	LR: 1e-05	Loss 0.9624 (0.9647)	Prec@1 66.406 (67.468)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.0928 (0.9799)	Prec@1 65.625 (66.982)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.9189 (0.9792)	Prec@1 66.797 (67.081)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.9634 (0.9772)	Prec@1 65.625 (67.122)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.9126 (0.9763)	Prec@1 69.922 (67.107)	
Total train loss: 0.9761

Train time: 17.303654193878174
 * Prec@1 68.840 Prec@5 96.660 Loss 0.9399
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 23.691035747528076

Epoch: [25][38/196]	LR: 1e-05	Loss 1.0732 (0.9846)	Prec@1 59.766 (66.847)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.9551 (0.9805)	Prec@1 70.312 (67.198)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.8584 (0.9756)	Prec@1 69.531 (67.351)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.0176 (0.9779)	Prec@1 66.016 (67.135)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.9287 (0.9753)	Prec@1 69.141 (67.177)	
Total train loss: 0.9754

Train time: 17.105664491653442
 * Prec@1 68.780 Prec@5 96.690 Loss 0.9458
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 20.830631732940674

Epoch: [26][38/196]	LR: 1e-05	Loss 0.8413 (0.9622)	Prec@1 70.312 (66.917)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.7490 (0.9650)	Prec@1 76.953 (67.233)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.0742 (0.9689)	Prec@1 67.188 (67.184)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.9448 (0.9738)	Prec@1 71.094 (67.105)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.0176 (0.9770)	Prec@1 65.234 (67.023)	
Total train loss: 0.9772

Train time: 16.576341152191162
 * Prec@1 68.890 Prec@5 96.630 Loss 0.9434
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 20.46319007873535

Epoch: [27][38/196]	LR: 1e-05	Loss 1.0107 (0.9679)	Prec@1 66.797 (67.348)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.9331 (0.9789)	Prec@1 68.750 (67.082)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.9751 (0.9765)	Prec@1 66.406 (67.071)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.9453 (0.9749)	Prec@1 66.797 (67.085)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.9209 (0.9737)	Prec@1 71.875 (67.183)	
Total train loss: 0.9740

Train time: 16.725208520889282
 * Prec@1 68.930 Prec@5 96.680 Loss 0.9424
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 26.082083463668823

Epoch: [28][38/196]	LR: 1e-05	Loss 1.0303 (0.9757)	Prec@1 64.844 (66.867)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.9502 (0.9772)	Prec@1 66.016 (66.932)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.0146 (0.9775)	Prec@1 66.016 (67.074)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.9673 (0.9776)	Prec@1 66.406 (67.047)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.8530 (0.9761)	Prec@1 69.141 (67.047)	
Total train loss: 0.9761

Train time: 18.940909147262573
 * Prec@1 68.780 Prec@5 96.720 Loss 0.9390
Best acc: 68.980
--------------------------------------------------------------------------------
Test time: 23.340561389923096

Epoch: [29][38/196]	LR: 1e-05	Loss 0.9248 (0.9661)	Prec@1 66.406 (66.957)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.9551 (0.9700)	Prec@1 68.359 (67.032)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.0234 (0.9684)	Prec@1 67.969 (67.141)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.0537 (0.9734)	Prec@1 64.453 (67.087)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.9854 (0.9758)	Prec@1 67.188 (67.051)	
Total train loss: 0.9759

Train time: 17.219468116760254
 * Prec@1 69.030 Prec@5 96.640 Loss 0.9409
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 20.94585609436035

Epoch: [30][38/196]	LR: 1e-05	Loss 1.0742 (0.9779)	Prec@1 66.797 (67.338)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.0898 (0.9773)	Prec@1 65.625 (67.193)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.9541 (0.9743)	Prec@1 66.797 (67.334)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.1494 (0.9776)	Prec@1 64.062 (67.190)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.8345 (0.9754)	Prec@1 73.438 (67.194)	
Total train loss: 0.9753

Train time: 17.03244376182556
 * Prec@1 68.740 Prec@5 96.670 Loss 0.9385
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 22.19664478302002

Epoch: [31][38/196]	LR: 1e-05	Loss 1.0186 (0.9972)	Prec@1 64.844 (66.456)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.9971 (0.9871)	Prec@1 66.016 (66.627)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.8315 (0.9781)	Prec@1 73.438 (66.950)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.0820 (0.9718)	Prec@1 65.625 (67.268)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.9434 (0.9770)	Prec@1 67.578 (67.057)	
Total train loss: 0.9772

Train time: 16.90680980682373
 * Prec@1 68.870 Prec@5 96.720 Loss 0.9448
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 20.52610754966736

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.9893 (0.9684)	Prec@1 66.406 (67.628)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.0518 (0.9744)	Prec@1 62.891 (67.248)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.9790 (0.9722)	Prec@1 68.750 (67.234)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.8447 (0.9736)	Prec@1 69.922 (67.243)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.8804 (0.9756)	Prec@1 69.922 (67.115)	
Total train loss: 0.9757

Train time: 18.74298882484436
 * Prec@1 68.860 Prec@5 96.710 Loss 0.9414
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 22.334879159927368

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.0312 (0.9748)	Prec@1 66.797 (67.087)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.9658 (0.9749)	Prec@1 71.875 (67.027)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.0176 (0.9741)	Prec@1 67.188 (67.101)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.0156 (0.9736)	Prec@1 63.281 (67.087)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.8682 (0.9761)	Prec@1 68.359 (67.087)	
Total train loss: 0.9762

Train time: 18.339684009552002
 * Prec@1 68.980 Prec@5 96.630 Loss 0.9414
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 25.810328006744385

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.0625 (0.9937)	Prec@1 64.062 (66.446)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.9375 (0.9888)	Prec@1 66.797 (66.356)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.9790 (0.9821)	Prec@1 70.312 (66.794)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.0361 (0.9808)	Prec@1 64.062 (66.839)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.0244 (0.9767)	Prec@1 64.844 (67.047)	
Total train loss: 0.9767

Train time: 18.90375781059265
 * Prec@1 68.870 Prec@5 96.640 Loss 0.9390
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 22.814826250076294

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.9688 (0.9777)	Prec@1 67.969 (67.318)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.8882 (0.9850)	Prec@1 72.656 (66.922)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.9985 (0.9784)	Prec@1 63.281 (67.107)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.8950 (0.9794)	Prec@1 69.141 (67.047)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.9048 (0.9743)	Prec@1 69.531 (67.127)	
Total train loss: 0.9747

Train time: 16.804062366485596
 * Prec@1 68.880 Prec@5 96.690 Loss 0.9448
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 19.720677614212036

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.9351 (0.9740)	Prec@1 66.406 (67.017)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.8271 (0.9799)	Prec@1 73.438 (66.767)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.0508 (0.9783)	Prec@1 68.750 (66.790)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.9307 (0.9764)	Prec@1 69.922 (66.822)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.0049 (0.9759)	Prec@1 65.234 (66.981)	
Total train loss: 0.9758

Train time: 17.4021954536438
 * Prec@1 68.810 Prec@5 96.680 Loss 0.9438
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 21.30160665512085

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.0635 (0.9841)	Prec@1 64.062 (67.167)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.8467 (0.9775)	Prec@1 71.875 (67.523)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.1807 (0.9789)	Prec@1 61.328 (67.157)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.9038 (0.9790)	Prec@1 66.797 (67.167)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.8491 (0.9753)	Prec@1 74.609 (67.198)	
Total train loss: 0.9755

Train time: 19.389416217803955
 * Prec@1 68.850 Prec@5 96.660 Loss 0.9438
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 22.806873321533203

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.9053 (0.9586)	Prec@1 73.828 (68.009)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.9834 (0.9782)	Prec@1 67.578 (67.077)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.8652 (0.9778)	Prec@1 67.969 (67.144)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.9536 (0.9773)	Prec@1 68.359 (67.120)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.0225 (0.9761)	Prec@1 68.359 (67.103)	
Total train loss: 0.9763

Train time: 18.55819034576416
 * Prec@1 68.820 Prec@5 96.680 Loss 0.9414
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 21.796225786209106

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.9116 (0.9871)	Prec@1 69.531 (66.366)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.0039 (0.9891)	Prec@1 66.406 (66.582)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.8911 (0.9812)	Prec@1 69.141 (66.884)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.8442 (0.9775)	Prec@1 71.875 (67.040)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.9937 (0.9762)	Prec@1 64.062 (67.089)	
Total train loss: 0.9764

Train time: 17.343854665756226
 * Prec@1 68.830 Prec@5 96.680 Loss 0.9434
Best acc: 69.030
--------------------------------------------------------------------------------
Test time: 21.45212435722351


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 17.410 Prec@5 76.990 Loss 3.9160
Pre-trained Prec@1 with 19 layers frozen: 17.40999984741211 	 Loss: 3.916015625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 1.6855 (2.1516)	Prec@1 55.078 (51.532)	
Epoch: [0][77/196]	LR: 0.01	Loss 1.6748 (1.9605)	Prec@1 56.250 (53.626)	
Epoch: [0][116/196]	LR: 0.01	Loss 1.5498 (1.8421)	Prec@1 59.375 (55.078)	
Epoch: [0][155/196]	LR: 0.01	Loss 1.3018 (1.7653)	Prec@1 62.891 (55.797)	
Epoch: [0][194/196]	LR: 0.01	Loss 1.5674 (1.7121)	Prec@1 52.734 (56.152)	
Total train loss: 1.7121

Train time: 159.3075466156006
 * Prec@1 61.220 Prec@5 94.500 Loss 1.4316
Best acc: 61.220
--------------------------------------------------------------------------------
Test time: 164.64550828933716

Epoch: [1][38/196]	LR: 0.01	Loss 1.2656 (1.3366)	Prec@1 58.984 (59.215)	
Epoch: [1][77/196]	LR: 0.01	Loss 1.3633 (1.3407)	Prec@1 53.906 (58.564)	
Epoch: [1][116/196]	LR: 0.01	Loss 1.2998 (1.3236)	Prec@1 56.250 (58.377)	
Epoch: [1][155/196]	LR: 0.01	Loss 1.3916 (1.3105)	Prec@1 57.422 (58.263)	
Epoch: [1][194/196]	LR: 0.01	Loss 1.2725 (1.2960)	Prec@1 58.594 (58.271)	
Total train loss: 1.2963

Train time: 14.497020483016968
 * Prec@1 61.390 Prec@5 94.610 Loss 1.1602
Best acc: 61.390
--------------------------------------------------------------------------------
Test time: 17.395182609558105

Epoch: [2][38/196]	LR: 0.01	Loss 1.2266 (1.2508)	Prec@1 58.594 (57.242)	
Epoch: [2][77/196]	LR: 0.01	Loss 1.1357 (1.2318)	Prec@1 58.203 (58.063)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.3350 (1.2291)	Prec@1 53.906 (58.160)	
Epoch: [2][155/196]	LR: 0.01	Loss 1.0645 (1.2238)	Prec@1 63.281 (58.333)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.3047 (1.2259)	Prec@1 56.641 (58.371)	
Total train loss: 1.2263

Train time: 13.325852394104004
 * Prec@1 61.880 Prec@5 94.720 Loss 1.1436
Best acc: 61.880
--------------------------------------------------------------------------------
Test time: 17.41323733329773

Epoch: [3][38/196]	LR: 0.01	Loss 1.3340 (1.2235)	Prec@1 54.688 (58.994)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.1836 (1.2173)	Prec@1 60.156 (58.849)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.4141 (1.2156)	Prec@1 57.031 (58.841)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.2002 (1.2155)	Prec@1 55.859 (58.879)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.4131 (1.2156)	Prec@1 52.344 (58.990)	
Total train loss: 1.2154

Train time: 12.180537462234497
 * Prec@1 61.950 Prec@5 94.820 Loss 1.1484
Best acc: 61.950
--------------------------------------------------------------------------------
Test time: 16.032909870147705

Epoch: [4][38/196]	LR: 0.01	Loss 1.2910 (1.2351)	Prec@1 57.422 (58.273)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.1006 (1.2304)	Prec@1 61.719 (58.428)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.2334 (1.2251)	Prec@1 61.328 (58.654)	
Epoch: [4][155/196]	LR: 0.01	Loss 1.1914 (1.2249)	Prec@1 60.547 (58.789)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.2197 (1.2207)	Prec@1 58.984 (58.916)	
Total train loss: 1.2209

Train time: 12.902299165725708
 * Prec@1 61.990 Prec@5 94.800 Loss 1.1650
Best acc: 61.990
--------------------------------------------------------------------------------
Test time: 20.127400636672974

Epoch: [5][38/196]	LR: 0.01	Loss 1.3965 (1.2331)	Prec@1 56.250 (59.215)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.3213 (1.2458)	Prec@1 55.859 (58.749)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.2285 (1.2471)	Prec@1 60.547 (58.858)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.2607 (1.2403)	Prec@1 54.688 (59.100)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.2158 (1.2382)	Prec@1 56.250 (59.155)	
Total train loss: 1.2384

Train time: 13.32148814201355
 * Prec@1 62.160 Prec@5 94.830 Loss 1.1914
Best acc: 62.160
--------------------------------------------------------------------------------
Test time: 16.263429641723633

Epoch: [6][38/196]	LR: 0.01	Loss 1.3037 (1.2237)	Prec@1 57.031 (59.435)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.2881 (1.2409)	Prec@1 56.641 (58.819)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.2207 (1.2433)	Prec@1 57.812 (58.774)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.4463 (1.2434)	Prec@1 58.594 (58.927)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.2695 (1.2511)	Prec@1 60.547 (58.798)	
Total train loss: 1.2511

Train time: 12.667030811309814
 * Prec@1 62.000 Prec@5 94.770 Loss 1.2285
Best acc: 62.160
--------------------------------------------------------------------------------
Test time: 15.611295700073242

Epoch: [7][38/196]	LR: 0.01	Loss 1.1406 (1.2591)	Prec@1 61.719 (58.934)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.3037 (1.2547)	Prec@1 56.641 (58.909)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.3887 (1.2681)	Prec@1 52.344 (58.804)	
Epoch: [7][155/196]	LR: 0.01	Loss 1.3506 (1.2634)	Prec@1 58.594 (58.782)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.2988 (1.2579)	Prec@1 62.109 (59.000)	
Total train loss: 1.2578

Train time: 12.23067021369934
 * Prec@1 62.050 Prec@5 94.770 Loss 1.2188
Best acc: 62.160
--------------------------------------------------------------------------------
Test time: 15.553902864456177

Epoch: [8][38/196]	LR: 0.001	Loss 1.2266 (1.2325)	Prec@1 61.719 (59.635)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.2256 (1.2383)	Prec@1 59.766 (59.555)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.3730 (1.2470)	Prec@1 58.984 (59.458)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.3057 (1.2545)	Prec@1 58.203 (59.202)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.4131 (1.2603)	Prec@1 57.422 (59.050)	
Total train loss: 1.2606

Train time: 12.59207010269165
 * Prec@1 61.960 Prec@5 94.800 Loss 1.2295
Best acc: 62.160
--------------------------------------------------------------------------------
Test time: 17.46403193473816

Epoch: [9][38/196]	LR: 0.001	Loss 1.1523 (1.2298)	Prec@1 61.719 (59.836)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.5107 (1.2517)	Prec@1 51.172 (59.049)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.3828 (1.2565)	Prec@1 56.250 (59.071)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.4062 (1.2607)	Prec@1 53.906 (59.017)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.3203 (1.2583)	Prec@1 53.906 (58.984)	
Total train loss: 1.2582

Train time: 13.481589078903198
 * Prec@1 62.210 Prec@5 94.710 Loss 1.2188
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 17.647839546203613

Epoch: [10][38/196]	LR: 0.001	Loss 1.2070 (1.2704)	Prec@1 57.422 (58.564)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.2402 (1.2591)	Prec@1 57.422 (59.039)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.5400 (1.2618)	Prec@1 54.688 (58.968)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.2744 (1.2582)	Prec@1 58.203 (59.039)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.4102 (1.2595)	Prec@1 57.031 (59.010)	
Total train loss: 1.2595

Train time: 12.976768016815186
 * Prec@1 62.000 Prec@5 94.850 Loss 1.2295
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 16.20554208755493

Epoch: [11][38/196]	LR: 0.001	Loss 1.3818 (1.2589)	Prec@1 57.812 (59.054)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.2920 (1.2525)	Prec@1 57.422 (58.964)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.3623 (1.2506)	Prec@1 58.594 (59.218)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.3896 (1.2565)	Prec@1 57.031 (58.964)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.4834 (1.2579)	Prec@1 53.906 (59.000)	
Total train loss: 1.2580

Train time: 12.299556493759155
 * Prec@1 62.110 Prec@5 94.750 Loss 1.2188
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 15.187263488769531

Epoch: [12][38/196]	LR: 0.001	Loss 1.2695 (1.2543)	Prec@1 58.984 (59.105)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.0723 (1.2604)	Prec@1 64.453 (59.105)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.4043 (1.2596)	Prec@1 54.688 (58.984)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.1943 (1.2579)	Prec@1 59.375 (59.034)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.2100 (1.2571)	Prec@1 62.109 (59.079)	
Total train loss: 1.2570

Train time: 12.366294622421265
 * Prec@1 62.200 Prec@5 94.800 Loss 1.2188
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 16.508882761001587

Epoch: [13][38/196]	LR: 0.001	Loss 1.0547 (1.2553)	Prec@1 62.891 (58.454)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.3574 (1.2502)	Prec@1 60.156 (58.914)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.2744 (1.2572)	Prec@1 62.109 (58.777)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.2334 (1.2616)	Prec@1 61.328 (58.797)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.1230 (1.2568)	Prec@1 63.281 (59.008)	
Total train loss: 1.2571

Train time: 12.944264888763428
 * Prec@1 62.080 Prec@5 94.740 Loss 1.2188
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 15.919736862182617

Epoch: [14][38/196]	LR: 0.001	Loss 1.3467 (1.2489)	Prec@1 57.812 (58.894)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.4160 (1.2550)	Prec@1 53.125 (58.849)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.1992 (1.2563)	Prec@1 62.891 (59.024)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.2510 (1.2575)	Prec@1 57.031 (58.972)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.4131 (1.2575)	Prec@1 58.203 (59.002)	
Total train loss: 1.2577

Train time: 13.055713891983032
 * Prec@1 62.010 Prec@5 94.870 Loss 1.2236
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 15.736610889434814

Epoch: [15][38/196]	LR: 0.001	Loss 1.1143 (1.2468)	Prec@1 59.375 (59.615)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.3398 (1.2591)	Prec@1 57.422 (58.879)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.2695 (1.2636)	Prec@1 59.766 (58.878)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.2021 (1.2572)	Prec@1 59.375 (59.072)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.3564 (1.2593)	Prec@1 53.906 (59.026)	
Total train loss: 1.2594

Train time: 12.147975444793701
 * Prec@1 62.050 Prec@5 94.790 Loss 1.2217
Best acc: 62.210
--------------------------------------------------------------------------------
Test time: 14.914481401443481

Epoch: [16][38/196]	LR: 0.0001	Loss 1.2412 (1.2628)	Prec@1 57.812 (58.914)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.2080 (1.2586)	Prec@1 60.547 (59.080)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.2559 (1.2629)	Prec@1 60.547 (59.138)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.1807 (1.2640)	Prec@1 59.375 (59.004)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.1484 (1.2580)	Prec@1 57.812 (59.032)	
Total train loss: 1.2580

Train time: 12.102505207061768
 * Prec@1 62.320 Prec@5 94.840 Loss 1.2217
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 14.888933658599854

Epoch: [17][38/196]	LR: 0.0001	Loss 1.2344 (1.2744)	Prec@1 60.547 (58.804)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.2666 (1.2633)	Prec@1 61.719 (58.804)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.1729 (1.2613)	Prec@1 63.281 (58.844)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.4395 (1.2615)	Prec@1 58.203 (58.869)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.2822 (1.2577)	Prec@1 55.469 (58.956)	
Total train loss: 1.2578

Train time: 12.408113718032837
 * Prec@1 62.100 Prec@5 94.870 Loss 1.2119
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.192918300628662

Epoch: [18][38/196]	LR: 0.0001	Loss 1.3096 (1.2493)	Prec@1 62.500 (59.285)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.1211 (1.2405)	Prec@1 64.453 (59.385)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.2383 (1.2525)	Prec@1 61.328 (59.091)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.1260 (1.2593)	Prec@1 60.938 (58.977)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.3633 (1.2567)	Prec@1 55.469 (59.006)	
Total train loss: 1.2570

Train time: 12.745487451553345
 * Prec@1 62.100 Prec@5 94.830 Loss 1.2227
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.204991102218628

Epoch: [19][38/196]	LR: 0.0001	Loss 1.4424 (1.2479)	Prec@1 58.984 (59.345)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.2295 (1.2534)	Prec@1 60.938 (59.395)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.4541 (1.2604)	Prec@1 53.906 (59.024)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.2236 (1.2579)	Prec@1 57.422 (59.027)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.3564 (1.2600)	Prec@1 57.031 (59.052)	
Total train loss: 1.2599

Train time: 12.315990686416626
 * Prec@1 62.190 Prec@5 94.800 Loss 1.2188
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.540147066116333

Epoch: [20][38/196]	LR: 0.0001	Loss 1.0947 (1.2465)	Prec@1 63.672 (59.385)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.2666 (1.2420)	Prec@1 58.594 (59.550)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.2246 (1.2525)	Prec@1 61.719 (59.171)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.3848 (1.2570)	Prec@1 58.984 (58.954)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.3623 (1.2575)	Prec@1 57.031 (59.046)	
Total train loss: 1.2575

Train time: 12.511197328567505
 * Prec@1 62.100 Prec@5 94.850 Loss 1.2109
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.377657890319824

Epoch: [21][38/196]	LR: 0.0001	Loss 1.2236 (1.2669)	Prec@1 59.766 (58.954)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.3467 (1.2640)	Prec@1 56.250 (58.949)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.1904 (1.2521)	Prec@1 62.109 (59.255)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.2344 (1.2552)	Prec@1 56.641 (59.250)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.4258 (1.2574)	Prec@1 54.688 (59.181)	
Total train loss: 1.2572

Train time: 12.496710062026978
 * Prec@1 62.040 Prec@5 94.810 Loss 1.2188
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.78524136543274

Epoch: [22][38/196]	LR: 0.0001	Loss 1.2178 (1.2371)	Prec@1 60.156 (58.774)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.3350 (1.2473)	Prec@1 56.641 (58.889)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.2314 (1.2507)	Prec@1 62.500 (59.251)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.2705 (1.2538)	Prec@1 58.203 (59.012)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.2881 (1.2597)	Prec@1 58.594 (58.854)	
Total train loss: 1.2602

Train time: 12.996467113494873
 * Prec@1 62.040 Prec@5 94.820 Loss 1.2217
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.211618423461914

Epoch: [23][38/196]	LR: 0.0001	Loss 1.2139 (1.2409)	Prec@1 61.328 (59.125)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.3105 (1.2494)	Prec@1 53.906 (58.694)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.1943 (1.2555)	Prec@1 61.719 (58.607)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.2930 (1.2570)	Prec@1 60.547 (58.754)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.3223 (1.2589)	Prec@1 61.719 (58.872)	
Total train loss: 1.2585

Train time: 12.703561782836914
 * Prec@1 62.070 Prec@5 94.730 Loss 1.2188
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.620660066604614

Epoch: [24][38/196]	LR: 1e-05	Loss 1.1514 (1.2833)	Prec@1 62.891 (58.704)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.1904 (1.2604)	Prec@1 60.156 (59.125)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.2451 (1.2653)	Prec@1 59.766 (58.837)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.2334 (1.2648)	Prec@1 57.422 (58.984)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.1973 (1.2591)	Prec@1 62.109 (59.058)	
Total train loss: 1.2592

Train time: 13.707807064056396
 * Prec@1 62.150 Prec@5 94.790 Loss 1.2236
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 17.20263910293579

Epoch: [25][38/196]	LR: 1e-05	Loss 1.2168 (1.2419)	Prec@1 57.031 (59.125)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.3193 (1.2508)	Prec@1 56.250 (59.175)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.0459 (1.2563)	Prec@1 62.891 (58.978)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.1777 (1.2526)	Prec@1 61.328 (59.090)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.2852 (1.2575)	Prec@1 58.984 (58.912)	
Total train loss: 1.2576

Train time: 12.432992458343506
 * Prec@1 62.140 Prec@5 94.800 Loss 1.2305
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.770690679550171

Epoch: [26][38/196]	LR: 1e-05	Loss 1.3760 (1.2586)	Prec@1 61.328 (58.794)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.3389 (1.2676)	Prec@1 54.688 (58.474)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.2197 (1.2594)	Prec@1 60.938 (58.884)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.2559 (1.2546)	Prec@1 56.641 (59.002)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.3604 (1.2581)	Prec@1 58.984 (58.876)	
Total train loss: 1.2577

Train time: 13.527579069137573
 * Prec@1 61.960 Prec@5 94.790 Loss 1.2148
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.28725838661194

Epoch: [27][38/196]	LR: 1e-05	Loss 1.2881 (1.2533)	Prec@1 56.250 (59.024)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.2705 (1.2572)	Prec@1 56.641 (58.864)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.3115 (1.2582)	Prec@1 53.906 (58.864)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.1162 (1.2562)	Prec@1 64.062 (59.072)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.1465 (1.2584)	Prec@1 60.156 (58.978)	
Total train loss: 1.2584

Train time: 13.964237928390503
 * Prec@1 61.990 Prec@5 94.760 Loss 1.2227
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 18.113366842269897

Epoch: [28][38/196]	LR: 1e-05	Loss 1.2588 (1.2556)	Prec@1 59.375 (57.913)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.1689 (1.2551)	Prec@1 58.594 (58.474)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.3047 (1.2645)	Prec@1 57.422 (58.457)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.1533 (1.2610)	Prec@1 62.891 (58.746)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.1104 (1.2571)	Prec@1 64.844 (58.926)	
Total train loss: 1.2573

Train time: 14.78688097000122
 * Prec@1 62.140 Prec@5 94.800 Loss 1.2217
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 19.100637674331665

Epoch: [29][38/196]	LR: 1e-05	Loss 1.2070 (1.2651)	Prec@1 60.547 (58.784)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.1455 (1.2622)	Prec@1 61.719 (58.719)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.3213 (1.2578)	Prec@1 57.031 (58.904)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.2031 (1.2572)	Prec@1 61.328 (58.784)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.4463 (1.2585)	Prec@1 58.203 (58.836)	
Total train loss: 1.2580

Train time: 14.356679439544678
 * Prec@1 62.040 Prec@5 94.930 Loss 1.2207
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 18.03470516204834

Epoch: [30][38/196]	LR: 1e-05	Loss 1.4092 (1.2600)	Prec@1 55.078 (59.235)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.3203 (1.2690)	Prec@1 57.812 (58.824)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.3867 (1.2600)	Prec@1 51.953 (59.028)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.4424 (1.2568)	Prec@1 56.250 (59.120)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.2070 (1.2587)	Prec@1 57.031 (58.962)	
Total train loss: 1.2589

Train time: 13.678327322006226
 * Prec@1 62.020 Prec@5 94.890 Loss 1.2100
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.760117292404175

Epoch: [31][38/196]	LR: 1e-05	Loss 1.0781 (1.2717)	Prec@1 64.453 (58.413)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.1982 (1.2693)	Prec@1 60.938 (58.684)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.2207 (1.2696)	Prec@1 61.719 (58.600)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.2363 (1.2662)	Prec@1 59.375 (58.797)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.1045 (1.2590)	Prec@1 63.281 (58.886)	
Total train loss: 1.2590

Train time: 12.738255500793457
 * Prec@1 62.110 Prec@5 94.750 Loss 1.2217
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.097816944122314

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.1191 (1.2618)	Prec@1 63.672 (58.884)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.2490 (1.2655)	Prec@1 61.328 (58.719)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.2363 (1.2609)	Prec@1 56.250 (58.881)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.1973 (1.2577)	Prec@1 60.938 (59.039)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.2256 (1.2584)	Prec@1 62.109 (59.028)	
Total train loss: 1.2585

Train time: 12.260034322738647
 * Prec@1 61.970 Prec@5 94.760 Loss 1.2158
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.49463129043579

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.2559 (1.2753)	Prec@1 57.812 (58.423)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.3477 (1.2657)	Prec@1 53.906 (58.584)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.4580 (1.2598)	Prec@1 57.422 (58.744)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.2402 (1.2562)	Prec@1 56.641 (58.944)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.3994 (1.2588)	Prec@1 52.344 (58.914)	
Total train loss: 1.2589

Train time: 12.439097166061401
 * Prec@1 61.940 Prec@5 94.810 Loss 1.2324
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.623072624206543

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.2725 (1.2687)	Prec@1 60.547 (59.034)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.4209 (1.2675)	Prec@1 53.906 (58.959)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.2227 (1.2639)	Prec@1 57.812 (59.024)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.3643 (1.2667)	Prec@1 56.250 (58.844)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.2109 (1.2581)	Prec@1 57.812 (59.044)	
Total train loss: 1.2580

Train time: 12.420807361602783
 * Prec@1 62.190 Prec@5 94.760 Loss 1.2129
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.14405369758606

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.2773 (1.2491)	Prec@1 58.594 (59.165)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.2959 (1.2584)	Prec@1 54.297 (58.999)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.2354 (1.2617)	Prec@1 59.766 (58.884)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.3496 (1.2561)	Prec@1 58.984 (59.052)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.3623 (1.2589)	Prec@1 55.469 (58.956)	
Total train loss: 1.2586

Train time: 12.653044700622559
 * Prec@1 61.970 Prec@5 94.780 Loss 1.2275
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.10763931274414

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.3369 (1.2723)	Prec@1 55.859 (57.973)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.0576 (1.2626)	Prec@1 67.578 (58.644)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.2402 (1.2614)	Prec@1 60.547 (58.841)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.0693 (1.2561)	Prec@1 63.672 (59.070)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.2324 (1.2564)	Prec@1 58.984 (59.032)	
Total train loss: 1.2564

Train time: 12.483416080474854
 * Prec@1 62.110 Prec@5 94.850 Loss 1.2227
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 18.665452241897583

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.2988 (1.2695)	Prec@1 58.594 (58.914)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.0908 (1.2716)	Prec@1 65.625 (58.449)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.1045 (1.2690)	Prec@1 63.672 (58.397)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.1982 (1.2604)	Prec@1 64.453 (58.877)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.3652 (1.2587)	Prec@1 59.766 (58.876)	
Total train loss: 1.2583

Train time: 12.420697927474976
 * Prec@1 62.070 Prec@5 94.840 Loss 1.2275
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.867324829101562

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.9766 (1.2340)	Prec@1 68.359 (59.896)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.2236 (1.2490)	Prec@1 60.547 (59.320)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.1221 (1.2557)	Prec@1 60.938 (59.165)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.3164 (1.2564)	Prec@1 57.812 (59.140)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.3555 (1.2566)	Prec@1 56.641 (59.185)	
Total train loss: 1.2569

Train time: 12.636852741241455
 * Prec@1 62.050 Prec@5 94.900 Loss 1.2227
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 15.703288316726685

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.1279 (1.2555)	Prec@1 62.891 (59.345)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.2842 (1.2567)	Prec@1 59.375 (59.180)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.0918 (1.2486)	Prec@1 60.547 (59.472)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.2568 (1.2553)	Prec@1 61.328 (59.172)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.3213 (1.2584)	Prec@1 57.031 (59.010)	
Total train loss: 1.2583

Train time: 13.0896155834198
 * Prec@1 62.230 Prec@5 94.810 Loss 1.2188
Best acc: 62.320
--------------------------------------------------------------------------------
Test time: 16.486894607543945

