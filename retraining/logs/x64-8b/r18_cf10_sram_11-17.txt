
      ==> Arguments:
          dataset: cifar10
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.08
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 11
Savedir:  ../pretrained_models/frozen/x64-8b/sram/cifar10/resnet18
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar10/resnet18/train/relu11
Test path:  /home/nano01/a/esoufler/activations/x64-8b/sram/one_batch/cifar10/resnet18/test/relu11
ResNet18(
  (conv12): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): QConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=512, out_features=10, bias=False)
  (bn19): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 9.610 Prec@5 52.050 Loss 2.2969
Avg Loading time: 0.2955 seconds
Avg Batch time: 0.3181 seconds

Pre-trained Prec@1 with 11 layers frozen: 9.609999656677246 	 Loss: 2.296875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.08	DT: 0.000 (1.926)	BT: 0.026 (1.957)	Loss 0.5811 (0.7474)	Prec@1 82.812 (77.344)	
Epoch: [0][155/391]	LR: 0.08	DT: 2.259 (3.871)	BT: 2.295 (3.904)	Loss 0.6616 (0.6914)	Prec@1 78.906 (77.865)	
Epoch: [0][233/391]	LR: 0.08	DT: 6.431 (5.216)	BT: 6.465 (5.250)	Loss 0.4402 (0.6721)	Prec@1 84.375 (78.062)	
Epoch: [0][311/391]	LR: 0.08	DT: 0.000 (5.850)	BT: 0.026 (5.885)	Loss 0.5796 (0.6510)	Prec@1 80.469 (78.513)	
Epoch: [0][389/391]	LR: 0.08	DT: 1.217 (6.232)	BT: 1.252 (6.266)	Loss 0.5381 (0.6308)	Prec@1 80.469 (79.111)	
Total train loss: 0.6309
Avg Loading time: 6.2160 seconds
Avg Batch time: 6.2505 seconds

Train time: 2443.975432395935
 * Prec@1 68.360 Prec@5 97.240 Loss 0.9985
Avg Loading time: 4.6416 seconds
Avg Batch time: 4.6562 seconds

Best acc: 68.360
--------------------------------------------------------------------------------
Test time: 368.7926027774811

Epoch: [1][77/391]	LR: 0.08	DT: 0.000 (4.767)	BT: 0.026 (4.798)	Loss 0.5337 (0.5240)	Prec@1 82.812 (82.272)	
Epoch: [1][155/391]	LR: 0.08	DT: 0.000 (4.996)	BT: 0.027 (5.027)	Loss 0.6143 (0.5229)	Prec@1 77.344 (82.151)	
Epoch: [1][233/391]	LR: 0.08	DT: 0.000 (4.884)	BT: 0.027 (4.915)	Loss 0.3865 (0.5076)	Prec@1 87.500 (82.692)	
Epoch: [1][311/391]	LR: 0.08	DT: 0.000 (4.445)	BT: 0.024 (4.476)	Loss 0.3823 (0.4975)	Prec@1 88.281 (83.033)	
Epoch: [1][389/391]	LR: 0.08	DT: 0.000 (4.215)	BT: 0.023 (4.246)	Loss 0.5117 (0.4926)	Prec@1 82.812 (83.197)	
Total train loss: 0.4925
Avg Loading time: 4.2040 seconds
Avg Batch time: 4.2348 seconds

Train time: 1655.8688147068024
 * Prec@1 71.820 Prec@5 98.990 Loss 0.8784
Avg Loading time: 3.0376 seconds
Avg Batch time: 3.0527 seconds

Best acc: 71.820
--------------------------------------------------------------------------------
Test time: 242.12473726272583

Epoch: [2][77/391]	LR: 0.08	DT: 2.459 (2.724)	BT: 2.499 (2.756)	Loss 0.3984 (0.4500)	Prec@1 88.281 (84.405)	
Epoch: [2][155/391]	LR: 0.08	DT: 0.000 (2.529)	BT: 0.027 (2.560)	Loss 0.3958 (0.4434)	Prec@1 85.938 (84.660)	
Epoch: [2][233/391]	LR: 0.08	DT: 0.000 (2.179)	BT: 0.027 (2.210)	Loss 0.3291 (0.4391)	Prec@1 90.625 (84.936)	
Epoch: [2][311/391]	LR: 0.08	DT: 0.000 (1.957)	BT: 0.027 (1.989)	Loss 0.4314 (0.4313)	Prec@1 83.594 (85.204)	
Epoch: [2][389/391]	LR: 0.08	DT: 0.000 (1.812)	BT: 0.025 (1.843)	Loss 0.3789 (0.4237)	Prec@1 86.719 (85.461)	
Total train loss: 0.4236
Avg Loading time: 1.8078 seconds
Avg Batch time: 1.8388 seconds

Train time: 719.0136685371399
 * Prec@1 65.990 Prec@5 97.760 Loss 0.9570
Avg Loading time: 1.9329 seconds
Avg Batch time: 1.9457 seconds

Best acc: 71.820
--------------------------------------------------------------------------------
Test time: 154.26658177375793

Epoch: [3][77/391]	LR: 0.08	DT: 0.000 (2.776)	BT: 0.026 (2.808)	Loss 0.3442 (0.3575)	Prec@1 86.719 (87.660)	
Epoch: [3][155/391]	LR: 0.08	DT: 0.000 (2.895)	BT: 0.026 (2.928)	Loss 0.4609 (0.3583)	Prec@1 85.156 (87.595)	
Epoch: [3][233/391]	LR: 0.08	DT: 0.257 (3.026)	BT: 0.292 (3.058)	Loss 0.3167 (0.3608)	Prec@1 87.500 (87.523)	
Epoch: [3][311/391]	LR: 0.08	DT: 0.000 (3.117)	BT: 0.027 (3.149)	Loss 0.4597 (0.3653)	Prec@1 85.156 (87.337)	
Epoch: [3][389/391]	LR: 0.08	DT: 0.000 (3.138)	BT: 0.025 (3.170)	Loss 0.3149 (0.3649)	Prec@1 92.188 (87.372)	
Total train loss: 0.3650
Avg Loading time: 3.1303 seconds
Avg Batch time: 3.1621 seconds

Train time: 1236.4125187397003
 * Prec@1 83.900 Prec@5 99.360 Loss 0.4807
Avg Loading time: 3.4395 seconds
Avg Batch time: 3.4536 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 273.85018515586853

Epoch: [4][77/391]	LR: 0.08	DT: 0.000 (3.345)	BT: 0.026 (3.378)	Loss 0.3994 (0.3306)	Prec@1 86.719 (88.612)	
Epoch: [4][155/391]	LR: 0.08	DT: 0.000 (3.307)	BT: 0.026 (3.340)	Loss 0.2791 (0.3226)	Prec@1 89.844 (88.842)	
Epoch: [4][233/391]	LR: 0.08	DT: 4.915 (3.216)	BT: 4.949 (3.249)	Loss 0.2448 (0.3276)	Prec@1 91.406 (88.652)	
Epoch: [4][311/391]	LR: 0.08	DT: 0.472 (3.058)	BT: 0.505 (3.091)	Loss 0.3523 (0.3308)	Prec@1 85.156 (88.544)	
Epoch: [4][389/391]	LR: 0.08	DT: 0.000 (3.003)	BT: 0.026 (3.035)	Loss 0.2683 (0.3282)	Prec@1 91.406 (88.652)	
Total train loss: 0.3283
Avg Loading time: 2.9955 seconds
Avg Batch time: 3.0277 seconds

Train time: 1183.902898311615
 * Prec@1 48.890 Prec@5 89.520 Loss 1.8545
Avg Loading time: 2.8767 seconds
Avg Batch time: 2.8915 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 228.97558546066284

Epoch: [5][77/391]	LR: 0.08	DT: 0.000 (2.859)	BT: 0.026 (2.891)	Loss 0.2551 (0.2899)	Prec@1 89.844 (90.024)	
Epoch: [5][155/391]	LR: 0.08	DT: 2.181 (2.867)	BT: 2.217 (2.899)	Loss 0.3772 (0.2914)	Prec@1 85.938 (89.884)	
Epoch: [5][233/391]	LR: 0.08	DT: 7.709 (2.646)	BT: 7.746 (2.677)	Loss 0.3293 (0.3022)	Prec@1 86.719 (89.446)	
Epoch: [5][311/391]	LR: 0.08	DT: 0.000 (2.393)	BT: 0.026 (2.424)	Loss 0.4902 (0.3014)	Prec@1 86.719 (89.526)	
Epoch: [5][389/391]	LR: 0.08	DT: 0.000 (2.217)	BT: 0.027 (2.247)	Loss 0.3748 (0.3091)	Prec@1 88.281 (89.281)	
Total train loss: 0.3092
Avg Loading time: 2.2111 seconds
Avg Batch time: 2.2418 seconds

Train time: 876.5977921485901
 * Prec@1 49.840 Prec@5 95.610 Loss 2.3965
Avg Loading time: 2.6499 seconds
Avg Batch time: 2.6643 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 210.99912309646606

Epoch: [6][77/391]	LR: 0.08	DT: 0.000 (3.087)	BT: 0.028 (3.119)	Loss 0.3037 (0.3268)	Prec@1 88.281 (88.532)	
Epoch: [6][155/391]	LR: 0.08	DT: 0.000 (3.176)	BT: 0.027 (3.208)	Loss 0.2122 (0.3153)	Prec@1 92.188 (89.068)	
Epoch: [6][233/391]	LR: 0.08	DT: 0.000 (3.227)	BT: 0.028 (3.259)	Loss 0.2903 (0.3148)	Prec@1 89.062 (89.049)	
Epoch: [6][311/391]	LR: 0.08	DT: 0.000 (3.215)	BT: 0.027 (3.248)	Loss 0.3093 (0.3120)	Prec@1 92.188 (89.120)	
Epoch: [6][389/391]	LR: 0.08	DT: 0.000 (3.241)	BT: 0.026 (3.274)	Loss 0.4207 (0.3186)	Prec@1 84.375 (88.892)	
Total train loss: 0.3186
Avg Loading time: 3.2329 seconds
Avg Batch time: 3.2653 seconds

Train time: 1276.7692654132843
 * Prec@1 83.050 Prec@5 99.120 Loss 0.5093
Avg Loading time: 2.8497 seconds
Avg Batch time: 2.8638 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 226.77838253974915

Epoch: [7][77/391]	LR: 0.08	DT: 0.000 (2.896)	BT: 0.026 (2.928)	Loss 0.2664 (0.2705)	Prec@1 92.188 (90.685)	
Epoch: [7][155/391]	LR: 0.08	DT: 0.000 (2.870)	BT: 0.026 (2.902)	Loss 0.2458 (0.2751)	Prec@1 92.188 (90.600)	
Epoch: [7][233/391]	LR: 0.08	DT: 8.179 (2.822)	BT: 8.218 (2.853)	Loss 0.2871 (0.2755)	Prec@1 87.500 (90.411)	
Epoch: [7][311/391]	LR: 0.08	DT: 0.000 (2.783)	BT: 0.026 (2.814)	Loss 0.3218 (0.2769)	Prec@1 89.062 (90.312)	
Epoch: [7][389/391]	LR: 0.08	DT: 0.000 (2.853)	BT: 0.026 (2.883)	Loss 0.3162 (0.2831)	Prec@1 89.062 (90.108)	
Total train loss: 0.2831
Avg Loading time: 2.8454 seconds
Avg Batch time: 2.8761 seconds

Train time: 1124.6108076572418
 * Prec@1 16.590 Prec@5 71.210 Loss 4.6094
Avg Loading time: 3.4013 seconds
Avg Batch time: 3.4156 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 270.33428955078125

Epoch: [8][77/391]	LR: 0.08	DT: 1.619 (4.050)	BT: 1.652 (4.081)	Loss 0.2981 (0.2905)	Prec@1 89.062 (89.483)	
Epoch: [8][155/391]	LR: 0.08	DT: 0.000 (3.748)	BT: 0.025 (3.780)	Loss 0.3083 (0.2949)	Prec@1 89.062 (89.578)	
Epoch: [8][233/391]	LR: 0.08	DT: 7.309 (3.350)	BT: 7.343 (3.382)	Loss 0.2886 (0.2908)	Prec@1 89.844 (89.820)	
Epoch: [8][311/391]	LR: 0.08	DT: 0.000 (2.979)	BT: 0.027 (3.011)	Loss 0.3469 (0.2931)	Prec@1 90.625 (89.711)	
Epoch: [8][389/391]	LR: 0.08	DT: 0.000 (2.802)	BT: 0.026 (2.833)	Loss 0.3311 (0.2897)	Prec@1 87.500 (89.886)	
Total train loss: 0.2898
Avg Loading time: 2.7949 seconds
Avg Batch time: 2.8263 seconds

Train time: 1105.1233115196228
 * Prec@1 77.560 Prec@5 97.740 Loss 0.7197
Avg Loading time: 3.3267 seconds
Avg Batch time: 3.3408 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 264.43887543678284

Epoch: [9][77/391]	LR: 0.08	DT: 0.000 (3.609)	BT: 0.026 (3.641)	Loss 0.4409 (0.2570)	Prec@1 85.938 (90.976)	
Epoch: [9][155/391]	LR: 0.08	DT: 2.290 (3.981)	BT: 2.326 (4.014)	Loss 0.2385 (0.2899)	Prec@1 91.406 (89.829)	
Epoch: [9][233/391]	LR: 0.08	DT: 0.000 (3.948)	BT: 0.028 (3.980)	Loss 0.4583 (0.3067)	Prec@1 84.375 (89.229)	
Epoch: [9][311/391]	LR: 0.08	DT: 0.000 (3.744)	BT: 0.029 (3.776)	Loss 0.2537 (0.3069)	Prec@1 89.844 (89.253)	
Epoch: [9][389/391]	LR: 0.08	DT: 0.000 (3.644)	BT: 0.026 (3.676)	Loss 0.2007 (0.3110)	Prec@1 92.969 (89.143)	
Total train loss: 0.3109
Avg Loading time: 3.6349 seconds
Avg Batch time: 3.6670 seconds

Train time: 1433.8675034046173
 * Prec@1 75.640 Prec@5 97.350 Loss 0.7217
Avg Loading time: 3.6761 seconds
Avg Batch time: 3.6902 seconds

Best acc: 83.900
--------------------------------------------------------------------------------
Test time: 292.0370752811432

Epoch: [10][77/391]	LR: 0.016	DT: 0.000 (3.717)	BT: 0.026 (3.749)	Loss 0.2717 (0.2752)	Prec@1 87.500 (90.365)	
Epoch: [10][155/391]	LR: 0.016	DT: 0.000 (3.667)	BT: 0.027 (3.699)	Loss 0.1495 (0.2650)	Prec@1 96.094 (90.840)	
Epoch: [10][233/391]	LR: 0.016	DT: 0.000 (3.644)	BT: 0.027 (3.676)	Loss 0.3245 (0.2658)	Prec@1 90.625 (90.769)	
Epoch: [10][311/391]	LR: 0.016	DT: 0.000 (3.575)	BT: 0.025 (3.607)	Loss 0.2537 (0.2656)	Prec@1 92.188 (90.850)	
Epoch: [10][389/391]	LR: 0.016	DT: 0.000 (3.553)	BT: 0.026 (3.585)	Loss 0.2524 (0.2638)	Prec@1 91.406 (90.909)	
Total train loss: 0.2638
Avg Loading time: 3.5439 seconds
Avg Batch time: 3.5761 seconds

Train time: 1398.3024969100952
 * Prec@1 87.260 Prec@5 99.670 Loss 0.3818
Avg Loading time: 3.2414 seconds
Avg Batch time: 3.2555 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 258.53194212913513

Epoch: [11][77/391]	LR: 0.016	DT: 0.000 (2.850)	BT: 0.026 (2.883)	Loss 0.2233 (0.2635)	Prec@1 92.188 (90.885)	
Epoch: [11][155/391]	LR: 0.016	DT: 0.000 (2.407)	BT: 0.025 (2.439)	Loss 0.3381 (0.2587)	Prec@1 88.281 (91.066)	
Epoch: [11][233/391]	LR: 0.016	DT: 0.728 (2.194)	BT: 0.767 (2.225)	Loss 0.3088 (0.2623)	Prec@1 90.625 (90.986)	
Epoch: [11][311/391]	LR: 0.016	DT: 0.000 (2.047)	BT: 0.026 (2.078)	Loss 0.1830 (0.2624)	Prec@1 92.188 (90.948)	
Epoch: [11][389/391]	LR: 0.016	DT: 0.000 (2.212)	BT: 0.026 (2.243)	Loss 0.3711 (0.2624)	Prec@1 85.938 (90.929)	
Total train loss: 0.2621
Avg Loading time: 2.2063 seconds
Avg Batch time: 2.2373 seconds

Train time: 874.8545191287994
 * Prec@1 84.540 Prec@5 99.480 Loss 0.4348
Avg Loading time: 2.5574 seconds
Avg Batch time: 2.5704 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 203.5526065826416

Epoch: [12][77/391]	LR: 0.016	DT: 0.000 (3.203)	BT: 0.024 (3.234)	Loss 0.3098 (0.2720)	Prec@1 89.062 (90.675)	
Epoch: [12][155/391]	LR: 0.016	DT: 0.000 (3.225)	BT: 0.026 (3.255)	Loss 0.1648 (0.2669)	Prec@1 93.750 (91.026)	
Epoch: [12][233/391]	LR: 0.016	DT: 0.000 (3.337)	BT: 0.027 (3.367)	Loss 0.1881 (0.2687)	Prec@1 92.188 (90.905)	
Epoch: [12][311/391]	LR: 0.016	DT: 0.000 (3.389)	BT: 0.024 (3.419)	Loss 0.3462 (0.2715)	Prec@1 88.281 (90.725)	
Epoch: [12][389/391]	LR: 0.016	DT: 0.165 (3.438)	BT: 0.191 (3.469)	Loss 0.2815 (0.2748)	Prec@1 87.500 (90.601)	
Total train loss: 0.2750
Avg Loading time: 3.4297 seconds
Avg Batch time: 3.4606 seconds

Train time: 1353.1582608222961
 * Prec@1 86.120 Prec@5 99.600 Loss 0.4211
Avg Loading time: 3.6475 seconds
Avg Batch time: 3.6624 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 289.84750628471375

Epoch: [13][77/391]	LR: 0.016	DT: 0.126 (3.700)	BT: 0.152 (3.734)	Loss 0.2015 (0.2746)	Prec@1 93.750 (90.355)	
Epoch: [13][155/391]	LR: 0.016	DT: 1.669 (3.773)	BT: 1.701 (3.807)	Loss 0.2522 (0.2777)	Prec@1 90.625 (90.355)	
Epoch: [13][233/391]	LR: 0.016	DT: 0.000 (3.790)	BT: 0.026 (3.824)	Loss 0.2158 (0.2783)	Prec@1 92.188 (90.331)	
Epoch: [13][311/391]	LR: 0.016	DT: 0.000 (3.615)	BT: 0.026 (3.648)	Loss 0.2871 (0.2766)	Prec@1 89.062 (90.447)	
Epoch: [13][389/391]	LR: 0.016	DT: 0.080 (3.539)	BT: 0.106 (3.573)	Loss 0.2218 (0.2796)	Prec@1 89.062 (90.333)	
Total train loss: 0.2796
Avg Loading time: 3.5304 seconds
Avg Batch time: 3.5635 seconds

Train time: 1393.363056898117
 * Prec@1 85.260 Prec@5 99.400 Loss 0.4612
Avg Loading time: 2.8890 seconds
Avg Batch time: 2.9021 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 229.81723523139954

Epoch: [14][77/391]	LR: 0.016	DT: 0.000 (2.127)	BT: 0.026 (2.155)	Loss 0.3572 (0.3000)	Prec@1 88.281 (89.373)	
Epoch: [14][155/391]	LR: 0.016	DT: 0.000 (1.856)	BT: 0.025 (1.884)	Loss 0.3901 (0.3012)	Prec@1 89.844 (89.458)	
Epoch: [14][233/391]	LR: 0.016	DT: 0.000 (1.797)	BT: 0.026 (1.824)	Loss 0.2947 (0.2988)	Prec@1 89.844 (89.567)	
Epoch: [14][311/391]	LR: 0.016	DT: 0.000 (2.026)	BT: 0.030 (2.054)	Loss 0.3120 (0.2983)	Prec@1 89.062 (89.583)	
Epoch: [14][389/391]	LR: 0.016	DT: 0.000 (2.194)	BT: 0.022 (2.222)	Loss 0.3479 (0.3024)	Prec@1 85.156 (89.473)	
Total train loss: 0.3024
Avg Loading time: 2.1883 seconds
Avg Batch time: 2.2167 seconds

Train time: 866.7657794952393
 * Prec@1 84.490 Prec@5 99.350 Loss 0.4895
Avg Loading time: 3.7649 seconds
Avg Batch time: 3.7791 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 299.07559633255005

Epoch: [15][77/391]	LR: 0.016	DT: 1.040 (3.724)	BT: 1.079 (3.755)	Loss 0.4143 (0.3300)	Prec@1 84.375 (88.191)	
Epoch: [15][155/391]	LR: 0.016	DT: 0.000 (3.840)	BT: 0.027 (3.872)	Loss 0.1953 (0.3219)	Prec@1 92.969 (88.727)	
Epoch: [15][233/391]	LR: 0.016	DT: 0.000 (3.819)	BT: 0.027 (3.851)	Loss 0.3154 (0.3208)	Prec@1 87.500 (88.802)	
Epoch: [15][311/391]	LR: 0.016	DT: 0.000 (3.719)	BT: 0.025 (3.751)	Loss 0.3284 (0.3210)	Prec@1 85.938 (88.867)	
Epoch: [15][389/391]	LR: 0.016	DT: 0.000 (3.717)	BT: 0.025 (3.749)	Loss 0.2717 (0.3190)	Prec@1 85.938 (88.882)	
Total train loss: 0.3193
Avg Loading time: 3.7073 seconds
Avg Batch time: 3.7391 seconds

Train time: 1462.0232734680176
 * Prec@1 84.290 Prec@5 99.130 Loss 0.4968
Avg Loading time: 3.4640 seconds
Avg Batch time: 3.4777 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 275.2772283554077

Epoch: [16][77/391]	LR: 0.016	DT: 0.000 (3.139)	BT: 0.025 (3.172)	Loss 0.2864 (0.3061)	Prec@1 88.281 (89.433)	
Epoch: [16][155/391]	LR: 0.016	DT: 0.061 (3.154)	BT: 0.087 (3.186)	Loss 0.3677 (0.3190)	Prec@1 89.062 (88.912)	
Epoch: [16][233/391]	LR: 0.016	DT: 0.000 (3.106)	BT: 0.026 (3.138)	Loss 0.3188 (0.3232)	Prec@1 89.062 (88.802)	
Epoch: [16][311/391]	LR: 0.016	DT: 0.000 (3.081)	BT: 0.025 (3.112)	Loss 0.4153 (0.3230)	Prec@1 83.594 (88.780)	
Epoch: [16][389/391]	LR: 0.016	DT: 0.000 (3.108)	BT: 0.025 (3.140)	Loss 0.3044 (0.3194)	Prec@1 87.500 (88.866)	
Total train loss: 0.3194
Avg Loading time: 3.1004 seconds
Avg Batch time: 3.1316 seconds

Train time: 1224.5195667743683
 * Prec@1 85.970 Prec@5 99.490 Loss 0.4167
Avg Loading time: 2.7531 seconds
Avg Batch time: 2.7662 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 219.05810928344727

Epoch: [17][77/391]	LR: 0.016	DT: 0.000 (2.362)	BT: 0.026 (2.393)	Loss 0.3154 (0.3160)	Prec@1 89.062 (89.012)	
Epoch: [17][155/391]	LR: 0.016	DT: 2.123 (2.383)	BT: 2.160 (2.414)	Loss 0.3062 (0.3092)	Prec@1 89.844 (89.118)	
Epoch: [17][233/391]	LR: 0.016	DT: 11.665 (2.770)	BT: 11.702 (2.801)	Loss 0.2756 (0.3048)	Prec@1 91.406 (89.286)	
Epoch: [17][311/391]	LR: 0.016	DT: 0.000 (2.793)	BT: 0.025 (2.824)	Loss 0.3298 (0.3041)	Prec@1 87.500 (89.328)	
Epoch: [17][389/391]	LR: 0.016	DT: 0.000 (2.886)	BT: 0.025 (2.917)	Loss 0.3882 (0.3025)	Prec@1 86.719 (89.353)	
Total train loss: 0.3024
Avg Loading time: 2.8789 seconds
Avg Batch time: 2.9097 seconds

Train time: 1137.7524816989899
 * Prec@1 85.890 Prec@5 99.510 Loss 0.4192
Avg Loading time: 3.3704 seconds
Avg Batch time: 3.3853 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 267.98024225234985

Epoch: [18][77/391]	LR: 0.016	DT: 0.000 (3.383)	BT: 0.025 (3.415)	Loss 0.2734 (0.2941)	Prec@1 90.625 (89.854)	
Epoch: [18][155/391]	LR: 0.016	DT: 2.302 (3.058)	BT: 2.335 (3.089)	Loss 0.2461 (0.3026)	Prec@1 91.406 (89.673)	
Epoch: [18][233/391]	LR: 0.016	DT: 2.270 (2.961)	BT: 2.303 (2.992)	Loss 0.2419 (0.2994)	Prec@1 92.188 (89.744)	
Epoch: [18][311/391]	LR: 0.016	DT: 0.000 (2.895)	BT: 0.026 (2.926)	Loss 0.3535 (0.3008)	Prec@1 88.281 (89.628)	
Epoch: [18][389/391]	LR: 0.016	DT: 0.000 (2.819)	BT: 0.025 (2.849)	Loss 0.3528 (0.3046)	Prec@1 84.375 (89.459)	
Total train loss: 0.3045
Avg Loading time: 2.8114 seconds
Avg Batch time: 2.8415 seconds

Train time: 1111.0906357765198
 * Prec@1 84.710 Prec@5 99.390 Loss 0.4448
Avg Loading time: 3.0487 seconds
Avg Batch time: 3.0629 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 242.51398253440857

Epoch: [19][77/391]	LR: 0.016	DT: 0.000 (3.203)	BT: 0.025 (3.236)	Loss 0.2578 (0.2992)	Prec@1 92.969 (89.393)	
Epoch: [19][155/391]	LR: 0.016	DT: 0.550 (3.246)	BT: 0.586 (3.280)	Loss 0.3567 (0.3110)	Prec@1 88.281 (89.088)	
Epoch: [19][233/391]	LR: 0.016	DT: 6.027 (3.254)	BT: 6.062 (3.288)	Loss 0.3030 (0.3225)	Prec@1 87.500 (88.685)	
Epoch: [19][311/391]	LR: 0.016	DT: 0.000 (3.214)	BT: 0.026 (3.247)	Loss 0.3748 (0.3260)	Prec@1 88.281 (88.597)	
Epoch: [19][389/391]	LR: 0.016	DT: 0.000 (3.041)	BT: 0.025 (3.074)	Loss 0.3191 (0.3269)	Prec@1 88.281 (88.598)	
Total train loss: 0.3272
Avg Loading time: 3.0332 seconds
Avg Batch time: 3.0658 seconds

Train time: 1198.7732610702515
 * Prec@1 85.240 Prec@5 99.490 Loss 0.4380
Avg Loading time: 2.3529 seconds
Avg Batch time: 2.3661 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 187.45422053337097

Epoch: [20][77/391]	LR: 0.0032	DT: 0.008 (1.986)	BT: 0.033 (2.016)	Loss 0.2942 (0.3068)	Prec@1 89.844 (89.724)	
Epoch: [20][155/391]	LR: 0.0032	DT: 2.838 (2.206)	BT: 2.876 (2.238)	Loss 0.2883 (0.3181)	Prec@1 91.406 (89.153)	
Epoch: [20][233/391]	LR: 0.0032	DT: 1.705 (2.333)	BT: 1.735 (2.364)	Loss 0.2330 (0.3177)	Prec@1 92.188 (89.193)	
Epoch: [20][311/391]	LR: 0.0032	DT: 0.000 (2.318)	BT: 0.026 (2.350)	Loss 0.3369 (0.3156)	Prec@1 90.625 (89.135)	
Epoch: [20][389/391]	LR: 0.0032	DT: 0.000 (2.335)	BT: 0.025 (2.366)	Loss 0.3054 (0.3144)	Prec@1 89.844 (89.163)	
Total train loss: 0.3142
Avg Loading time: 2.3290 seconds
Avg Batch time: 2.3603 seconds

Train time: 922.9103119373322
 * Prec@1 86.230 Prec@5 99.470 Loss 0.4165
Avg Loading time: 2.7141 seconds
Avg Batch time: 2.7287 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 216.1371545791626

Epoch: [21][77/391]	LR: 0.0032	DT: 0.000 (2.758)	BT: 0.026 (2.790)	Loss 0.3447 (0.3155)	Prec@1 87.500 (88.582)	
Epoch: [21][155/391]	LR: 0.0032	DT: 0.000 (2.791)	BT: 0.026 (2.822)	Loss 0.2025 (0.3115)	Prec@1 92.969 (89.007)	
Epoch: [21][233/391]	LR: 0.0032	DT: 0.000 (2.861)	BT: 0.026 (2.892)	Loss 0.2010 (0.3118)	Prec@1 95.312 (89.036)	
Epoch: [21][311/391]	LR: 0.0032	DT: 0.000 (2.888)	BT: 0.021 (2.919)	Loss 0.2456 (0.3114)	Prec@1 92.188 (89.045)	
Epoch: [21][389/391]	LR: 0.0032	DT: 0.000 (2.950)	BT: 0.026 (2.980)	Loss 0.2153 (0.3110)	Prec@1 92.969 (89.073)	
Total train loss: 0.3110
Avg Loading time: 2.9422 seconds
Avg Batch time: 2.9720 seconds

Train time: 1162.1172504425049
 * Prec@1 86.340 Prec@5 99.510 Loss 0.4155
Avg Loading time: 3.3010 seconds
Avg Batch time: 3.3147 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 262.3857171535492

Epoch: [22][77/391]	LR: 0.0032	DT: 1.371 (3.390)	BT: 1.398 (3.423)	Loss 0.3508 (0.3129)	Prec@1 88.281 (89.143)	
Epoch: [22][155/391]	LR: 0.0032	DT: 0.000 (3.336)	BT: 0.027 (3.369)	Loss 0.3633 (0.3107)	Prec@1 85.156 (89.228)	
Epoch: [22][233/391]	LR: 0.0032	DT: 0.000 (3.334)	BT: 0.026 (3.367)	Loss 0.3142 (0.3118)	Prec@1 89.062 (89.209)	
Epoch: [22][311/391]	LR: 0.0032	DT: 0.000 (3.212)	BT: 0.025 (3.244)	Loss 0.2394 (0.3103)	Prec@1 89.844 (89.290)	
Epoch: [22][389/391]	LR: 0.0032	DT: 0.000 (3.100)	BT: 0.026 (3.132)	Loss 0.3069 (0.3120)	Prec@1 89.844 (89.123)	
Total train loss: 0.3117
Avg Loading time: 3.0921 seconds
Avg Batch time: 3.1238 seconds

Train time: 1221.4834969043732
 * Prec@1 86.200 Prec@5 99.480 Loss 0.4136
Avg Loading time: 2.0925 seconds
Avg Batch time: 2.1053 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 166.8515386581421

Epoch: [23][77/391]	LR: 0.0032	DT: 0.000 (1.695)	BT: 0.025 (1.722)	Loss 0.3477 (0.3141)	Prec@1 86.719 (88.782)	
Epoch: [23][155/391]	LR: 0.0032	DT: 0.000 (2.026)	BT: 0.027 (2.054)	Loss 0.3899 (0.3214)	Prec@1 89.062 (88.872)	
Epoch: [23][233/391]	LR: 0.0032	DT: 3.424 (2.286)	BT: 3.460 (2.314)	Loss 0.4856 (0.3144)	Prec@1 86.719 (89.179)	
Epoch: [23][311/391]	LR: 0.0032	DT: 0.000 (2.257)	BT: 0.025 (2.286)	Loss 0.3970 (0.3124)	Prec@1 86.719 (89.243)	
Epoch: [23][389/391]	LR: 0.0032	DT: 0.000 (2.297)	BT: 0.025 (2.326)	Loss 0.4485 (0.3104)	Prec@1 85.938 (89.291)	
Total train loss: 0.3104
Avg Loading time: 2.2912 seconds
Avg Batch time: 2.3202 seconds

Train time: 907.2582552433014
 * Prec@1 86.140 Prec@5 99.490 Loss 0.4155
Avg Loading time: 3.0029 seconds
Avg Batch time: 3.0160 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 238.7972502708435

Epoch: [24][77/391]	LR: 0.0032	DT: 0.000 (3.221)	BT: 0.025 (3.250)	Loss 0.2415 (0.3147)	Prec@1 89.844 (89.002)	
Epoch: [24][155/391]	LR: 0.0032	DT: 0.000 (3.305)	BT: 0.025 (3.335)	Loss 0.3206 (0.3157)	Prec@1 89.062 (89.078)	
Epoch: [24][233/391]	LR: 0.0032	DT: 0.000 (3.353)	BT: 0.027 (3.384)	Loss 0.2832 (0.3089)	Prec@1 90.625 (89.276)	
Epoch: [24][311/391]	LR: 0.0032	DT: 0.000 (3.248)	BT: 0.025 (3.278)	Loss 0.3716 (0.3071)	Prec@1 90.625 (89.315)	
Epoch: [24][389/391]	LR: 0.0032	DT: 0.000 (3.226)	BT: 0.027 (3.256)	Loss 0.3113 (0.3101)	Prec@1 89.844 (89.233)	
Total train loss: 0.3102
Avg Loading time: 3.2175 seconds
Avg Batch time: 3.2473 seconds

Train time: 1269.7605345249176
 * Prec@1 86.100 Prec@5 99.490 Loss 0.4160
Avg Loading time: 3.1310 seconds
Avg Batch time: 3.1452 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 249.0365912914276

Epoch: [25][77/391]	LR: 0.0032	DT: 0.000 (3.308)	BT: 0.025 (3.341)	Loss 0.1959 (0.3183)	Prec@1 92.969 (88.882)	
Epoch: [25][155/391]	LR: 0.0032	DT: 0.000 (3.172)	BT: 0.025 (3.203)	Loss 0.2383 (0.3080)	Prec@1 92.188 (89.243)	
Epoch: [25][233/391]	LR: 0.0032	DT: 9.762 (3.044)	BT: 9.796 (3.075)	Loss 0.3186 (0.3095)	Prec@1 89.062 (89.296)	
Epoch: [25][311/391]	LR: 0.0032	DT: 0.000 (2.926)	BT: 0.026 (2.957)	Loss 0.3967 (0.3090)	Prec@1 86.719 (89.180)	
Epoch: [25][389/391]	LR: 0.0032	DT: 0.000 (2.758)	BT: 0.025 (2.789)	Loss 0.1653 (0.3080)	Prec@1 94.531 (89.271)	
Total train loss: 0.3080
Avg Loading time: 2.7512 seconds
Avg Batch time: 2.7816 seconds

Train time: 1087.643724679947
 * Prec@1 86.170 Prec@5 99.470 Loss 0.4136
Avg Loading time: 1.9704 seconds
Avg Batch time: 1.9823 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 157.1410722732544

Epoch: [26][77/391]	LR: 0.0032	DT: 1.294 (1.971)	BT: 1.332 (2.000)	Loss 0.3291 (0.3102)	Prec@1 88.281 (89.133)	
Epoch: [26][155/391]	LR: 0.0032	DT: 0.168 (2.286)	BT: 0.193 (2.316)	Loss 0.3862 (0.3061)	Prec@1 87.500 (89.288)	
Epoch: [26][233/391]	LR: 0.0032	DT: 25.027 (2.599)	BT: 25.083 (2.629)	Loss 0.3875 (0.3078)	Prec@1 88.281 (89.203)	
Epoch: [26][311/391]	LR: 0.0032	DT: 0.001 (2.619)	BT: 0.023 (2.648)	Loss 0.2874 (0.3089)	Prec@1 89.062 (89.213)	
Epoch: [26][389/391]	LR: 0.0032	DT: 0.000 (2.663)	BT: 0.038 (2.693)	Loss 0.2600 (0.3089)	Prec@1 90.625 (89.263)	
Total train loss: 0.3088
Avg Loading time: 2.6565 seconds
Avg Batch time: 2.6865 seconds

Train time: 1050.4955592155457
 * Prec@1 86.100 Prec@5 99.470 Loss 0.4155
Avg Loading time: 3.4187 seconds
Avg Batch time: 3.4321 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 271.6466977596283

Epoch: [27][77/391]	LR: 0.0032	DT: 0.000 (3.735)	BT: 0.026 (3.769)	Loss 0.3098 (0.3024)	Prec@1 91.406 (89.683)	
Epoch: [27][155/391]	LR: 0.0032	DT: 1.512 (3.688)	BT: 1.539 (3.722)	Loss 0.3362 (0.3040)	Prec@1 88.281 (89.553)	
Epoch: [27][233/391]	LR: 0.0032	DT: 0.000 (3.668)	BT: 0.026 (3.701)	Loss 0.2930 (0.3063)	Prec@1 90.625 (89.423)	
Epoch: [27][311/391]	LR: 0.0032	DT: 0.010 (3.572)	BT: 0.038 (3.605)	Loss 0.3015 (0.3075)	Prec@1 89.844 (89.305)	
Epoch: [27][389/391]	LR: 0.0032	DT: 0.000 (3.450)	BT: 0.025 (3.483)	Loss 0.2294 (0.3095)	Prec@1 90.625 (89.265)	
Total train loss: 0.3097
Avg Loading time: 3.4407 seconds
Avg Batch time: 3.4739 seconds

Train time: 1358.362582206726
 * Prec@1 86.190 Prec@5 99.540 Loss 0.4143
Avg Loading time: 2.9994 seconds
Avg Batch time: 3.0133 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 238.5851194858551

Epoch: [28][77/391]	LR: 0.0032	DT: 0.000 (3.174)	BT: 0.025 (3.207)	Loss 0.2859 (0.2999)	Prec@1 88.281 (89.513)	
Epoch: [28][155/391]	LR: 0.0032	DT: 0.000 (3.142)	BT: 0.025 (3.175)	Loss 0.2380 (0.3037)	Prec@1 91.406 (89.353)	
Epoch: [28][233/391]	LR: 0.0032	DT: 3.169 (3.231)	BT: 3.205 (3.263)	Loss 0.3396 (0.3050)	Prec@1 92.188 (89.386)	
Epoch: [28][311/391]	LR: 0.0032	DT: 0.000 (3.061)	BT: 0.025 (3.092)	Loss 0.3345 (0.3047)	Prec@1 85.156 (89.416)	
Epoch: [28][389/391]	LR: 0.0032	DT: 0.000 (2.926)	BT: 0.025 (2.957)	Loss 0.2441 (0.3081)	Prec@1 92.188 (89.301)	
Total train loss: 0.3081
Avg Loading time: 2.9187 seconds
Avg Batch time: 2.9497 seconds

Train time: 1153.3759870529175
 * Prec@1 86.150 Prec@5 99.510 Loss 0.4153
Avg Loading time: 2.3050 seconds
Avg Batch time: 2.3177 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 183.63897919654846

Epoch: [29][77/391]	LR: 0.0032	DT: 0.000 (2.856)	BT: 0.026 (2.888)	Loss 0.3250 (0.3088)	Prec@1 88.281 (89.293)	
Epoch: [29][155/391]	LR: 0.0032	DT: 0.000 (3.021)	BT: 0.026 (3.053)	Loss 0.2935 (0.3087)	Prec@1 89.062 (89.333)	
Epoch: [29][233/391]	LR: 0.0032	DT: 0.871 (2.999)	BT: 0.905 (3.031)	Loss 0.2722 (0.3074)	Prec@1 92.969 (89.303)	
Epoch: [29][311/391]	LR: 0.0032	DT: 0.000 (2.883)	BT: 0.026 (2.915)	Loss 0.3792 (0.3076)	Prec@1 86.719 (89.340)	
Epoch: [29][389/391]	LR: 0.0032	DT: 0.000 (2.935)	BT: 0.026 (2.967)	Loss 0.3801 (0.3094)	Prec@1 84.375 (89.233)	
Total train loss: 0.3094
Avg Loading time: 2.9279 seconds
Avg Batch time: 2.9592 seconds

Train time: 1157.1172552108765
 * Prec@1 86.120 Prec@5 99.450 Loss 0.4131
Avg Loading time: 3.0630 seconds
Avg Batch time: 3.0772 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 243.63943123817444

Epoch: [30][77/391]	LR: 0.00064	DT: 0.000 (2.689)	BT: 0.025 (2.720)	Loss 0.2336 (0.3090)	Prec@1 92.969 (89.463)	
Epoch: [30][155/391]	LR: 0.00064	DT: 0.000 (2.721)	BT: 0.026 (2.753)	Loss 0.2483 (0.3089)	Prec@1 91.406 (89.303)	
Epoch: [30][233/391]	LR: 0.00064	DT: 0.540 (2.753)	BT: 0.566 (2.785)	Loss 0.2766 (0.3055)	Prec@1 89.844 (89.430)	
Epoch: [30][311/391]	LR: 0.00064	DT: 0.000 (2.711)	BT: 0.025 (2.743)	Loss 0.3276 (0.3060)	Prec@1 89.844 (89.416)	
Epoch: [30][389/391]	LR: 0.00064	DT: 0.000 (2.760)	BT: 0.025 (2.791)	Loss 0.4150 (0.3065)	Prec@1 85.156 (89.345)	
Total train loss: 0.3067
Avg Loading time: 2.7527 seconds
Avg Batch time: 2.7842 seconds

Train time: 1088.6617827415466
 * Prec@1 86.090 Prec@5 99.500 Loss 0.4143
Avg Loading time: 3.2581 seconds
Avg Batch time: 3.2729 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 259.0960268974304

Epoch: [31][77/391]	LR: 0.00064	DT: 0.000 (3.368)	BT: 0.026 (3.399)	Loss 0.3479 (0.3172)	Prec@1 85.156 (89.153)	
Epoch: [31][155/391]	LR: 0.00064	DT: 0.000 (3.326)	BT: 0.025 (3.357)	Loss 0.3870 (0.3053)	Prec@1 86.719 (89.498)	
Epoch: [31][233/391]	LR: 0.00064	DT: 8.315 (3.282)	BT: 8.342 (3.312)	Loss 0.2603 (0.3077)	Prec@1 90.625 (89.497)	
Epoch: [31][311/391]	LR: 0.00064	DT: 0.000 (2.966)	BT: 0.025 (2.996)	Loss 0.2751 (0.3058)	Prec@1 90.625 (89.566)	
Epoch: [31][389/391]	LR: 0.00064	DT: 0.000 (2.814)	BT: 0.023 (2.843)	Loss 0.3904 (0.3080)	Prec@1 88.281 (89.427)	
Total train loss: 0.3080
Avg Loading time: 2.8067 seconds
Avg Batch time: 2.8361 seconds

Train time: 1108.961773633957
 * Prec@1 86.150 Prec@5 99.480 Loss 0.4121
Avg Loading time: 2.3225 seconds
Avg Batch time: 2.3353 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 185.0220205783844

Epoch: [32][77/391]	LR: 0.00064	DT: 0.448 (2.909)	BT: 0.481 (2.939)	Loss 0.2202 (0.2978)	Prec@1 92.188 (89.834)	
Epoch: [32][155/391]	LR: 0.00064	DT: 0.424 (2.716)	BT: 0.461 (2.746)	Loss 0.3564 (0.2983)	Prec@1 86.719 (89.628)	
Epoch: [32][233/391]	LR: 0.00064	DT: 0.000 (2.730)	BT: 0.027 (2.761)	Loss 0.3682 (0.3044)	Prec@1 81.250 (89.463)	
Epoch: [32][311/391]	LR: 0.00064	DT: 0.000 (2.711)	BT: 0.029 (2.742)	Loss 0.2761 (0.3041)	Prec@1 89.062 (89.458)	
Epoch: [32][389/391]	LR: 0.00064	DT: 0.384 (2.694)	BT: 0.418 (2.725)	Loss 0.3005 (0.3058)	Prec@1 88.281 (89.367)	
Total train loss: 0.3059
Avg Loading time: 2.6867 seconds
Avg Batch time: 2.7179 seconds

Train time: 1062.7508206367493
 * Prec@1 86.050 Prec@5 99.480 Loss 0.4155
Avg Loading time: 2.7179 seconds
Avg Batch time: 2.7328 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 216.44909238815308

Epoch: [33][77/391]	LR: 0.00064	DT: 0.000 (2.874)	BT: 0.025 (2.906)	Loss 0.3213 (0.3013)	Prec@1 87.500 (89.163)	
Epoch: [33][155/391]	LR: 0.00064	DT: 0.000 (2.975)	BT: 0.027 (3.008)	Loss 0.4514 (0.2994)	Prec@1 85.938 (89.398)	
Epoch: [33][233/391]	LR: 0.00064	DT: 0.000 (3.016)	BT: 0.027 (3.048)	Loss 0.3330 (0.3002)	Prec@1 89.844 (89.450)	
Epoch: [33][311/391]	LR: 0.00064	DT: 0.000 (3.003)	BT: 0.031 (3.035)	Loss 0.2297 (0.3026)	Prec@1 91.406 (89.443)	
Epoch: [33][389/391]	LR: 0.00064	DT: 0.000 (3.036)	BT: 0.026 (3.068)	Loss 0.3147 (0.3047)	Prec@1 89.062 (89.351)	
Total train loss: 0.3048
Avg Loading time: 3.0283 seconds
Avg Batch time: 3.0601 seconds

Train time: 1196.5432343482971
 * Prec@1 86.020 Prec@5 99.480 Loss 0.4148
Avg Loading time: 3.3334 seconds
Avg Batch time: 3.3478 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 265.0006182193756

Epoch: [34][77/391]	LR: 0.00064	DT: 0.000 (3.391)	BT: 0.027 (3.423)	Loss 0.3555 (0.3191)	Prec@1 87.500 (89.193)	
Epoch: [34][155/391]	LR: 0.00064	DT: 0.000 (3.328)	BT: 0.027 (3.360)	Loss 0.2426 (0.3128)	Prec@1 92.969 (89.088)	
Epoch: [34][233/391]	LR: 0.00064	DT: 0.000 (3.180)	BT: 0.026 (3.212)	Loss 0.3311 (0.3123)	Prec@1 88.281 (89.176)	
Epoch: [34][311/391]	LR: 0.00064	DT: 0.000 (2.957)	BT: 0.025 (2.989)	Loss 0.3560 (0.3093)	Prec@1 89.844 (89.350)	
Epoch: [34][389/391]	LR: 0.00064	DT: 0.000 (2.712)	BT: 0.025 (2.743)	Loss 0.3281 (0.3076)	Prec@1 86.719 (89.427)	
Total train loss: 0.3076
Avg Loading time: 2.7048 seconds
Avg Batch time: 2.7357 seconds

Train time: 1069.7200148105621
 * Prec@1 86.240 Prec@5 99.450 Loss 0.4141
Avg Loading time: 1.6326 seconds
Avg Batch time: 1.6459 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 130.56002259254456

Epoch: [35][77/391]	LR: 0.00064	DT: 0.000 (2.396)	BT: 0.028 (2.427)	Loss 0.2006 (0.3052)	Prec@1 94.531 (89.233)	
Epoch: [35][155/391]	LR: 0.00064	DT: 3.273 (2.578)	BT: 3.307 (2.609)	Loss 0.3628 (0.3080)	Prec@1 85.938 (89.248)	
Epoch: [35][233/391]	LR: 0.00064	DT: 0.000 (2.585)	BT: 0.026 (2.616)	Loss 0.2869 (0.3050)	Prec@1 89.844 (89.360)	
Epoch: [35][311/391]	LR: 0.00064	DT: 0.000 (2.499)	BT: 0.027 (2.530)	Loss 0.2097 (0.3032)	Prec@1 91.406 (89.471)	
Epoch: [35][389/391]	LR: 0.00064	DT: 0.000 (2.539)	BT: 0.026 (2.569)	Loss 0.4253 (0.3030)	Prec@1 82.812 (89.427)	
Total train loss: 0.3032
Avg Loading time: 2.5322 seconds
Avg Batch time: 2.5628 seconds

Train time: 1002.1109991073608
 * Prec@1 86.190 Prec@5 99.480 Loss 0.4148
Avg Loading time: 3.2568 seconds
Avg Batch time: 3.2711 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 258.9525637626648

Epoch: [36][77/391]	LR: 0.00064	DT: 0.941 (3.346)	BT: 0.978 (3.380)	Loss 0.2590 (0.3069)	Prec@1 92.969 (89.193)	
Epoch: [36][155/391]	LR: 0.00064	DT: 1.610 (3.359)	BT: 1.646 (3.394)	Loss 0.3564 (0.3093)	Prec@1 85.938 (89.203)	
Epoch: [36][233/391]	LR: 0.00064	DT: 4.302 (3.337)	BT: 4.337 (3.372)	Loss 0.2312 (0.3069)	Prec@1 92.188 (89.286)	
Epoch: [36][311/391]	LR: 0.00064	DT: 0.000 (3.219)	BT: 0.027 (3.253)	Loss 0.2549 (0.3075)	Prec@1 93.750 (89.300)	
Epoch: [36][389/391]	LR: 0.00064	DT: 0.000 (3.198)	BT: 0.025 (3.231)	Loss 0.3103 (0.3061)	Prec@1 89.062 (89.319)	
Total train loss: 0.3060
Avg Loading time: 3.1895 seconds
Avg Batch time: 3.2230 seconds

Train time: 1260.2351467609406
 * Prec@1 86.130 Prec@5 99.450 Loss 0.4155
Avg Loading time: 3.2033 seconds
Avg Batch time: 3.2180 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 254.76266479492188

Epoch: [37][77/391]	LR: 0.00064	DT: 0.757 (2.942)	BT: 0.797 (2.974)	Loss 0.3850 (0.3190)	Prec@1 85.938 (88.742)	
Epoch: [37][155/391]	LR: 0.00064	DT: 1.711 (2.849)	BT: 1.745 (2.882)	Loss 0.3127 (0.3113)	Prec@1 90.625 (89.233)	
Epoch: [37][233/391]	LR: 0.00064	DT: 8.979 (2.777)	BT: 9.012 (2.809)	Loss 0.3054 (0.3081)	Prec@1 89.062 (89.280)	
Epoch: [37][311/391]	LR: 0.00064	DT: 0.000 (2.512)	BT: 0.025 (2.543)	Loss 0.3889 (0.3090)	Prec@1 87.500 (89.243)	
Epoch: [37][389/391]	LR: 0.00064	DT: 1.233 (2.358)	BT: 1.265 (2.389)	Loss 0.3372 (0.3066)	Prec@1 85.938 (89.307)	
Total train loss: 0.3068
Avg Loading time: 2.3517 seconds
Avg Batch time: 2.3827 seconds

Train time: 931.685257434845
 * Prec@1 86.100 Prec@5 99.460 Loss 0.4136
Avg Loading time: 2.0995 seconds
Avg Batch time: 2.1114 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 167.32997727394104

Epoch: [38][77/391]	LR: 0.00064	DT: 0.000 (2.951)	BT: 0.025 (2.982)	Loss 0.3552 (0.2910)	Prec@1 87.500 (89.924)	
Epoch: [38][155/391]	LR: 0.00064	DT: 0.000 (3.087)	BT: 0.026 (3.117)	Loss 0.2009 (0.3018)	Prec@1 89.844 (89.473)	
Epoch: [38][233/391]	LR: 0.00064	DT: 0.000 (3.129)	BT: 0.028 (3.159)	Loss 0.3389 (0.3053)	Prec@1 89.062 (89.406)	
Epoch: [38][311/391]	LR: 0.00064	DT: 0.000 (2.979)	BT: 0.022 (3.009)	Loss 0.3347 (0.3065)	Prec@1 89.844 (89.365)	
Epoch: [38][389/391]	LR: 0.00064	DT: 0.000 (2.928)	BT: 0.025 (2.958)	Loss 0.3552 (0.3067)	Prec@1 85.938 (89.351)	
Total train loss: 0.3067
Avg Loading time: 2.9206 seconds
Avg Batch time: 2.9505 seconds

Train time: 1153.6965627670288
 * Prec@1 86.190 Prec@5 99.490 Loss 0.4167
Avg Loading time: 2.8380 seconds
Avg Batch time: 2.8507 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 225.7508409023285

Epoch: [39][77/391]	LR: 0.00064	DT: 0.000 (3.304)	BT: 0.028 (3.338)	Loss 0.3291 (0.3047)	Prec@1 87.500 (89.283)	
Epoch: [39][155/391]	LR: 0.00064	DT: 0.000 (3.303)	BT: 0.025 (3.336)	Loss 0.2257 (0.3075)	Prec@1 92.188 (89.303)	
Epoch: [39][233/391]	LR: 0.00064	DT: 4.593 (3.286)	BT: 4.628 (3.319)	Loss 0.2255 (0.3080)	Prec@1 91.406 (89.186)	
Epoch: [39][311/391]	LR: 0.00064	DT: 0.000 (3.119)	BT: 0.026 (3.151)	Loss 0.2388 (0.3070)	Prec@1 91.406 (89.205)	
Epoch: [39][389/391]	LR: 0.00064	DT: 0.000 (3.018)	BT: 0.026 (3.049)	Loss 0.2413 (0.3068)	Prec@1 91.406 (89.191)	
Total train loss: 0.3069
Avg Loading time: 3.0099 seconds
Avg Batch time: 3.0417 seconds

Train time: 1189.3609037399292
 * Prec@1 86.170 Prec@5 99.440 Loss 0.4143
Avg Loading time: 2.7820 seconds
Avg Batch time: 2.7952 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 221.36783862113953

Epoch: [40][77/391]	LR: 0.00012800000000000002	DT: 0.077 (2.759)	BT: 0.107 (2.791)	Loss 0.2571 (0.3074)	Prec@1 92.188 (88.902)	
Epoch: [40][155/391]	LR: 0.00012800000000000002	DT: 0.000 (2.863)	BT: 0.025 (2.895)	Loss 0.3596 (0.3075)	Prec@1 87.500 (89.068)	
Epoch: [40][233/391]	LR: 0.00012800000000000002	DT: 2.255 (2.912)	BT: 2.291 (2.944)	Loss 0.3179 (0.3085)	Prec@1 87.500 (89.086)	
Epoch: [40][311/391]	LR: 0.00012800000000000002	DT: 0.000 (2.702)	BT: 0.026 (2.733)	Loss 0.3218 (0.3047)	Prec@1 87.500 (89.255)	
Epoch: [40][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.586)	BT: 0.025 (2.617)	Loss 0.3247 (0.3056)	Prec@1 85.156 (89.207)	
Total train loss: 0.3057
Avg Loading time: 2.5797 seconds
Avg Batch time: 2.6104 seconds

Train time: 1020.7431457042694
 * Prec@1 86.140 Prec@5 99.470 Loss 0.4143
Avg Loading time: 2.3534 seconds
Avg Batch time: 2.3672 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 187.53285384178162

Epoch: [41][77/391]	LR: 0.00012800000000000002	DT: 0.000 (3.391)	BT: 0.021 (3.421)	Loss 0.2439 (0.3116)	Prec@1 92.188 (89.333)	
Epoch: [41][155/391]	LR: 0.00012800000000000002	DT: 0.000 (3.259)	BT: 0.027 (3.289)	Loss 0.2279 (0.3078)	Prec@1 93.750 (89.343)	
Epoch: [41][233/391]	LR: 0.00012800000000000002	DT: 0.000 (3.241)	BT: 0.027 (3.272)	Loss 0.3240 (0.3051)	Prec@1 92.188 (89.370)	
Epoch: [41][311/391]	LR: 0.00012800000000000002	DT: 0.000 (3.015)	BT: 0.025 (3.045)	Loss 0.4038 (0.3046)	Prec@1 84.375 (89.438)	
Epoch: [41][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.841)	BT: 0.025 (2.871)	Loss 0.3110 (0.3060)	Prec@1 86.719 (89.327)	
Total train loss: 0.3061
Avg Loading time: 2.8336 seconds
Avg Batch time: 2.8634 seconds

Train time: 1119.6350808143616
 * Prec@1 86.150 Prec@5 99.470 Loss 0.4143
Avg Loading time: 3.0788 seconds
Avg Batch time: 3.0928 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 244.87807202339172

Epoch: [42][77/391]	LR: 0.00012800000000000002	DT: 0.000 (2.703)	BT: 0.026 (2.734)	Loss 0.3101 (0.2897)	Prec@1 90.625 (89.673)	
Epoch: [42][155/391]	LR: 0.00012800000000000002	DT: 0.000 (2.714)	BT: 0.029 (2.746)	Loss 0.3623 (0.2993)	Prec@1 87.500 (89.348)	
Epoch: [42][233/391]	LR: 0.00012800000000000002	DT: 0.900 (2.762)	BT: 0.943 (2.794)	Loss 0.2461 (0.3059)	Prec@1 92.188 (89.126)	
Epoch: [42][311/391]	LR: 0.00012800000000000002	DT: 0.000 (2.726)	BT: 0.022 (2.758)	Loss 0.2573 (0.3049)	Prec@1 89.844 (89.230)	
Epoch: [42][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.774)	BT: 0.024 (2.807)	Loss 0.2095 (0.3040)	Prec@1 93.750 (89.267)	
Total train loss: 0.3041
Avg Loading time: 2.7673 seconds
Avg Batch time: 2.7998 seconds

Train time: 1094.7725958824158
 * Prec@1 86.150 Prec@5 99.460 Loss 0.4148
Avg Loading time: 3.2197 seconds
Avg Batch time: 3.2344 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 256.0533323287964

Epoch: [43][77/391]	LR: 0.00012800000000000002	DT: 0.000 (3.140)	BT: 0.026 (3.172)	Loss 0.3384 (0.3091)	Prec@1 89.062 (89.253)	
Epoch: [43][155/391]	LR: 0.00012800000000000002	DT: 0.589 (3.204)	BT: 0.625 (3.237)	Loss 0.3530 (0.3056)	Prec@1 85.938 (89.378)	
Epoch: [43][233/391]	LR: 0.00012800000000000002	DT: 0.000 (3.091)	BT: 0.027 (3.123)	Loss 0.2454 (0.3045)	Prec@1 92.188 (89.490)	
Epoch: [43][311/391]	LR: 0.00012800000000000002	DT: 0.000 (2.823)	BT: 0.026 (2.855)	Loss 0.3213 (0.3015)	Prec@1 89.062 (89.531)	
Epoch: [43][389/391]	LR: 0.00012800000000000002	DT: 0.170 (2.694)	BT: 0.197 (2.725)	Loss 0.3103 (0.3047)	Prec@1 89.844 (89.439)	
Total train loss: 0.3047
Avg Loading time: 2.6868 seconds
Avg Batch time: 2.7178 seconds

Train time: 1062.7514142990112
 * Prec@1 86.240 Prec@5 99.500 Loss 0.4136
Avg Loading time: 2.3317 seconds
Avg Batch time: 2.3441 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 185.7487497329712

Epoch: [44][77/391]	LR: 0.00012800000000000002	DT: 0.000 (3.121)	BT: 0.026 (3.154)	Loss 0.2988 (0.2971)	Prec@1 90.625 (89.413)	
Epoch: [44][155/391]	LR: 0.00012800000000000002	DT: 0.000 (3.133)	BT: 0.027 (3.165)	Loss 0.3704 (0.2978)	Prec@1 85.156 (89.488)	
Epoch: [44][233/391]	LR: 0.00012800000000000002	DT: 0.136 (3.310)	BT: 0.169 (3.342)	Loss 0.2834 (0.3018)	Prec@1 89.844 (89.330)	
Epoch: [44][311/391]	LR: 0.00012800000000000002	DT: 0.000 (3.252)	BT: 0.021 (3.284)	Loss 0.4260 (0.3030)	Prec@1 84.375 (89.373)	
Epoch: [44][389/391]	LR: 0.00012800000000000002	DT: 0.000 (3.192)	BT: 0.026 (3.224)	Loss 0.5171 (0.3045)	Prec@1 84.375 (89.389)	
Total train loss: 0.3044
Avg Loading time: 3.1841 seconds
Avg Batch time: 3.2155 seconds

Train time: 1257.3305764198303
 * Prec@1 86.230 Prec@5 99.490 Loss 0.4155
Avg Loading time: 2.9600 seconds
Avg Batch time: 2.9739 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 235.44174122810364

Epoch: [45][77/391]	LR: 0.00012800000000000002	DT: 0.912 (2.980)	BT: 0.947 (3.012)	Loss 0.3469 (0.2991)	Prec@1 86.719 (89.553)	
Epoch: [45][155/391]	LR: 0.00012800000000000002	DT: 0.000 (3.085)	BT: 0.027 (3.117)	Loss 0.3665 (0.3016)	Prec@1 86.719 (89.588)	
Epoch: [45][233/391]	LR: 0.00012800000000000002	DT: 3.101 (3.163)	BT: 3.135 (3.196)	Loss 0.3325 (0.3024)	Prec@1 88.281 (89.456)	
Epoch: [45][311/391]	LR: 0.00012800000000000002	DT: 0.000 (3.113)	BT: 0.025 (3.146)	Loss 0.2959 (0.3015)	Prec@1 89.844 (89.528)	
Epoch: [45][389/391]	LR: 0.00012800000000000002	DT: 0.000 (3.130)	BT: 0.025 (3.161)	Loss 0.2384 (0.3024)	Prec@1 92.969 (89.501)	
Total train loss: 0.3025
Avg Loading time: 3.1217 seconds
Avg Batch time: 3.1534 seconds

Train time: 1233.0486326217651
 * Prec@1 86.270 Prec@5 99.460 Loss 0.4141
Avg Loading time: 3.0989 seconds
Avg Batch time: 3.1118 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 246.36887431144714

Epoch: [46][77/391]	LR: 0.00012800000000000002	DT: 0.000 (3.209)	BT: 0.025 (3.241)	Loss 0.2678 (0.3137)	Prec@1 91.406 (89.073)	
Epoch: [46][155/391]	LR: 0.00012800000000000002	DT: 0.000 (3.206)	BT: 0.026 (3.238)	Loss 0.3347 (0.3057)	Prec@1 90.625 (89.428)	
Epoch: [46][233/391]	LR: 0.00012800000000000002	DT: 5.749 (2.972)	BT: 5.784 (3.002)	Loss 0.3655 (0.3058)	Prec@1 85.156 (89.353)	
Epoch: [46][311/391]	LR: 0.00012800000000000002	DT: 0.000 (2.775)	BT: 0.026 (2.806)	Loss 0.3469 (0.3047)	Prec@1 88.281 (89.393)	
Epoch: [46][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.528)	BT: 0.025 (2.559)	Loss 0.3044 (0.3041)	Prec@1 89.844 (89.465)	
Total train loss: 0.3041
Avg Loading time: 2.5219 seconds
Avg Batch time: 2.5522 seconds

Train time: 997.9592430591583
 * Prec@1 86.210 Prec@5 99.470 Loss 0.4153
Avg Loading time: 2.3661 seconds
Avg Batch time: 2.3779 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 188.37288117408752

Epoch: [47][77/391]	LR: 0.00012800000000000002	DT: 0.000 (2.984)	BT: 0.025 (3.015)	Loss 0.3350 (0.3181)	Prec@1 86.719 (88.572)	
Epoch: [47][155/391]	LR: 0.00012800000000000002	DT: 0.000 (2.936)	BT: 0.025 (2.967)	Loss 0.2571 (0.3138)	Prec@1 91.406 (88.977)	
Epoch: [47][233/391]	LR: 0.00012800000000000002	DT: 0.000 (2.928)	BT: 0.026 (2.958)	Loss 0.4729 (0.3101)	Prec@1 81.250 (89.146)	
Epoch: [47][311/391]	LR: 0.00012800000000000002	DT: 0.000 (2.839)	BT: 0.025 (2.870)	Loss 0.2878 (0.3092)	Prec@1 91.406 (89.218)	
Epoch: [47][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.856)	BT: 0.025 (2.887)	Loss 0.4263 (0.3085)	Prec@1 85.156 (89.333)	
Total train loss: 0.3085
Avg Loading time: 2.8488 seconds
Avg Batch time: 2.8794 seconds

Train time: 1125.886202096939
 * Prec@1 86.210 Prec@5 99.480 Loss 0.4148
Avg Loading time: 3.2308 seconds
Avg Batch time: 3.2442 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 256.83369612693787

Epoch: [48][77/391]	LR: 0.00012800000000000002	DT: 0.000 (3.409)	BT: 0.026 (3.440)	Loss 0.3740 (0.3033)	Prec@1 87.500 (89.623)	
Epoch: [48][155/391]	LR: 0.00012800000000000002	DT: 0.000 (3.387)	BT: 0.026 (3.418)	Loss 0.3052 (0.3091)	Prec@1 91.406 (89.338)	
Epoch: [48][233/391]	LR: 0.00012800000000000002	DT: 0.000 (3.386)	BT: 0.026 (3.416)	Loss 0.3066 (0.3115)	Prec@1 87.500 (89.219)	
Epoch: [48][311/391]	LR: 0.00012800000000000002	DT: 0.000 (3.271)	BT: 0.022 (3.301)	Loss 0.2402 (0.3070)	Prec@1 91.406 (89.350)	
Epoch: [48][389/391]	LR: 0.00012800000000000002	DT: 0.000 (3.220)	BT: 0.024 (3.250)	Loss 0.3706 (0.3051)	Prec@1 83.594 (89.409)	
Total train loss: 0.3052
Avg Loading time: 3.2115 seconds
Avg Batch time: 3.2415 seconds

Train time: 1267.497089624405
 * Prec@1 86.220 Prec@5 99.480 Loss 0.4143
Avg Loading time: 3.1637 seconds
Avg Batch time: 3.1788 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 251.65850043296814

Epoch: [49][77/391]	LR: 0.00012800000000000002	DT: 0.000 (2.853)	BT: 0.025 (2.884)	Loss 0.2734 (0.3131)	Prec@1 88.281 (88.942)	
Epoch: [49][155/391]	LR: 0.00012800000000000002	DT: 0.000 (2.732)	BT: 0.025 (2.763)	Loss 0.3362 (0.3035)	Prec@1 89.062 (89.468)	
Epoch: [49][233/391]	LR: 0.00012800000000000002	DT: 0.000 (2.367)	BT: 0.026 (2.399)	Loss 0.3550 (0.3035)	Prec@1 86.719 (89.520)	
Epoch: [49][311/391]	LR: 0.00012800000000000002	DT: 1.514 (2.224)	BT: 1.547 (2.256)	Loss 0.3267 (0.3039)	Prec@1 88.281 (89.466)	
Epoch: [49][389/391]	LR: 0.00012800000000000002	DT: 0.000 (2.166)	BT: 0.024 (2.197)	Loss 0.2632 (0.3045)	Prec@1 92.969 (89.383)	
Total train loss: 0.3045
Avg Loading time: 2.1607 seconds
Avg Batch time: 2.1914 seconds

Train time: 856.8772683143616
 * Prec@1 86.100 Prec@5 99.450 Loss 0.4136
Avg Loading time: 2.9678 seconds
Avg Batch time: 2.9826 seconds

Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 236.1502788066864


      ==> Arguments:
          dataset: cifar10
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.08
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 13
Savedir:  ../pretrained_models/frozen/x64-8b/rram/cifar10/resnet18
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar10/resnet18/train/relu13
Test path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar10/resnet18/test/relu13
ResNet18(
  (conv14): QConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): QConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=512, out_features=10, bias=False)
  (bn19): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 9.850 Prec@5 55.590 Loss 2.2988
Avg Loading time: 7.0260 seconds
Avg Batch time: 7.0462 seconds

Pre-trained Prec@1 with 13 layers frozen: 9.84999942779541 	 Loss: 2.298828125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.08	DT: 0.000 (7.747)	BT: 0.020 (7.773)	Loss 0.6807 (0.8845)	Prec@1 75.781 (72.446)	
Epoch: [0][155/391]	LR: 0.08	DT: 0.000 (7.871)	BT: 0.018 (7.896)	Loss 0.4727 (0.7598)	Prec@1 85.156 (75.366)	
Epoch: [0][233/391]	LR: 0.08	DT: 0.000 (7.885)	BT: 0.021 (7.910)	Loss 0.5781 (0.6878)	Prec@1 82.031 (77.567)	
Epoch: [0][311/391]	LR: 0.08	DT: 0.000 (7.524)	BT: 0.019 (7.549)	Loss 0.5576 (0.6386)	Prec@1 79.688 (78.929)	
Epoch: [0][389/391]	LR: 0.08	DT: 0.108 (7.460)	BT: 0.127 (7.485)	Loss 0.6909 (0.6128)	Prec@1 79.688 (79.609)	
Total train loss: 0.6127
Avg Loading time: 7.4408 seconds
Avg Batch time: 7.4662 seconds

Train time: 2919.363095521927
 * Prec@1 47.240 Prec@5 96.430 Loss 1.3135
Avg Loading time: 2.5662 seconds
Avg Batch time: 2.5783 seconds

Best acc: 47.240
--------------------------------------------------------------------------------
Test time: 204.54372572898865

Epoch: [1][77/391]	LR: 0.08	DT: 0.216 (2.318)	BT: 0.237 (2.341)	Loss 0.4348 (0.4697)	Prec@1 83.594 (84.155)	
Epoch: [1][155/391]	LR: 0.08	DT: 0.000 (2.246)	BT: 0.019 (2.270)	Loss 0.4758 (0.4528)	Prec@1 84.375 (84.660)	
Epoch: [1][233/391]	LR: 0.08	DT: 4.887 (2.518)	BT: 4.920 (2.542)	Loss 0.3950 (0.4387)	Prec@1 85.156 (85.009)	
Epoch: [1][311/391]	LR: 0.08	DT: 0.000 (2.652)	BT: 0.020 (2.675)	Loss 0.6362 (0.4378)	Prec@1 75.000 (84.986)	
Epoch: [1][389/391]	LR: 0.08	DT: 0.000 (2.775)	BT: 0.020 (2.799)	Loss 0.4846 (0.4474)	Prec@1 83.594 (84.704)	
Total train loss: 0.4473
Avg Loading time: 2.7684 seconds
Avg Batch time: 2.7917 seconds

Train time: 1091.6243846416473
