
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 55.640 Prec@5 82.230 Loss 1.8340
Pre-trained Prec@1 with 11 layers frozen: 55.63999938964844 	 Loss: 1.833984375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6719 (1.3034)	Prec@1 57.031 (63.442)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.4668 (1.3756)	Prec@1 55.078 (61.328)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.3896 (1.3751)	Prec@1 58.984 (61.131)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.4492 (1.3540)	Prec@1 60.156 (61.729)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.1572 (1.3339)	Prec@1 64.844 (62.238)	
Total train loss: 1.3336

Train time: 289.03459310531616
 * Prec@1 53.130 Prec@5 82.390 Loss 1.7949
Best acc: 53.130
--------------------------------------------------------------------------------
Test time: 319.85746693611145

Epoch: [1][38/196]	LR: 0.1	Loss 1.1133 (1.0647)	Prec@1 67.969 (69.561)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0586 (1.0669)	Prec@1 70.703 (69.401)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.2002 (1.0775)	Prec@1 65.234 (68.797)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0791 (1.0852)	Prec@1 71.094 (68.590)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.1230 (1.0851)	Prec@1 65.625 (68.632)	
Total train loss: 1.0853

Train time: 570.0436432361603
 * Prec@1 56.410 Prec@5 84.000 Loss 1.7217
Best acc: 56.410
--------------------------------------------------------------------------------
Test time: 704.4825971126556

Epoch: [2][38/196]	LR: 0.1	Loss 0.9106 (0.9232)	Prec@1 71.094 (72.947)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0732 (0.9193)	Prec@1 67.969 (73.022)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9648 (0.9374)	Prec@1 71.484 (72.429)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.1475 (0.9520)	Prec@1 66.797 (71.965)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9067 (0.9628)	Prec@1 73.828 (71.647)	
Total train loss: 0.9628

Train time: 88.34381341934204
 * Prec@1 56.130 Prec@5 83.900 Loss 1.7998
Best acc: 56.410
--------------------------------------------------------------------------------
Test time: 102.30158162117004

Epoch: [3][38/196]	LR: 0.1	Loss 0.7070 (0.7954)	Prec@1 81.250 (76.673)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8354 (0.7965)	Prec@1 76.953 (76.457)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9399 (0.8133)	Prec@1 71.094 (75.908)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9810 (0.8321)	Prec@1 72.266 (75.381)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9521 (0.8495)	Prec@1 71.484 (74.884)	
Total train loss: 0.8497

Train time: 23.030269384384155
 * Prec@1 56.940 Prec@5 84.340 Loss 1.7773
Best acc: 56.940
--------------------------------------------------------------------------------
Test time: 26.300706386566162

Epoch: [4][38/196]	LR: 0.1	Loss 0.7046 (0.7236)	Prec@1 79.688 (78.696)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7393 (0.7237)	Prec@1 77.344 (78.606)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8008 (0.7427)	Prec@1 73.828 (77.905)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.6978 (0.7606)	Prec@1 80.078 (77.281)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8687 (0.7770)	Prec@1 77.344 (76.781)	
Total train loss: 0.7773

Train time: 16.735838413238525
 * Prec@1 57.460 Prec@5 84.510 Loss 1.7246
Best acc: 57.460
--------------------------------------------------------------------------------
Test time: 21.48652219772339

Epoch: [5][38/196]	LR: 0.1	Loss 0.6597 (0.6387)	Prec@1 79.297 (80.980)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6909 (0.6433)	Prec@1 81.641 (80.814)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7891 (0.6624)	Prec@1 74.219 (80.175)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7827 (0.6817)	Prec@1 74.609 (79.487)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.7744 (0.6989)	Prec@1 78.906 (78.914)	
Total train loss: 0.6993

Train time: 20.17022180557251
 * Prec@1 58.270 Prec@5 84.890 Loss 1.7246
Best acc: 58.270
--------------------------------------------------------------------------------
Test time: 23.831010341644287

Epoch: [6][38/196]	LR: 0.1	Loss 0.6650 (0.6111)	Prec@1 75.391 (81.490)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5947 (0.6006)	Prec@1 82.031 (81.921)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.7109 (0.6077)	Prec@1 78.516 (81.661)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7588 (0.6242)	Prec@1 75.781 (81.130)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.6489 (0.6389)	Prec@1 80.469 (80.805)	
Total train loss: 0.6392

Train time: 19.32566213607788
 * Prec@1 58.070 Prec@5 84.560 Loss 1.7266
Best acc: 58.270
--------------------------------------------------------------------------------
Test time: 22.996647596359253

Epoch: [7][38/196]	LR: 0.1	Loss 0.3894 (0.5131)	Prec@1 88.672 (85.026)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5161 (0.5193)	Prec@1 83.594 (84.515)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6602 (0.5349)	Prec@1 79.297 (83.858)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6938 (0.5581)	Prec@1 80.078 (83.015)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6226 (0.5791)	Prec@1 78.125 (82.358)	
Total train loss: 0.5800

Train time: 18.2911274433136
 * Prec@1 56.130 Prec@5 82.880 Loss 1.8916
Best acc: 58.270
--------------------------------------------------------------------------------
Test time: 23.039688110351562

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3315 (0.4008)	Prec@1 89.844 (88.431)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3245 (0.3808)	Prec@1 92.969 (89.148)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3000 (0.3626)	Prec@1 91.797 (89.830)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2522 (0.3458)	Prec@1 94.141 (90.450)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2534 (0.3345)	Prec@1 93.359 (90.879)	
Total train loss: 0.3349

Train time: 20.659457683563232
 * Prec@1 64.280 Prec@5 88.090 Loss 1.4629
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 24.081860780715942

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2236 (0.2488)	Prec@1 94.922 (94.171)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3069 (0.2439)	Prec@1 90.625 (94.481)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2469 (0.2423)	Prec@1 94.531 (94.391)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2379 (0.2424)	Prec@1 94.922 (94.328)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2416 (0.2420)	Prec@1 94.922 (94.357)	
Total train loss: 0.2423

Train time: 19.272698402404785
 * Prec@1 64.550 Prec@5 87.780 Loss 1.4844
Best acc: 64.550
--------------------------------------------------------------------------------
Test time: 24.12356472015381

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2255 (0.2127)	Prec@1 93.750 (95.523)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2354 (0.2110)	Prec@1 94.531 (95.488)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2040 (0.2104)	Prec@1 95.703 (95.550)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2072 (0.2103)	Prec@1 95.703 (95.583)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2407 (0.2118)	Prec@1 96.484 (95.567)	
Total train loss: 0.2119

Train time: 19.211440324783325
 * Prec@1 64.610 Prec@5 87.760 Loss 1.5010
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 23.094616889953613

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1584 (0.1827)	Prec@1 96.875 (96.705)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.2112 (0.1820)	Prec@1 94.531 (96.725)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1968 (0.1848)	Prec@1 95.312 (96.544)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2218 (0.1875)	Prec@1 94.141 (96.444)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2013 (0.1899)	Prec@1 96.484 (96.360)	
Total train loss: 0.1900

Train time: 20.450331449508667
 * Prec@1 63.950 Prec@5 87.370 Loss 1.5264
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.904167890548706

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1884 (0.1743)	Prec@1 96.484 (96.985)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1980 (0.1714)	Prec@1 96.484 (97.025)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1946 (0.1715)	Prec@1 95.703 (96.982)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1187 (0.1729)	Prec@1 99.609 (96.980)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2144 (0.1750)	Prec@1 94.922 (96.849)	
Total train loss: 0.1750

Train time: 19.242562294006348
 * Prec@1 63.800 Prec@5 87.320 Loss 1.5342
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 22.592252731323242

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1678 (0.1637)	Prec@1 97.266 (97.396)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1724 (0.1612)	Prec@1 96.484 (97.466)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1639 (0.1625)	Prec@1 97.266 (97.453)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1711 (0.1628)	Prec@1 97.656 (97.426)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2225 (0.1636)	Prec@1 94.922 (97.394)	
Total train loss: 0.1638

Train time: 19.149428129196167
 * Prec@1 64.020 Prec@5 87.090 Loss 1.5420
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 22.283613681793213

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1439 (0.1491)	Prec@1 98.438 (98.027)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1666 (0.1490)	Prec@1 98.438 (97.932)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1305 (0.1510)	Prec@1 98.438 (97.823)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1693 (0.1513)	Prec@1 98.047 (97.774)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1447 (0.1520)	Prec@1 98.047 (97.718)	
Total train loss: 0.1522

Train time: 19.617730855941772
 * Prec@1 63.770 Prec@5 86.900 Loss 1.5557
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.55528163909912

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2095 (0.1404)	Prec@1 96.094 (98.127)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1505 (0.1407)	Prec@1 98.438 (98.147)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1204 (0.1422)	Prec@1 98.438 (98.080)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1746 (0.1445)	Prec@1 97.656 (98.042)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1372 (0.1452)	Prec@1 97.656 (98.007)	
Total train loss: 0.1453

Train time: 19.336761236190796
 * Prec@1 63.550 Prec@5 86.900 Loss 1.5703
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 23.100330591201782

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1339 (0.1317)	Prec@1 98.438 (98.458)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1311 (0.1310)	Prec@1 98.047 (98.397)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1248 (0.1317)	Prec@1 99.219 (98.461)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1388 (0.1310)	Prec@1 99.219 (98.470)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1323 (0.1309)	Prec@1 98.828 (98.530)	
Total train loss: 0.1310

Train time: 20.754713535308838
 * Prec@1 63.690 Prec@5 86.720 Loss 1.5723
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 25.710424661636353

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1179 (0.1326)	Prec@1 99.609 (98.638)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1172 (0.1324)	Prec@1 99.219 (98.548)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1226 (0.1315)	Prec@1 99.219 (98.574)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1249 (0.1305)	Prec@1 98.438 (98.545)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1466 (0.1315)	Prec@1 98.438 (98.508)	
Total train loss: 0.1316

Train time: 20.042758464813232
 * Prec@1 63.560 Prec@5 86.960 Loss 1.5732
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 23.240516901016235

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1154 (0.1271)	Prec@1 98.047 (98.708)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1277 (0.1272)	Prec@1 98.828 (98.678)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1153 (0.1281)	Prec@1 98.438 (98.648)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1248 (0.1293)	Prec@1 98.828 (98.618)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1519 (0.1290)	Prec@1 97.656 (98.604)	
Total train loss: 0.1292

Train time: 18.65935254096985
 * Prec@1 63.510 Prec@5 86.930 Loss 1.5645
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 21.526063442230225

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1570 (0.1266)	Prec@1 97.656 (98.628)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1337 (0.1276)	Prec@1 97.656 (98.548)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1285 (0.1291)	Prec@1 98.828 (98.518)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1140 (0.1281)	Prec@1 98.828 (98.558)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1196 (0.1275)	Prec@1 99.609 (98.582)	
Total train loss: 0.1276

Train time: 19.595181703567505
 * Prec@1 63.930 Prec@5 86.920 Loss 1.5645
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.171175241470337

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1376 (0.1292)	Prec@1 99.219 (98.538)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1349 (0.1292)	Prec@1 97.656 (98.518)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1149 (0.1287)	Prec@1 98.828 (98.488)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1153 (0.1288)	Prec@1 99.219 (98.508)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1368 (0.1287)	Prec@1 97.656 (98.564)	
Total train loss: 0.1288

Train time: 19.948957920074463
 * Prec@1 63.790 Prec@5 86.840 Loss 1.5693
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 23.312159061431885

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1293 (0.1289)	Prec@1 97.656 (98.528)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1257 (0.1309)	Prec@1 99.219 (98.438)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0974 (0.1294)	Prec@1 99.609 (98.514)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1516 (0.1293)	Prec@1 98.828 (98.530)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1309 (0.1301)	Prec@1 98.047 (98.516)	
Total train loss: 0.1303

Train time: 20.673479080200195
 * Prec@1 63.430 Prec@5 86.780 Loss 1.5693
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.753390073776245

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1271 (0.1298)	Prec@1 99.219 (98.588)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1333 (0.1301)	Prec@1 98.438 (98.518)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1179 (0.1287)	Prec@1 99.609 (98.551)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1151 (0.1283)	Prec@1 99.609 (98.593)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1256 (0.1292)	Prec@1 98.438 (98.590)	
Total train loss: 0.1293

Train time: 19.47006106376648
 * Prec@1 63.640 Prec@5 86.660 Loss 1.5732
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 22.91776394844055

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1332 (0.1298)	Prec@1 98.438 (98.458)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1248 (0.1297)	Prec@1 98.828 (98.528)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1118 (0.1309)	Prec@1 99.219 (98.484)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1078 (0.1307)	Prec@1 100.000 (98.510)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1271 (0.1296)	Prec@1 98.828 (98.558)	
Total train loss: 0.1297

Train time: 21.385897636413574
 * Prec@1 63.860 Prec@5 86.700 Loss 1.5645
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 25.807018041610718

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1266 (0.1295)	Prec@1 98.828 (98.538)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1301 (0.1291)	Prec@1 97.656 (98.518)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1586 (0.1286)	Prec@1 98.047 (98.578)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1476 (0.1283)	Prec@1 98.047 (98.595)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1281 (0.1286)	Prec@1 98.438 (98.592)	
Total train loss: 0.1288

Train time: 18.681493759155273
 * Prec@1 63.660 Prec@5 86.610 Loss 1.5811
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 21.855948448181152

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1194 (0.1300)	Prec@1 99.609 (98.638)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1122 (0.1278)	Prec@1 99.609 (98.683)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1301 (0.1282)	Prec@1 100.000 (98.658)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1223 (0.1284)	Prec@1 99.219 (98.605)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1256 (0.1283)	Prec@1 98.047 (98.576)	
Total train loss: 0.1285

Train time: 19.364350080490112
 * Prec@1 63.720 Prec@5 86.800 Loss 1.5684
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 22.828368425369263

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1063 (0.1281)	Prec@1 99.609 (98.708)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1310 (0.1263)	Prec@1 98.438 (98.703)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1176 (0.1280)	Prec@1 99.219 (98.658)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1353 (0.1283)	Prec@1 98.047 (98.663)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0953 (0.1282)	Prec@1 100.000 (98.654)	
Total train loss: 0.1285

Train time: 19.198811054229736
 * Prec@1 63.770 Prec@5 86.770 Loss 1.5674
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 23.487027168273926

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1263 (0.1274)	Prec@1 98.438 (98.598)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1317 (0.1281)	Prec@1 97.656 (98.583)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1165 (0.1291)	Prec@1 98.828 (98.514)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1072 (0.1293)	Prec@1 100.000 (98.495)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0963 (0.1284)	Prec@1 99.609 (98.568)	
Total train loss: 0.1287

Train time: 20.768919229507446
 * Prec@1 63.710 Prec@5 86.830 Loss 1.5625
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.036423206329346

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1476 (0.1260)	Prec@1 97.656 (98.528)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1403 (0.1263)	Prec@1 97.656 (98.583)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1570 (0.1273)	Prec@1 96.484 (98.574)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1455 (0.1275)	Prec@1 97.266 (98.548)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0981 (0.1279)	Prec@1 99.609 (98.562)	
Total train loss: 0.1282

Train time: 19.114442110061646
 * Prec@1 63.750 Prec@5 86.690 Loss 1.5684
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 22.444336414337158

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1260 (0.1300)	Prec@1 97.656 (98.528)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1118 (0.1292)	Prec@1 99.609 (98.578)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1029 (0.1299)	Prec@1 99.609 (98.534)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1106 (0.1290)	Prec@1 99.219 (98.545)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1304 (0.1287)	Prec@1 98.438 (98.578)	
Total train loss: 0.1288

Train time: 19.70032811164856
 * Prec@1 63.450 Prec@5 86.870 Loss 1.5664
Best acc: 64.610
--------------------------------------------------------------------------------
Test time: 24.281938552856445


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 49.640 Prec@5 77.550 Loss 2.1641
Pre-trained Prec@1 with 13 layers frozen: 49.63999938964844 	 Loss: 2.1640625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.4150 (1.2368)	Prec@1 57.422 (64.754)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3066 (1.2989)	Prec@1 60.938 (62.775)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.3096 (1.2997)	Prec@1 62.891 (62.927)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.3535 (1.2974)	Prec@1 61.328 (62.983)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2764 (1.2880)	Prec@1 63.281 (63.285)	
Total train loss: 1.2885

Train time: 148.74668884277344
 * Prec@1 52.980 Prec@5 81.680 Loss 1.8271
Best acc: 52.980
--------------------------------------------------------------------------------
Test time: 152.77457308769226

Epoch: [1][38/196]	LR: 0.1	Loss 0.8633 (1.0230)	Prec@1 75.391 (70.443)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.1348 (1.0375)	Prec@1 68.750 (69.922)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.2100 (1.0548)	Prec@1 66.406 (69.294)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0801 (1.0597)	Prec@1 66.016 (69.148)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.1748 (1.0609)	Prec@1 65.625 (69.081)	
Total train loss: 1.0609

Train time: 17.874549865722656
 * Prec@1 56.910 Prec@5 84.480 Loss 1.7012
Best acc: 56.910
--------------------------------------------------------------------------------
Test time: 21.33951997756958

Epoch: [2][38/196]	LR: 0.1	Loss 0.9336 (0.8761)	Prec@1 73.047 (74.619)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9678 (0.8780)	Prec@1 69.922 (74.259)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9106 (0.8913)	Prec@1 71.484 (73.785)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.8081 (0.9087)	Prec@1 76.953 (73.275)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9575 (0.9171)	Prec@1 71.094 (72.999)	
Total train loss: 0.9175

Train time: 15.254200220108032
 * Prec@1 59.320 Prec@5 86.090 Loss 1.5449
Best acc: 59.320
--------------------------------------------------------------------------------
Test time: 18.513110637664795

Epoch: [3][38/196]	LR: 0.1	Loss 0.8232 (0.7745)	Prec@1 77.734 (77.083)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8472 (0.7818)	Prec@1 75.000 (76.993)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8491 (0.8000)	Prec@1 71.875 (76.459)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.8945 (0.8181)	Prec@1 73.047 (75.829)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9058 (0.8256)	Prec@1 74.219 (75.593)	
Total train loss: 0.8258

Train time: 18.139875888824463
 * Prec@1 59.770 Prec@5 85.920 Loss 1.6064
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 22.90301752090454

Epoch: [4][38/196]	LR: 0.1	Loss 0.6753 (0.6727)	Prec@1 78.516 (80.278)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7612 (0.6921)	Prec@1 77.344 (79.372)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7095 (0.7129)	Prec@1 81.250 (78.699)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.7666 (0.7266)	Prec@1 76.953 (78.283)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8428 (0.7409)	Prec@1 73.828 (77.790)	
Total train loss: 0.7411

Train time: 19.635567665100098
 * Prec@1 57.920 Prec@5 84.510 Loss 1.8125
Best acc: 59.770
--------------------------------------------------------------------------------
Test time: 23.024521350860596

Epoch: [5][38/196]	LR: 0.1	Loss 0.6650 (0.6248)	Prec@1 82.812 (81.701)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6392 (0.6304)	Prec@1 79.297 (81.375)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.6055 (0.6438)	Prec@1 81.250 (80.756)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7451 (0.6604)	Prec@1 76.953 (80.178)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8354 (0.6728)	Prec@1 74.609 (79.746)	
Total train loss: 0.6732

Train time: 18.48476791381836
 * Prec@1 60.350 Prec@5 86.020 Loss 1.6074
Best acc: 60.350
--------------------------------------------------------------------------------
Test time: 22.258166313171387

Epoch: [6][38/196]	LR: 0.1	Loss 0.5957 (0.5591)	Prec@1 80.078 (83.093)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5518 (0.5615)	Prec@1 81.641 (82.873)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.5967 (0.5715)	Prec@1 80.859 (82.579)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.5620 (0.5911)	Prec@1 81.250 (82.044)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7793 (0.6050)	Prec@1 76.953 (81.579)	
Total train loss: 0.6054

Train time: 18.88178825378418
 * Prec@1 58.900 Prec@5 84.850 Loss 1.7637
Best acc: 60.350
--------------------------------------------------------------------------------
Test time: 23.047905206680298

Epoch: [7][38/196]	LR: 0.1	Loss 0.4404 (0.4928)	Prec@1 88.672 (85.467)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.6333 (0.4986)	Prec@1 81.250 (85.051)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.5015 (0.5167)	Prec@1 83.594 (84.405)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6963 (0.5351)	Prec@1 78.516 (83.736)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6372 (0.5512)	Prec@1 78.906 (83.193)	
Total train loss: 0.5517

Train time: 20.042749881744385
 * Prec@1 58.870 Prec@5 85.470 Loss 1.7793
Best acc: 60.350
--------------------------------------------------------------------------------
Test time: 23.850115537643433

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3254 (0.4025)	Prec@1 92.969 (88.652)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2703 (0.3708)	Prec@1 93.750 (89.678)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2822 (0.3481)	Prec@1 92.578 (90.568)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2798 (0.3341)	Prec@1 94.922 (91.016)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.3013 (0.3259)	Prec@1 91.406 (91.276)	
Total train loss: 0.3260

Train time: 18.537084579467773
 * Prec@1 64.280 Prec@5 87.980 Loss 1.4619
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.226351976394653

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2253 (0.2370)	Prec@1 94.141 (94.862)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2290 (0.2358)	Prec@1 94.922 (94.832)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2209 (0.2381)	Prec@1 96.094 (94.675)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2092 (0.2377)	Prec@1 95.312 (94.664)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2451 (0.2373)	Prec@1 93.359 (94.647)	
Total train loss: 0.2374

Train time: 17.390010118484497
 * Prec@1 64.280 Prec@5 88.000 Loss 1.4805
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 21.791378498077393

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2649 (0.2051)	Prec@1 94.922 (95.964)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1565 (0.2046)	Prec@1 96.875 (95.853)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1675 (0.2036)	Prec@1 97.656 (95.930)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2391 (0.2052)	Prec@1 96.094 (95.833)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2316 (0.2068)	Prec@1 94.922 (95.817)	
Total train loss: 0.2070

Train time: 18.187755584716797
 * Prec@1 63.950 Prec@5 87.530 Loss 1.5059
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 21.898175954818726

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2200 (0.1861)	Prec@1 95.312 (96.324)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1677 (0.1838)	Prec@1 97.656 (96.575)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1917 (0.1866)	Prec@1 96.484 (96.424)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1819 (0.1866)	Prec@1 96.484 (96.447)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1686 (0.1870)	Prec@1 97.266 (96.448)	
Total train loss: 0.1874

Train time: 19.545648097991943
 * Prec@1 64.110 Prec@5 87.420 Loss 1.5195
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.39328908920288

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1708 (0.1636)	Prec@1 95.703 (97.366)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1707 (0.1651)	Prec@1 96.484 (97.371)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2443 (0.1695)	Prec@1 94.531 (97.169)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1798 (0.1717)	Prec@1 97.656 (97.130)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1699 (0.1731)	Prec@1 96.484 (97.067)	
Total train loss: 0.1733

Train time: 19.69775390625
 * Prec@1 63.720 Prec@5 87.270 Loss 1.5361
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.856910228729248

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1656 (0.1550)	Prec@1 98.047 (97.606)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1670 (0.1555)	Prec@1 96.094 (97.596)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1471 (0.1561)	Prec@1 97.266 (97.579)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1831 (0.1584)	Prec@1 96.484 (97.501)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2024 (0.1599)	Prec@1 97.266 (97.440)	
Total train loss: 0.1599

Train time: 19.247158527374268
 * Prec@1 63.590 Prec@5 87.190 Loss 1.5449
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.336318254470825

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1477 (0.1481)	Prec@1 96.875 (97.947)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1479 (0.1485)	Prec@1 97.656 (97.907)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1239 (0.1479)	Prec@1 98.047 (97.937)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1317 (0.1496)	Prec@1 99.609 (97.879)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1554 (0.1502)	Prec@1 96.484 (97.847)	
Total train loss: 0.1504

Train time: 18.75949716567993
 * Prec@1 63.700 Prec@5 87.140 Loss 1.5498
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.60074734687805

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1460 (0.1415)	Prec@1 98.438 (98.167)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1140 (0.1419)	Prec@1 98.828 (98.112)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1230 (0.1428)	Prec@1 98.828 (98.064)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1505 (0.1433)	Prec@1 98.047 (98.072)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1460 (0.1441)	Prec@1 98.047 (98.037)	
Total train loss: 0.1444

Train time: 19.211142778396606
 * Prec@1 63.310 Prec@5 86.960 Loss 1.5605
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.783586978912354

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1002 (0.1264)	Prec@1 99.609 (98.668)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1316 (0.1265)	Prec@1 98.438 (98.663)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1469 (0.1261)	Prec@1 98.438 (98.618)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1443 (0.1270)	Prec@1 98.047 (98.638)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1091 (0.1288)	Prec@1 98.828 (98.578)	
Total train loss: 0.1291

Train time: 19.307705879211426
 * Prec@1 63.100 Prec@5 86.780 Loss 1.5752
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.78014874458313

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1329 (0.1311)	Prec@1 98.438 (98.578)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1295 (0.1305)	Prec@1 97.656 (98.493)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1265 (0.1303)	Prec@1 99.219 (98.528)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1556 (0.1298)	Prec@1 98.047 (98.530)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1256 (0.1300)	Prec@1 98.828 (98.574)	
Total train loss: 0.1303

Train time: 20.256139039993286
 * Prec@1 63.220 Prec@5 87.010 Loss 1.5713
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.95747709274292

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1182 (0.1279)	Prec@1 99.609 (98.578)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1619 (0.1285)	Prec@1 98.047 (98.578)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1263 (0.1298)	Prec@1 98.438 (98.534)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1370 (0.1295)	Prec@1 98.438 (98.538)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1290 (0.1293)	Prec@1 97.656 (98.532)	
Total train loss: 0.1296

Train time: 18.53754734992981
 * Prec@1 63.240 Prec@5 87.000 Loss 1.5732
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.747984170913696

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1122 (0.1271)	Prec@1 98.828 (98.558)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1411 (0.1281)	Prec@1 98.828 (98.598)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1261 (0.1285)	Prec@1 97.266 (98.588)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1263 (0.1293)	Prec@1 98.438 (98.568)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1285 (0.1289)	Prec@1 98.438 (98.572)	
Total train loss: 0.1290

Train time: 18.174794673919678
 * Prec@1 63.340 Prec@5 86.930 Loss 1.5713
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 21.9112069606781

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1179 (0.1253)	Prec@1 99.219 (98.808)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1241 (0.1266)	Prec@1 98.828 (98.698)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1310 (0.1278)	Prec@1 99.219 (98.681)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1016 (0.1291)	Prec@1 99.609 (98.615)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1471 (0.1282)	Prec@1 96.875 (98.606)	
Total train loss: 0.1285

Train time: 18.852383613586426
 * Prec@1 63.110 Prec@5 87.090 Loss 1.5703
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.467307567596436

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1410 (0.1290)	Prec@1 98.047 (98.397)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1175 (0.1287)	Prec@1 98.828 (98.458)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1003 (0.1290)	Prec@1 99.219 (98.528)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1284 (0.1286)	Prec@1 97.266 (98.565)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1515 (0.1292)	Prec@1 98.047 (98.556)	
Total train loss: 0.1292

Train time: 19.060187339782715
 * Prec@1 63.150 Prec@5 86.990 Loss 1.5674
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.467878103256226

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1323 (0.1309)	Prec@1 98.438 (98.588)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1124 (0.1278)	Prec@1 100.000 (98.718)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1339 (0.1281)	Prec@1 98.438 (98.688)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1591 (0.1286)	Prec@1 97.266 (98.688)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1656 (0.1286)	Prec@1 96.484 (98.644)	
Total train loss: 0.1287

Train time: 18.491363048553467
 * Prec@1 63.480 Prec@5 86.990 Loss 1.5684
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.068825006484985

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1240 (0.1319)	Prec@1 98.047 (98.568)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1318 (0.1312)	Prec@1 99.609 (98.588)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1317 (0.1290)	Prec@1 98.828 (98.614)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0973 (0.1289)	Prec@1 99.219 (98.578)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1370 (0.1286)	Prec@1 98.047 (98.588)	
Total train loss: 0.1287

Train time: 18.572270393371582
 * Prec@1 63.340 Prec@5 87.110 Loss 1.5752
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.22598171234131

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1290 (0.1270)	Prec@1 98.828 (98.608)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1672 (0.1297)	Prec@1 97.656 (98.498)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1187 (0.1280)	Prec@1 98.438 (98.534)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1320 (0.1274)	Prec@1 98.438 (98.540)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1367 (0.1282)	Prec@1 98.047 (98.572)	
Total train loss: 0.1283

Train time: 20.82796621322632
 * Prec@1 63.280 Prec@5 87.060 Loss 1.5684
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 25.112809896469116

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1296 (0.1313)	Prec@1 98.438 (98.518)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1301 (0.1289)	Prec@1 99.219 (98.593)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1014 (0.1285)	Prec@1 100.000 (98.631)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1088 (0.1283)	Prec@1 99.219 (98.638)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1203 (0.1280)	Prec@1 98.828 (98.660)	
Total train loss: 0.1282

Train time: 18.080017566680908
 * Prec@1 63.010 Prec@5 86.890 Loss 1.5771
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 21.647541761398315

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1153 (0.1306)	Prec@1 98.047 (98.598)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1272 (0.1278)	Prec@1 98.438 (98.608)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1225 (0.1272)	Prec@1 98.828 (98.628)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1390 (0.1275)	Prec@1 98.438 (98.608)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1299 (0.1275)	Prec@1 97.656 (98.586)	
Total train loss: 0.1277

Train time: 19.43760323524475
 * Prec@1 63.260 Prec@5 86.910 Loss 1.5752
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.132894277572632

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1141 (0.1249)	Prec@1 99.609 (98.578)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1088 (0.1268)	Prec@1 99.219 (98.648)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1549 (0.1277)	Prec@1 97.656 (98.601)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1482 (0.1278)	Prec@1 98.047 (98.568)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1381 (0.1275)	Prec@1 98.438 (98.562)	
Total train loss: 0.1277

Train time: 18.218244075775146
 * Prec@1 63.530 Prec@5 87.190 Loss 1.5684
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 22.572402715682983

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1487 (0.1274)	Prec@1 98.047 (98.548)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1276 (0.1278)	Prec@1 98.828 (98.608)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1464 (0.1281)	Prec@1 98.828 (98.614)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1218 (0.1283)	Prec@1 98.047 (98.628)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1390 (0.1293)	Prec@1 98.438 (98.590)	
Total train loss: 0.1296

Train time: 19.54007387161255
 * Prec@1 63.160 Prec@5 87.040 Loss 1.5771
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 23.13383722305298

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1376 (0.1218)	Prec@1 98.047 (98.818)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1033 (0.1243)	Prec@1 98.828 (98.693)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1218 (0.1269)	Prec@1 99.609 (98.638)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1166 (0.1279)	Prec@1 98.438 (98.585)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1230 (0.1281)	Prec@1 98.438 (98.588)	
Total train loss: 0.1284

Train time: 18.23694396018982
 * Prec@1 63.120 Prec@5 86.960 Loss 1.5742
Best acc: 64.280
--------------------------------------------------------------------------------
Test time: 21.96526050567627


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 47.800 Prec@5 75.580 Loss 2.3027
Pre-trained Prec@1 with 15 layers frozen: 47.79999923706055 	 Loss: 2.302734375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.2754 (1.3137)	Prec@1 61.328 (63.261)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3535 (1.3205)	Prec@1 61.719 (62.595)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.0596 (1.2993)	Prec@1 68.359 (63.014)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1738 (1.2779)	Prec@1 66.016 (63.572)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2549 (1.2685)	Prec@1 65.625 (63.748)	
Total train loss: 1.2679

Train time: 183.65008521080017
 * Prec@1 57.200 Prec@5 84.430 Loss 1.6260
Best acc: 57.200
--------------------------------------------------------------------------------
Test time: 187.5249524116516

Epoch: [1][38/196]	LR: 0.1	Loss 0.9663 (1.0152)	Prec@1 71.484 (70.232)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.9688 (1.0292)	Prec@1 72.266 (70.017)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.0010 (1.0405)	Prec@1 73.438 (69.671)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1836 (1.0456)	Prec@1 65.625 (69.516)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2246 (1.0484)	Prec@1 65.234 (69.461)	
Total train loss: 1.0483

Train time: 18.01216721534729
 * Prec@1 57.810 Prec@5 84.320 Loss 1.6885
Best acc: 57.810
--------------------------------------------------------------------------------
Test time: 21.02383542060852

Epoch: [2][38/196]	LR: 0.1	Loss 0.8379 (0.9011)	Prec@1 73.047 (73.297)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0625 (0.9194)	Prec@1 67.578 (72.801)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9424 (0.9305)	Prec@1 70.703 (72.463)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0938 (0.9286)	Prec@1 68.359 (72.481)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9756 (0.9385)	Prec@1 67.969 (72.232)	
Total train loss: 0.9388

Train time: 13.868780374526978
 * Prec@1 58.250 Prec@5 85.090 Loss 1.6387
Best acc: 58.250
--------------------------------------------------------------------------------
Test time: 16.67017650604248

Epoch: [3][38/196]	LR: 0.1	Loss 0.8169 (0.8148)	Prec@1 78.125 (76.022)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.0059 (0.8321)	Prec@1 74.219 (75.506)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.7012 (0.8462)	Prec@1 79.297 (75.053)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9155 (0.8577)	Prec@1 69.922 (74.544)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9414 (0.8675)	Prec@1 72.266 (74.261)	
Total train loss: 0.8673

Train time: 14.945271015167236
 * Prec@1 58.860 Prec@5 85.610 Loss 1.6240
Best acc: 58.860
--------------------------------------------------------------------------------
Test time: 18.349015474319458

Epoch: [4][38/196]	LR: 0.1	Loss 0.7656 (0.7275)	Prec@1 77.344 (78.646)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8154 (0.7456)	Prec@1 77.734 (77.845)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8604 (0.7656)	Prec@1 73.828 (77.173)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.7524 (0.7811)	Prec@1 76.562 (76.728)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8730 (0.7969)	Prec@1 72.266 (76.314)	
Total train loss: 0.7969

Train time: 18.306346654891968
 * Prec@1 56.890 Prec@5 84.860 Loss 1.7324
Best acc: 58.860
--------------------------------------------------------------------------------
Test time: 22.05978560447693

Epoch: [5][38/196]	LR: 0.1	Loss 0.6602 (0.6839)	Prec@1 78.516 (79.597)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.7632 (0.7000)	Prec@1 76.562 (79.137)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7720 (0.7106)	Prec@1 76.953 (78.703)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.6777 (0.7286)	Prec@1 80.078 (78.188)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8115 (0.7391)	Prec@1 73.438 (77.855)	
Total train loss: 0.7394

Train time: 17.093416213989258
 * Prec@1 57.220 Prec@5 84.360 Loss 1.7715
Best acc: 58.860
--------------------------------------------------------------------------------
Test time: 20.442127466201782

Epoch: [6][38/196]	LR: 0.1	Loss 0.5728 (0.6215)	Prec@1 83.594 (81.210)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.7056 (0.6285)	Prec@1 76.562 (80.714)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.8384 (0.6509)	Prec@1 73.047 (80.091)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6675 (0.6682)	Prec@1 79.297 (79.557)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7852 (0.6825)	Prec@1 77.344 (79.201)	
Total train loss: 0.6828

Train time: 18.009639978408813
 * Prec@1 59.450 Prec@5 85.600 Loss 1.7021
Best acc: 59.450
--------------------------------------------------------------------------------
Test time: 21.22514009475708

Epoch: [7][38/196]	LR: 0.1	Loss 0.5474 (0.5637)	Prec@1 81.641 (83.454)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5410 (0.5829)	Prec@1 83.984 (82.517)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.5986 (0.5964)	Prec@1 81.250 (82.202)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6743 (0.6217)	Prec@1 80.469 (81.283)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.8818 (0.6412)	Prec@1 71.484 (80.609)	
Total train loss: 0.6414

Train time: 17.749534368515015
 * Prec@1 57.140 Prec@5 83.980 Loss 1.8066
Best acc: 59.450
--------------------------------------------------------------------------------
Test time: 22.029923915863037

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3889 (0.4802)	Prec@1 90.625 (86.118)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3821 (0.4515)	Prec@1 87.891 (87.114)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4363 (0.4322)	Prec@1 86.719 (87.760)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3735 (0.4235)	Prec@1 89.844 (88.111)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4041 (0.4160)	Prec@1 87.891 (88.343)	
Total train loss: 0.4161

Train time: 18.79946494102478
 * Prec@1 64.760 Prec@5 88.090 Loss 1.4404
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.78067898750305

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.4180 (0.3349)	Prec@1 88.281 (91.296)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3340 (0.3283)	Prec@1 92.578 (91.687)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.3213 (0.3322)	Prec@1 90.234 (91.463)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3623 (0.3340)	Prec@1 90.625 (91.466)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.4111 (0.3363)	Prec@1 87.500 (91.420)	
Total train loss: 0.3363

Train time: 19.189110040664673
 * Prec@1 64.310 Prec@5 87.850 Loss 1.4717
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.724428415298462

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2773 (0.3054)	Prec@1 94.922 (92.798)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3120 (0.3073)	Prec@1 91.797 (92.568)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2595 (0.3085)	Prec@1 94.531 (92.528)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.3167 (0.3071)	Prec@1 92.578 (92.558)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2246 (0.3081)	Prec@1 94.531 (92.546)	
Total train loss: 0.3082

Train time: 17.78461456298828
 * Prec@1 64.100 Prec@5 87.900 Loss 1.4844
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.509958505630493

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2708 (0.2734)	Prec@1 93.359 (93.920)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.3340 (0.2817)	Prec@1 91.016 (93.635)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2563 (0.2836)	Prec@1 96.094 (93.530)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2340 (0.2834)	Prec@1 95.703 (93.505)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.3469 (0.2854)	Prec@1 90.625 (93.379)	
Total train loss: 0.2856

Train time: 18.47953724861145
 * Prec@1 64.040 Prec@5 87.580 Loss 1.4941
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.09652042388916

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1991 (0.2661)	Prec@1 95.703 (94.020)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2742 (0.2643)	Prec@1 94.141 (94.236)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2362 (0.2667)	Prec@1 93.750 (94.131)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2927 (0.2670)	Prec@1 92.188 (94.058)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2642 (0.2707)	Prec@1 92.578 (93.942)	
Total train loss: 0.2711

Train time: 18.224557161331177
 * Prec@1 63.620 Prec@5 87.390 Loss 1.5244
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.51461386680603

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2444 (0.2508)	Prec@1 96.484 (94.852)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2284 (0.2510)	Prec@1 94.922 (94.832)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2725 (0.2546)	Prec@1 93.750 (94.668)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.2834 (0.2562)	Prec@1 92.969 (94.564)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2815 (0.2587)	Prec@1 94.922 (94.495)	
Total train loss: 0.2588

Train time: 17.90155529975891
 * Prec@1 63.650 Prec@5 87.190 Loss 1.5371
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.981366395950317

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.2954 (0.2406)	Prec@1 92.969 (95.142)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2089 (0.2445)	Prec@1 96.484 (94.967)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2346 (0.2449)	Prec@1 95.703 (94.895)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2615 (0.2475)	Prec@1 93.359 (94.734)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2329 (0.2503)	Prec@1 95.703 (94.603)	
Total train loss: 0.2508

Train time: 18.4381844997406
 * Prec@1 63.400 Prec@5 87.040 Loss 1.5615
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.51916790008545

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1805 (0.2306)	Prec@1 97.656 (95.583)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2998 (0.2345)	Prec@1 95.312 (95.312)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2683 (0.2329)	Prec@1 94.922 (95.429)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2267 (0.2332)	Prec@1 94.922 (95.408)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.2686 (0.2361)	Prec@1 94.531 (95.286)	
Total train loss: 0.2361

Train time: 18.01507830619812
 * Prec@1 63.250 Prec@5 86.730 Loss 1.5645
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.58090591430664

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1763 (0.2221)	Prec@1 98.047 (95.903)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1957 (0.2182)	Prec@1 96.875 (96.029)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1497 (0.2186)	Prec@1 98.828 (96.050)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2295 (0.2165)	Prec@1 95.703 (96.114)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2532 (0.2162)	Prec@1 92.969 (96.126)	
Total train loss: 0.2163

Train time: 18.656035661697388
 * Prec@1 63.340 Prec@5 86.950 Loss 1.5674
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.457411766052246

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1824 (0.2112)	Prec@1 97.266 (96.354)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1906 (0.2134)	Prec@1 97.266 (96.314)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1697 (0.2150)	Prec@1 96.875 (96.284)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2316 (0.2183)	Prec@1 96.484 (96.121)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2281 (0.2170)	Prec@1 96.875 (96.152)	
Total train loss: 0.2171

Train time: 18.58777356147766
 * Prec@1 63.520 Prec@5 86.910 Loss 1.5674
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.927444458007812

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2051 (0.2128)	Prec@1 96.875 (96.334)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1982 (0.2154)	Prec@1 96.484 (96.194)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1877 (0.2154)	Prec@1 96.484 (96.194)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2181 (0.2142)	Prec@1 96.484 (96.229)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.2313 (0.2140)	Prec@1 96.484 (96.238)	
Total train loss: 0.2143

Train time: 17.81908416748047
 * Prec@1 63.420 Prec@5 86.930 Loss 1.5674
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 20.918570518493652

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2324 (0.2153)	Prec@1 95.312 (96.144)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1851 (0.2154)	Prec@1 95.703 (96.119)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.2411 (0.2121)	Prec@1 92.578 (96.174)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1931 (0.2135)	Prec@1 97.266 (96.184)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.2101 (0.2134)	Prec@1 98.438 (96.232)	
Total train loss: 0.2137

Train time: 18.489847421646118
 * Prec@1 63.380 Prec@5 86.920 Loss 1.5586
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.410083293914795

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2300 (0.2152)	Prec@1 94.922 (96.334)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1897 (0.2150)	Prec@1 97.656 (96.314)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1821 (0.2152)	Prec@1 97.266 (96.197)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2170 (0.2154)	Prec@1 95.312 (96.159)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1716 (0.2135)	Prec@1 98.047 (96.252)	
Total train loss: 0.2136

Train time: 18.976442098617554
 * Prec@1 63.250 Prec@5 86.860 Loss 1.5693
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.845770835876465

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2313 (0.2166)	Prec@1 96.094 (95.954)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2104 (0.2139)	Prec@1 96.484 (96.099)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2250 (0.2143)	Prec@1 97.656 (96.167)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1838 (0.2134)	Prec@1 96.484 (96.151)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.2272 (0.2140)	Prec@1 96.094 (96.112)	
Total train loss: 0.2139

Train time: 17.336939811706543
 * Prec@1 63.260 Prec@5 86.970 Loss 1.5605
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.025194883346558

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2362 (0.2117)	Prec@1 95.312 (96.324)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2288 (0.2168)	Prec@1 94.531 (96.089)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2389 (0.2161)	Prec@1 95.703 (96.090)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2159 (0.2150)	Prec@1 95.703 (96.071)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1954 (0.2141)	Prec@1 97.656 (96.104)	
Total train loss: 0.2143

Train time: 17.26199769973755
 * Prec@1 63.210 Prec@5 86.800 Loss 1.5723
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.09110116958618

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1941 (0.2089)	Prec@1 96.875 (96.364)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.2773 (0.2110)	Prec@1 92.188 (96.359)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1954 (0.2123)	Prec@1 97.266 (96.327)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1941 (0.2118)	Prec@1 97.656 (96.322)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2174 (0.2125)	Prec@1 95.703 (96.262)	
Total train loss: 0.2126

Train time: 17.15595030784607
 * Prec@1 63.270 Prec@5 86.980 Loss 1.5645
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 20.733445644378662

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.2700 (0.2119)	Prec@1 94.141 (96.134)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2292 (0.2148)	Prec@1 96.094 (96.229)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2079 (0.2135)	Prec@1 94.922 (96.267)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1837 (0.2120)	Prec@1 96.875 (96.304)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1865 (0.2121)	Prec@1 97.656 (96.290)	
Total train loss: 0.2126

Train time: 18.94377088546753
 * Prec@1 63.370 Prec@5 86.910 Loss 1.5566
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.16322088241577

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2311 (0.2210)	Prec@1 95.312 (95.944)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2235 (0.2172)	Prec@1 95.703 (96.154)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1865 (0.2167)	Prec@1 97.266 (96.247)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1716 (0.2137)	Prec@1 98.438 (96.307)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1942 (0.2121)	Prec@1 96.484 (96.338)	
Total train loss: 0.2124

Train time: 19.028777360916138
 * Prec@1 63.410 Prec@5 86.950 Loss 1.5645
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 23.197504997253418

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.2489 (0.2161)	Prec@1 94.531 (96.264)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1952 (0.2104)	Prec@1 98.047 (96.424)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2345 (0.2124)	Prec@1 95.312 (96.327)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1879 (0.2111)	Prec@1 97.656 (96.339)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1915 (0.2115)	Prec@1 97.656 (96.320)	
Total train loss: 0.2117

Train time: 18.999972581863403
 * Prec@1 63.430 Prec@5 86.940 Loss 1.5625
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.03865075111389

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.2208 (0.2058)	Prec@1 95.312 (96.605)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.2581 (0.2111)	Prec@1 94.531 (96.334)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2003 (0.2118)	Prec@1 95.703 (96.261)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1965 (0.2121)	Prec@1 96.094 (96.221)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.2397 (0.2117)	Prec@1 96.484 (96.258)	
Total train loss: 0.2119

Train time: 18.108153104782104
 * Prec@1 63.200 Prec@5 86.780 Loss 1.5664
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 21.839266538619995

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2183 (0.2106)	Prec@1 96.875 (96.364)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2284 (0.2127)	Prec@1 96.094 (96.384)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2255 (0.2120)	Prec@1 96.094 (96.327)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1675 (0.2116)	Prec@1 97.266 (96.359)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1846 (0.2115)	Prec@1 96.875 (96.380)	
Total train loss: 0.2116

Train time: 18.213009357452393
 * Prec@1 63.160 Prec@5 87.050 Loss 1.5684
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 22.12882971763611

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1675 (0.2086)	Prec@1 97.266 (96.534)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2686 (0.2131)	Prec@1 93.359 (96.294)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1687 (0.2133)	Prec@1 98.047 (96.281)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1655 (0.2135)	Prec@1 97.656 (96.234)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.2297 (0.2129)	Prec@1 94.141 (96.244)	
Total train loss: 0.2130

Train time: 16.898380756378174
 * Prec@1 63.320 Prec@5 86.910 Loss 1.5684
Best acc: 64.760
--------------------------------------------------------------------------------
Test time: 20.413225650787354


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 43.000 Prec@5 72.450 Loss 2.6406
Pre-trained Prec@1 with 17 layers frozen: 43.0 	 Loss: 2.640625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.3320 (1.3166)	Prec@1 61.328 (63.211)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.2422 (1.2748)	Prec@1 65.625 (64.138)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.0781 (1.2453)	Prec@1 71.484 (64.760)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.0420 (1.2299)	Prec@1 67.578 (65.097)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.0664 (1.2117)	Prec@1 69.141 (65.597)	
Total train loss: 1.2116

Train time: 167.06740403175354
 * Prec@1 58.650 Prec@5 84.750 Loss 1.6240
Best acc: 58.650
--------------------------------------------------------------------------------
Test time: 170.94927501678467

Epoch: [1][38/196]	LR: 0.1	Loss 0.9004 (1.0038)	Prec@1 73.828 (71.144)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.9893 (1.0110)	Prec@1 71.484 (70.738)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.0332 (1.0196)	Prec@1 69.531 (70.463)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1045 (1.0218)	Prec@1 68.359 (70.400)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0547 (1.0262)	Prec@1 66.406 (70.258)	
Total train loss: 1.0264

Train time: 19.712188243865967
 * Prec@1 58.980 Prec@5 85.400 Loss 1.5771
Best acc: 58.980
--------------------------------------------------------------------------------
Test time: 23.550514936447144

Epoch: [2][38/196]	LR: 0.1	Loss 0.9277 (0.9058)	Prec@1 69.922 (73.478)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0371 (0.9104)	Prec@1 69.922 (73.523)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9956 (0.9278)	Prec@1 73.047 (73.114)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.8843 (0.9405)	Prec@1 73.828 (72.706)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9785 (0.9433)	Prec@1 68.750 (72.448)	
Total train loss: 0.9437

Train time: 14.467342853546143
 * Prec@1 60.350 Prec@5 86.360 Loss 1.5107
Best acc: 60.350
--------------------------------------------------------------------------------
Test time: 17.334309816360474

Epoch: [3][38/196]	LR: 0.1	Loss 0.7588 (0.8256)	Prec@1 78.125 (75.751)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8921 (0.8577)	Prec@1 71.094 (74.514)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9907 (0.8738)	Prec@1 74.219 (74.048)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.0039 (0.8838)	Prec@1 71.094 (73.638)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.7368 (0.8918)	Prec@1 76.562 (73.492)	
Total train loss: 0.8919

Train time: 14.54170536994934
 * Prec@1 60.730 Prec@5 86.240 Loss 1.5449
Best acc: 60.730
--------------------------------------------------------------------------------
Test time: 18.40797472000122

Epoch: [4][38/196]	LR: 0.1	Loss 0.7227 (0.7925)	Prec@1 79.688 (76.793)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8271 (0.8095)	Prec@1 76.172 (76.252)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7529 (0.8198)	Prec@1 80.078 (75.865)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8076 (0.8245)	Prec@1 77.344 (75.729)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.7900 (0.8362)	Prec@1 72.656 (75.331)	
Total train loss: 0.8363

Train time: 17.659419298171997
 * Prec@1 61.130 Prec@5 86.620 Loss 1.5469
Best acc: 61.130
--------------------------------------------------------------------------------
Test time: 21.434662580490112

Epoch: [5][38/196]	LR: 0.1	Loss 0.7065 (0.7590)	Prec@1 78.906 (77.764)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.8838 (0.7703)	Prec@1 76.953 (77.294)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7109 (0.7838)	Prec@1 80.859 (76.976)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7788 (0.7969)	Prec@1 80.469 (76.537)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.9131 (0.8050)	Prec@1 70.312 (76.204)	
Total train loss: 0.8058

Train time: 18.19374418258667
 * Prec@1 60.200 Prec@5 86.350 Loss 1.5898
Best acc: 61.130
--------------------------------------------------------------------------------
Test time: 21.764559984207153

Epoch: [6][38/196]	LR: 0.1	Loss 0.7183 (0.7122)	Prec@1 77.734 (78.896)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.6655 (0.7240)	Prec@1 76.953 (78.360)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.8311 (0.7438)	Prec@1 77.344 (77.885)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7822 (0.7552)	Prec@1 75.000 (77.469)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.9487 (0.7619)	Prec@1 72.266 (77.242)	
Total train loss: 0.7628

Train time: 17.69660973548889
 * Prec@1 59.270 Prec@5 84.820 Loss 1.7549
Best acc: 61.130
--------------------------------------------------------------------------------
Test time: 21.183480739593506

Epoch: [7][38/196]	LR: 0.1	Loss 0.7129 (0.6783)	Prec@1 80.078 (79.577)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.7490 (0.6940)	Prec@1 77.344 (78.891)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6465 (0.7081)	Prec@1 82.422 (78.599)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.8091 (0.7208)	Prec@1 73.828 (78.077)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.8711 (0.7328)	Prec@1 73.047 (77.734)	
Total train loss: 0.7329

Train time: 18.688502073287964
 * Prec@1 60.500 Prec@5 86.020 Loss 1.6162
Best acc: 61.130
--------------------------------------------------------------------------------
Test time: 22.794665575027466

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.5103 (0.6000)	Prec@1 85.156 (82.141)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.6094 (0.5791)	Prec@1 81.641 (83.058)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4502 (0.5676)	Prec@1 88.281 (83.447)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.5137 (0.5598)	Prec@1 84.375 (83.757)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4465 (0.5576)	Prec@1 87.891 (83.924)	
Total train loss: 0.5578

Train time: 17.78853988647461
 * Prec@1 63.890 Prec@5 87.950 Loss 1.4346
Best acc: 63.890
--------------------------------------------------------------------------------
Test time: 20.89561915397644

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.4688 (0.5011)	Prec@1 86.328 (85.757)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.4839 (0.5045)	Prec@1 89.062 (85.772)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.5356 (0.5078)	Prec@1 82.812 (85.560)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.5879 (0.5093)	Prec@1 84.375 (85.554)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.4666 (0.5104)	Prec@1 85.547 (85.553)	
Total train loss: 0.5105

Train time: 18.597057342529297
 * Prec@1 64.150 Prec@5 87.810 Loss 1.4512
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 22.25675129890442

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.4343 (0.4661)	Prec@1 89.062 (87.240)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.4946 (0.4776)	Prec@1 86.328 (86.969)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.5122 (0.4799)	Prec@1 85.547 (86.866)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.5122 (0.4876)	Prec@1 85.547 (86.564)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.5742 (0.4904)	Prec@1 83.594 (86.448)	
Total train loss: 0.4906

Train time: 18.053555727005005
 * Prec@1 63.670 Prec@5 87.690 Loss 1.4717
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 22.02940034866333

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.4060 (0.4717)	Prec@1 88.281 (86.919)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.5806 (0.4653)	Prec@1 84.766 (87.134)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.4683 (0.4723)	Prec@1 87.109 (86.906)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.4233 (0.4738)	Prec@1 89.062 (86.874)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.4424 (0.4746)	Prec@1 89.062 (86.871)	
Total train loss: 0.4746

Train time: 18.082890510559082
 * Prec@1 63.770 Prec@5 87.440 Loss 1.4795
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.774062156677246

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.4097 (0.4320)	Prec@1 88.672 (88.632)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.4419 (0.4514)	Prec@1 83.203 (87.700)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.4260 (0.4600)	Prec@1 90.234 (87.473)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3960 (0.4628)	Prec@1 85.938 (87.372)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.5205 (0.4625)	Prec@1 86.328 (87.414)	
Total train loss: 0.4628

Train time: 16.804153203964233
 * Prec@1 63.630 Prec@5 87.470 Loss 1.4834
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 20.230690002441406

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.5010 (0.4462)	Prec@1 87.891 (88.081)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.4580 (0.4419)	Prec@1 85.547 (88.086)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.5103 (0.4465)	Prec@1 87.891 (87.954)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.4617 (0.4533)	Prec@1 85.938 (87.715)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.4143 (0.4540)	Prec@1 89.062 (87.660)	
Total train loss: 0.4544

Train time: 16.687395095825195
 * Prec@1 63.230 Prec@5 87.160 Loss 1.5000
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 20.96151065826416

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.4749 (0.4290)	Prec@1 87.500 (88.792)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.4573 (0.4362)	Prec@1 87.109 (88.467)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.4617 (0.4400)	Prec@1 85.156 (88.211)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3975 (0.4428)	Prec@1 87.500 (88.103)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.4944 (0.4458)	Prec@1 84.766 (88.005)	
Total train loss: 0.4462

Train time: 18.197991609573364
 * Prec@1 63.390 Prec@5 87.200 Loss 1.5098
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.34448742866516

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.4351 (0.4358)	Prec@1 88.672 (88.582)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.3992 (0.4395)	Prec@1 87.500 (88.356)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.4673 (0.4374)	Prec@1 89.062 (88.311)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.5010 (0.4401)	Prec@1 87.500 (88.266)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.4338 (0.4408)	Prec@1 87.109 (88.239)	
Total train loss: 0.4408

Train time: 18.17743444442749
 * Prec@1 63.250 Prec@5 87.100 Loss 1.5137
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.78723382949829

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.3604 (0.4190)	Prec@1 92.188 (89.042)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.4023 (0.4203)	Prec@1 91.016 (88.927)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.4758 (0.4186)	Prec@1 86.719 (89.012)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.4370 (0.4206)	Prec@1 89.453 (88.940)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.3560 (0.4181)	Prec@1 91.797 (89.050)	
Total train loss: 0.4182

Train time: 17.787461042404175
 * Prec@1 63.160 Prec@5 87.110 Loss 1.5195
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.36662268638611

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.4097 (0.4238)	Prec@1 91.016 (89.473)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.4033 (0.4177)	Prec@1 89.453 (89.428)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.4290 (0.4168)	Prec@1 89.062 (89.470)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.4221 (0.4170)	Prec@1 88.281 (89.443)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.3679 (0.4173)	Prec@1 89.844 (89.361)	
Total train loss: 0.4176

Train time: 17.780757188796997
 * Prec@1 63.370 Prec@5 87.150 Loss 1.5088
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.44002079963684

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.3752 (0.4158)	Prec@1 89.062 (88.992)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.4182 (0.4178)	Prec@1 87.500 (89.093)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.4216 (0.4168)	Prec@1 90.234 (89.073)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.3784 (0.4158)	Prec@1 91.016 (89.168)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.4451 (0.4180)	Prec@1 87.500 (89.135)	
Total train loss: 0.4181

Train time: 17.924890518188477
 * Prec@1 63.370 Prec@5 87.070 Loss 1.5107
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.213258743286133

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.4553 (0.4167)	Prec@1 87.891 (89.243)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.4265 (0.4143)	Prec@1 89.844 (89.513)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.4309 (0.4161)	Prec@1 88.281 (89.256)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.3430 (0.4177)	Prec@1 91.016 (89.238)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.4790 (0.4164)	Prec@1 87.500 (89.269)	
Total train loss: 0.4166

Train time: 16.937540292739868
 * Prec@1 63.370 Prec@5 87.140 Loss 1.5186
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.119570016860962

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.4004 (0.4161)	Prec@1 91.406 (89.333)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.4275 (0.4192)	Prec@1 87.500 (89.108)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.4036 (0.4137)	Prec@1 87.891 (89.343)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.3923 (0.4120)	Prec@1 89.062 (89.413)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.3953 (0.4155)	Prec@1 90.625 (89.315)	
Total train loss: 0.4155

Train time: 18.984360218048096
 * Prec@1 63.220 Prec@5 87.100 Loss 1.5059
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 22.300185918807983

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.4224 (0.4112)	Prec@1 89.844 (89.774)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.4705 (0.4107)	Prec@1 86.328 (89.784)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.4163 (0.4133)	Prec@1 89.062 (89.597)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.3955 (0.4137)	Prec@1 89.453 (89.558)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.4883 (0.4143)	Prec@1 88.281 (89.499)	
Total train loss: 0.4146

Train time: 17.751450061798096
 * Prec@1 63.210 Prec@5 87.190 Loss 1.5186
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.600611925125122

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.3906 (0.4255)	Prec@1 89.453 (88.922)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.4490 (0.4172)	Prec@1 86.719 (89.148)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.4089 (0.4153)	Prec@1 88.281 (89.173)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.4656 (0.4160)	Prec@1 85.938 (89.240)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.4827 (0.4163)	Prec@1 87.500 (89.287)	
Total train loss: 0.4165

Train time: 17.41674041748047
 * Prec@1 63.310 Prec@5 87.080 Loss 1.5195
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.187165021896362

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.3923 (0.4206)	Prec@1 89.062 (89.123)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.3796 (0.4194)	Prec@1 89.062 (89.318)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.3662 (0.4154)	Prec@1 92.578 (89.256)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.3684 (0.4163)	Prec@1 88.672 (89.143)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.4133 (0.4157)	Prec@1 88.672 (89.217)	
Total train loss: 0.4160

Train time: 15.752622604370117
 * Prec@1 63.140 Prec@5 87.140 Loss 1.5068
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 19.65575933456421

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.4878 (0.4109)	Prec@1 85.547 (89.673)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.4170 (0.4085)	Prec@1 89.844 (89.663)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.4460 (0.4089)	Prec@1 89.062 (89.587)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.3757 (0.4110)	Prec@1 91.406 (89.488)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.4404 (0.4110)	Prec@1 88.281 (89.527)	
Total train loss: 0.4111

Train time: 18.129927396774292
 * Prec@1 63.250 Prec@5 87.170 Loss 1.5156
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 21.47349762916565

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.3147 (0.4067)	Prec@1 94.531 (89.493)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.4180 (0.4126)	Prec@1 91.016 (89.298)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.4592 (0.4166)	Prec@1 87.891 (89.196)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.3606 (0.4145)	Prec@1 90.625 (89.350)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.4180 (0.4146)	Prec@1 89.453 (89.345)	
Total train loss: 0.4146

Train time: 18.401837587356567
 * Prec@1 63.360 Prec@5 87.060 Loss 1.5176
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 22.769147634506226

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.4353 (0.4270)	Prec@1 89.062 (88.882)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.4272 (0.4151)	Prec@1 90.234 (89.293)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.3755 (0.4152)	Prec@1 91.797 (89.290)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.3848 (0.4153)	Prec@1 88.672 (89.323)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.4319 (0.4143)	Prec@1 90.234 (89.275)	
Total train loss: 0.4146

Train time: 16.56965684890747
 * Prec@1 63.090 Prec@5 87.060 Loss 1.5215
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 19.690881490707397

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.4146 (0.4128)	Prec@1 88.672 (88.992)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.4297 (0.4116)	Prec@1 91.016 (89.308)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.4102 (0.4140)	Prec@1 89.453 (89.366)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.3906 (0.4143)	Prec@1 90.234 (89.398)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.3389 (0.4129)	Prec@1 91.406 (89.387)	
Total train loss: 0.4131

Train time: 16.71898078918457
 * Prec@1 63.330 Prec@5 86.980 Loss 1.5156
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 20.61672329902649

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.4165 (0.4201)	Prec@1 87.500 (89.323)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.4426 (0.4123)	Prec@1 89.453 (89.543)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.3545 (0.4155)	Prec@1 92.188 (89.393)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.3801 (0.4145)	Prec@1 91.016 (89.413)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.3799 (0.4129)	Prec@1 91.016 (89.469)	
Total train loss: 0.4134

Train time: 17.21314835548401
 * Prec@1 63.450 Prec@5 87.070 Loss 1.5107
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 20.974956274032593

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.3999 (0.4184)	Prec@1 88.281 (89.183)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.3396 (0.4103)	Prec@1 90.625 (89.473)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.3337 (0.4113)	Prec@1 92.188 (89.410)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.4136 (0.4112)	Prec@1 88.672 (89.468)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.3718 (0.4144)	Prec@1 90.234 (89.369)	
Total train loss: 0.4145

Train time: 16.915261268615723
 * Prec@1 63.310 Prec@5 87.090 Loss 1.5107
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 20.780353784561157


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 39.810 Prec@5 70.090 Loss 2.9297
Pre-trained Prec@1 with 19 layers frozen: 39.80999755859375 	 Loss: 2.9296875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.2842 (1.3286)	Prec@1 67.188 (63.201)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.0811 (1.2802)	Prec@1 69.141 (64.473)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.2158 (1.2629)	Prec@1 66.797 (64.860)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1855 (1.2409)	Prec@1 64.453 (65.397)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2256 (1.2324)	Prec@1 65.234 (65.561)	
Total train loss: 1.2325

Train time: 92.96033143997192
 * Prec@1 61.670 Prec@5 86.250 Loss 1.4619
Best acc: 61.670
--------------------------------------------------------------------------------
Test time: 96.24271726608276

Epoch: [1][38/196]	LR: 0.1	Loss 1.1016 (1.1418)	Prec@1 66.406 (68.069)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.1045 (1.1363)	Prec@1 68.359 (68.144)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1162 (1.1448)	Prec@1 65.234 (67.648)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0166 (1.1444)	Prec@1 69.141 (67.568)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2402 (1.1439)	Prec@1 64.453 (67.558)	
Total train loss: 1.1440

Train time: 12.806929588317871
 * Prec@1 62.060 Prec@5 86.780 Loss 1.4434
Best acc: 62.060
--------------------------------------------------------------------------------
Test time: 15.218377590179443

Epoch: [2][38/196]	LR: 0.1	Loss 1.1328 (1.0891)	Prec@1 68.359 (68.980)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9819 (1.0970)	Prec@1 72.656 (68.845)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.1025 (1.1130)	Prec@1 69.531 (68.413)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0801 (1.1155)	Prec@1 66.406 (68.184)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.1396 (1.1173)	Prec@1 67.578 (68.105)	
Total train loss: 1.1170

Train time: 12.353801965713501
 * Prec@1 62.270 Prec@5 86.750 Loss 1.4424
Best acc: 62.270
--------------------------------------------------------------------------------
Test time: 14.57928204536438

Epoch: [3][38/196]	LR: 0.1	Loss 1.0879 (1.0873)	Prec@1 70.312 (68.740)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.0156 (1.0850)	Prec@1 74.219 (68.865)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9956 (1.0884)	Prec@1 71.484 (68.647)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.0898 (1.0943)	Prec@1 69.141 (68.487)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9858 (1.1020)	Prec@1 70.703 (68.405)	
Total train loss: 1.1019

Train time: 11.527125358581543
 * Prec@1 62.290 Prec@5 87.100 Loss 1.4385
Best acc: 62.290
--------------------------------------------------------------------------------
Test time: 14.301238775253296

Epoch: [4][38/196]	LR: 0.1	Loss 1.0195 (1.0455)	Prec@1 71.484 (69.992)	
Epoch: [4][77/196]	LR: 0.1	Loss 1.0068 (1.0587)	Prec@1 72.266 (69.782)	
Epoch: [4][116/196]	LR: 0.1	Loss 1.0752 (1.0716)	Prec@1 67.188 (69.444)	
Epoch: [4][155/196]	LR: 0.1	Loss 1.1797 (1.0797)	Prec@1 63.672 (69.118)	
Epoch: [4][194/196]	LR: 0.1	Loss 1.1494 (1.0860)	Prec@1 67.188 (68.982)	
Total train loss: 1.0867

Train time: 13.01800537109375
 * Prec@1 62.370 Prec@5 86.890 Loss 1.4453
Best acc: 62.370
--------------------------------------------------------------------------------
Test time: 15.929197788238525

Epoch: [5][38/196]	LR: 0.1	Loss 1.0781 (1.0718)	Prec@1 71.094 (69.511)	
Epoch: [5][77/196]	LR: 0.1	Loss 1.1270 (1.0772)	Prec@1 66.406 (69.236)	
Epoch: [5][116/196]	LR: 0.1	Loss 1.0957 (1.0834)	Prec@1 69.922 (69.000)	
Epoch: [5][155/196]	LR: 0.1	Loss 1.0342 (1.0780)	Prec@1 73.828 (69.181)	
Epoch: [5][194/196]	LR: 0.1	Loss 1.0010 (1.0806)	Prec@1 71.875 (69.077)	
Total train loss: 1.0802

Train time: 12.052154302597046
 * Prec@1 62.700 Prec@5 87.390 Loss 1.4316
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 14.82998275756836

Epoch: [6][38/196]	LR: 0.1	Loss 1.1162 (1.0420)	Prec@1 68.750 (69.631)	
Epoch: [6][77/196]	LR: 0.1	Loss 1.1367 (1.0599)	Prec@1 67.578 (69.326)	
Epoch: [6][116/196]	LR: 0.1	Loss 1.0830 (1.0661)	Prec@1 67.578 (69.227)	
Epoch: [6][155/196]	LR: 0.1	Loss 1.1553 (1.0689)	Prec@1 66.406 (69.136)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.8818 (1.0742)	Prec@1 73.047 (68.918)	
Total train loss: 1.0742

Train time: 12.864290952682495
 * Prec@1 62.720 Prec@5 87.080 Loss 1.4297
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.283942937850952

Epoch: [7][38/196]	LR: 0.1	Loss 1.0420 (1.0502)	Prec@1 70.312 (69.882)	
Epoch: [7][77/196]	LR: 0.1	Loss 1.0479 (1.0536)	Prec@1 71.875 (69.792)	
Epoch: [7][116/196]	LR: 0.1	Loss 1.1436 (1.0546)	Prec@1 66.797 (69.661)	
Epoch: [7][155/196]	LR: 0.1	Loss 1.1641 (1.0586)	Prec@1 70.703 (69.531)	
Epoch: [7][194/196]	LR: 0.1	Loss 1.0850 (1.0689)	Prec@1 68.359 (69.339)	
Total train loss: 1.0690

Train time: 12.242981910705566
 * Prec@1 62.450 Prec@5 87.200 Loss 1.4463
Best acc: 62.720
--------------------------------------------------------------------------------
Test time: 15.137410879135132

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.9077 (1.0057)	Prec@1 72.656 (70.984)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 1.0654 (1.0213)	Prec@1 67.188 (70.368)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.9668 (1.0262)	Prec@1 71.484 (70.343)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 1.0654 (1.0220)	Prec@1 67.969 (70.500)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 1.0361 (1.0193)	Prec@1 67.188 (70.555)	
Total train loss: 1.0194

Train time: 12.599418878555298
 * Prec@1 63.110 Prec@5 87.380 Loss 1.4150
Best acc: 63.110
--------------------------------------------------------------------------------
Test time: 15.595322608947754

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 1.1699 (1.0154)	Prec@1 69.141 (70.673)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.9521 (1.0143)	Prec@1 70.312 (70.513)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.8184 (1.0082)	Prec@1 78.516 (70.773)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.9878 (1.0081)	Prec@1 75.781 (70.923)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 1.0215 (1.0103)	Prec@1 70.312 (70.875)	
Total train loss: 1.0109

Train time: 12.215991497039795
 * Prec@1 63.140 Prec@5 87.400 Loss 1.4180
Best acc: 63.140
--------------------------------------------------------------------------------
Test time: 15.134416103363037

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.9819 (1.0154)	Prec@1 70.312 (70.903)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 1.1289 (0.9973)	Prec@1 66.797 (71.304)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.9761 (1.0033)	Prec@1 71.875 (71.007)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.9941 (1.0096)	Prec@1 72.266 (70.853)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.9624 (1.0076)	Prec@1 69.922 (70.976)	
Total train loss: 1.0077

Train time: 12.711842775344849
 * Prec@1 63.020 Prec@5 87.440 Loss 1.4150
Best acc: 63.140
--------------------------------------------------------------------------------
Test time: 15.163978576660156

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.9087 (1.0110)	Prec@1 73.828 (70.503)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 1.1074 (1.0101)	Prec@1 68.750 (70.553)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.9424 (1.0068)	Prec@1 71.875 (70.877)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 1.1201 (1.0078)	Prec@1 67.969 (70.941)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 1.0254 (1.0061)	Prec@1 69.141 (70.903)	
Total train loss: 1.0063

Train time: 12.340274333953857
 * Prec@1 63.240 Prec@5 87.420 Loss 1.4160
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.26764965057373

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 1.1260 (1.0148)	Prec@1 67.578 (70.863)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 1.0176 (1.0145)	Prec@1 70.703 (70.743)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 1.0723 (1.0054)	Prec@1 70.312 (71.157)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.9390 (1.0098)	Prec@1 75.391 (71.074)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.9980 (1.0065)	Prec@1 70.703 (71.144)	
Total train loss: 1.0064

Train time: 12.768249034881592
 * Prec@1 63.170 Prec@5 87.460 Loss 1.4160
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.763473749160767

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 1.1465 (1.0054)	Prec@1 67.188 (71.084)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 1.0430 (1.0079)	Prec@1 68.750 (71.249)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.9761 (1.0035)	Prec@1 74.609 (71.167)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 1.0010 (1.0060)	Prec@1 75.391 (71.041)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.9932 (1.0076)	Prec@1 72.656 (70.923)	
Total train loss: 1.0078

Train time: 12.17662525177002
 * Prec@1 63.020 Prec@5 87.490 Loss 1.4229
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.13241720199585

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 1.0166 (1.0047)	Prec@1 72.266 (70.974)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 1.0244 (1.0014)	Prec@1 68.359 (71.084)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 1.0303 (1.0010)	Prec@1 70.703 (71.114)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 1.0225 (1.0025)	Prec@1 73.047 (71.184)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.9932 (1.0059)	Prec@1 70.703 (71.072)	
Total train loss: 1.0060

Train time: 12.435762166976929
 * Prec@1 62.940 Prec@5 87.480 Loss 1.4189
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 14.928388833999634

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.9268 (1.0082)	Prec@1 70.703 (70.903)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 1.0332 (1.0115)	Prec@1 69.922 (70.723)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 1.0693 (1.0034)	Prec@1 71.094 (71.070)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.9233 (1.0050)	Prec@1 72.266 (70.989)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.9673 (1.0049)	Prec@1 71.484 (71.046)	
Total train loss: 1.0052

Train time: 12.397501945495605
 * Prec@1 63.230 Prec@5 87.410 Loss 1.4180
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.2653226852417

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.9683 (0.9811)	Prec@1 71.875 (71.715)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.9360 (0.9975)	Prec@1 73.047 (71.359)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 1.0820 (1.0041)	Prec@1 68.359 (71.161)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 1.0439 (1.0042)	Prec@1 72.656 (71.169)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 1.0850 (0.9992)	Prec@1 70.703 (71.314)	
Total train loss: 0.9991

Train time: 12.475398778915405
 * Prec@1 63.100 Prec@5 87.540 Loss 1.4180
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.439137697219849

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.9951 (0.9967)	Prec@1 70.312 (71.224)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.9995 (0.9923)	Prec@1 70.312 (71.349)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 1.1182 (1.0011)	Prec@1 66.406 (71.154)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.9814 (1.0007)	Prec@1 70.703 (71.174)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.9902 (0.9978)	Prec@1 71.875 (71.216)	
Total train loss: 0.9977

Train time: 13.527414798736572
 * Prec@1 63.090 Prec@5 87.490 Loss 1.4180
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 16.345942735671997

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.9634 (0.9893)	Prec@1 74.219 (71.565)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.9805 (0.9968)	Prec@1 74.609 (71.439)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.9243 (1.0008)	Prec@1 73.828 (71.197)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 1.0742 (0.9977)	Prec@1 66.797 (71.274)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.9541 (1.0004)	Prec@1 75.000 (71.232)	
Total train loss: 1.0002

Train time: 11.897832870483398
 * Prec@1 62.990 Prec@5 87.440 Loss 1.4180
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 14.220969915390015

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 1.0166 (1.0122)	Prec@1 69.141 (70.302)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.9419 (0.9939)	Prec@1 74.219 (71.219)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 1.0889 (0.9967)	Prec@1 69.922 (71.187)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.9458 (0.9982)	Prec@1 76.172 (71.201)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 1.0068 (0.9948)	Prec@1 69.531 (71.236)	
Total train loss: 0.9946

Train time: 12.086656093597412
 * Prec@1 63.070 Prec@5 87.380 Loss 1.4150
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 14.899907350540161

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 1.0176 (0.9858)	Prec@1 69.531 (71.304)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 1.0566 (0.9956)	Prec@1 72.656 (71.104)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.9570 (0.9996)	Prec@1 70.312 (71.314)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 1.1299 (0.9992)	Prec@1 66.406 (71.399)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.9233 (0.9972)	Prec@1 74.219 (71.438)	
Total train loss: 0.9973

Train time: 12.369714736938477
 * Prec@1 63.080 Prec@5 87.540 Loss 1.4189
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 15.208271265029907

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.9985 (0.9982)	Prec@1 72.266 (71.144)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.9609 (1.0123)	Prec@1 70.312 (70.733)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 1.0488 (0.9999)	Prec@1 67.578 (71.147)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.9795 (0.9962)	Prec@1 71.484 (71.214)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 1.0801 (0.9982)	Prec@1 69.141 (71.264)	
Total train loss: 0.9982

Train time: 11.788109064102173
 * Prec@1 63.040 Prec@5 87.540 Loss 1.4180
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 14.636727094650269

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 1.0664 (0.9981)	Prec@1 67.969 (71.184)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.9849 (1.0143)	Prec@1 70.703 (70.583)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 1.0059 (1.0136)	Prec@1 74.219 (70.580)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.9287 (1.0057)	Prec@1 70.703 (70.753)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.9893 (1.0009)	Prec@1 71.484 (70.970)	
Total train loss: 1.0010

Train time: 12.149091720581055
 * Prec@1 63.250 Prec@5 87.470 Loss 1.4160
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 14.464026927947998

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.9741 (0.9991)	Prec@1 73.828 (71.274)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 1.0039 (1.0068)	Prec@1 71.875 (71.044)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.9702 (1.0082)	Prec@1 71.094 (70.957)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 1.0195 (1.0009)	Prec@1 72.656 (71.141)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.9033 (0.9972)	Prec@1 74.219 (71.328)	
Total train loss: 0.9970

Train time: 11.716724872589111
 * Prec@1 63.070 Prec@5 87.530 Loss 1.4180
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 14.557371616363525

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 1.0967 (1.0161)	Prec@1 66.406 (70.723)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.9414 (0.9999)	Prec@1 72.656 (71.389)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 1.0361 (0.9910)	Prec@1 70.312 (71.585)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.9741 (0.9933)	Prec@1 72.656 (71.432)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.8774 (0.9956)	Prec@1 71.484 (71.340)	
Total train loss: 0.9957

Train time: 12.190665245056152
 * Prec@1 63.200 Prec@5 87.370 Loss 1.4160
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 14.490521907806396

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 1.1318 (1.0016)	Prec@1 70.703 (71.394)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.8647 (0.9926)	Prec@1 74.219 (71.570)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 1.0352 (0.9964)	Prec@1 70.312 (71.468)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 1.0781 (0.9981)	Prec@1 69.922 (71.334)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.9849 (0.9984)	Prec@1 70.703 (71.248)	
Total train loss: 0.9982

Train time: 12.19579005241394
 * Prec@1 63.150 Prec@5 87.450 Loss 1.4180
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 15.637173414230347

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 1.0518 (1.0068)	Prec@1 68.750 (70.703)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.9351 (1.0058)	Prec@1 71.875 (70.928)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.9277 (1.0067)	Prec@1 71.094 (70.900)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.9971 (1.0032)	Prec@1 69.141 (71.074)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.9541 (0.9992)	Prec@1 69.922 (71.106)	
Total train loss: 0.9994

Train time: 12.416064977645874
 * Prec@1 63.110 Prec@5 87.480 Loss 1.4180
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 14.768544435501099

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 1.0176 (1.0032)	Prec@1 69.922 (71.474)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 1.0508 (0.9952)	Prec@1 68.359 (71.369)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 1.0566 (0.9956)	Prec@1 69.922 (71.378)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 1.2178 (0.9962)	Prec@1 64.062 (71.267)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.9224 (0.9970)	Prec@1 73.828 (71.248)	
Total train loss: 0.9970

Train time: 12.17473316192627
 * Prec@1 63.010 Prec@5 87.440 Loss 1.4199
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 15.091469049453735

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 1.0801 (1.0059)	Prec@1 70.312 (70.893)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 1.1162 (1.0060)	Prec@1 70.312 (70.994)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.9302 (1.0002)	Prec@1 73.047 (71.187)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.9492 (0.9971)	Prec@1 69.922 (71.379)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.9917 (0.9999)	Prec@1 72.656 (71.194)	
Total train loss: 1.0004

Train time: 12.522578477859497
 * Prec@1 63.150 Prec@5 87.470 Loss 1.4150
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 14.870760917663574

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.9248 (0.9797)	Prec@1 75.391 (72.306)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 1.0303 (0.9944)	Prec@1 68.750 (71.595)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 1.0156 (1.0025)	Prec@1 69.531 (71.284)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 1.1787 (0.9960)	Prec@1 62.891 (71.359)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 1.0303 (0.9979)	Prec@1 72.266 (71.176)	
Total train loss: 0.9983

Train time: 12.332038164138794
 * Prec@1 63.080 Prec@5 87.380 Loss 1.4160
Best acc: 63.250
--------------------------------------------------------------------------------
Test time: 15.836856126785278

