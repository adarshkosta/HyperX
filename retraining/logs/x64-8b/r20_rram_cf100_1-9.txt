
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv2): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 68.440 Prec@5 90.230 Loss 1.1943
Pre-trained Prec@1 with 1 layers frozen: 68.43999481201172 	 Loss: 1.1943359375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.5044 (0.5172)	Prec@1 87.500 (85.327)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.6260 (0.5277)	Prec@1 80.859 (85.246)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.5698 (0.5431)	Prec@1 82.812 (84.779)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.8232 (0.5617)	Prec@1 75.000 (84.255)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.6875 (0.5830)	Prec@1 82.812 (83.628)	
Total train loss: 0.5834

Train time: 275.6216399669647
 * Prec@1 60.510 Prec@5 84.900 Loss 1.5645
Best acc: 60.510
--------------------------------------------------------------------------------
Test time: 300.00056171417236

Epoch: [1][38/196]	LR: 0.01	Loss 0.5879 (0.6640)	Prec@1 82.031 (81.010)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.7319 (0.6911)	Prec@1 79.688 (80.394)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.8418 (0.7030)	Prec@1 75.781 (80.292)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.8442 (0.7113)	Prec@1 78.516 (79.965)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.8340 (0.7079)	Prec@1 76.953 (80.090)	
Total train loss: 0.7080

Train time: 205.13469672203064
 * Prec@1 63.240 Prec@5 87.010 Loss 1.4492
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 216.3919005393982

Epoch: [2][38/196]	LR: 0.01	Loss 0.7935 (0.7298)	Prec@1 77.344 (79.377)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.8379 (0.7318)	Prec@1 76.172 (79.187)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.7373 (0.7655)	Prec@1 78.906 (78.058)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.9287 (0.7930)	Prec@1 73.047 (77.251)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.1045 (0.8502)	Prec@1 70.703 (75.619)	
Total train loss: 0.8505

Train time: 28.768012523651123
 * Prec@1 51.090 Prec@5 78.780 Loss 2.1562
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 34.17060923576355

Epoch: [3][38/196]	LR: 0.01	Loss 1.3193 (1.1630)	Prec@1 63.281 (67.388)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.7070 (1.2712)	Prec@1 55.859 (64.568)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.5508 (1.3349)	Prec@1 56.641 (62.734)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.4180 (1.3664)	Prec@1 60.938 (62.039)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.4561 (1.3880)	Prec@1 57.422 (61.540)	
Total train loss: 1.3886

Train time: 30.535922527313232
 * Prec@1 40.960 Prec@5 70.800 Loss 2.4023
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 35.8915650844574

Epoch: [4][38/196]	LR: 0.01	Loss 1.4160 (1.5044)	Prec@1 64.062 (59.175)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.7002 (1.5410)	Prec@1 53.125 (58.308)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.5654 (1.5928)	Prec@1 59.375 (57.472)	
Epoch: [4][155/196]	LR: 0.01	Loss 1.9912 (1.6656)	Prec@1 44.531 (55.834)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.8262 (1.7129)	Prec@1 49.609 (54.545)	
Total train loss: 1.7134

Train time: 23.512765407562256
 * Prec@1 33.570 Prec@5 63.700 Loss 2.9258
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.279748678207397

Epoch: [5][38/196]	LR: 0.01	Loss 2.0840 (1.9669)	Prec@1 45.703 (48.968)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.9482 (1.9817)	Prec@1 46.875 (47.982)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.7617 (1.9541)	Prec@1 51.562 (48.371)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.9219 (1.9497)	Prec@1 50.391 (48.458)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.8086 (1.9395)	Prec@1 51.562 (48.730)	
Total train loss: 1.9392

Train time: 25.60037612915039
 * Prec@1 23.980 Prec@5 52.030 Loss 3.9805
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 31.23387861251831

Epoch: [6][38/196]	LR: 0.01	Loss 1.9043 (1.9278)	Prec@1 50.391 (48.658)	
Epoch: [6][77/196]	LR: 0.01	Loss 2.0820 (1.9543)	Prec@1 45.703 (48.282)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.9375 (2.0120)	Prec@1 50.391 (47.045)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.7725 (1.9583)	Prec@1 51.953 (48.245)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.6787 (1.9188)	Prec@1 53.516 (49.283)	
Total train loss: 1.9193

Train time: 20.700278759002686
 * Prec@1 13.080 Prec@5 34.910 Loss 3.9609
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.274898052215576

Epoch: [7][38/196]	LR: 0.01	Loss 1.8652 (1.8070)	Prec@1 50.000 (52.113)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.7344 (1.7935)	Prec@1 55.469 (52.439)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.7129 (1.7864)	Prec@1 54.688 (52.631)	
Epoch: [7][155/196]	LR: 0.01	Loss 1.7852 (1.7818)	Prec@1 50.000 (52.732)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.6904 (1.7963)	Prec@1 58.203 (52.534)	
Total train loss: 1.7967

Train time: 23.069806337356567
 * Prec@1 41.640 Prec@5 72.260 Loss 2.3594
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.213404893875122

Epoch: [8][38/196]	LR: 0.001	Loss 1.7510 (1.7973)	Prec@1 59.375 (52.584)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.6475 (1.7801)	Prec@1 61.328 (53.300)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.8438 (1.7763)	Prec@1 48.828 (53.355)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.5605 (1.7701)	Prec@1 60.938 (53.571)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.8066 (1.7662)	Prec@1 52.734 (53.736)	
Total train loss: 1.7666

Train time: 21.54499650001526
 * Prec@1 50.310 Prec@5 79.590 Loss 1.8926
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.856168270111084

Epoch: [9][38/196]	LR: 0.001	Loss 1.7334 (1.7369)	Prec@1 54.688 (54.307)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.6113 (1.7379)	Prec@1 60.938 (54.577)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.8613 (1.7368)	Prec@1 50.781 (54.617)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.8320 (1.7419)	Prec@1 51.562 (54.364)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.7773 (1.7445)	Prec@1 53.125 (54.343)	
Total train loss: 1.7448

Train time: 23.584743976593018
 * Prec@1 50.300 Prec@5 79.690 Loss 1.8896
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.159136533737183

Epoch: [10][38/196]	LR: 0.001	Loss 1.6221 (1.7543)	Prec@1 57.422 (53.446)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.9033 (1.7441)	Prec@1 52.344 (54.041)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.7686 (1.7476)	Prec@1 54.688 (54.000)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.7227 (1.7370)	Prec@1 52.734 (54.329)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.5244 (1.7369)	Prec@1 59.375 (54.401)	
Total train loss: 1.7370

Train time: 21.857823371887207
 * Prec@1 50.180 Prec@5 79.600 Loss 1.8867
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.021437168121338

Epoch: [11][38/196]	LR: 0.001	Loss 1.7217 (1.7220)	Prec@1 56.641 (54.908)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.7500 (1.7277)	Prec@1 50.000 (54.718)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.7119 (1.7253)	Prec@1 55.078 (54.798)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.8135 (1.7366)	Prec@1 53.125 (54.380)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.7373 (1.7348)	Prec@1 52.344 (54.439)	
Total train loss: 1.7347

Train time: 20.686760187149048
 * Prec@1 50.560 Prec@5 79.820 Loss 1.8799
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 25.6407208442688

Epoch: [12][38/196]	LR: 0.001	Loss 1.7646 (1.7279)	Prec@1 52.734 (54.667)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.6748 (1.7197)	Prec@1 55.469 (54.843)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.6963 (1.7327)	Prec@1 56.641 (54.454)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.7090 (1.7304)	Prec@1 54.688 (54.627)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.7227 (1.7310)	Prec@1 54.297 (54.629)	
Total train loss: 1.7316

Train time: 23.673197031021118
 * Prec@1 50.540 Prec@5 79.970 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.239145517349243

Epoch: [13][38/196]	LR: 0.001	Loss 1.7090 (1.7243)	Prec@1 54.688 (54.898)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.7178 (1.7224)	Prec@1 53.125 (54.958)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.6777 (1.7250)	Prec@1 55.859 (54.784)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.6455 (1.7269)	Prec@1 57.031 (54.825)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.7119 (1.7297)	Prec@1 54.297 (54.692)	
Total train loss: 1.7301

Train time: 22.469717502593994
 * Prec@1 50.730 Prec@5 79.950 Loss 1.8877
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 29.2802414894104

Epoch: [14][38/196]	LR: 0.001	Loss 1.6064 (1.7453)	Prec@1 56.250 (54.357)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.6855 (1.7350)	Prec@1 56.250 (54.462)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.8096 (1.7295)	Prec@1 52.734 (54.531)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.8662 (1.7328)	Prec@1 50.781 (54.560)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.6270 (1.7284)	Prec@1 57.422 (54.768)	
Total train loss: 1.7281

Train time: 21.604494094848633
 * Prec@1 50.860 Prec@5 79.960 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.059411764144897

Epoch: [15][38/196]	LR: 0.001	Loss 1.8037 (1.7009)	Prec@1 51.562 (55.409)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.7588 (1.7085)	Prec@1 53.516 (55.268)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.6699 (1.7177)	Prec@1 56.641 (54.838)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.8027 (1.7282)	Prec@1 50.781 (54.627)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.7529 (1.7253)	Prec@1 54.297 (54.738)	
Total train loss: 1.7253

Train time: 21.90856122970581
 * Prec@1 50.600 Prec@5 79.950 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 26.4760639667511

Epoch: [16][38/196]	LR: 0.0001	Loss 1.6475 (1.7132)	Prec@1 54.688 (54.838)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.8506 (1.7232)	Prec@1 48.828 (54.447)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.7842 (1.7241)	Prec@1 53.906 (54.501)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.7021 (1.7218)	Prec@1 54.688 (54.693)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.8506 (1.7201)	Prec@1 53.125 (54.786)	
Total train loss: 1.7205

Train time: 22.832563877105713
 * Prec@1 50.890 Prec@5 79.830 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 29.114020347595215

Epoch: [17][38/196]	LR: 0.0001	Loss 1.6611 (1.7344)	Prec@1 53.125 (54.758)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.7305 (1.7319)	Prec@1 55.859 (54.652)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.6211 (1.7263)	Prec@1 58.984 (54.651)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.7617 (1.7215)	Prec@1 55.469 (54.780)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.7002 (1.7217)	Prec@1 55.859 (54.772)	
Total train loss: 1.7218

Train time: 22.492656469345093
 * Prec@1 50.660 Prec@5 79.910 Loss 1.8828
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.736771821975708

Epoch: [18][38/196]	LR: 0.0001	Loss 1.8008 (1.7174)	Prec@1 57.422 (55.038)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.7891 (1.7115)	Prec@1 52.734 (55.163)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.8516 (1.7182)	Prec@1 50.000 (54.965)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.6143 (1.7155)	Prec@1 60.938 (55.073)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.7666 (1.7167)	Prec@1 53.516 (54.970)	
Total train loss: 1.7171

Train time: 20.016063451766968
 * Prec@1 50.650 Prec@5 79.850 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 26.657190799713135

Epoch: [19][38/196]	LR: 0.0001	Loss 1.7373 (1.7146)	Prec@1 55.078 (54.808)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.7275 (1.7119)	Prec@1 55.078 (54.883)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.6562 (1.7210)	Prec@1 55.469 (54.698)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.7188 (1.7265)	Prec@1 53.125 (54.690)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.7744 (1.7211)	Prec@1 53.125 (54.862)	
Total train loss: 1.7210

Train time: 22.83987307548523
 * Prec@1 50.640 Prec@5 79.740 Loss 1.8867
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.229377031326294

Epoch: [20][38/196]	LR: 0.0001	Loss 1.6797 (1.7061)	Prec@1 53.125 (55.619)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.8457 (1.7095)	Prec@1 53.125 (55.359)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.5479 (1.7195)	Prec@1 60.547 (54.908)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.7832 (1.7186)	Prec@1 52.734 (55.028)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.6387 (1.7171)	Prec@1 53.516 (55.012)	
Total train loss: 1.7172

Train time: 22.16712188720703
 * Prec@1 50.720 Prec@5 79.880 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.170011281967163

Epoch: [21][38/196]	LR: 0.0001	Loss 1.7236 (1.7382)	Prec@1 55.469 (54.467)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.6641 (1.7169)	Prec@1 57.422 (54.988)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.7168 (1.7271)	Prec@1 54.688 (54.661)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.7422 (1.7215)	Prec@1 53.125 (54.815)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.7285 (1.7217)	Prec@1 51.953 (54.792)	
Total train loss: 1.7214

Train time: 23.236912965774536
 * Prec@1 50.790 Prec@5 79.790 Loss 1.8896
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.338451385498047

Epoch: [22][38/196]	LR: 0.0001	Loss 1.6426 (1.7183)	Prec@1 57.812 (54.938)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.6592 (1.7332)	Prec@1 53.906 (54.447)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.7832 (1.7319)	Prec@1 54.688 (54.554)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.8027 (1.7265)	Prec@1 50.391 (54.710)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.7373 (1.7200)	Prec@1 53.906 (54.790)	
Total train loss: 1.7199

Train time: 23.16047430038452
 * Prec@1 50.780 Prec@5 79.900 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.563000202178955

Epoch: [23][38/196]	LR: 0.0001	Loss 1.6201 (1.7102)	Prec@1 59.375 (55.078)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.7852 (1.7169)	Prec@1 53.906 (55.008)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.5449 (1.7215)	Prec@1 59.766 (55.108)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.7568 (1.7210)	Prec@1 53.906 (54.853)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.7324 (1.7208)	Prec@1 55.078 (54.706)	
Total train loss: 1.7206

Train time: 22.608800172805786
 * Prec@1 50.780 Prec@5 79.820 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.895925045013428

Epoch: [24][38/196]	LR: 1e-05	Loss 1.7344 (1.7248)	Prec@1 53.906 (54.988)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.7949 (1.7242)	Prec@1 51.172 (54.652)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.7090 (1.7251)	Prec@1 55.859 (54.567)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.7744 (1.7229)	Prec@1 53.516 (54.778)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.7490 (1.7210)	Prec@1 56.250 (54.862)	
Total train loss: 1.7206

Train time: 23.321701526641846
 * Prec@1 50.670 Prec@5 79.790 Loss 1.8799
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 29.09086036682129

Epoch: [25][38/196]	LR: 1e-05	Loss 1.7705 (1.7127)	Prec@1 51.953 (54.637)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.6484 (1.7128)	Prec@1 57.031 (54.808)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.6475 (1.7159)	Prec@1 56.641 (54.874)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.6318 (1.7213)	Prec@1 58.984 (54.753)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.8154 (1.7199)	Prec@1 56.250 (54.850)	
Total train loss: 1.7199

Train time: 22.295124769210815
 * Prec@1 50.930 Prec@5 80.010 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.194908142089844

Epoch: [26][38/196]	LR: 1e-05	Loss 1.7236 (1.7232)	Prec@1 58.203 (54.667)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.7900 (1.7163)	Prec@1 52.344 (54.878)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.6797 (1.7210)	Prec@1 51.953 (54.734)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.7510 (1.7235)	Prec@1 52.344 (54.680)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.7773 (1.7210)	Prec@1 49.219 (54.780)	
Total train loss: 1.7208

Train time: 23.473348140716553
 * Prec@1 50.720 Prec@5 79.900 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 29.066624641418457

Epoch: [27][38/196]	LR: 1e-05	Loss 1.6738 (1.6995)	Prec@1 55.469 (55.609)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.6992 (1.7160)	Prec@1 57.422 (55.153)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.8740 (1.7177)	Prec@1 51.562 (54.788)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.5781 (1.7142)	Prec@1 59.766 (54.835)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.9355 (1.7194)	Prec@1 48.828 (54.696)	
Total train loss: 1.7196

Train time: 22.582384824752808
 * Prec@1 50.670 Prec@5 79.760 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.18740224838257

Epoch: [28][38/196]	LR: 1e-05	Loss 1.7637 (1.7188)	Prec@1 49.219 (54.627)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.6543 (1.7085)	Prec@1 55.469 (55.013)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.7734 (1.7114)	Prec@1 55.469 (55.018)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.7539 (1.7164)	Prec@1 52.734 (54.870)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.8916 (1.7217)	Prec@1 51.562 (54.838)	
Total train loss: 1.7215

Train time: 22.152987718582153
 * Prec@1 50.730 Prec@5 79.850 Loss 1.8867
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 27.94913077354431

Epoch: [29][38/196]	LR: 1e-05	Loss 1.6260 (1.7306)	Prec@1 54.688 (54.768)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.7393 (1.7297)	Prec@1 51.562 (54.462)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.8350 (1.7253)	Prec@1 53.906 (54.661)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.6035 (1.7186)	Prec@1 56.250 (54.795)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.6660 (1.7187)	Prec@1 53.906 (54.866)	
Total train loss: 1.7187

Train time: 21.855486154556274
 * Prec@1 50.650 Prec@5 79.940 Loss 1.8877
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 26.98586678504944

Epoch: [30][38/196]	LR: 1e-05	Loss 1.6250 (1.7081)	Prec@1 56.641 (55.188)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.7744 (1.7260)	Prec@1 51.953 (54.933)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.7715 (1.7211)	Prec@1 53.516 (55.101)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.7334 (1.7189)	Prec@1 57.422 (55.078)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.5918 (1.7206)	Prec@1 60.547 (55.078)	
Total train loss: 1.7208

Train time: 22.038896799087524
 * Prec@1 50.840 Prec@5 79.930 Loss 1.8799
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.684183835983276

Epoch: [31][38/196]	LR: 1e-05	Loss 1.5820 (1.7433)	Prec@1 56.641 (54.167)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.7002 (1.7226)	Prec@1 53.906 (54.828)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.8076 (1.7255)	Prec@1 51.172 (54.751)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.6572 (1.7226)	Prec@1 58.984 (54.738)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.8164 (1.7200)	Prec@1 52.344 (54.854)	
Total train loss: 1.7199

Train time: 23.70365023612976
 * Prec@1 50.700 Prec@5 79.930 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.087907314300537

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.6270 (1.7273)	Prec@1 55.859 (55.028)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.6816 (1.7285)	Prec@1 53.516 (54.763)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.6299 (1.7208)	Prec@1 56.641 (54.824)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.6641 (1.7194)	Prec@1 57.812 (54.828)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.7725 (1.7196)	Prec@1 51.953 (54.828)	
Total train loss: 1.7201

Train time: 23.08577537536621
 * Prec@1 50.840 Prec@5 79.990 Loss 1.8828
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.392887353897095

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.6475 (1.7228)	Prec@1 56.641 (54.207)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.6602 (1.7131)	Prec@1 56.641 (54.738)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.7129 (1.7155)	Prec@1 57.031 (54.691)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.6533 (1.7180)	Prec@1 56.250 (54.647)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.8232 (1.7207)	Prec@1 54.297 (54.698)	
Total train loss: 1.7205

Train time: 23.948434352874756
 * Prec@1 50.910 Prec@5 79.890 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.26541304588318

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.5850 (1.7117)	Prec@1 55.078 (55.178)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.7344 (1.7216)	Prec@1 52.734 (54.552)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.6113 (1.7190)	Prec@1 55.469 (54.714)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.7881 (1.7191)	Prec@1 51.953 (54.585)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.8008 (1.7219)	Prec@1 52.344 (54.649)	
Total train loss: 1.7224

Train time: 23.949279308319092
 * Prec@1 50.950 Prec@5 79.810 Loss 1.8799
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 30.89492392539978

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.6719 (1.7077)	Prec@1 56.250 (55.058)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.6299 (1.7212)	Prec@1 57.422 (54.843)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.7275 (1.7141)	Prec@1 52.734 (54.821)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.7842 (1.7160)	Prec@1 53.125 (54.768)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.8271 (1.7200)	Prec@1 53.906 (54.736)	
Total train loss: 1.7200

Train time: 24.130988121032715
 * Prec@1 50.770 Prec@5 79.970 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 28.876010417938232

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.7539 (1.7146)	Prec@1 50.000 (55.168)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.7178 (1.7246)	Prec@1 54.688 (54.838)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.7422 (1.7209)	Prec@1 52.734 (54.888)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.7178 (1.7220)	Prec@1 57.031 (54.758)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.6699 (1.7198)	Prec@1 57.031 (54.742)	
Total train loss: 1.7203

Train time: 14.306990385055542
 * Prec@1 50.580 Prec@5 79.960 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 18.372487783432007

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.8447 (1.7065)	Prec@1 52.734 (55.819)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.5664 (1.7069)	Prec@1 60.156 (55.689)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.7910 (1.7147)	Prec@1 54.688 (55.235)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.6650 (1.7110)	Prec@1 56.641 (55.238)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.7588 (1.7181)	Prec@1 52.734 (54.988)	
Total train loss: 1.7187

Train time: 42.770498275756836
 * Prec@1 50.600 Prec@5 79.770 Loss 1.8848
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 52.83856654167175

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 1.6455 (1.7296)	Prec@1 56.250 (54.297)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.7539 (1.7073)	Prec@1 52.734 (54.923)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.8740 (1.7131)	Prec@1 50.781 (54.928)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.8037 (1.7201)	Prec@1 54.297 (54.803)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.6689 (1.7200)	Prec@1 55.469 (54.874)	
Total train loss: 1.7204

Train time: 62.204140186309814
 * Prec@1 50.740 Prec@5 79.870 Loss 1.8828
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 76.59421491622925

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.7617 (1.7257)	Prec@1 52.734 (54.738)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.7695 (1.7199)	Prec@1 54.297 (55.048)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.7354 (1.7171)	Prec@1 57.422 (55.182)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.8037 (1.7224)	Prec@1 51.953 (54.853)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.6787 (1.7196)	Prec@1 55.469 (54.848)	
Total train loss: 1.7201

Train time: 35.00907039642334
 * Prec@1 50.860 Prec@5 80.000 Loss 1.8818
Best acc: 63.240
--------------------------------------------------------------------------------
Test time: 40.10084867477417


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 68.200 Prec@5 89.820 Loss 1.2217
Pre-trained Prec@1 with 3 layers frozen: 68.19999694824219 	 Loss: 1.2216796875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.6943 (0.5311)	Prec@1 78.516 (85.216)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.5679 (0.5432)	Prec@1 89.062 (84.896)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.5630 (0.5555)	Prec@1 84.375 (84.505)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.6899 (0.5749)	Prec@1 79.688 (83.867)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.5942 (0.5905)	Prec@1 82.031 (83.452)	
Total train loss: 0.5910

Train time: 99.01108980178833
 * Prec@1 62.700 Prec@5 87.250 Loss 1.5381
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 104.49769973754883

Epoch: [1][38/196]	LR: 0.01	Loss 0.6812 (0.6289)	Prec@1 80.469 (82.292)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.5664 (0.6547)	Prec@1 84.375 (81.335)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.8052 (0.6716)	Prec@1 77.734 (80.789)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.8369 (0.6957)	Prec@1 76.172 (80.058)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.9619 (0.7187)	Prec@1 71.484 (79.339)	
Total train loss: 0.7190

Train time: 24.083603143692017
 * Prec@1 43.300 Prec@5 72.040 Loss 2.4668
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.619925498962402

Epoch: [2][38/196]	LR: 0.01	Loss 0.8989 (0.8937)	Prec@1 69.922 (74.339)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.9331 (0.9067)	Prec@1 71.875 (74.038)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.2256 (0.9500)	Prec@1 65.625 (72.766)	
Epoch: [2][155/196]	LR: 0.01	Loss 1.3047 (1.0349)	Prec@1 67.188 (70.808)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.2852 (1.1008)	Prec@1 62.109 (69.337)	
Total train loss: 1.1017

Train time: 23.51390838623047
 * Prec@1 6.320 Prec@5 19.440 Loss 6.0977
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.718408823013306

Epoch: [3][38/196]	LR: 0.01	Loss 1.4795 (1.4174)	Prec@1 60.547 (61.168)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.6484 (1.4486)	Prec@1 55.078 (60.462)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.7949 (1.5027)	Prec@1 52.344 (59.335)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.6934 (1.5630)	Prec@1 57.031 (57.978)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.6270 (1.5882)	Prec@1 57.422 (57.462)	
Total train loss: 1.5887

Train time: 22.851318836212158
 * Prec@1 42.070 Prec@5 71.720 Loss 2.3320
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.171793222427368

Epoch: [4][38/196]	LR: 0.01	Loss 1.8838 (1.7821)	Prec@1 51.172 (53.606)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.7852 (1.7980)	Prec@1 53.516 (53.295)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.9023 (1.8475)	Prec@1 50.391 (51.983)	
Epoch: [4][155/196]	LR: 0.01	Loss 1.7852 (1.8520)	Prec@1 50.781 (51.740)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.8604 (1.8508)	Prec@1 50.391 (51.669)	
Total train loss: 1.8511

Train time: 22.741543769836426
 * Prec@1 27.930 Prec@5 55.790 Loss 3.5527
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.824091911315918

Epoch: [5][38/196]	LR: 0.01	Loss 1.8115 (1.8112)	Prec@1 51.953 (52.143)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.7207 (1.8208)	Prec@1 50.000 (51.763)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.6182 (1.8158)	Prec@1 54.297 (51.769)	
Epoch: [5][155/196]	LR: 0.01	Loss 2.0293 (1.8287)	Prec@1 45.312 (51.412)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.8203 (1.8448)	Prec@1 53.516 (51.056)	
Total train loss: 1.8450

Train time: 22.521864891052246
 * Prec@1 26.220 Prec@5 54.810 Loss 3.1172
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 24.987851858139038

Epoch: [6][38/196]	LR: 0.01	Loss 1.9443 (1.9071)	Prec@1 49.219 (50.140)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.9092 (1.8769)	Prec@1 48.438 (50.406)	
Epoch: [6][116/196]	LR: 0.01	Loss 2.1484 (1.9099)	Prec@1 43.750 (49.783)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.9043 (1.9225)	Prec@1 48.047 (49.447)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.9150 (1.9115)	Prec@1 48.047 (49.501)	
Total train loss: 1.9116

Train time: 22.56721329689026
 * Prec@1 20.760 Prec@5 44.300 Loss 4.3945
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.582244396209717

Epoch: [7][38/196]	LR: 0.01	Loss 1.9121 (1.8346)	Prec@1 50.781 (51.663)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.9727 (1.8689)	Prec@1 46.094 (50.280)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.7637 (1.8561)	Prec@1 53.516 (50.431)	
Epoch: [7][155/196]	LR: 0.01	Loss 1.6816 (1.8391)	Prec@1 52.344 (50.874)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.5225 (1.8308)	Prec@1 60.156 (51.042)	
Total train loss: 1.8308

Train time: 23.052492141723633
 * Prec@1 2.250 Prec@5 7.830 Loss inf
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.643451929092407

Epoch: [8][38/196]	LR: 0.001	Loss 1.5869 (1.6999)	Prec@1 57.031 (54.367)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.4873 (1.6797)	Prec@1 60.547 (54.462)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.6289 (1.6729)	Prec@1 58.984 (54.501)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.6836 (1.6699)	Prec@1 56.641 (54.582)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.7109 (1.6673)	Prec@1 51.562 (54.645)	
Total train loss: 1.6682

Train time: 22.348390579223633
 * Prec@1 50.890 Prec@5 80.330 Loss 1.9004
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 26.17753505706787

Epoch: [9][38/196]	LR: 0.001	Loss 1.3311 (1.6525)	Prec@1 62.891 (54.898)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.7568 (1.6490)	Prec@1 55.469 (55.339)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.6836 (1.6562)	Prec@1 49.609 (54.878)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.6641 (1.6463)	Prec@1 53.516 (55.148)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.5742 (1.6381)	Prec@1 57.031 (55.385)	
Total train loss: 1.6382

Train time: 21.697627067565918
 * Prec@1 50.760 Prec@5 80.480 Loss 1.8916
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 25.666168689727783

Epoch: [10][38/196]	LR: 0.001	Loss 1.6689 (1.6223)	Prec@1 55.078 (55.038)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.8057 (1.6266)	Prec@1 53.125 (55.429)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.7227 (1.6288)	Prec@1 52.734 (55.319)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.4541 (1.6293)	Prec@1 60.547 (55.404)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.5820 (1.6302)	Prec@1 56.641 (55.425)	
Total train loss: 1.6306

Train time: 23.05776858329773
 * Prec@1 48.340 Prec@5 78.800 Loss 2.0273
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.30441117286682

Epoch: [11][38/196]	LR: 0.001	Loss 1.8213 (1.6021)	Prec@1 51.562 (55.669)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.6475 (1.6240)	Prec@1 56.641 (55.444)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.7188 (1.6329)	Prec@1 52.344 (55.215)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.7031 (1.6282)	Prec@1 55.469 (55.339)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.6875 (1.6274)	Prec@1 56.641 (55.397)	
Total train loss: 1.6274

Train time: 22.482884407043457
 * Prec@1 50.920 Prec@5 80.540 Loss 1.8867
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.99245262145996

Epoch: [12][38/196]	LR: 0.001	Loss 1.4863 (1.6065)	Prec@1 62.500 (56.490)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.8291 (1.6208)	Prec@1 53.516 (56.075)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.6611 (1.6203)	Prec@1 51.953 (55.793)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.5332 (1.6249)	Prec@1 55.469 (55.772)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.6279 (1.6276)	Prec@1 57.812 (55.763)	
Total train loss: 1.6280

Train time: 23.31450581550598
 * Prec@1 48.310 Prec@5 78.640 Loss 2.0215
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 29.674604415893555

Epoch: [13][38/196]	LR: 0.001	Loss 1.7217 (1.6244)	Prec@1 56.641 (55.899)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.5000 (1.6158)	Prec@1 59.766 (56.035)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.6328 (1.6276)	Prec@1 55.859 (55.712)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.6924 (1.6297)	Prec@1 56.250 (55.566)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.5029 (1.6295)	Prec@1 61.328 (55.697)	
Total train loss: 1.6296

Train time: 23.206883192062378
 * Prec@1 50.650 Prec@5 80.570 Loss 1.9053
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.810577154159546

Epoch: [14][38/196]	LR: 0.001	Loss 1.5459 (1.6523)	Prec@1 53.125 (54.708)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.6221 (1.6301)	Prec@1 55.469 (55.519)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.7725 (1.6372)	Prec@1 51.953 (55.295)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.4355 (1.6256)	Prec@1 62.500 (55.717)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.6104 (1.6296)	Prec@1 52.734 (55.643)	
Total train loss: 1.6298

Train time: 21.661264181137085
 * Prec@1 50.860 Prec@5 80.320 Loss 1.8896
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 26.587212800979614

Epoch: [15][38/196]	LR: 0.001	Loss 1.4883 (1.6322)	Prec@1 58.203 (55.589)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.7207 (1.6077)	Prec@1 55.078 (56.230)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.8887 (1.6130)	Prec@1 48.047 (56.103)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.4883 (1.6238)	Prec@1 60.938 (55.787)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.5938 (1.6271)	Prec@1 60.547 (55.665)	
Total train loss: 1.6269

Train time: 22.08497166633606
 * Prec@1 50.120 Prec@5 80.140 Loss 1.9180
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.148667573928833

Epoch: [16][38/196]	LR: 0.0001	Loss 1.7568 (1.6314)	Prec@1 50.000 (55.609)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.6494 (1.6214)	Prec@1 55.859 (55.924)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.5693 (1.6259)	Prec@1 56.641 (55.766)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.5449 (1.6186)	Prec@1 58.594 (56.032)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.7373 (1.6216)	Prec@1 50.000 (55.809)	
Total train loss: 1.6214

Train time: 22.206576108932495
 * Prec@1 50.820 Prec@5 80.450 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 26.20656418800354

Epoch: [17][38/196]	LR: 0.0001	Loss 1.5752 (1.6380)	Prec@1 60.938 (55.379)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.4434 (1.6305)	Prec@1 62.500 (55.419)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.6396 (1.6259)	Prec@1 57.031 (55.536)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.4971 (1.6244)	Prec@1 58.984 (55.549)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.4922 (1.6193)	Prec@1 57.812 (55.785)	
Total train loss: 1.6196

Train time: 23.845258474349976
 * Prec@1 50.720 Prec@5 80.440 Loss 1.8965
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.530324697494507

Epoch: [18][38/196]	LR: 0.0001	Loss 1.7061 (1.6313)	Prec@1 56.250 (55.329)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.7334 (1.6359)	Prec@1 54.688 (55.133)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.6055 (1.6250)	Prec@1 53.516 (55.519)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.5977 (1.6213)	Prec@1 55.469 (55.672)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.7334 (1.6227)	Prec@1 49.219 (55.563)	
Total train loss: 1.6227

Train time: 22.244093418121338
 * Prec@1 50.630 Prec@5 80.440 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.291072607040405

Epoch: [19][38/196]	LR: 0.0001	Loss 1.5977 (1.6302)	Prec@1 55.469 (55.659)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.8076 (1.6299)	Prec@1 49.609 (55.479)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.5244 (1.6174)	Prec@1 55.859 (55.923)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.6826 (1.6217)	Prec@1 55.469 (55.807)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.5859 (1.6229)	Prec@1 55.859 (55.739)	
Total train loss: 1.6230

Train time: 22.655841827392578
 * Prec@1 50.560 Prec@5 80.420 Loss 1.8975
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 26.06084442138672

Epoch: [20][38/196]	LR: 0.0001	Loss 1.6484 (1.6286)	Prec@1 54.688 (55.819)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.6416 (1.6285)	Prec@1 51.562 (55.584)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.3320 (1.6162)	Prec@1 59.766 (55.929)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.4229 (1.6230)	Prec@1 62.109 (55.749)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.7100 (1.6217)	Prec@1 53.906 (55.713)	
Total train loss: 1.6218

Train time: 22.2469482421875
 * Prec@1 50.800 Prec@5 80.550 Loss 1.8867
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 25.99304223060608

Epoch: [21][38/196]	LR: 0.0001	Loss 1.6602 (1.6267)	Prec@1 54.297 (55.970)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.6768 (1.6265)	Prec@1 58.594 (55.814)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.5576 (1.6195)	Prec@1 56.641 (55.789)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.5283 (1.6175)	Prec@1 55.469 (55.879)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.7510 (1.6178)	Prec@1 54.688 (55.839)	
Total train loss: 1.6180

Train time: 22.674104928970337
 * Prec@1 50.950 Prec@5 80.490 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.01746416091919

Epoch: [22][38/196]	LR: 0.0001	Loss 1.6924 (1.6391)	Prec@1 48.438 (55.499)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.5635 (1.6222)	Prec@1 53.516 (55.759)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.5654 (1.6179)	Prec@1 56.250 (55.803)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.7373 (1.6226)	Prec@1 51.562 (55.629)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.5859 (1.6211)	Prec@1 52.344 (55.601)	
Total train loss: 1.6214

Train time: 22.737369775772095
 * Prec@1 50.740 Prec@5 80.490 Loss 1.8867
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.45621156692505

Epoch: [23][38/196]	LR: 0.0001	Loss 1.5889 (1.6217)	Prec@1 56.641 (55.439)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.6445 (1.6328)	Prec@1 55.469 (55.258)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.5703 (1.6354)	Prec@1 58.984 (55.312)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.6514 (1.6234)	Prec@1 55.469 (55.526)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.4600 (1.6200)	Prec@1 58.203 (55.669)	
Total train loss: 1.6202

Train time: 23.168926000595093
 * Prec@1 51.000 Prec@5 80.580 Loss 1.8877
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 29.090611934661865

Epoch: [24][38/196]	LR: 1e-05	Loss 1.5918 (1.6318)	Prec@1 57.812 (56.110)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.7227 (1.6142)	Prec@1 56.641 (56.205)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.8496 (1.6149)	Prec@1 54.297 (56.056)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.7285 (1.6159)	Prec@1 53.516 (55.980)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.6504 (1.6180)	Prec@1 54.688 (55.944)	
Total train loss: 1.6181

Train time: 23.292407751083374
 * Prec@1 50.840 Prec@5 80.610 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.282070875167847

Epoch: [25][38/196]	LR: 1e-05	Loss 1.6172 (1.6377)	Prec@1 55.078 (55.489)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.7422 (1.6261)	Prec@1 53.906 (55.674)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.6338 (1.6137)	Prec@1 55.469 (55.993)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.5840 (1.6198)	Prec@1 58.984 (55.812)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.4326 (1.6213)	Prec@1 56.250 (55.715)	
Total train loss: 1.6216

Train time: 23.11699151992798
 * Prec@1 50.860 Prec@5 80.610 Loss 1.8877
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.245202779769897

Epoch: [26][38/196]	LR: 1e-05	Loss 1.6699 (1.6085)	Prec@1 53.125 (55.869)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.7295 (1.6301)	Prec@1 53.516 (55.334)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.5713 (1.6268)	Prec@1 55.859 (55.402)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.5527 (1.6213)	Prec@1 57.812 (55.662)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.5947 (1.6191)	Prec@1 60.156 (55.819)	
Total train loss: 1.6195

Train time: 23.590561628341675
 * Prec@1 50.920 Prec@5 80.510 Loss 1.8916
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 29.132585763931274

Epoch: [27][38/196]	LR: 1e-05	Loss 1.6152 (1.6243)	Prec@1 56.641 (55.439)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.6299 (1.6363)	Prec@1 55.469 (55.113)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.4580 (1.6256)	Prec@1 60.547 (55.475)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.4736 (1.6217)	Prec@1 54.297 (55.574)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.6094 (1.6208)	Prec@1 56.641 (55.705)	
Total train loss: 1.6207

Train time: 23.03447389602661
 * Prec@1 50.920 Prec@5 80.500 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 27.636433601379395

Epoch: [28][38/196]	LR: 1e-05	Loss 1.5850 (1.6169)	Prec@1 55.469 (55.669)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.6562 (1.6192)	Prec@1 53.516 (55.784)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.6250 (1.6183)	Prec@1 56.250 (55.736)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.6025 (1.6169)	Prec@1 53.125 (55.724)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.6250 (1.6212)	Prec@1 58.984 (55.645)	
Total train loss: 1.6214

Train time: 23.252402782440186
 * Prec@1 50.820 Prec@5 80.530 Loss 1.8867
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.18734574317932

Epoch: [29][38/196]	LR: 1e-05	Loss 1.6768 (1.6124)	Prec@1 51.953 (55.789)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.5439 (1.6046)	Prec@1 59.766 (56.115)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.5518 (1.6219)	Prec@1 58.984 (55.863)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.5869 (1.6197)	Prec@1 60.547 (55.889)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.8740 (1.6213)	Prec@1 50.391 (55.847)	
Total train loss: 1.6211

Train time: 12.815696954727173
 * Prec@1 50.730 Prec@5 80.300 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 16.57979440689087

Epoch: [30][38/196]	LR: 1e-05	Loss 1.5742 (1.6169)	Prec@1 57.812 (55.709)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.5918 (1.6177)	Prec@1 56.641 (55.654)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.5615 (1.6236)	Prec@1 55.078 (55.465)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.5010 (1.6221)	Prec@1 57.422 (55.521)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.5518 (1.6195)	Prec@1 56.250 (55.601)	
Total train loss: 1.6194

Train time: 12.178075075149536
 * Prec@1 51.000 Prec@5 80.590 Loss 1.8867
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 16.67464280128479

Epoch: [31][38/196]	LR: 1e-05	Loss 1.5215 (1.6150)	Prec@1 59.375 (55.749)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.6045 (1.6273)	Prec@1 56.641 (55.684)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.5889 (1.6226)	Prec@1 54.688 (55.973)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.6807 (1.6290)	Prec@1 54.688 (55.644)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.5615 (1.6219)	Prec@1 57.812 (55.659)	
Total train loss: 1.6224

Train time: 12.392668962478638
 * Prec@1 50.660 Prec@5 80.540 Loss 1.8926
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 15.96316146850586

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.7051 (1.6091)	Prec@1 53.516 (55.509)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.8193 (1.6146)	Prec@1 50.781 (55.924)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.6221 (1.6109)	Prec@1 59.375 (56.253)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.5117 (1.6146)	Prec@1 58.594 (56.055)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.4922 (1.6194)	Prec@1 58.203 (55.847)	
Total train loss: 1.6196

Train time: 12.389450550079346
 * Prec@1 50.480 Prec@5 80.370 Loss 1.8945
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 16.019177198410034

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.7207 (1.6214)	Prec@1 50.781 (55.950)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.6621 (1.6203)	Prec@1 58.984 (56.090)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.7773 (1.6109)	Prec@1 51.172 (55.986)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.6797 (1.6169)	Prec@1 53.516 (55.832)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.4873 (1.6167)	Prec@1 60.547 (55.835)	
Total train loss: 1.6176

Train time: 12.209874868392944
 * Prec@1 51.070 Prec@5 80.510 Loss 1.8848
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 15.84076476097107

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.4326 (1.6240)	Prec@1 58.594 (55.369)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.6006 (1.6083)	Prec@1 57.812 (56.035)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.7080 (1.6179)	Prec@1 54.688 (55.879)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.6553 (1.6174)	Prec@1 54.688 (55.934)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.7930 (1.6211)	Prec@1 51.953 (55.825)	
Total train loss: 1.6213

Train time: 12.262621879577637
 * Prec@1 50.920 Prec@5 80.420 Loss 1.8896
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 15.752560377120972

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.6055 (1.6347)	Prec@1 55.469 (55.789)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.6953 (1.6389)	Prec@1 51.953 (55.644)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.6299 (1.6350)	Prec@1 55.859 (55.572)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.7842 (1.6244)	Prec@1 53.516 (55.777)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.5547 (1.6219)	Prec@1 53.906 (55.785)	
Total train loss: 1.6224

Train time: 12.235612154006958
 * Prec@1 50.740 Prec@5 80.370 Loss 1.8945
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 16.036120891571045

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.6768 (1.6110)	Prec@1 54.297 (55.599)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.7432 (1.6085)	Prec@1 52.734 (55.714)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.7012 (1.6017)	Prec@1 56.250 (56.000)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.5332 (1.6100)	Prec@1 58.984 (55.864)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.6172 (1.6197)	Prec@1 54.297 (55.705)	
Total train loss: 1.6199

Train time: 16.23118829727173
 * Prec@1 51.030 Prec@5 80.920 Loss 1.8848
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 21.040339708328247

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.6514 (1.5953)	Prec@1 53.906 (56.240)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.7285 (1.6102)	Prec@1 50.781 (56.120)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.5889 (1.6180)	Prec@1 55.859 (55.970)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.4961 (1.6168)	Prec@1 62.891 (56.027)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.5352 (1.6180)	Prec@1 57.812 (55.859)	
Total train loss: 1.6182

Train time: 37.02189636230469
 * Prec@1 50.800 Prec@5 80.450 Loss 1.8916
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 47.84938287734985

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 1.7627 (1.6143)	Prec@1 53.516 (55.288)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.4268 (1.6136)	Prec@1 60.938 (55.809)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.7021 (1.6223)	Prec@1 55.469 (55.883)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.6641 (1.6235)	Prec@1 56.641 (55.882)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.4482 (1.6202)	Prec@1 61.719 (55.970)	
Total train loss: 1.6200

Train time: 39.21126341819763
 * Prec@1 50.860 Prec@5 80.550 Loss 1.8877
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 42.58795785903931

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.5928 (1.6526)	Prec@1 55.078 (55.148)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.4922 (1.6208)	Prec@1 59.375 (55.809)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.6133 (1.6194)	Prec@1 58.203 (55.776)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.6064 (1.6227)	Prec@1 53.906 (55.659)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.6455 (1.6194)	Prec@1 55.078 (55.833)	
Total train loss: 1.6196

Train time: 22.44184446334839
 * Prec@1 51.050 Prec@5 80.560 Loss 1.8848
Best acc: 62.700
--------------------------------------------------------------------------------
Test time: 28.62337827682495


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 67.360 Prec@5 89.460 Loss 1.2480
Pre-trained Prec@1 with 5 layers frozen: 67.36000061035156 	 Loss: 1.248046875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.5562 (0.5642)	Prec@1 85.156 (83.874)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.7646 (0.6111)	Prec@1 77.734 (82.257)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.6689 (0.6617)	Prec@1 82.812 (80.923)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.8423 (0.6845)	Prec@1 73.828 (80.364)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.6978 (0.6978)	Prec@1 80.078 (79.912)	
Total train loss: 0.6980

Train time: 103.68093037605286
 * Prec@1 63.620 Prec@5 87.510 Loss 1.3926
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 110.78714728355408

Epoch: [1][38/196]	LR: 0.01	Loss 0.9019 (0.7758)	Prec@1 76.172 (77.624)	
Epoch: [1][77/196]	LR: 0.01	Loss 1.1045 (0.8519)	Prec@1 66.797 (75.601)	
Epoch: [1][116/196]	LR: 0.01	Loss 1.1748 (0.9235)	Prec@1 66.797 (73.354)	
Epoch: [1][155/196]	LR: 0.01	Loss 1.1660 (0.9676)	Prec@1 67.188 (72.050)	
Epoch: [1][194/196]	LR: 0.01	Loss 1.1904 (0.9984)	Prec@1 66.797 (71.198)	
Total train loss: 0.9987

Train time: 22.827372074127197
 * Prec@1 48.860 Prec@5 78.070 Loss 2.2480
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 28.364299535751343

Epoch: [2][38/196]	LR: 0.01	Loss 1.2773 (1.1955)	Prec@1 64.062 (66.536)	
Epoch: [2][77/196]	LR: 0.01	Loss 1.2725 (1.2475)	Prec@1 65.625 (64.909)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.4414 (1.2928)	Prec@1 58.594 (63.715)	
Epoch: [2][155/196]	LR: 0.01	Loss 1.6240 (1.3475)	Prec@1 53.125 (62.337)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.5859 (1.4148)	Prec@1 56.250 (60.793)	
Total train loss: 1.4151

Train time: 22.181159257888794
 * Prec@1 33.400 Prec@5 64.750 Loss 3.2520
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.16621971130371

Epoch: [3][38/196]	LR: 0.01	Loss 1.7305 (1.6892)	Prec@1 56.250 (54.207)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.6650 (1.7030)	Prec@1 54.297 (54.097)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.5889 (1.6624)	Prec@1 56.250 (54.938)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.6660 (1.6658)	Prec@1 57.422 (55.011)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.8496 (1.6756)	Prec@1 49.219 (54.880)	
Total train loss: 1.6762

Train time: 21.44468903541565
 * Prec@1 1.160 Prec@5 5.530 Loss 5.2578
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.71241855621338

Epoch: [4][38/196]	LR: 0.01	Loss 1.6982 (1.9210)	Prec@1 57.812 (49.629)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.7529 (1.7818)	Prec@1 55.859 (52.674)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.6865 (1.8036)	Prec@1 56.250 (52.277)	
Epoch: [4][155/196]	LR: 0.01	Loss 1.8369 (1.8075)	Prec@1 55.469 (52.266)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.9658 (1.8099)	Prec@1 47.656 (52.250)	
Total train loss: 1.8104

Train time: 22.053752422332764
 * Prec@1 42.450 Prec@5 72.440 Loss 2.6504
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.645456790924072

Epoch: [5][38/196]	LR: 0.01	Loss 1.7324 (1.6896)	Prec@1 55.469 (54.607)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.7441 (1.7337)	Prec@1 55.078 (53.741)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.6875 (1.7668)	Prec@1 52.344 (52.728)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.7881 (1.7532)	Prec@1 54.297 (52.910)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.5791 (1.7632)	Prec@1 58.594 (52.574)	
Total train loss: 1.7631

Train time: 20.60214877128601
 * Prec@1 8.840 Prec@5 26.590 Loss inf
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.0991632938385

Epoch: [6][38/196]	LR: 0.01	Loss 1.7246 (1.7192)	Prec@1 55.469 (54.337)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.6436 (1.6959)	Prec@1 56.641 (54.562)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.5186 (1.6789)	Prec@1 55.859 (54.778)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.4736 (1.6601)	Prec@1 55.859 (54.940)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.7207 (1.6623)	Prec@1 54.297 (54.645)	
Total train loss: 1.6629

Train time: 18.525054216384888
 * Prec@1 1.740 Prec@5 8.600 Loss inf
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 23.93773865699768

Epoch: [7][38/196]	LR: 0.01	Loss 1.7422 (1.7460)	Prec@1 49.219 (53.165)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.6406 (1.7125)	Prec@1 58.203 (53.716)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.5605 (1.6813)	Prec@1 56.641 (54.260)	
Epoch: [7][155/196]	LR: 0.01	Loss 1.6172 (1.6510)	Prec@1 55.859 (55.073)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.7285 (1.6384)	Prec@1 51.562 (55.355)	
Total train loss: 1.6386

Train time: 21.180683135986328
 * Prec@1 4.280 Prec@5 14.030 Loss inf
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.499345779418945

Epoch: [8][38/196]	LR: 0.001	Loss 1.4893 (1.5330)	Prec@1 55.078 (57.632)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.4072 (1.5158)	Prec@1 60.547 (58.048)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.4629 (1.5075)	Prec@1 58.203 (58.063)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.5234 (1.5094)	Prec@1 57.031 (57.985)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.5039 (1.5059)	Prec@1 56.250 (58.039)	
Total train loss: 1.5063

Train time: 20.05915093421936
 * Prec@1 52.930 Prec@5 82.110 Loss 1.7949
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.808916568756104

Epoch: [9][38/196]	LR: 0.001	Loss 1.4336 (1.4890)	Prec@1 59.375 (58.223)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.4082 (1.4795)	Prec@1 60.547 (58.754)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.6426 (1.4865)	Prec@1 53.516 (58.474)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.5537 (1.4894)	Prec@1 55.859 (58.481)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.4189 (1.4831)	Prec@1 59.766 (58.650)	
Total train loss: 1.4833

Train time: 21.95156717300415
 * Prec@1 53.560 Prec@5 82.610 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.738851308822632

Epoch: [10][38/196]	LR: 0.001	Loss 1.4707 (1.4594)	Prec@1 58.984 (59.355)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.4346 (1.4651)	Prec@1 63.281 (59.210)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.5215 (1.4739)	Prec@1 58.984 (58.908)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.3604 (1.4768)	Prec@1 62.891 (58.877)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.4355 (1.4738)	Prec@1 58.984 (58.964)	
Total train loss: 1.4741

Train time: 20.363096952438354
 * Prec@1 51.830 Prec@5 81.440 Loss 1.8525
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.06766676902771

Epoch: [11][38/196]	LR: 0.001	Loss 1.5674 (1.4982)	Prec@1 55.859 (58.173)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.4365 (1.4883)	Prec@1 60.547 (58.459)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.4375 (1.4832)	Prec@1 59.766 (58.564)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.4102 (1.4719)	Prec@1 56.641 (58.884)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.3584 (1.4748)	Prec@1 63.672 (58.798)	
Total train loss: 1.4758

Train time: 19.707053661346436
 * Prec@1 54.020 Prec@5 82.530 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.379145622253418

Epoch: [12][38/196]	LR: 0.001	Loss 1.5732 (1.4587)	Prec@1 53.516 (59.054)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.6113 (1.4788)	Prec@1 54.688 (58.579)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.6523 (1.4689)	Prec@1 52.344 (58.914)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.3340 (1.4637)	Prec@1 62.109 (58.942)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.4873 (1.4661)	Prec@1 58.594 (58.918)	
Total train loss: 1.4664

Train time: 21.548681259155273
 * Prec@1 53.500 Prec@5 82.900 Loss 1.7637
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 28.153281688690186

Epoch: [13][38/196]	LR: 0.001	Loss 1.3486 (1.4585)	Prec@1 60.547 (59.185)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.3887 (1.4572)	Prec@1 61.719 (59.215)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.4092 (1.4544)	Prec@1 57.422 (59.408)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.5146 (1.4553)	Prec@1 60.156 (59.362)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.4746 (1.4646)	Prec@1 57.422 (59.131)	
Total train loss: 1.4652

Train time: 21.114696979522705
 * Prec@1 53.740 Prec@5 82.570 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.605680465698242

Epoch: [14][38/196]	LR: 0.001	Loss 1.6152 (1.5038)	Prec@1 55.078 (58.023)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.4131 (1.5017)	Prec@1 63.281 (58.273)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.2812 (1.4953)	Prec@1 63.281 (58.390)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.3711 (1.4845)	Prec@1 61.328 (58.676)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.4648 (1.4799)	Prec@1 60.156 (58.852)	
Total train loss: 1.4807

Train time: 21.110899209976196
 * Prec@1 53.730 Prec@5 82.640 Loss 1.7715
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.5362765789032

Epoch: [15][38/196]	LR: 0.001	Loss 1.4609 (1.4498)	Prec@1 58.203 (59.475)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.5781 (1.4713)	Prec@1 52.344 (58.854)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.5811 (1.4765)	Prec@1 55.078 (58.794)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.5557 (1.4786)	Prec@1 55.859 (58.671)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.3730 (1.4738)	Prec@1 59.375 (58.830)	
Total train loss: 1.4742

Train time: 22.3412504196167
 * Prec@1 53.500 Prec@5 82.630 Loss 1.7695
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.47551465034485

Epoch: [16][38/196]	LR: 0.0001	Loss 1.4072 (1.4972)	Prec@1 62.109 (58.403)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.5615 (1.4718)	Prec@1 57.812 (58.769)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.3877 (1.4660)	Prec@1 60.156 (59.028)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.4639 (1.4640)	Prec@1 58.984 (59.095)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.4570 (1.4664)	Prec@1 56.641 (58.996)	
Total train loss: 1.4672

Train time: 23.221661806106567
 * Prec@1 53.230 Prec@5 82.600 Loss 1.7725
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.20578169822693

Epoch: [17][38/196]	LR: 0.0001	Loss 1.5254 (1.4623)	Prec@1 58.984 (59.365)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.4004 (1.4650)	Prec@1 64.453 (59.445)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.6260 (1.4749)	Prec@1 55.859 (58.961)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.3672 (1.4704)	Prec@1 58.594 (59.004)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.4150 (1.4668)	Prec@1 56.641 (59.046)	
Total train loss: 1.4670

Train time: 22.87403917312622
 * Prec@1 53.670 Prec@5 82.720 Loss 1.7617
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.47043776512146

Epoch: [18][38/196]	LR: 0.0001	Loss 1.2900 (1.4546)	Prec@1 63.672 (59.315)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.2832 (1.4489)	Prec@1 62.500 (59.465)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.6172 (1.4684)	Prec@1 56.641 (58.924)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.6348 (1.4684)	Prec@1 57.812 (59.052)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.4902 (1.4645)	Prec@1 57.422 (59.077)	
Total train loss: 1.4648

Train time: 22.693044424057007
 * Prec@1 53.640 Prec@5 82.640 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 28.05697751045227

Epoch: [19][38/196]	LR: 0.0001	Loss 1.3652 (1.4227)	Prec@1 60.547 (59.696)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.4932 (1.4470)	Prec@1 58.984 (59.290)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.4697 (1.4584)	Prec@1 59.766 (59.135)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.4521 (1.4644)	Prec@1 57.812 (58.954)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.4463 (1.4664)	Prec@1 61.328 (58.980)	
Total train loss: 1.4671

Train time: 23.213812351226807
 * Prec@1 53.600 Prec@5 82.730 Loss 1.7617
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.56111192703247

Epoch: [20][38/196]	LR: 0.0001	Loss 1.6377 (1.4558)	Prec@1 54.297 (59.245)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.6426 (1.4515)	Prec@1 52.344 (59.565)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.4072 (1.4623)	Prec@1 60.547 (59.135)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.4219 (1.4647)	Prec@1 59.375 (59.044)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.4355 (1.4649)	Prec@1 56.641 (59.042)	
Total train loss: 1.4653

Train time: 22.046959161758423
 * Prec@1 53.500 Prec@5 82.590 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.0527286529541

Epoch: [21][38/196]	LR: 0.0001	Loss 1.3467 (1.4692)	Prec@1 60.938 (58.944)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.3799 (1.4562)	Prec@1 59.766 (59.415)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.5947 (1.4535)	Prec@1 53.906 (59.358)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.5957 (1.4620)	Prec@1 55.859 (59.147)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.6475 (1.4666)	Prec@1 56.641 (59.069)	
Total train loss: 1.4668

Train time: 22.71918559074402
 * Prec@1 53.750 Prec@5 82.750 Loss 1.7637
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.096301317214966

Epoch: [22][38/196]	LR: 0.0001	Loss 1.5283 (1.4596)	Prec@1 58.594 (59.255)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.5205 (1.4585)	Prec@1 59.766 (59.485)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.4980 (1.4582)	Prec@1 58.594 (59.251)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.4316 (1.4618)	Prec@1 59.766 (59.125)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.4951 (1.4644)	Prec@1 60.938 (59.087)	
Total train loss: 1.4644

Train time: 22.425766468048096
 * Prec@1 53.680 Prec@5 82.620 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.489320755004883

Epoch: [23][38/196]	LR: 0.0001	Loss 1.4219 (1.4759)	Prec@1 63.281 (58.904)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.4922 (1.4655)	Prec@1 56.250 (58.979)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.3047 (1.4682)	Prec@1 65.625 (59.101)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.4043 (1.4650)	Prec@1 60.938 (59.162)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.5088 (1.4667)	Prec@1 55.078 (59.097)	
Total train loss: 1.4672

Train time: 23.19793200492859
 * Prec@1 53.560 Prec@5 82.690 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.776224851608276

Epoch: [24][38/196]	LR: 1e-05	Loss 1.4072 (1.4502)	Prec@1 58.203 (58.834)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.3398 (1.4621)	Prec@1 62.109 (58.824)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.3936 (1.4619)	Prec@1 60.938 (58.898)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.3594 (1.4654)	Prec@1 59.375 (58.839)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.3857 (1.4655)	Prec@1 59.375 (58.966)	
Total train loss: 1.4655

Train time: 22.01716446876526
 * Prec@1 53.550 Prec@5 82.590 Loss 1.7646
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 28.104289054870605

Epoch: [25][38/196]	LR: 1e-05	Loss 1.3477 (1.4636)	Prec@1 62.500 (58.393)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.4355 (1.4562)	Prec@1 60.547 (58.744)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.5166 (1.4603)	Prec@1 59.375 (58.918)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.3047 (1.4636)	Prec@1 61.719 (58.877)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.4580 (1.4661)	Prec@1 58.594 (58.964)	
Total train loss: 1.4659

Train time: 21.00770401954651
 * Prec@1 53.560 Prec@5 82.700 Loss 1.7695
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 23.361415147781372

Epoch: [26][38/196]	LR: 1e-05	Loss 1.3926 (1.4465)	Prec@1 60.547 (59.365)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.5430 (1.4582)	Prec@1 55.469 (59.110)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.5508 (1.4628)	Prec@1 57.422 (59.078)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.5996 (1.4632)	Prec@1 58.203 (59.165)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.5127 (1.4635)	Prec@1 61.328 (59.087)	
Total train loss: 1.4638

Train time: 21.190144538879395
 * Prec@1 53.230 Prec@5 82.640 Loss 1.7715
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.40261697769165

Epoch: [27][38/196]	LR: 1e-05	Loss 1.3340 (1.4617)	Prec@1 63.672 (58.854)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.5459 (1.4660)	Prec@1 57.031 (58.839)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.5342 (1.4671)	Prec@1 56.641 (58.881)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.5254 (1.4702)	Prec@1 58.203 (58.844)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.4424 (1.4631)	Prec@1 62.109 (59.081)	
Total train loss: 1.4632

Train time: 22.467471837997437
 * Prec@1 53.530 Prec@5 82.630 Loss 1.7715
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.55039405822754

Epoch: [28][38/196]	LR: 1e-05	Loss 1.3691 (1.4527)	Prec@1 59.375 (59.625)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.3936 (1.4546)	Prec@1 60.156 (59.465)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.3643 (1.4610)	Prec@1 60.938 (59.482)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.5088 (1.4672)	Prec@1 60.938 (59.307)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.6094 (1.4654)	Prec@1 55.859 (59.325)	
Total train loss: 1.4656

Train time: 22.659414768218994
 * Prec@1 53.420 Prec@5 82.780 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.945704460144043

Epoch: [29][38/196]	LR: 1e-05	Loss 1.4258 (1.4643)	Prec@1 59.766 (58.904)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.4043 (1.4653)	Prec@1 60.547 (59.100)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.4307 (1.4706)	Prec@1 59.766 (58.801)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.3594 (1.4710)	Prec@1 64.844 (58.987)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.3838 (1.4654)	Prec@1 62.500 (59.149)	
Total train loss: 1.4653

Train time: 12.408677577972412
 * Prec@1 53.840 Prec@5 82.670 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 14.614244937896729

Epoch: [30][38/196]	LR: 1e-05	Loss 1.6094 (1.4761)	Prec@1 54.297 (58.664)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.3926 (1.4635)	Prec@1 62.500 (59.105)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.7178 (1.4750)	Prec@1 50.781 (58.704)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.3643 (1.4690)	Prec@1 60.547 (58.917)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.5430 (1.4670)	Prec@1 58.203 (58.926)	
Total train loss: 1.4673

Train time: 23.726304054260254
 * Prec@1 53.770 Prec@5 82.690 Loss 1.7617
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 33.291356563568115

Epoch: [31][38/196]	LR: 1e-05	Loss 1.5293 (1.4546)	Prec@1 54.688 (59.846)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.4102 (1.4670)	Prec@1 58.984 (59.265)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.6025 (1.4668)	Prec@1 58.203 (59.235)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.2773 (1.4642)	Prec@1 66.016 (59.215)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.6182 (1.4643)	Prec@1 54.688 (59.209)	
Total train loss: 1.4647

Train time: 49.200111865997314
 * Prec@1 53.530 Prec@5 82.620 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 58.29292345046997

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.5596 (1.4636)	Prec@1 57.031 (58.894)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.4453 (1.4482)	Prec@1 57.422 (59.430)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.6211 (1.4662)	Prec@1 54.297 (58.951)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.6104 (1.4656)	Prec@1 55.469 (58.957)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.5459 (1.4662)	Prec@1 60.547 (59.071)	
Total train loss: 1.4666

Train time: 38.68832468986511
 * Prec@1 53.550 Prec@5 82.730 Loss 1.7695
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 44.342658281326294

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.3428 (1.4794)	Prec@1 62.109 (58.874)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.5000 (1.4772)	Prec@1 61.719 (58.909)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.3818 (1.4639)	Prec@1 63.672 (59.078)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.4697 (1.4667)	Prec@1 60.938 (58.974)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.5938 (1.4650)	Prec@1 60.547 (59.056)	
Total train loss: 1.4652

Train time: 20.248631477355957
 * Prec@1 53.620 Prec@5 82.750 Loss 1.7598
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.349682569503784

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.4229 (1.4653)	Prec@1 58.203 (58.814)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.3193 (1.4608)	Prec@1 59.375 (59.160)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.5576 (1.4678)	Prec@1 55.859 (58.911)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.3965 (1.4672)	Prec@1 61.328 (58.864)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.4248 (1.4646)	Prec@1 60.938 (59.044)	
Total train loss: 1.4646

Train time: 20.47294282913208
 * Prec@1 53.400 Prec@5 82.720 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.209568977355957

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.5459 (1.4755)	Prec@1 57.812 (58.884)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.4209 (1.4667)	Prec@1 58.203 (59.125)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.4580 (1.4664)	Prec@1 60.547 (59.011)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.4570 (1.4612)	Prec@1 59.375 (59.132)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.5693 (1.4649)	Prec@1 56.641 (58.980)	
Total train loss: 1.4652

Train time: 20.245826244354248
 * Prec@1 53.560 Prec@5 82.850 Loss 1.7676
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 25.58976435661316

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.1611 (1.4492)	Prec@1 64.844 (59.285)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.4492 (1.4641)	Prec@1 57.812 (58.729)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.3350 (1.4685)	Prec@1 63.281 (58.754)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.4473 (1.4615)	Prec@1 58.984 (59.039)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.4248 (1.4665)	Prec@1 56.250 (58.838)	
Total train loss: 1.4666

Train time: 19.483724355697632
 * Prec@1 53.710 Prec@5 82.600 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 24.408074617385864

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.6816 (1.4619)	Prec@1 55.469 (59.615)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.5000 (1.4665)	Prec@1 57.812 (59.140)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.4912 (1.4673)	Prec@1 58.984 (59.014)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.5479 (1.4687)	Prec@1 56.250 (58.899)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.3896 (1.4671)	Prec@1 64.844 (58.994)	
Total train loss: 1.4675

Train time: 21.082338333129883
 * Prec@1 53.380 Prec@5 82.490 Loss 1.7744
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.811625242233276

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 1.3193 (1.4635)	Prec@1 62.891 (58.544)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.5615 (1.4750)	Prec@1 56.250 (58.494)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.3701 (1.4608)	Prec@1 60.156 (58.911)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.4463 (1.4571)	Prec@1 58.203 (59.032)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.4883 (1.4641)	Prec@1 60.547 (58.884)	
Total train loss: 1.4643

Train time: 22.2151997089386
 * Prec@1 53.850 Prec@5 82.780 Loss 1.7598
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 27.251785278320312

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.5107 (1.4629)	Prec@1 57.422 (58.724)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.4268 (1.4606)	Prec@1 60.156 (58.984)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.5107 (1.4618)	Prec@1 58.594 (58.931)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.5625 (1.4648)	Prec@1 59.375 (58.989)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.3154 (1.4644)	Prec@1 62.500 (59.109)	
Total train loss: 1.4647

Train time: 20.36183738708496
 * Prec@1 53.790 Prec@5 82.750 Loss 1.7666
Best acc: 63.620
--------------------------------------------------------------------------------
Test time: 26.279038429260254


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 66.420 Prec@5 88.950 Loss 1.2998
Pre-trained Prec@1 with 7 layers frozen: 66.41999816894531 	 Loss: 1.2998046875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.6011 (0.5653)	Prec@1 82.422 (84.145)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.6821 (0.5847)	Prec@1 79.688 (83.423)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.8418 (0.6304)	Prec@1 73.828 (81.995)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.9507 (0.6829)	Prec@1 73.828 (80.354)	
Epoch: [0][194/196]	LR: 0.01	Loss 1.2578 (0.7373)	Prec@1 62.891 (78.758)	
Total train loss: 0.7378

Train time: 98.97068810462952
 * Prec@1 42.400 Prec@5 70.290 Loss 2.4902
Best acc: 42.400
--------------------------------------------------------------------------------
Test time: 106.17384219169617

Epoch: [1][38/196]	LR: 0.01	Loss 1.2422 (1.2258)	Prec@1 69.141 (66.006)	
Epoch: [1][77/196]	LR: 0.01	Loss 1.6191 (1.2719)	Prec@1 52.344 (64.628)	
Epoch: [1][116/196]	LR: 0.01	Loss 1.4805 (1.3343)	Prec@1 58.984 (63.091)	
Epoch: [1][155/196]	LR: 0.01	Loss 1.6084 (1.3720)	Prec@1 54.688 (62.107)	
Epoch: [1][194/196]	LR: 0.01	Loss 1.4111 (1.4015)	Prec@1 64.062 (61.490)	
Total train loss: 1.4019

Train time: 23.13006854057312
 * Prec@1 51.170 Prec@5 80.560 Loss 1.8770
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 30.888086080551147

Epoch: [2][38/196]	LR: 0.01	Loss 1.3955 (1.4243)	Prec@1 62.500 (61.368)	
Epoch: [2][77/196]	LR: 0.01	Loss 1.3848 (1.4094)	Prec@1 61.328 (61.448)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.6152 (1.4303)	Prec@1 58.594 (60.867)	
Epoch: [2][155/196]	LR: 0.01	Loss 1.6152 (1.4828)	Prec@1 58.594 (59.948)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.6914 (1.5116)	Prec@1 53.516 (59.207)	
Total train loss: 1.5119

Train time: 22.33593463897705
 * Prec@1 18.960 Prec@5 44.790 Loss 4.0781
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 27.311996936798096

Epoch: [3][38/196]	LR: 0.01	Loss 1.4746 (1.6915)	Prec@1 60.547 (55.429)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.6963 (1.6946)	Prec@1 55.859 (54.783)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.7178 (1.7134)	Prec@1 57.812 (54.664)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.9736 (1.7290)	Prec@1 44.922 (54.257)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.6748 (1.7357)	Prec@1 51.172 (53.898)	
Total train loss: 1.7357

Train time: 20.55914831161499
 * Prec@1 34.880 Prec@5 64.570 Loss 3.0039
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 25.91507339477539

Epoch: [4][38/196]	LR: 0.01	Loss 1.6309 (1.7825)	Prec@1 52.734 (52.664)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.7754 (1.8276)	Prec@1 51.562 (51.552)	
Epoch: [4][116/196]	LR: 0.01	Loss 2.0586 (1.8613)	Prec@1 45.312 (50.775)	
Epoch: [4][155/196]	LR: 0.01	Loss 2.0117 (1.8576)	Prec@1 45.312 (50.911)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.8809 (1.8578)	Prec@1 51.172 (50.789)	
Total train loss: 1.8579

Train time: 20.734207153320312
 * Prec@1 10.530 Prec@5 30.190 Loss 4.6523
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 25.02361750602722

Epoch: [5][38/196]	LR: 0.01	Loss 1.7031 (1.8367)	Prec@1 53.125 (51.012)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.6455 (1.7832)	Prec@1 55.078 (52.304)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.5928 (1.7324)	Prec@1 58.203 (53.312)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.7305 (1.7219)	Prec@1 51.562 (53.420)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.6934 (1.7136)	Prec@1 54.688 (53.586)	
Total train loss: 1.7136

Train time: 20.345335483551025
 * Prec@1 30.920 Prec@5 60.090 Loss 3.0449
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 25.97795796394348

Epoch: [6][38/196]	LR: 0.01	Loss 1.6895 (1.6926)	Prec@1 52.734 (53.656)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.6084 (1.6806)	Prec@1 56.250 (54.347)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.8193 (1.6942)	Prec@1 50.000 (53.839)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.7607 (1.7098)	Prec@1 50.000 (53.425)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.3271 (1.7025)	Prec@1 63.672 (53.620)	
Total train loss: 1.7021

Train time: 21.302186250686646
 * Prec@1 25.520 Prec@5 56.190 Loss 3.4395
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 27.205376148223877

Epoch: [7][38/196]	LR: 0.01	Loss 1.8135 (1.6351)	Prec@1 50.391 (55.278)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.6416 (1.6589)	Prec@1 56.641 (54.718)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.7568 (1.6561)	Prec@1 55.078 (54.824)	
Epoch: [7][155/196]	LR: 0.01	Loss 2.0078 (1.6771)	Prec@1 44.922 (54.530)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.6533 (1.6873)	Prec@1 56.250 (54.283)	
Total train loss: 1.6873

Train time: 21.59821105003357
 * Prec@1 8.950 Prec@5 26.180 Loss 5.9336
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 26.75088930130005

Epoch: [8][38/196]	LR: 0.001	Loss 1.8252 (1.6345)	Prec@1 50.781 (55.569)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.6113 (1.5977)	Prec@1 54.688 (56.455)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.4785 (1.5763)	Prec@1 61.719 (56.901)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.6709 (1.5741)	Prec@1 53.906 (56.879)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.5332 (1.5717)	Prec@1 58.203 (57.053)	
Total train loss: 1.5714

Train time: 20.139695644378662
 * Prec@1 49.010 Prec@5 79.080 Loss 2.0332
Best acc: 51.170
--------------------------------------------------------------------------------
Test time: 25.43159532546997

Epoch: [9][38/196]	LR: 0.001	Loss 1.5957 (1.5531)	Prec@1 55.078 (57.171)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.5635 (1.5511)	Prec@1 55.469 (57.452)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.4844 (1.5578)	Prec@1 59.375 (57.292)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.6016 (1.5542)	Prec@1 56.641 (57.479)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.4688 (1.5485)	Prec@1 57.422 (57.600)	
Total train loss: 1.5488

Train time: 21.37161636352539
 * Prec@1 52.670 Prec@5 81.390 Loss 1.8369
Best acc: 52.670
--------------------------------------------------------------------------------
Test time: 26.283400535583496

Epoch: [10][38/196]	LR: 0.001	Loss 1.5137 (1.5703)	Prec@1 57.031 (56.711)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.5352 (1.5505)	Prec@1 58.594 (57.352)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.5752 (1.5524)	Prec@1 58.594 (57.388)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.4824 (1.5478)	Prec@1 53.516 (57.552)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.5840 (1.5454)	Prec@1 55.859 (57.620)	
Total train loss: 1.5455

Train time: 20.885926246643066
 * Prec@1 53.060 Prec@5 81.850 Loss 1.8174
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 31.313478231430054

Epoch: [11][38/196]	LR: 0.001	Loss 1.4658 (1.5487)	Prec@1 60.156 (57.532)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.6611 (1.5440)	Prec@1 52.344 (57.622)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.5742 (1.5493)	Prec@1 58.203 (57.632)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.5088 (1.5396)	Prec@1 58.203 (57.800)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.4482 (1.5435)	Prec@1 58.594 (57.710)	
Total train loss: 1.5439

Train time: 20.91130304336548
 * Prec@1 52.950 Prec@5 81.820 Loss 1.8242
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 25.587777376174927

Epoch: [12][38/196]	LR: 0.001	Loss 1.4609 (1.5614)	Prec@1 62.500 (56.961)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.5068 (1.5538)	Prec@1 55.859 (57.377)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.6123 (1.5498)	Prec@1 54.688 (57.569)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.6016 (1.5520)	Prec@1 57.031 (57.409)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.5146 (1.5542)	Prec@1 56.250 (57.296)	
Total train loss: 1.5542

Train time: 21.224163055419922
 * Prec@1 52.740 Prec@5 81.650 Loss 1.8340
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 25.609344244003296

Epoch: [13][38/196]	LR: 0.001	Loss 1.7354 (1.5911)	Prec@1 54.297 (56.661)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.4199 (1.5576)	Prec@1 61.328 (57.387)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.6094 (1.5555)	Prec@1 57.812 (57.555)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.4629 (1.5461)	Prec@1 58.984 (57.883)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.4209 (1.5484)	Prec@1 63.281 (57.662)	
Total train loss: 1.5486

Train time: 22.424548149108887
 * Prec@1 52.970 Prec@5 81.710 Loss 1.8242
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 26.19754958152771

Epoch: [14][38/196]	LR: 0.001	Loss 1.4346 (1.5675)	Prec@1 60.547 (56.661)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.4092 (1.5549)	Prec@1 61.719 (57.242)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.4395 (1.5588)	Prec@1 62.891 (57.502)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.6533 (1.5570)	Prec@1 54.688 (57.457)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.3350 (1.5512)	Prec@1 66.406 (57.588)	
Total train loss: 1.5513

Train time: 20.593312978744507
 * Prec@1 52.450 Prec@5 81.190 Loss 1.8516
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 25.33251643180847

Epoch: [15][38/196]	LR: 0.001	Loss 1.5713 (1.5663)	Prec@1 56.641 (57.562)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.5850 (1.5572)	Prec@1 53.516 (57.552)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.8115 (1.5593)	Prec@1 53.125 (57.719)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.6377 (1.5563)	Prec@1 57.031 (57.562)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.4814 (1.5533)	Prec@1 60.547 (57.562)	
Total train loss: 1.5529

Train time: 21.882986307144165
 * Prec@1 52.100 Prec@5 81.160 Loss 1.8496
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 26.269643783569336

Epoch: [16][38/196]	LR: 0.0001	Loss 1.5186 (1.5441)	Prec@1 59.375 (57.352)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.5967 (1.5558)	Prec@1 50.781 (57.131)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.5283 (1.5527)	Prec@1 58.203 (57.449)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.4795 (1.5504)	Prec@1 61.328 (57.587)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.6377 (1.5485)	Prec@1 56.641 (57.658)	
Total train loss: 1.5488

Train time: 22.196994304656982
 * Prec@1 52.930 Prec@5 81.590 Loss 1.8271
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 26.458634853363037

Epoch: [17][38/196]	LR: 0.0001	Loss 1.4590 (1.5277)	Prec@1 58.984 (57.812)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.4600 (1.5344)	Prec@1 59.766 (57.667)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.5928 (1.5340)	Prec@1 55.859 (57.702)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.4131 (1.5429)	Prec@1 65.234 (57.525)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.3115 (1.5482)	Prec@1 64.453 (57.518)	
Total train loss: 1.5482

Train time: 21.396061182022095
 * Prec@1 52.840 Prec@5 81.570 Loss 1.8340
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 26.199259996414185

Epoch: [18][38/196]	LR: 0.0001	Loss 1.4990 (1.5274)	Prec@1 58.984 (58.183)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.5654 (1.5387)	Prec@1 59.375 (57.993)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.4590 (1.5383)	Prec@1 60.547 (57.999)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.5684 (1.5434)	Prec@1 58.984 (57.885)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.4053 (1.5450)	Prec@1 62.891 (57.786)	
Total train loss: 1.5449

Train time: 22.217882871627808
 * Prec@1 52.800 Prec@5 81.670 Loss 1.8193
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 28.128410577774048

Epoch: [19][38/196]	LR: 0.0001	Loss 1.5400 (1.5488)	Prec@1 57.031 (57.382)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.4971 (1.5371)	Prec@1 58.203 (57.828)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.5234 (1.5421)	Prec@1 58.594 (57.726)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.3740 (1.5458)	Prec@1 62.500 (57.625)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.7588 (1.5436)	Prec@1 53.516 (57.702)	
Total train loss: 1.5429

Train time: 20.508564472198486
 * Prec@1 52.980 Prec@5 81.700 Loss 1.8252
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 25.659422397613525

Epoch: [20][38/196]	LR: 0.0001	Loss 1.5244 (1.5390)	Prec@1 60.938 (58.253)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.5615 (1.5365)	Prec@1 58.984 (58.113)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.5010 (1.5391)	Prec@1 57.812 (57.929)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.6289 (1.5417)	Prec@1 55.859 (57.905)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.6680 (1.5429)	Prec@1 55.859 (57.784)	
Total train loss: 1.5430

Train time: 21.852888584136963
 * Prec@1 52.990 Prec@5 81.820 Loss 1.8203
Best acc: 53.060
--------------------------------------------------------------------------------
Test time: 26.60218572616577

Epoch: [21][38/196]	LR: 0.0001	Loss 1.4824 (1.5233)	Prec@1 60.547 (58.474)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.4336 (1.5294)	Prec@1 60.156 (58.078)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.3389 (1.5286)	Prec@1 62.109 (58.123)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.7441 (1.5396)	Prec@1 53.906 (57.888)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.5156 (1.5420)	Prec@1 59.375 (57.815)	
Total train loss: 1.5418

Train time: 22.497969150543213
 * Prec@1 53.070 Prec@5 81.650 Loss 1.8223
Best acc: 53.070
--------------------------------------------------------------------------------
Test time: 27.620890378952026

Epoch: [22][38/196]	LR: 0.0001	Loss 1.5166 (1.5291)	Prec@1 57.422 (58.103)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.7461 (1.5387)	Prec@1 51.172 (57.958)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.6387 (1.5427)	Prec@1 52.344 (57.749)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.5547 (1.5444)	Prec@1 55.469 (57.627)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.6631 (1.5417)	Prec@1 50.781 (57.782)	
Total train loss: 1.5419

Train time: 18.622891902923584
 * Prec@1 52.900 Prec@5 81.550 Loss 1.8203
Best acc: 53.070
--------------------------------------------------------------------------------
Test time: 24.38084840774536

Epoch: [23][38/196]	LR: 0.0001	Loss 1.7490 (1.5621)	Prec@1 51.953 (57.592)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.6592 (1.5587)	Prec@1 57.422 (57.457)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.6436 (1.5524)	Prec@1 50.781 (57.495)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.6309 (1.5567)	Prec@1 54.688 (57.277)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.3643 (1.5462)	Prec@1 58.203 (57.550)	
Total train loss: 1.5465

Train time: 24.59540557861328
 * Prec@1 52.740 Prec@5 81.580 Loss 1.8271
Best acc: 53.070
--------------------------------------------------------------------------------
Test time: 33.48660731315613

Epoch: [24][38/196]	LR: 1e-05	Loss 1.7031 (1.5357)	Prec@1 53.125 (58.454)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.7412 (1.5407)	Prec@1 51.562 (58.268)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.4561 (1.5457)	Prec@1 60.156 (57.836)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.6240 (1.5431)	Prec@1 57.031 (57.863)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.5898 (1.5450)	Prec@1 54.688 (57.706)	
Total train loss: 1.5448

Train time: 61.63510751724243
 * Prec@1 53.100 Prec@5 81.800 Loss 1.8271
Best acc: 53.100
--------------------------------------------------------------------------------
Test time: 76.8268768787384

Epoch: [25][38/196]	LR: 1e-05	Loss 1.6758 (1.5668)	Prec@1 51.953 (57.051)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.7285 (1.5501)	Prec@1 56.641 (57.522)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.4297 (1.5469)	Prec@1 58.984 (57.682)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.5430 (1.5433)	Prec@1 57.031 (57.858)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.5352 (1.5425)	Prec@1 58.203 (57.877)	
Total train loss: 1.5422

Train time: 51.849437952041626
 * Prec@1 52.980 Prec@5 81.730 Loss 1.8223
Best acc: 53.100
--------------------------------------------------------------------------------
Test time: 58.86089324951172

Epoch: [26][38/196]	LR: 1e-05	Loss 1.5566 (1.5574)	Prec@1 57.812 (57.522)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.5732 (1.5613)	Prec@1 55.859 (57.417)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.5078 (1.5579)	Prec@1 57.812 (57.669)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.6738 (1.5552)	Prec@1 55.078 (57.730)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.5156 (1.5450)	Prec@1 57.031 (57.901)	
Total train loss: 1.5451

Train time: 20.733516693115234
 * Prec@1 52.860 Prec@5 81.840 Loss 1.8203
Best acc: 53.100
--------------------------------------------------------------------------------
Test time: 24.463948965072632

Epoch: [27][38/196]	LR: 1e-05	Loss 1.6504 (1.5339)	Prec@1 55.469 (58.554)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.7148 (1.5314)	Prec@1 48.828 (58.313)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.4844 (1.5354)	Prec@1 59.766 (58.160)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.5107 (1.5391)	Prec@1 58.594 (57.998)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.5137 (1.5412)	Prec@1 57.422 (57.861)	
Total train loss: 1.5411

Train time: 18.957270622253418
 * Prec@1 52.970 Prec@5 81.730 Loss 1.8223
Best acc: 53.100
--------------------------------------------------------------------------------
Test time: 22.729048490524292

Epoch: [28][38/196]	LR: 1e-05	Loss 1.5615 (1.5353)	Prec@1 58.984 (58.213)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.5166 (1.5484)	Prec@1 58.594 (57.833)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.5439 (1.5493)	Prec@1 56.641 (57.836)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.6709 (1.5456)	Prec@1 56.641 (57.873)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.5801 (1.5461)	Prec@1 56.250 (57.682)	
Total train loss: 1.5461

Train time: 20.220594882965088
 * Prec@1 53.120 Prec@5 81.790 Loss 1.8242
Best acc: 53.120
--------------------------------------------------------------------------------
Test time: 26.029146432876587

Epoch: [29][38/196]	LR: 1e-05	Loss 1.3867 (1.5520)	Prec@1 60.547 (56.951)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.5869 (1.5571)	Prec@1 55.859 (57.081)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.4678 (1.5466)	Prec@1 62.500 (57.462)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.5195 (1.5463)	Prec@1 58.984 (57.727)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.5439 (1.5417)	Prec@1 58.594 (57.810)	
Total train loss: 1.5424

Train time: 19.737104177474976
 * Prec@1 52.920 Prec@5 81.590 Loss 1.8291
Best acc: 53.120
--------------------------------------------------------------------------------
Test time: 23.27744960784912

Epoch: [30][38/196]	LR: 1e-05	Loss 1.3594 (1.5353)	Prec@1 62.109 (58.113)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.4326 (1.5351)	Prec@1 60.547 (58.013)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.4873 (1.5362)	Prec@1 59.766 (57.873)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.3877 (1.5378)	Prec@1 62.109 (57.903)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.4678 (1.5431)	Prec@1 61.719 (57.764)	
Total train loss: 1.5437

Train time: 21.237160444259644
 * Prec@1 53.170 Prec@5 81.830 Loss 1.8174
Best acc: 53.170
--------------------------------------------------------------------------------
Test time: 26.48483109474182

Epoch: [31][38/196]	LR: 1e-05	Loss 1.6982 (1.5357)	Prec@1 55.469 (57.462)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.5859 (1.5354)	Prec@1 56.250 (57.727)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.4336 (1.5326)	Prec@1 61.328 (57.823)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.4502 (1.5391)	Prec@1 62.500 (57.868)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.7490 (1.5392)	Prec@1 51.953 (57.869)	
Total train loss: 1.5397

Train time: 19.388331413269043
 * Prec@1 53.050 Prec@5 81.660 Loss 1.8193
Best acc: 53.170
--------------------------------------------------------------------------------
Test time: 24.363327026367188

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.4170 (1.5563)	Prec@1 62.500 (58.013)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.5605 (1.5570)	Prec@1 55.469 (57.722)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.6045 (1.5469)	Prec@1 54.297 (57.956)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.4980 (1.5403)	Prec@1 59.766 (58.070)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.4551 (1.5432)	Prec@1 59.375 (57.901)	
Total train loss: 1.5433

Train time: 20.06467056274414
 * Prec@1 53.060 Prec@5 81.810 Loss 1.8203
Best acc: 53.170
--------------------------------------------------------------------------------
Test time: 25.332650661468506

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.4668 (1.5340)	Prec@1 58.984 (58.253)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.5527 (1.5515)	Prec@1 54.688 (57.352)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.5264 (1.5526)	Prec@1 57.422 (57.305)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.5020 (1.5435)	Prec@1 56.641 (57.690)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.4922 (1.5432)	Prec@1 60.156 (57.716)	
Total train loss: 1.5439

Train time: 20.990610361099243
 * Prec@1 53.050 Prec@5 81.750 Loss 1.8242
Best acc: 53.170
--------------------------------------------------------------------------------
Test time: 24.854861974716187

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.5049 (1.5536)	Prec@1 54.688 (57.161)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.4629 (1.5669)	Prec@1 61.328 (56.901)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.5410 (1.5464)	Prec@1 53.906 (57.459)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.7324 (1.5409)	Prec@1 50.781 (57.795)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.5342 (1.5419)	Prec@1 57.812 (57.746)	
Total train loss: 1.5423

Train time: 20.97708034515381
 * Prec@1 53.190 Prec@5 81.730 Loss 1.8223
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 25.752877235412598

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.5127 (1.5209)	Prec@1 61.328 (58.303)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.8076 (1.5321)	Prec@1 50.000 (57.828)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.5107 (1.5336)	Prec@1 53.906 (57.699)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.5791 (1.5370)	Prec@1 59.375 (57.757)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.5664 (1.5398)	Prec@1 58.984 (57.752)	
Total train loss: 1.5398

Train time: 20.96663761138916
 * Prec@1 53.030 Prec@5 81.770 Loss 1.8252
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 25.51117777824402

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.5010 (1.5366)	Prec@1 59.375 (58.063)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.4961 (1.5323)	Prec@1 60.938 (58.183)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.5381 (1.5350)	Prec@1 56.641 (58.116)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.4102 (1.5398)	Prec@1 64.062 (57.980)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.5605 (1.5423)	Prec@1 59.766 (57.903)	
Total train loss: 1.5424

Train time: 21.43214750289917
 * Prec@1 53.130 Prec@5 81.680 Loss 1.8223
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 25.41258144378662

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.4463 (1.5358)	Prec@1 60.156 (58.243)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.5713 (1.5511)	Prec@1 58.594 (57.687)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.5869 (1.5486)	Prec@1 55.469 (57.656)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.4678 (1.5416)	Prec@1 60.547 (57.850)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.4346 (1.5399)	Prec@1 58.203 (57.863)	
Total train loss: 1.5401

Train time: 19.66122317314148
 * Prec@1 53.090 Prec@5 81.740 Loss 1.8252
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 24.34914493560791

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 1.4141 (1.5535)	Prec@1 61.719 (57.853)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.4912 (1.5538)	Prec@1 58.203 (57.722)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.6543 (1.5444)	Prec@1 53.906 (58.043)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.4775 (1.5460)	Prec@1 64.844 (57.843)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.5508 (1.5424)	Prec@1 57.812 (57.865)	
Total train loss: 1.5431

Train time: 21.010412216186523
 * Prec@1 52.990 Prec@5 81.530 Loss 1.8252
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 25.376019716262817

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.7520 (1.5453)	Prec@1 52.734 (57.412)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.5273 (1.5450)	Prec@1 58.984 (57.577)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.4639 (1.5358)	Prec@1 58.594 (57.873)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.5771 (1.5444)	Prec@1 53.516 (57.707)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.6143 (1.5404)	Prec@1 54.688 (57.784)	
Total train loss: 1.5404

Train time: 20.07236623764038
 * Prec@1 52.990 Prec@5 81.710 Loss 1.8164
Best acc: 53.190
--------------------------------------------------------------------------------
Test time: 24.398396730422974


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 54.560 Prec@5 81.160 Loss 1.9043
Pre-trained Prec@1 with 9 layers frozen: 54.55999755859375 	 Loss: 1.904296875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.6973 (0.6932)	Prec@1 80.469 (80.208)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.7109 (0.6810)	Prec@1 78.906 (80.409)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.7441 (0.6911)	Prec@1 78.906 (80.268)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.7432 (0.6981)	Prec@1 78.516 (80.138)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.8052 (0.7164)	Prec@1 75.781 (79.595)	
Total train loss: 0.7170

Train time: 100.95398449897766
 * Prec@1 49.320 Prec@5 76.600 Loss 2.2441
Best acc: 49.320
--------------------------------------------------------------------------------
Test time: 106.15772795677185

Epoch: [1][38/196]	LR: 0.01	Loss 0.7251 (0.7732)	Prec@1 78.906 (78.105)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.7485 (0.7782)	Prec@1 76.953 (77.494)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.8535 (0.7920)	Prec@1 72.656 (76.913)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.8110 (0.7976)	Prec@1 75.781 (76.853)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.9253 (0.8115)	Prec@1 75.391 (76.579)	
Total train loss: 0.8121

Train time: 20.996890783309937
 * Prec@1 57.240 Prec@5 84.320 Loss 1.7168
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 26.970348358154297

Epoch: [2][38/196]	LR: 0.01	Loss 1.0410 (0.9581)	Prec@1 71.484 (73.578)	
Epoch: [2][77/196]	LR: 0.01	Loss 1.1074 (1.0124)	Prec@1 69.922 (72.251)	
Epoch: [2][116/196]	LR: 0.01	Loss 1.3008 (1.0718)	Prec@1 65.234 (70.740)	
Epoch: [2][155/196]	LR: 0.01	Loss 1.3467 (1.1337)	Prec@1 63.281 (69.298)	
Epoch: [2][194/196]	LR: 0.01	Loss 1.4219 (1.1707)	Prec@1 59.766 (68.287)	
Total train loss: 1.1717

Train time: 21.6919047832489
 * Prec@1 15.970 Prec@5 35.780 Loss 4.6367
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 26.214723587036133

Epoch: [3][38/196]	LR: 0.01	Loss 1.3701 (1.3930)	Prec@1 62.109 (62.129)	
Epoch: [3][77/196]	LR: 0.01	Loss 1.2520 (1.4488)	Prec@1 66.797 (60.537)	
Epoch: [3][116/196]	LR: 0.01	Loss 1.4258 (1.4363)	Prec@1 62.109 (60.741)	
Epoch: [3][155/196]	LR: 0.01	Loss 1.4551 (1.4056)	Prec@1 61.328 (61.466)	
Epoch: [3][194/196]	LR: 0.01	Loss 1.2520 (1.3866)	Prec@1 64.844 (61.917)	
Total train loss: 1.3870

Train time: 20.462376594543457
 * Prec@1 53.950 Prec@5 81.690 Loss 1.7920
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 25.058085203170776

Epoch: [4][38/196]	LR: 0.01	Loss 1.4375 (1.3976)	Prec@1 56.641 (61.619)	
Epoch: [4][77/196]	LR: 0.01	Loss 1.3867 (1.4030)	Prec@1 58.984 (61.403)	
Epoch: [4][116/196]	LR: 0.01	Loss 1.5762 (1.4316)	Prec@1 58.203 (60.887)	
Epoch: [4][155/196]	LR: 0.01	Loss 1.3545 (1.4269)	Prec@1 61.719 (60.975)	
Epoch: [4][194/196]	LR: 0.01	Loss 1.4014 (1.4203)	Prec@1 62.891 (61.152)	
Total train loss: 1.4205

Train time: 18.067529439926147
 * Prec@1 3.310 Prec@5 13.870 Loss inf
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 21.739956378936768

Epoch: [5][38/196]	LR: 0.01	Loss 1.4707 (1.3836)	Prec@1 58.203 (61.759)	
Epoch: [5][77/196]	LR: 0.01	Loss 1.6348 (1.4574)	Prec@1 57.812 (60.071)	
Epoch: [5][116/196]	LR: 0.01	Loss 1.4668 (1.4676)	Prec@1 60.938 (59.836)	
Epoch: [5][155/196]	LR: 0.01	Loss 1.5332 (1.4668)	Prec@1 58.203 (59.791)	
Epoch: [5][194/196]	LR: 0.01	Loss 1.4863 (1.4917)	Prec@1 58.203 (59.229)	
Total train loss: 1.4919

Train time: 20.149855375289917
 * Prec@1 50.220 Prec@5 79.530 Loss 1.9648
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 24.54679250717163

Epoch: [6][38/196]	LR: 0.01	Loss 1.5205 (1.4421)	Prec@1 62.500 (61.208)	
Epoch: [6][77/196]	LR: 0.01	Loss 1.3096 (1.4353)	Prec@1 65.625 (61.303)	
Epoch: [6][116/196]	LR: 0.01	Loss 1.3545 (1.4287)	Prec@1 62.891 (61.318)	
Epoch: [6][155/196]	LR: 0.01	Loss 1.5762 (1.4371)	Prec@1 55.469 (61.090)	
Epoch: [6][194/196]	LR: 0.01	Loss 1.4287 (1.4456)	Prec@1 62.500 (61.102)	
Total train loss: 1.4455

Train time: 19.491515398025513
 * Prec@1 49.050 Prec@5 77.900 Loss 2.0059
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 23.95909023284912

Epoch: [7][38/196]	LR: 0.01	Loss 1.4082 (1.4050)	Prec@1 60.938 (62.250)	
Epoch: [7][77/196]	LR: 0.01	Loss 1.3330 (1.4161)	Prec@1 67.188 (62.144)	
Epoch: [7][116/196]	LR: 0.01	Loss 1.3779 (1.3948)	Prec@1 67.188 (62.827)	
Epoch: [7][155/196]	LR: 0.01	Loss 1.3447 (1.3791)	Prec@1 65.625 (63.101)	
Epoch: [7][194/196]	LR: 0.01	Loss 1.4785 (1.3941)	Prec@1 60.547 (62.680)	
Total train loss: 1.3944

Train time: 19.583738565444946
 * Prec@1 48.460 Prec@5 78.430 Loss 2.0020
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 26.942556619644165

Epoch: [8][38/196]	LR: 0.001	Loss 1.4023 (1.4081)	Prec@1 66.016 (62.460)	
Epoch: [8][77/196]	LR: 0.001	Loss 1.3926 (1.4090)	Prec@1 64.453 (62.250)	
Epoch: [8][116/196]	LR: 0.001	Loss 1.4014 (1.4077)	Prec@1 62.891 (62.243)	
Epoch: [8][155/196]	LR: 0.001	Loss 1.2832 (1.4076)	Prec@1 65.625 (62.109)	
Epoch: [8][194/196]	LR: 0.001	Loss 1.3652 (1.3988)	Prec@1 62.109 (62.360)	
Total train loss: 1.3994

Train time: 18.89189076423645
 * Prec@1 56.010 Prec@5 83.500 Loss 1.6699
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 23.397122859954834

Epoch: [9][38/196]	LR: 0.001	Loss 1.3359 (1.3757)	Prec@1 64.844 (62.640)	
Epoch: [9][77/196]	LR: 0.001	Loss 1.3438 (1.3740)	Prec@1 62.891 (62.655)	
Epoch: [9][116/196]	LR: 0.001	Loss 1.3389 (1.3813)	Prec@1 63.281 (62.674)	
Epoch: [9][155/196]	LR: 0.001	Loss 1.2871 (1.3816)	Prec@1 67.969 (62.733)	
Epoch: [9][194/196]	LR: 0.001	Loss 1.4443 (1.3865)	Prec@1 61.328 (62.612)	
Total train loss: 1.3862

Train time: 19.063049793243408
 * Prec@1 55.910 Prec@5 83.650 Loss 1.6689
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 25.091182947158813

Epoch: [10][38/196]	LR: 0.001	Loss 1.2012 (1.3550)	Prec@1 70.312 (63.381)	
Epoch: [10][77/196]	LR: 0.001	Loss 1.5146 (1.3713)	Prec@1 56.250 (63.111)	
Epoch: [10][116/196]	LR: 0.001	Loss 1.4941 (1.3716)	Prec@1 58.203 (63.038)	
Epoch: [10][155/196]	LR: 0.001	Loss 1.4062 (1.3772)	Prec@1 62.109 (62.963)	
Epoch: [10][194/196]	LR: 0.001	Loss 1.4023 (1.3804)	Prec@1 62.109 (62.953)	
Total train loss: 1.3805

Train time: 20.017475128173828
 * Prec@1 56.040 Prec@5 83.720 Loss 1.6621
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 23.808956384658813

Epoch: [11][38/196]	LR: 0.001	Loss 1.3750 (1.3924)	Prec@1 64.453 (62.480)	
Epoch: [11][77/196]	LR: 0.001	Loss 1.4180 (1.3892)	Prec@1 62.109 (62.375)	
Epoch: [11][116/196]	LR: 0.001	Loss 1.1221 (1.3827)	Prec@1 68.359 (62.690)	
Epoch: [11][155/196]	LR: 0.001	Loss 1.4492 (1.3830)	Prec@1 61.719 (62.673)	
Epoch: [11][194/196]	LR: 0.001	Loss 1.3594 (1.3799)	Prec@1 62.891 (62.939)	
Total train loss: 1.3803

Train time: 20.613324403762817
 * Prec@1 56.380 Prec@5 83.890 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 25.728027820587158

Epoch: [12][38/196]	LR: 0.001	Loss 1.3896 (1.3733)	Prec@1 61.328 (62.841)	
Epoch: [12][77/196]	LR: 0.001	Loss 1.3213 (1.3803)	Prec@1 66.016 (62.871)	
Epoch: [12][116/196]	LR: 0.001	Loss 1.5215 (1.3810)	Prec@1 60.156 (62.944)	
Epoch: [12][155/196]	LR: 0.001	Loss 1.4922 (1.3775)	Prec@1 58.594 (62.943)	
Epoch: [12][194/196]	LR: 0.001	Loss 1.2793 (1.3796)	Prec@1 67.188 (62.897)	
Total train loss: 1.3798

Train time: 20.052440643310547
 * Prec@1 55.860 Prec@5 83.510 Loss 1.6689
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 25.477850437164307

Epoch: [13][38/196]	LR: 0.001	Loss 1.3477 (1.3737)	Prec@1 64.062 (62.881)	
Epoch: [13][77/196]	LR: 0.001	Loss 1.4443 (1.3680)	Prec@1 56.641 (63.136)	
Epoch: [13][116/196]	LR: 0.001	Loss 1.3594 (1.3745)	Prec@1 65.234 (62.851)	
Epoch: [13][155/196]	LR: 0.001	Loss 1.3721 (1.3744)	Prec@1 60.156 (62.853)	
Epoch: [13][194/196]	LR: 0.001	Loss 1.4072 (1.3773)	Prec@1 63.281 (62.810)	
Total train loss: 1.3778

Train time: 20.44467306137085
 * Prec@1 56.180 Prec@5 83.790 Loss 1.6592
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 23.157820463180542

Epoch: [14][38/196]	LR: 0.001	Loss 1.3350 (1.3536)	Prec@1 62.500 (63.361)	
Epoch: [14][77/196]	LR: 0.001	Loss 1.4951 (1.3600)	Prec@1 57.422 (63.356)	
Epoch: [14][116/196]	LR: 0.001	Loss 1.3398 (1.3732)	Prec@1 61.328 (62.897)	
Epoch: [14][155/196]	LR: 0.001	Loss 1.4941 (1.3732)	Prec@1 55.859 (62.813)	
Epoch: [14][194/196]	LR: 0.001	Loss 1.3320 (1.3764)	Prec@1 64.453 (62.865)	
Total train loss: 1.3769

Train time: 10.345735549926758
 * Prec@1 56.360 Prec@5 83.800 Loss 1.6611
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 12.371110439300537

Epoch: [15][38/196]	LR: 0.001	Loss 1.4385 (1.3828)	Prec@1 61.328 (62.240)	
Epoch: [15][77/196]	LR: 0.001	Loss 1.3672 (1.3706)	Prec@1 61.719 (62.986)	
Epoch: [15][116/196]	LR: 0.001	Loss 1.2568 (1.3739)	Prec@1 63.672 (62.967)	
Epoch: [15][155/196]	LR: 0.001	Loss 1.1025 (1.3725)	Prec@1 71.094 (62.961)	
Epoch: [15][194/196]	LR: 0.001	Loss 1.2168 (1.3737)	Prec@1 70.312 (62.855)	
Total train loss: 1.3738

Train time: 10.401690244674683
 * Prec@1 56.180 Prec@5 83.620 Loss 1.6592
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.187700510025024

Epoch: [16][38/196]	LR: 0.0001	Loss 1.3350 (1.3771)	Prec@1 62.891 (62.810)	
Epoch: [16][77/196]	LR: 0.0001	Loss 1.3330 (1.3656)	Prec@1 63.281 (63.321)	
Epoch: [16][116/196]	LR: 0.0001	Loss 1.3154 (1.3674)	Prec@1 66.016 (63.218)	
Epoch: [16][155/196]	LR: 0.0001	Loss 1.3379 (1.3694)	Prec@1 66.797 (63.249)	
Epoch: [16][194/196]	LR: 0.0001	Loss 1.4854 (1.3691)	Prec@1 58.203 (63.223)	
Total train loss: 1.3694

Train time: 10.518647193908691
 * Prec@1 56.490 Prec@5 83.640 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.874449729919434

Epoch: [17][38/196]	LR: 0.0001	Loss 1.4170 (1.3548)	Prec@1 61.719 (64.153)	
Epoch: [17][77/196]	LR: 0.0001	Loss 1.3252 (1.3534)	Prec@1 65.625 (63.867)	
Epoch: [17][116/196]	LR: 0.0001	Loss 1.4326 (1.3603)	Prec@1 58.984 (63.588)	
Epoch: [17][155/196]	LR: 0.0001	Loss 1.4766 (1.3632)	Prec@1 62.500 (63.467)	
Epoch: [17][194/196]	LR: 0.0001	Loss 1.3555 (1.3677)	Prec@1 64.062 (63.283)	
Total train loss: 1.3676

Train time: 10.773739576339722
 * Prec@1 56.270 Prec@5 83.520 Loss 1.6572
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.20072054862976

Epoch: [18][38/196]	LR: 0.0001	Loss 1.3857 (1.3601)	Prec@1 60.938 (63.482)	
Epoch: [18][77/196]	LR: 0.0001	Loss 1.3154 (1.3606)	Prec@1 63.672 (63.181)	
Epoch: [18][116/196]	LR: 0.0001	Loss 1.3838 (1.3640)	Prec@1 60.938 (63.128)	
Epoch: [18][155/196]	LR: 0.0001	Loss 1.3311 (1.3663)	Prec@1 65.234 (63.058)	
Epoch: [18][194/196]	LR: 0.0001	Loss 1.3906 (1.3657)	Prec@1 61.719 (63.085)	
Total train loss: 1.3660

Train time: 10.754100799560547
 * Prec@1 56.280 Prec@5 83.760 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 14.250994443893433

Epoch: [19][38/196]	LR: 0.0001	Loss 1.3984 (1.3814)	Prec@1 63.672 (62.490)	
Epoch: [19][77/196]	LR: 0.0001	Loss 1.3809 (1.3597)	Prec@1 61.719 (63.241)	
Epoch: [19][116/196]	LR: 0.0001	Loss 1.4521 (1.3675)	Prec@1 63.281 (63.034)	
Epoch: [19][155/196]	LR: 0.0001	Loss 1.4375 (1.3704)	Prec@1 60.547 (63.051)	
Epoch: [19][194/196]	LR: 0.0001	Loss 1.3096 (1.3682)	Prec@1 64.453 (63.121)	
Total train loss: 1.3684

Train time: 10.7476167678833
 * Prec@1 56.280 Prec@5 83.680 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.777827262878418

Epoch: [20][38/196]	LR: 0.0001	Loss 1.4951 (1.3794)	Prec@1 58.594 (62.470)	
Epoch: [20][77/196]	LR: 0.0001	Loss 1.4160 (1.3644)	Prec@1 60.547 (63.156)	
Epoch: [20][116/196]	LR: 0.0001	Loss 1.4092 (1.3621)	Prec@1 65.234 (63.398)	
Epoch: [20][155/196]	LR: 0.0001	Loss 1.4639 (1.3692)	Prec@1 57.812 (63.324)	
Epoch: [20][194/196]	LR: 0.0001	Loss 1.2412 (1.3703)	Prec@1 64.844 (63.183)	
Total train loss: 1.3705

Train time: 10.510852336883545
 * Prec@1 56.150 Prec@5 83.780 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.629811763763428

Epoch: [21][38/196]	LR: 0.0001	Loss 1.5059 (1.3746)	Prec@1 60.547 (63.171)	
Epoch: [21][77/196]	LR: 0.0001	Loss 1.3701 (1.3714)	Prec@1 60.938 (63.011)	
Epoch: [21][116/196]	LR: 0.0001	Loss 1.4150 (1.3758)	Prec@1 61.328 (62.887)	
Epoch: [21][155/196]	LR: 0.0001	Loss 1.2402 (1.3766)	Prec@1 67.188 (62.828)	
Epoch: [21][194/196]	LR: 0.0001	Loss 1.4160 (1.3713)	Prec@1 61.328 (63.019)	
Total train loss: 1.3710

Train time: 10.44814920425415
 * Prec@1 56.240 Prec@5 83.610 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 14.109332799911499

Epoch: [22][38/196]	LR: 0.0001	Loss 1.3848 (1.3632)	Prec@1 61.328 (63.442)	
Epoch: [22][77/196]	LR: 0.0001	Loss 1.3643 (1.3688)	Prec@1 62.500 (63.401)	
Epoch: [22][116/196]	LR: 0.0001	Loss 1.2793 (1.3691)	Prec@1 64.453 (63.335)	
Epoch: [22][155/196]	LR: 0.0001	Loss 1.2725 (1.3694)	Prec@1 65.234 (63.196)	
Epoch: [22][194/196]	LR: 0.0001	Loss 1.4648 (1.3678)	Prec@1 59.375 (63.211)	
Total train loss: 1.3679

Train time: 10.37550687789917
 * Prec@1 56.150 Prec@5 83.680 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 12.671912908554077

Epoch: [23][38/196]	LR: 0.0001	Loss 1.3174 (1.3674)	Prec@1 64.062 (62.931)	
Epoch: [23][77/196]	LR: 0.0001	Loss 1.3945 (1.3589)	Prec@1 58.594 (63.186)	
Epoch: [23][116/196]	LR: 0.0001	Loss 1.4209 (1.3598)	Prec@1 65.234 (63.224)	
Epoch: [23][155/196]	LR: 0.0001	Loss 1.3887 (1.3604)	Prec@1 62.109 (63.286)	
Epoch: [23][194/196]	LR: 0.0001	Loss 1.3896 (1.3669)	Prec@1 63.672 (63.229)	
Total train loss: 1.3671

Train time: 10.292877435684204
 * Prec@1 56.150 Prec@5 83.560 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 12.931401491165161

Epoch: [24][38/196]	LR: 1e-05	Loss 1.5723 (1.3629)	Prec@1 55.469 (62.540)	
Epoch: [24][77/196]	LR: 1e-05	Loss 1.3271 (1.3671)	Prec@1 66.016 (63.121)	
Epoch: [24][116/196]	LR: 1e-05	Loss 1.2021 (1.3663)	Prec@1 63.672 (63.141)	
Epoch: [24][155/196]	LR: 1e-05	Loss 1.2881 (1.3669)	Prec@1 65.234 (63.043)	
Epoch: [24][194/196]	LR: 1e-05	Loss 1.4326 (1.3715)	Prec@1 60.938 (62.949)	
Total train loss: 1.3716

Train time: 10.264017343521118
 * Prec@1 56.220 Prec@5 83.690 Loss 1.6523
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 12.778105020523071

Epoch: [25][38/196]	LR: 1e-05	Loss 1.4121 (1.3700)	Prec@1 65.234 (63.051)	
Epoch: [25][77/196]	LR: 1e-05	Loss 1.2910 (1.3647)	Prec@1 64.453 (63.066)	
Epoch: [25][116/196]	LR: 1e-05	Loss 1.3359 (1.3707)	Prec@1 64.062 (62.951)	
Epoch: [25][155/196]	LR: 1e-05	Loss 1.3711 (1.3677)	Prec@1 63.672 (63.008)	
Epoch: [25][194/196]	LR: 1e-05	Loss 1.3379 (1.3702)	Prec@1 64.844 (62.981)	
Total train loss: 1.3701

Train time: 10.523037672042847
 * Prec@1 56.300 Prec@5 83.480 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.130555868148804

Epoch: [26][38/196]	LR: 1e-05	Loss 1.3496 (1.3643)	Prec@1 62.500 (63.532)	
Epoch: [26][77/196]	LR: 1e-05	Loss 1.5381 (1.3710)	Prec@1 58.203 (63.166)	
Epoch: [26][116/196]	LR: 1e-05	Loss 1.4258 (1.3697)	Prec@1 62.891 (63.154)	
Epoch: [26][155/196]	LR: 1e-05	Loss 1.3008 (1.3666)	Prec@1 65.625 (63.194)	
Epoch: [26][194/196]	LR: 1e-05	Loss 1.3965 (1.3700)	Prec@1 63.672 (63.053)	
Total train loss: 1.3708

Train time: 10.196127653121948
 * Prec@1 55.970 Prec@5 83.570 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.31192946434021

Epoch: [27][38/196]	LR: 1e-05	Loss 1.4004 (1.3723)	Prec@1 63.281 (63.512)	
Epoch: [27][77/196]	LR: 1e-05	Loss 1.3516 (1.3614)	Prec@1 62.109 (63.672)	
Epoch: [27][116/196]	LR: 1e-05	Loss 1.4258 (1.3689)	Prec@1 60.547 (63.281)	
Epoch: [27][155/196]	LR: 1e-05	Loss 1.2266 (1.3668)	Prec@1 66.406 (63.314)	
Epoch: [27][194/196]	LR: 1e-05	Loss 1.3730 (1.3683)	Prec@1 65.625 (63.283)	
Total train loss: 1.3682

Train time: 10.250431299209595
 * Prec@1 56.310 Prec@5 83.730 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.255046844482422

Epoch: [28][38/196]	LR: 1e-05	Loss 1.3252 (1.3531)	Prec@1 62.109 (63.732)	
Epoch: [28][77/196]	LR: 1e-05	Loss 1.3809 (1.3540)	Prec@1 59.766 (63.542)	
Epoch: [28][116/196]	LR: 1e-05	Loss 1.6133 (1.3633)	Prec@1 55.078 (63.248)	
Epoch: [28][155/196]	LR: 1e-05	Loss 1.2510 (1.3674)	Prec@1 66.016 (63.131)	
Epoch: [28][194/196]	LR: 1e-05	Loss 1.3789 (1.3675)	Prec@1 66.406 (63.155)	
Total train loss: 1.3676

Train time: 10.61132264137268
 * Prec@1 56.250 Prec@5 83.790 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 14.721112489700317

Epoch: [29][38/196]	LR: 1e-05	Loss 1.5195 (1.3585)	Prec@1 58.984 (63.822)	
Epoch: [29][77/196]	LR: 1e-05	Loss 1.4199 (1.3693)	Prec@1 62.109 (63.562)	
Epoch: [29][116/196]	LR: 1e-05	Loss 1.2871 (1.3676)	Prec@1 63.672 (63.522)	
Epoch: [29][155/196]	LR: 1e-05	Loss 1.2695 (1.3688)	Prec@1 61.328 (63.351)	
Epoch: [29][194/196]	LR: 1e-05	Loss 1.2559 (1.3669)	Prec@1 67.188 (63.317)	
Total train loss: 1.3674

Train time: 10.608409881591797
 * Prec@1 56.260 Prec@5 83.700 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.852450370788574

Epoch: [30][38/196]	LR: 1e-05	Loss 1.4512 (1.3802)	Prec@1 60.547 (62.821)	
Epoch: [30][77/196]	LR: 1e-05	Loss 1.4180 (1.3790)	Prec@1 66.016 (62.735)	
Epoch: [30][116/196]	LR: 1e-05	Loss 1.3350 (1.3696)	Prec@1 64.453 (62.927)	
Epoch: [30][155/196]	LR: 1e-05	Loss 1.4023 (1.3693)	Prec@1 61.328 (63.081)	
Epoch: [30][194/196]	LR: 1e-05	Loss 1.4395 (1.3693)	Prec@1 59.375 (63.139)	
Total train loss: 1.3690

Train time: 10.408913850784302
 * Prec@1 56.380 Prec@5 83.660 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.10167121887207

Epoch: [31][38/196]	LR: 1e-05	Loss 1.2314 (1.3554)	Prec@1 64.844 (63.381)	
Epoch: [31][77/196]	LR: 1e-05	Loss 1.4248 (1.3674)	Prec@1 57.031 (63.006)	
Epoch: [31][116/196]	LR: 1e-05	Loss 1.3594 (1.3661)	Prec@1 62.891 (63.188)	
Epoch: [31][155/196]	LR: 1e-05	Loss 1.2305 (1.3680)	Prec@1 67.188 (63.231)	
Epoch: [31][194/196]	LR: 1e-05	Loss 1.2861 (1.3673)	Prec@1 67.969 (63.265)	
Total train loss: 1.3678

Train time: 10.419354677200317
 * Prec@1 56.110 Prec@5 83.750 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.669055461883545

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 1.3760 (1.3684)	Prec@1 63.672 (63.291)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 1.3672 (1.3756)	Prec@1 61.328 (63.076)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 1.2598 (1.3686)	Prec@1 69.922 (63.241)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 1.3779 (1.3671)	Prec@1 64.844 (63.329)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 1.2666 (1.3682)	Prec@1 65.625 (63.249)	
Total train loss: 1.3682

Train time: 10.189706802368164
 * Prec@1 56.290 Prec@5 83.730 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.093872308731079

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 1.4697 (1.3634)	Prec@1 59.766 (63.522)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 1.4941 (1.3619)	Prec@1 59.375 (63.386)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 1.2500 (1.3656)	Prec@1 62.109 (63.275)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 1.3359 (1.3728)	Prec@1 64.844 (63.103)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 1.4463 (1.3702)	Prec@1 59.375 (63.157)	
Total train loss: 1.3704

Train time: 10.430283308029175
 * Prec@1 56.300 Prec@5 83.730 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 16.400316953659058

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 1.3438 (1.3759)	Prec@1 59.375 (62.931)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 1.4355 (1.3793)	Prec@1 63.672 (62.856)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 1.3584 (1.3722)	Prec@1 59.766 (63.054)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 1.5127 (1.3710)	Prec@1 57.812 (63.111)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 1.3564 (1.3681)	Prec@1 61.719 (63.129)	
Total train loss: 1.3682

Train time: 10.922306537628174
 * Prec@1 56.490 Prec@5 83.670 Loss 1.6514
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.737541675567627

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 1.3018 (1.3612)	Prec@1 65.625 (63.672)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 1.2227 (1.3646)	Prec@1 67.188 (63.361)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 1.3447 (1.3673)	Prec@1 63.672 (63.375)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 1.2578 (1.3680)	Prec@1 61.719 (63.254)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 1.2852 (1.3687)	Prec@1 67.188 (63.259)	
Total train loss: 1.3686

Train time: 10.253962993621826
 * Prec@1 56.190 Prec@5 83.670 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.384584903717041

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 1.3223 (1.3610)	Prec@1 64.062 (63.251)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 1.4316 (1.3708)	Prec@1 61.719 (63.006)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 1.3574 (1.3747)	Prec@1 64.844 (62.987)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 1.3018 (1.3761)	Prec@1 67.188 (62.986)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 1.2412 (1.3708)	Prec@1 69.531 (63.111)	
Total train loss: 1.3709

Train time: 10.269125938415527
 * Prec@1 56.240 Prec@5 83.670 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.256200075149536

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 1.5312 (1.3670)	Prec@1 58.203 (63.742)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 1.3125 (1.3667)	Prec@1 66.016 (63.406)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 1.2969 (1.3701)	Prec@1 65.625 (63.148)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 1.3896 (1.3700)	Prec@1 59.375 (62.993)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 1.3174 (1.3681)	Prec@1 66.406 (63.097)	
Total train loss: 1.3684

Train time: 10.298211574554443
 * Prec@1 56.200 Prec@5 83.550 Loss 1.6543
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.316253423690796

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 1.3926 (1.3776)	Prec@1 60.156 (62.590)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 1.3809 (1.3774)	Prec@1 61.719 (62.670)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 1.2148 (1.3755)	Prec@1 69.141 (62.787)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 1.4492 (1.3723)	Prec@1 62.109 (62.953)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 1.2734 (1.3687)	Prec@1 64.453 (63.037)	
Total train loss: 1.3690

Train time: 10.350246906280518
 * Prec@1 56.240 Prec@5 83.580 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.659635543823242

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 1.3535 (1.3593)	Prec@1 64.453 (63.692)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 1.4648 (1.3641)	Prec@1 59.766 (63.457)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 1.4854 (1.3697)	Prec@1 57.812 (63.301)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 1.3916 (1.3641)	Prec@1 62.109 (63.439)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 1.3779 (1.3674)	Prec@1 62.109 (63.257)	
Total train loss: 1.3675

Train time: 10.384893894195557
 * Prec@1 56.160 Prec@5 83.630 Loss 1.6562
Best acc: 57.240
--------------------------------------------------------------------------------
Test time: 13.091747522354126

