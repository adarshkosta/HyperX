
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv2): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 91.290 Prec@5 99.660 Loss 0.3318
Pre-trained Prec@1 with 1 layers frozen: 91.29000091552734 	 Loss: 0.331787109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.0327 (0.0364)	Prec@1 98.828 (98.928)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.0344 (0.0368)	Prec@1 99.219 (98.903)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.0242 (0.0372)	Prec@1 99.609 (98.888)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.0285 (0.0369)	Prec@1 98.828 (98.923)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.0417 (0.0372)	Prec@1 98.828 (98.902)	
Total train loss: 0.0373

Train time: 18.6071560382843
 * Prec@1 91.490 Prec@5 99.660 Loss 0.3225
Best acc: 91.490
--------------------------------------------------------------------------------
Test time: 21.974025011062622

Epoch: [1][38/196]	LR: 0.01	Loss 0.0433 (0.0345)	Prec@1 98.828 (99.119)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.0184 (0.0346)	Prec@1 100.000 (99.033)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.0366 (0.0351)	Prec@1 99.219 (98.992)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.0453 (0.0349)	Prec@1 98.438 (98.998)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.0349 (0.0349)	Prec@1 98.828 (98.982)	
Total train loss: 0.0350

Train time: 15.535178184509277
 * Prec@1 91.510 Prec@5 99.680 Loss 0.3230
Best acc: 91.510
--------------------------------------------------------------------------------
Test time: 18.567044496536255

Epoch: [2][38/196]	LR: 0.01	Loss 0.0606 (0.0338)	Prec@1 97.656 (98.988)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.0266 (0.0332)	Prec@1 99.219 (99.048)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.0302 (0.0336)	Prec@1 99.219 (99.052)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.0338 (0.0343)	Prec@1 99.219 (99.023)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.0259 (0.0340)	Prec@1 99.219 (99.032)	
Total train loss: 0.0340

Train time: 22.15899109840393
 * Prec@1 91.560 Prec@5 99.670 Loss 0.3235
Best acc: 91.560
--------------------------------------------------------------------------------
Test time: 34.83433938026428

Epoch: [3][38/196]	LR: 0.01	Loss 0.0296 (0.0303)	Prec@1 99.219 (99.199)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.0376 (0.0318)	Prec@1 99.609 (99.204)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.0295 (0.0316)	Prec@1 100.000 (99.222)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.0352 (0.0320)	Prec@1 98.828 (99.211)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.0566 (0.0324)	Prec@1 98.047 (99.171)	
Total train loss: 0.0325

Train time: 175.35420155525208
 * Prec@1 91.640 Prec@5 99.650 Loss 0.3225
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 206.9596242904663

Epoch: [4][38/196]	LR: 0.01	Loss 0.0152 (0.0300)	Prec@1 100.000 (99.289)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.0185 (0.0323)	Prec@1 99.609 (99.114)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.0261 (0.0326)	Prec@1 99.609 (99.092)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.0300 (0.0328)	Prec@1 98.828 (99.071)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.0424 (0.0329)	Prec@1 98.828 (99.083)	
Total train loss: 0.0330

Train time: 197.6046051979065
 * Prec@1 91.530 Prec@5 99.620 Loss 0.3254
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 231.9675693511963

Epoch: [5][38/196]	LR: 0.01	Loss 0.0219 (0.0310)	Prec@1 99.609 (99.209)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.0309 (0.0315)	Prec@1 99.219 (99.159)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.0242 (0.0316)	Prec@1 99.219 (99.139)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.0350 (0.0318)	Prec@1 98.828 (99.124)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.0458 (0.0323)	Prec@1 98.438 (99.097)	
Total train loss: 0.0324

Train time: 32.082698822021484
 * Prec@1 91.590 Prec@5 99.650 Loss 0.3235
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 38.131181716918945

Epoch: [6][38/196]	LR: 0.01	Loss 0.0264 (0.0315)	Prec@1 99.219 (99.189)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.0319 (0.0315)	Prec@1 99.609 (99.159)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.0254 (0.0317)	Prec@1 99.609 (99.145)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.0475 (0.0323)	Prec@1 98.828 (99.131)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.0191 (0.0322)	Prec@1 99.609 (99.141)	
Total train loss: 0.0324

Train time: 23.085049629211426
 * Prec@1 91.570 Prec@5 99.650 Loss 0.3254
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 28.60176110267639

Epoch: [7][38/196]	LR: 0.01	Loss 0.0304 (0.0319)	Prec@1 99.219 (99.149)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.0256 (0.0333)	Prec@1 99.219 (99.109)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.0387 (0.0327)	Prec@1 98.828 (99.129)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.0135 (0.0321)	Prec@1 99.609 (99.136)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.0241 (0.0318)	Prec@1 99.219 (99.147)	
Total train loss: 0.0318

Train time: 26.783891916275024
 * Prec@1 91.520 Prec@5 99.690 Loss 0.3242
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 32.050912618637085

Epoch: [8][38/196]	LR: 0.001	Loss 0.0373 (0.0324)	Prec@1 99.219 (99.119)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.0320 (0.0353)	Prec@1 99.219 (99.013)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.0638 (0.0336)	Prec@1 98.438 (99.082)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.0427 (0.0327)	Prec@1 98.438 (99.106)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.0172 (0.0323)	Prec@1 100.000 (99.135)	
Total train loss: 0.0324

Train time: 25.580442667007446
 * Prec@1 91.590 Prec@5 99.700 Loss 0.3262
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 32.62851595878601

Epoch: [9][38/196]	LR: 0.001	Loss 0.0344 (0.0310)	Prec@1 98.047 (99.139)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.0326 (0.0292)	Prec@1 99.219 (99.254)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.0283 (0.0301)	Prec@1 99.219 (99.202)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.0294 (0.0309)	Prec@1 98.438 (99.156)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.0236 (0.0309)	Prec@1 99.609 (99.173)	
Total train loss: 0.0310

Train time: 20.80249524116516
 * Prec@1 91.540 Prec@5 99.670 Loss 0.3269
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 25.94118094444275

Epoch: [10][38/196]	LR: 0.001	Loss 0.0353 (0.0328)	Prec@1 98.828 (99.129)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.0198 (0.0329)	Prec@1 100.000 (99.119)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.0152 (0.0324)	Prec@1 99.609 (99.122)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.0542 (0.0316)	Prec@1 98.047 (99.151)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.0231 (0.0311)	Prec@1 99.609 (99.181)	
Total train loss: 0.0312

Train time: 21.747934818267822
 * Prec@1 91.480 Prec@5 99.670 Loss 0.3240
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.293331623077393

Epoch: [11][38/196]	LR: 0.001	Loss 0.0281 (0.0323)	Prec@1 99.219 (99.089)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.0562 (0.0319)	Prec@1 97.656 (99.119)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.0266 (0.0314)	Prec@1 99.609 (99.155)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.0552 (0.0315)	Prec@1 98.047 (99.129)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.0312 (0.0314)	Prec@1 98.828 (99.157)	
Total train loss: 0.0314

Train time: 21.91780710220337
 * Prec@1 91.560 Prec@5 99.650 Loss 0.3254
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.115874528884888

Epoch: [12][38/196]	LR: 0.001	Loss 0.0224 (0.0338)	Prec@1 99.609 (99.099)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.0428 (0.0326)	Prec@1 99.219 (99.169)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.0331 (0.0323)	Prec@1 98.828 (99.152)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.0244 (0.0317)	Prec@1 98.828 (99.184)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.0480 (0.0312)	Prec@1 98.438 (99.209)	
Total train loss: 0.0312

Train time: 21.317866563796997
 * Prec@1 91.570 Prec@5 99.680 Loss 0.3269
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 25.745309352874756

Epoch: [13][38/196]	LR: 0.001	Loss 0.0179 (0.0305)	Prec@1 99.609 (99.269)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.0377 (0.0309)	Prec@1 98.828 (99.254)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.0362 (0.0310)	Prec@1 99.219 (99.232)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.0118 (0.0311)	Prec@1 100.000 (99.234)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.0238 (0.0314)	Prec@1 99.609 (99.209)	
Total train loss: 0.0314

Train time: 23.11143970489502
 * Prec@1 91.530 Prec@5 99.660 Loss 0.3254
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 28.038859844207764

Epoch: [14][38/196]	LR: 0.001	Loss 0.0266 (0.0324)	Prec@1 99.609 (99.139)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.0464 (0.0313)	Prec@1 98.438 (99.194)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.0438 (0.0320)	Prec@1 99.219 (99.182)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.0249 (0.0316)	Prec@1 99.219 (99.204)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.0352 (0.0315)	Prec@1 98.828 (99.209)	
Total train loss: 0.0315

Train time: 20.33780527114868
 * Prec@1 91.600 Prec@5 99.670 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 26.875062465667725

Epoch: [15][38/196]	LR: 0.001	Loss 0.0201 (0.0313)	Prec@1 100.000 (99.119)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.0382 (0.0318)	Prec@1 99.219 (99.114)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.0246 (0.0320)	Prec@1 98.828 (99.165)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.0179 (0.0317)	Prec@1 99.609 (99.151)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.0383 (0.0316)	Prec@1 98.438 (99.163)	
Total train loss: 0.0316

Train time: 21.517489433288574
 * Prec@1 91.560 Prec@5 99.670 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 25.912028312683105

Epoch: [16][38/196]	LR: 0.0001	Loss 0.0407 (0.0313)	Prec@1 98.828 (99.109)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.0388 (0.0309)	Prec@1 98.828 (99.164)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.0245 (0.0305)	Prec@1 99.609 (99.189)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.0252 (0.0306)	Prec@1 100.000 (99.206)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.0289 (0.0315)	Prec@1 99.609 (99.183)	
Total train loss: 0.0315

Train time: 21.815282344818115
 * Prec@1 91.570 Prec@5 99.660 Loss 0.3264
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.149275064468384

Epoch: [17][38/196]	LR: 0.0001	Loss 0.0199 (0.0301)	Prec@1 99.609 (99.249)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.0233 (0.0311)	Prec@1 98.828 (99.179)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.0341 (0.0314)	Prec@1 99.219 (99.155)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.0360 (0.0320)	Prec@1 99.609 (99.126)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.0278 (0.0318)	Prec@1 99.219 (99.133)	
Total train loss: 0.0318

Train time: 24.263012886047363
 * Prec@1 91.570 Prec@5 99.690 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 30.40444326400757

Epoch: [18][38/196]	LR: 0.0001	Loss 0.0299 (0.0299)	Prec@1 99.219 (99.319)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.0249 (0.0314)	Prec@1 99.609 (99.224)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.0269 (0.0317)	Prec@1 99.219 (99.162)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.0212 (0.0313)	Prec@1 99.219 (99.176)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.0396 (0.0315)	Prec@1 98.828 (99.175)	
Total train loss: 0.0315

Train time: 21.80920958518982
 * Prec@1 91.470 Prec@5 99.630 Loss 0.3267
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.644859313964844

Epoch: [19][38/196]	LR: 0.0001	Loss 0.0453 (0.0301)	Prec@1 97.656 (99.219)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.0242 (0.0316)	Prec@1 100.000 (99.129)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.0356 (0.0318)	Prec@1 99.219 (99.102)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.0238 (0.0315)	Prec@1 99.219 (99.104)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.0519 (0.0318)	Prec@1 97.266 (99.109)	
Total train loss: 0.0319

Train time: 23.01924991607666
 * Prec@1 91.450 Prec@5 99.670 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.71661639213562

Epoch: [20][38/196]	LR: 0.0001	Loss 0.0422 (0.0314)	Prec@1 98.438 (99.189)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.0267 (0.0318)	Prec@1 99.219 (99.154)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.0475 (0.0321)	Prec@1 98.047 (99.132)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.0372 (0.0315)	Prec@1 99.219 (99.166)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.0199 (0.0316)	Prec@1 99.609 (99.145)	
Total train loss: 0.0316

Train time: 21.172349214553833
 * Prec@1 91.630 Prec@5 99.640 Loss 0.3267
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.107677221298218

Epoch: [21][38/196]	LR: 0.0001	Loss 0.0332 (0.0303)	Prec@1 99.219 (99.249)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.0389 (0.0298)	Prec@1 98.828 (99.249)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.0291 (0.0302)	Prec@1 99.609 (99.245)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.0226 (0.0308)	Prec@1 99.219 (99.229)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.0260 (0.0306)	Prec@1 99.219 (99.237)	
Total train loss: 0.0306

Train time: 21.71514320373535
 * Prec@1 91.610 Prec@5 99.660 Loss 0.3259
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 29.073867082595825

Epoch: [22][38/196]	LR: 0.0001	Loss 0.0358 (0.0295)	Prec@1 99.219 (99.299)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.0243 (0.0299)	Prec@1 99.609 (99.259)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.0175 (0.0306)	Prec@1 100.000 (99.235)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.0248 (0.0309)	Prec@1 99.609 (99.226)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.0375 (0.0314)	Prec@1 98.438 (99.169)	
Total train loss: 0.0315

Train time: 20.9224534034729
 * Prec@1 91.510 Prec@5 99.640 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 26.355963468551636

Epoch: [23][38/196]	LR: 0.0001	Loss 0.0324 (0.0290)	Prec@1 98.828 (99.249)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.0226 (0.0303)	Prec@1 100.000 (99.204)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.0325 (0.0301)	Prec@1 99.609 (99.199)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.0213 (0.0306)	Prec@1 99.219 (99.179)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.0259 (0.0309)	Prec@1 99.219 (99.179)	
Total train loss: 0.0309

Train time: 22.01718783378601
 * Prec@1 91.510 Prec@5 99.660 Loss 0.3247
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 28.50168251991272

Epoch: [24][38/196]	LR: 1e-05	Loss 0.0326 (0.0319)	Prec@1 99.609 (99.139)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.0360 (0.0317)	Prec@1 98.438 (99.134)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.0306 (0.0311)	Prec@1 98.828 (99.182)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.0234 (0.0312)	Prec@1 99.219 (99.191)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.0209 (0.0315)	Prec@1 99.219 (99.197)	
Total train loss: 0.0314

Train time: 21.695284366607666
 * Prec@1 91.480 Prec@5 99.650 Loss 0.3264
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 25.97032403945923

Epoch: [25][38/196]	LR: 1e-05	Loss 0.0512 (0.0313)	Prec@1 98.047 (99.149)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.0237 (0.0315)	Prec@1 98.828 (99.179)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.0460 (0.0320)	Prec@1 99.609 (99.139)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.0348 (0.0313)	Prec@1 99.219 (99.169)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.0474 (0.0314)	Prec@1 98.828 (99.175)	
Total train loss: 0.0314

Train time: 22.20571279525757
 * Prec@1 91.490 Prec@5 99.640 Loss 0.3250
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.169790744781494

Epoch: [26][38/196]	LR: 1e-05	Loss 0.0278 (0.0329)	Prec@1 99.219 (99.109)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.0241 (0.0321)	Prec@1 99.219 (99.114)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.0363 (0.0317)	Prec@1 98.828 (99.135)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.0421 (0.0316)	Prec@1 98.047 (99.139)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.0153 (0.0315)	Prec@1 100.000 (99.155)	
Total train loss: 0.0316

Train time: 22.40161943435669
 * Prec@1 91.440 Prec@5 99.640 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 28.473989009857178

Epoch: [27][38/196]	LR: 1e-05	Loss 0.0122 (0.0325)	Prec@1 100.000 (99.079)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.0294 (0.0325)	Prec@1 98.828 (99.064)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.0466 (0.0318)	Prec@1 98.828 (99.125)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.0456 (0.0313)	Prec@1 98.828 (99.154)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.0393 (0.0316)	Prec@1 98.828 (99.145)	
Total train loss: 0.0316

Train time: 22.645208597183228
 * Prec@1 91.540 Prec@5 99.650 Loss 0.3225
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.46695065498352

Epoch: [28][38/196]	LR: 1e-05	Loss 0.0465 (0.0320)	Prec@1 98.047 (99.079)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.0278 (0.0313)	Prec@1 98.828 (99.159)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.0279 (0.0315)	Prec@1 99.609 (99.145)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.0274 (0.0316)	Prec@1 99.219 (99.166)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.0301 (0.0311)	Prec@1 99.219 (99.189)	
Total train loss: 0.0311

Train time: 22.11215329170227
 * Prec@1 91.560 Prec@5 99.680 Loss 0.3250
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.814188718795776

Epoch: [29][38/196]	LR: 1e-05	Loss 0.0553 (0.0302)	Prec@1 98.828 (99.279)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.0251 (0.0308)	Prec@1 99.609 (99.234)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.0390 (0.0308)	Prec@1 99.219 (99.229)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.0212 (0.0308)	Prec@1 99.219 (99.184)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.0320 (0.0311)	Prec@1 98.828 (99.179)	
Total train loss: 0.0311

Train time: 22.11984157562256
 * Prec@1 91.530 Prec@5 99.660 Loss 0.3240
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 27.715799570083618

Epoch: [30][38/196]	LR: 1e-05	Loss 0.0512 (0.0311)	Prec@1 99.219 (99.249)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.0235 (0.0318)	Prec@1 100.000 (99.204)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.0414 (0.0321)	Prec@1 98.047 (99.155)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.0466 (0.0324)	Prec@1 98.047 (99.161)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.0152 (0.0316)	Prec@1 99.609 (99.187)	
Total train loss: 0.0317

Train time: 23.06334161758423
 * Prec@1 91.570 Prec@5 99.680 Loss 0.3257
Best acc: 91.640
--------------------------------------------------------------------------------
Test time: 28.319329023361206

Epoch: [31][38/196]	LR: 1e-05	Loss 0.0240 (0.0302)	Prec@1 99.609 (99.179)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.0217 (0.0307)	Prec@1 99.609 (99.184)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.0322 (0.0308)	Prec@1 99.219 (99.209)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.0239 (0.0314)	Prec@1 99.219 (99.189)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.0278 (0.0314)	Prec@1 99.219 (99.197)	
Total train loss: 0.0314

Train time: 22.611435890197754
 * Prec@1 91.650 Prec@5 99.640 Loss 0.3247
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 28.75606369972229

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.0314 (0.0313)	Prec@1 99.219 (99.119)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.0255 (0.0322)	Prec@1 99.609 (99.179)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.0306 (0.0316)	Prec@1 99.219 (99.212)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.0407 (0.0317)	Prec@1 99.609 (99.201)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.0228 (0.0309)	Prec@1 99.609 (99.227)	
Total train loss: 0.0308

Train time: 23.336182832717896
 * Prec@1 91.520 Prec@5 99.640 Loss 0.3235
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 29.36019015312195

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.0381 (0.0311)	Prec@1 98.438 (99.179)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.0472 (0.0302)	Prec@1 98.438 (99.199)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.0233 (0.0305)	Prec@1 99.609 (99.175)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.0345 (0.0306)	Prec@1 99.219 (99.186)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.0209 (0.0310)	Prec@1 99.609 (99.165)	
Total train loss: 0.0310

Train time: 21.451732635498047
 * Prec@1 91.570 Prec@5 99.670 Loss 0.3247
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 26.599512338638306

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.0375 (0.0323)	Prec@1 99.219 (99.249)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.0164 (0.0307)	Prec@1 99.609 (99.254)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.0382 (0.0311)	Prec@1 99.219 (99.202)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.0176 (0.0314)	Prec@1 100.000 (99.186)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.0411 (0.0319)	Prec@1 98.438 (99.179)	
Total train loss: 0.0319

Train time: 20.927809238433838
 * Prec@1 91.560 Prec@5 99.640 Loss 0.3252
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 25.386763334274292

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.0382 (0.0308)	Prec@1 98.828 (99.209)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.0316 (0.0307)	Prec@1 98.828 (99.189)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.0196 (0.0314)	Prec@1 100.000 (99.195)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.0284 (0.0315)	Prec@1 99.219 (99.159)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.0347 (0.0312)	Prec@1 99.219 (99.163)	
Total train loss: 0.0312

Train time: 23.326345920562744
 * Prec@1 91.480 Prec@5 99.640 Loss 0.3237
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 32.69658398628235

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.0305 (0.0300)	Prec@1 99.219 (99.299)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.0432 (0.0306)	Prec@1 98.438 (99.219)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.0253 (0.0299)	Prec@1 99.609 (99.249)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.0210 (0.0301)	Prec@1 100.000 (99.256)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.0528 (0.0307)	Prec@1 97.266 (99.229)	
Total train loss: 0.0307

Train time: 22.464789152145386
 * Prec@1 91.510 Prec@5 99.670 Loss 0.3269
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 25.634777069091797

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.0185 (0.0308)	Prec@1 99.609 (99.159)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.0249 (0.0321)	Prec@1 99.609 (99.099)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.0359 (0.0323)	Prec@1 98.828 (99.105)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.0307 (0.0322)	Prec@1 99.219 (99.116)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.0485 (0.0318)	Prec@1 98.438 (99.145)	
Total train loss: 0.0317

Train time: 23.085219860076904
 * Prec@1 91.550 Prec@5 99.680 Loss 0.3235
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 27.07784628868103

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.0340 (0.0319)	Prec@1 98.438 (99.058)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.0173 (0.0316)	Prec@1 100.000 (99.134)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.0297 (0.0319)	Prec@1 99.219 (99.132)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.0176 (0.0319)	Prec@1 99.609 (99.119)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.0309 (0.0319)	Prec@1 99.219 (99.129)	
Total train loss: 0.0319

Train time: 23.190391302108765
 * Prec@1 91.550 Prec@5 99.670 Loss 0.3271
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 29.638752937316895

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.0312 (0.0314)	Prec@1 98.828 (99.099)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.0216 (0.0316)	Prec@1 100.000 (99.124)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.0248 (0.0312)	Prec@1 99.219 (99.155)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.0272 (0.0314)	Prec@1 99.609 (99.176)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.0277 (0.0315)	Prec@1 99.609 (99.171)	
Total train loss: 0.0315

Train time: 24.14320945739746
 * Prec@1 91.600 Prec@5 99.660 Loss 0.3228
Best acc: 91.650
--------------------------------------------------------------------------------
Test time: 28.79988718032837


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv4): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 90.570 Prec@5 99.670 Loss 0.3572
Pre-trained Prec@1 with 3 layers frozen: 90.56999969482422 	 Loss: 0.357177734375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.0370 (0.0490)	Prec@1 98.828 (98.427)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.0380 (0.0477)	Prec@1 99.219 (98.493)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.0576 (0.0486)	Prec@1 97.656 (98.431)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.0233 (0.0476)	Prec@1 99.219 (98.465)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.0275 (0.0482)	Prec@1 99.219 (98.458)	
Total train loss: 0.0482

Train time: 70.57174038887024
 * Prec@1 91.250 Prec@5 99.630 Loss 0.3379
Best acc: 91.250
--------------------------------------------------------------------------------
Test time: 86.33550095558167

Epoch: [1][38/196]	LR: 0.01	Loss 0.0416 (0.0428)	Prec@1 99.219 (98.698)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.0374 (0.0449)	Prec@1 98.828 (98.623)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.0529 (0.0440)	Prec@1 98.438 (98.678)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.0332 (0.0444)	Prec@1 99.219 (98.650)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.0361 (0.0449)	Prec@1 98.828 (98.648)	
Total train loss: 0.0448

Train time: 56.4278302192688
 * Prec@1 91.370 Prec@5 99.580 Loss 0.3357
Best acc: 91.370
--------------------------------------------------------------------------------
Test time: 62.45312428474426

Epoch: [2][38/196]	LR: 0.01	Loss 0.0292 (0.0392)	Prec@1 100.000 (98.818)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.0647 (0.0404)	Prec@1 96.484 (98.763)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.0266 (0.0414)	Prec@1 99.609 (98.755)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.0470 (0.0428)	Prec@1 98.438 (98.695)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.0323 (0.0433)	Prec@1 99.609 (98.686)	
Total train loss: 0.0433

Train time: 16.748613357543945
 * Prec@1 91.340 Prec@5 99.610 Loss 0.3325
Best acc: 91.370
--------------------------------------------------------------------------------
Test time: 19.70058846473694

Epoch: [3][38/196]	LR: 0.01	Loss 0.0371 (0.0437)	Prec@1 98.828 (98.658)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.0389 (0.0440)	Prec@1 98.438 (98.708)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.0211 (0.0438)	Prec@1 99.609 (98.708)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.0445 (0.0428)	Prec@1 98.828 (98.720)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.0153 (0.0430)	Prec@1 100.000 (98.718)	
Total train loss: 0.0429

Train time: 11.97311019897461
 * Prec@1 91.390 Prec@5 99.610 Loss 0.3296
Best acc: 91.390
--------------------------------------------------------------------------------
Test time: 15.306755065917969

Epoch: [4][38/196]	LR: 0.01	Loss 0.0586 (0.0413)	Prec@1 98.828 (98.808)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.0591 (0.0417)	Prec@1 98.047 (98.758)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.0250 (0.0414)	Prec@1 99.609 (98.775)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.0341 (0.0416)	Prec@1 99.219 (98.771)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.0230 (0.0419)	Prec@1 99.219 (98.756)	
Total train loss: 0.0419

Train time: 11.779461145401001
 * Prec@1 91.310 Prec@5 99.610 Loss 0.3318
Best acc: 91.390
--------------------------------------------------------------------------------
Test time: 13.944681882858276

Epoch: [5][38/196]	LR: 0.01	Loss 0.0475 (0.0397)	Prec@1 98.828 (98.868)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.0419 (0.0397)	Prec@1 98.438 (98.848)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.0641 (0.0409)	Prec@1 98.047 (98.801)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.0264 (0.0413)	Prec@1 99.609 (98.773)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.0398 (0.0424)	Prec@1 99.219 (98.730)	
Total train loss: 0.0425

Train time: 11.774718284606934
 * Prec@1 91.400 Prec@5 99.570 Loss 0.3352
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 16.732887268066406

Epoch: [6][38/196]	LR: 0.01	Loss 0.0504 (0.0423)	Prec@1 98.438 (98.718)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.0479 (0.0411)	Prec@1 98.047 (98.748)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.0445 (0.0405)	Prec@1 98.438 (98.815)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.0548 (0.0405)	Prec@1 98.438 (98.838)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.0809 (0.0412)	Prec@1 96.875 (98.788)	
Total train loss: 0.0413

Train time: 16.70456099510193
 * Prec@1 91.280 Prec@5 99.610 Loss 0.3350
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 19.391727209091187

Epoch: [7][38/196]	LR: 0.01	Loss 0.0432 (0.0374)	Prec@1 98.828 (98.878)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.0308 (0.0394)	Prec@1 99.609 (98.823)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.0430 (0.0414)	Prec@1 98.047 (98.721)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.0244 (0.0412)	Prec@1 99.219 (98.745)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.0564 (0.0417)	Prec@1 98.828 (98.730)	
Total train loss: 0.0417

Train time: 16.75619077682495
 * Prec@1 91.280 Prec@5 99.590 Loss 0.3354
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 19.912999153137207

Epoch: [8][38/196]	LR: 0.001	Loss 0.0293 (0.0371)	Prec@1 99.219 (98.908)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.0505 (0.0374)	Prec@1 98.047 (98.933)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.0353 (0.0386)	Prec@1 99.219 (98.855)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.0494 (0.0396)	Prec@1 98.828 (98.823)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.0312 (0.0402)	Prec@1 99.219 (98.820)	
Total train loss: 0.0401

Train time: 18.020245790481567
 * Prec@1 91.280 Prec@5 99.620 Loss 0.3311
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 23.150479316711426

Epoch: [9][38/196]	LR: 0.001	Loss 0.0169 (0.0379)	Prec@1 99.219 (98.898)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.0429 (0.0391)	Prec@1 98.828 (98.843)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.0395 (0.0393)	Prec@1 98.828 (98.882)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.0313 (0.0396)	Prec@1 99.609 (98.846)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.0449 (0.0394)	Prec@1 99.219 (98.856)	
Total train loss: 0.0396

Train time: 18.57649540901184
 * Prec@1 91.290 Prec@5 99.580 Loss 0.3347
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 22.473922729492188

Epoch: [10][38/196]	LR: 0.001	Loss 0.0327 (0.0373)	Prec@1 99.609 (99.008)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.0361 (0.0366)	Prec@1 99.219 (99.048)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.0365 (0.0381)	Prec@1 99.219 (98.975)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.0246 (0.0392)	Prec@1 99.609 (98.913)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.0347 (0.0394)	Prec@1 99.609 (98.892)	
Total train loss: 0.0395

Train time: 17.780319452285767
 * Prec@1 91.260 Prec@5 99.570 Loss 0.3347
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 20.92286515235901

Epoch: [11][38/196]	LR: 0.001	Loss 0.0605 (0.0438)	Prec@1 98.438 (98.668)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.0514 (0.0405)	Prec@1 98.828 (98.813)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.0752 (0.0397)	Prec@1 97.656 (98.872)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.0343 (0.0396)	Prec@1 98.828 (98.861)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.0201 (0.0397)	Prec@1 99.609 (98.860)	
Total train loss: 0.0397

Train time: 16.961578130722046
 * Prec@1 91.220 Prec@5 99.610 Loss 0.3308
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 21.78394627571106

Epoch: [12][38/196]	LR: 0.001	Loss 0.0185 (0.0390)	Prec@1 100.000 (98.798)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.0581 (0.0402)	Prec@1 98.438 (98.843)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.0605 (0.0412)	Prec@1 97.656 (98.818)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.0338 (0.0402)	Prec@1 98.828 (98.848)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.0493 (0.0395)	Prec@1 97.656 (98.864)	
Total train loss: 0.0395

Train time: 21.60837173461914
 * Prec@1 91.170 Prec@5 99.630 Loss 0.3345
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 25.59804654121399

Epoch: [13][38/196]	LR: 0.001	Loss 0.0360 (0.0398)	Prec@1 98.438 (98.838)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.0578 (0.0412)	Prec@1 98.047 (98.743)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.0578 (0.0423)	Prec@1 97.656 (98.705)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.0619 (0.0411)	Prec@1 97.656 (98.778)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.0392 (0.0406)	Prec@1 99.219 (98.798)	
Total train loss: 0.0406

Train time: 21.72187852859497
 * Prec@1 91.320 Prec@5 99.570 Loss 0.3357
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 25.60038924217224

Epoch: [14][38/196]	LR: 0.001	Loss 0.0303 (0.0399)	Prec@1 99.219 (98.878)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.0627 (0.0396)	Prec@1 97.656 (98.838)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.0430 (0.0396)	Prec@1 99.219 (98.868)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.0336 (0.0390)	Prec@1 98.438 (98.898)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.0448 (0.0392)	Prec@1 98.438 (98.896)	
Total train loss: 0.0393

Train time: 22.219722986221313
 * Prec@1 91.220 Prec@5 99.600 Loss 0.3352
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.622554779052734

Epoch: [15][38/196]	LR: 0.001	Loss 0.0233 (0.0425)	Prec@1 99.219 (98.768)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.0294 (0.0407)	Prec@1 99.609 (98.833)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.0517 (0.0401)	Prec@1 98.828 (98.858)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.0491 (0.0405)	Prec@1 98.828 (98.831)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.0212 (0.0399)	Prec@1 100.000 (98.876)	
Total train loss: 0.0400

Train time: 21.320584774017334
 * Prec@1 91.340 Prec@5 99.610 Loss 0.3359
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 24.85841965675354

Epoch: [16][38/196]	LR: 0.0001	Loss 0.0676 (0.0394)	Prec@1 96.875 (98.778)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.0674 (0.0420)	Prec@1 98.828 (98.723)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.0520 (0.0409)	Prec@1 98.438 (98.808)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.0689 (0.0413)	Prec@1 98.047 (98.761)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.0273 (0.0402)	Prec@1 99.609 (98.812)	
Total train loss: 0.0402

Train time: 22.088106155395508
 * Prec@1 91.150 Prec@5 99.560 Loss 0.3335
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.60229778289795

Epoch: [17][38/196]	LR: 0.0001	Loss 0.0259 (0.0395)	Prec@1 99.609 (98.848)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.0332 (0.0390)	Prec@1 98.828 (98.843)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.0384 (0.0388)	Prec@1 98.828 (98.875)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.0410 (0.0398)	Prec@1 98.438 (98.846)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.0554 (0.0400)	Prec@1 98.047 (98.834)	
Total train loss: 0.0400

Train time: 22.882736444473267
 * Prec@1 91.170 Prec@5 99.610 Loss 0.3301
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.478658437728882

Epoch: [18][38/196]	LR: 0.0001	Loss 0.0521 (0.0396)	Prec@1 97.266 (98.838)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.0373 (0.0390)	Prec@1 99.219 (98.878)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.0408 (0.0394)	Prec@1 99.219 (98.888)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.0290 (0.0398)	Prec@1 99.609 (98.878)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.0413 (0.0398)	Prec@1 98.828 (98.854)	
Total train loss: 0.0398

Train time: 22.800188302993774
 * Prec@1 91.160 Prec@5 99.580 Loss 0.3350
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.07165002822876

Epoch: [19][38/196]	LR: 0.0001	Loss 0.0502 (0.0391)	Prec@1 98.438 (98.778)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.0417 (0.0386)	Prec@1 98.047 (98.848)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.0687 (0.0401)	Prec@1 98.047 (98.835)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.0530 (0.0393)	Prec@1 98.828 (98.881)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.0428 (0.0393)	Prec@1 98.047 (98.898)	
Total train loss: 0.0394

Train time: 21.959208965301514
 * Prec@1 91.200 Prec@5 99.610 Loss 0.3350
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 26.771809816360474

Epoch: [20][38/196]	LR: 0.0001	Loss 0.0341 (0.0426)	Prec@1 98.828 (98.728)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.0329 (0.0412)	Prec@1 99.219 (98.828)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.0445 (0.0394)	Prec@1 98.438 (98.888)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.0735 (0.0395)	Prec@1 97.266 (98.888)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.0492 (0.0394)	Prec@1 98.828 (98.896)	
Total train loss: 0.0394

Train time: 21.44922399520874
 * Prec@1 91.240 Prec@5 99.630 Loss 0.3345
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 25.834906578063965

Epoch: [21][38/196]	LR: 0.0001	Loss 0.0335 (0.0417)	Prec@1 99.609 (98.738)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.0386 (0.0403)	Prec@1 99.219 (98.858)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.0557 (0.0402)	Prec@1 98.047 (98.872)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.0474 (0.0409)	Prec@1 98.438 (98.821)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.0346 (0.0402)	Prec@1 99.219 (98.848)	
Total train loss: 0.0402

Train time: 22.994534492492676
 * Prec@1 91.290 Prec@5 99.580 Loss 0.3347
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.391090154647827

Epoch: [22][38/196]	LR: 0.0001	Loss 0.0267 (0.0378)	Prec@1 99.609 (98.938)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.0578 (0.0395)	Prec@1 97.266 (98.833)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.0301 (0.0389)	Prec@1 98.828 (98.868)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.0295 (0.0395)	Prec@1 98.828 (98.846)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.0723 (0.0403)	Prec@1 98.047 (98.812)	
Total train loss: 0.0404

Train time: 22.476670503616333
 * Prec@1 91.330 Prec@5 99.630 Loss 0.3357
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 29.681649923324585

Epoch: [23][38/196]	LR: 0.0001	Loss 0.0567 (0.0391)	Prec@1 98.047 (98.768)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.0581 (0.0397)	Prec@1 98.047 (98.768)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.0229 (0.0399)	Prec@1 99.609 (98.805)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.0524 (0.0401)	Prec@1 98.438 (98.796)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.0330 (0.0397)	Prec@1 98.828 (98.806)	
Total train loss: 0.0397

Train time: 23.302619457244873
 * Prec@1 91.330 Prec@5 99.580 Loss 0.3333
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 29.144532442092896

Epoch: [24][38/196]	LR: 1e-05	Loss 0.0364 (0.0355)	Prec@1 99.609 (99.028)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.0296 (0.0384)	Prec@1 99.219 (98.858)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.0585 (0.0387)	Prec@1 98.438 (98.875)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.0534 (0.0392)	Prec@1 97.266 (98.868)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.0317 (0.0389)	Prec@1 98.828 (98.872)	
Total train loss: 0.0389

Train time: 22.852531671524048
 * Prec@1 91.290 Prec@5 99.590 Loss 0.3369
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 26.764302968978882

Epoch: [25][38/196]	LR: 1e-05	Loss 0.0336 (0.0387)	Prec@1 98.828 (98.888)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.0313 (0.0395)	Prec@1 99.609 (98.913)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.0377 (0.0394)	Prec@1 98.438 (98.868)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.0420 (0.0398)	Prec@1 99.219 (98.861)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.0287 (0.0400)	Prec@1 99.219 (98.852)	
Total train loss: 0.0400

Train time: 21.863081455230713
 * Prec@1 91.280 Prec@5 99.610 Loss 0.3347
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 25.271445989608765

Epoch: [26][38/196]	LR: 1e-05	Loss 0.0376 (0.0394)	Prec@1 98.438 (98.898)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.0450 (0.0404)	Prec@1 98.828 (98.883)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.0479 (0.0394)	Prec@1 98.828 (98.862)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.0502 (0.0395)	Prec@1 99.219 (98.868)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.0363 (0.0396)	Prec@1 99.219 (98.874)	
Total train loss: 0.0397

Train time: 21.760426998138428
 * Prec@1 91.250 Prec@5 99.580 Loss 0.3333
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 26.934671640396118

Epoch: [27][38/196]	LR: 1e-05	Loss 0.0518 (0.0414)	Prec@1 98.438 (98.798)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.0396 (0.0407)	Prec@1 98.828 (98.798)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.0280 (0.0408)	Prec@1 98.828 (98.795)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.0418 (0.0407)	Prec@1 99.219 (98.786)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.0414 (0.0401)	Prec@1 98.438 (98.802)	
Total train loss: 0.0402

Train time: 21.837409496307373
 * Prec@1 91.260 Prec@5 99.590 Loss 0.3345
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 26.641699075698853

Epoch: [28][38/196]	LR: 1e-05	Loss 0.0547 (0.0383)	Prec@1 97.656 (98.768)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.0458 (0.0404)	Prec@1 98.438 (98.748)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.0256 (0.0398)	Prec@1 99.609 (98.818)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.0297 (0.0396)	Prec@1 99.609 (98.843)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.0382 (0.0396)	Prec@1 98.828 (98.842)	
Total train loss: 0.0396

Train time: 23.068129301071167
 * Prec@1 91.260 Prec@5 99.560 Loss 0.3320
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.165405988693237

Epoch: [29][38/196]	LR: 1e-05	Loss 0.0429 (0.0415)	Prec@1 98.828 (98.718)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.0642 (0.0400)	Prec@1 98.828 (98.828)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.0676 (0.0400)	Prec@1 97.656 (98.838)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.0445 (0.0399)	Prec@1 98.828 (98.831)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.0281 (0.0404)	Prec@1 99.609 (98.812)	
Total train loss: 0.0404

Train time: 21.854092121124268
 * Prec@1 91.300 Prec@5 99.610 Loss 0.3328
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.367477655410767

Epoch: [30][38/196]	LR: 1e-05	Loss 0.0409 (0.0401)	Prec@1 99.219 (98.858)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.0496 (0.0401)	Prec@1 97.656 (98.793)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.0329 (0.0395)	Prec@1 99.219 (98.851)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.0333 (0.0398)	Prec@1 99.219 (98.856)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.0440 (0.0405)	Prec@1 98.438 (98.806)	
Total train loss: 0.0404

Train time: 22.822548151016235
 * Prec@1 91.300 Prec@5 99.620 Loss 0.3333
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.315216302871704

Epoch: [31][38/196]	LR: 1e-05	Loss 0.0275 (0.0399)	Prec@1 99.219 (98.818)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.0756 (0.0399)	Prec@1 96.875 (98.818)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.0494 (0.0397)	Prec@1 98.828 (98.805)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.0358 (0.0395)	Prec@1 100.000 (98.821)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.0363 (0.0394)	Prec@1 98.828 (98.838)	
Total train loss: 0.0394

Train time: 21.975614547729492
 * Prec@1 91.320 Prec@5 99.610 Loss 0.3340
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 26.763992309570312

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.0514 (0.0407)	Prec@1 98.438 (98.858)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.0511 (0.0403)	Prec@1 98.438 (98.848)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.0378 (0.0398)	Prec@1 98.438 (98.858)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.0247 (0.0400)	Prec@1 99.219 (98.831)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.0420 (0.0397)	Prec@1 97.266 (98.842)	
Total train loss: 0.0397

Train time: 22.880794763565063
 * Prec@1 91.260 Prec@5 99.650 Loss 0.3340
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.519392728805542

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.0471 (0.0397)	Prec@1 98.828 (98.848)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.0325 (0.0409)	Prec@1 99.609 (98.793)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.0504 (0.0402)	Prec@1 98.047 (98.858)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.0326 (0.0392)	Prec@1 98.828 (98.893)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.0365 (0.0395)	Prec@1 98.828 (98.858)	
Total train loss: 0.0395

Train time: 22.731098651885986
 * Prec@1 91.210 Prec@5 99.620 Loss 0.3340
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.897084951400757

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.0323 (0.0383)	Prec@1 99.219 (98.868)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.0361 (0.0392)	Prec@1 98.828 (98.853)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.0507 (0.0395)	Prec@1 98.828 (98.855)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.0212 (0.0386)	Prec@1 100.000 (98.898)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.0428 (0.0387)	Prec@1 97.656 (98.850)	
Total train loss: 0.0387

Train time: 23.05902600288391
 * Prec@1 91.230 Prec@5 99.620 Loss 0.3350
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.389122247695923

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.0165 (0.0424)	Prec@1 99.609 (98.758)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.0329 (0.0399)	Prec@1 99.219 (98.843)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.0304 (0.0388)	Prec@1 98.828 (98.882)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.0515 (0.0389)	Prec@1 98.438 (98.881)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.0396 (0.0392)	Prec@1 98.828 (98.882)	
Total train loss: 0.0391

Train time: 24.14497995376587
 * Prec@1 91.200 Prec@5 99.580 Loss 0.3330
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 29.071470260620117

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.0303 (0.0412)	Prec@1 99.609 (98.798)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.0246 (0.0408)	Prec@1 100.000 (98.813)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.0710 (0.0400)	Prec@1 97.266 (98.835)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.0436 (0.0399)	Prec@1 98.828 (98.821)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.0545 (0.0403)	Prec@1 98.047 (98.788)	
Total train loss: 0.0405

Train time: 22.923223972320557
 * Prec@1 91.300 Prec@5 99.610 Loss 0.3328
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.148496389389038

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.0286 (0.0378)	Prec@1 99.609 (98.938)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.0654 (0.0397)	Prec@1 98.047 (98.868)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.0514 (0.0406)	Prec@1 97.656 (98.801)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.0385 (0.0401)	Prec@1 98.828 (98.811)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.0673 (0.0397)	Prec@1 98.438 (98.838)	
Total train loss: 0.0398

Train time: 23.18075466156006
 * Prec@1 91.370 Prec@5 99.610 Loss 0.3328
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.963395595550537

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.0380 (0.0384)	Prec@1 99.219 (98.818)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.0616 (0.0399)	Prec@1 97.266 (98.808)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.0261 (0.0405)	Prec@1 99.609 (98.815)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.0469 (0.0409)	Prec@1 98.828 (98.783)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.0431 (0.0404)	Prec@1 98.047 (98.800)	
Total train loss: 0.0405

Train time: 22.597821950912476
 * Prec@1 91.330 Prec@5 99.600 Loss 0.3315
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 28.318495988845825

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.0624 (0.0393)	Prec@1 98.438 (98.848)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.0467 (0.0389)	Prec@1 98.047 (98.818)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.0331 (0.0392)	Prec@1 98.438 (98.831)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.0385 (0.0396)	Prec@1 99.219 (98.828)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.0431 (0.0394)	Prec@1 98.438 (98.840)	
Total train loss: 0.0395

Train time: 23.051199197769165
 * Prec@1 91.300 Prec@5 99.630 Loss 0.3328
Best acc: 91.400
--------------------------------------------------------------------------------
Test time: 27.158124446868896


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv6): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 88.510 Prec@5 99.490 Loss 0.4568
Pre-trained Prec@1 with 5 layers frozen: 88.50999450683594 	 Loss: 0.456787109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.0501 (0.0658)	Prec@1 98.047 (97.837)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.0402 (0.0642)	Prec@1 98.438 (97.927)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.0490 (0.0632)	Prec@1 97.656 (97.947)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.0428 (0.0624)	Prec@1 99.219 (97.967)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.0254 (0.0615)	Prec@1 99.219 (97.999)	
Total train loss: 0.0618

Train time: 44.26790404319763
 * Prec@1 90.750 Prec@5 99.640 Loss 0.3467
Best acc: 90.750
--------------------------------------------------------------------------------
Test time: 52.75325870513916

Epoch: [1][38/196]	LR: 0.01	Loss 0.0421 (0.0527)	Prec@1 99.219 (98.367)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.0850 (0.0539)	Prec@1 96.484 (98.252)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.0414 (0.0546)	Prec@1 98.047 (98.210)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.0629 (0.0552)	Prec@1 98.047 (98.185)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.0392 (0.0563)	Prec@1 99.219 (98.131)	
Total train loss: 0.0562

Train time: 34.89456629753113
 * Prec@1 90.610 Prec@5 99.610 Loss 0.3464
Best acc: 90.750
--------------------------------------------------------------------------------
Test time: 40.22984862327576

Epoch: [2][38/196]	LR: 0.01	Loss 0.0395 (0.0550)	Prec@1 98.438 (98.177)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.0900 (0.0567)	Prec@1 98.047 (98.077)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.0784 (0.0554)	Prec@1 96.484 (98.164)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.0299 (0.0557)	Prec@1 99.609 (98.167)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.0459 (0.0549)	Prec@1 97.656 (98.201)	
Total train loss: 0.0549

Train time: 24.126232862472534
 * Prec@1 90.740 Prec@5 99.610 Loss 0.3467
Best acc: 90.750
--------------------------------------------------------------------------------
Test time: 30.055203914642334

Epoch: [3][38/196]	LR: 0.01	Loss 0.0229 (0.0499)	Prec@1 99.609 (98.448)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.0989 (0.0504)	Prec@1 97.266 (98.347)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.0656 (0.0513)	Prec@1 98.438 (98.327)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.0428 (0.0514)	Prec@1 98.828 (98.310)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.0307 (0.0519)	Prec@1 99.219 (98.325)	
Total train loss: 0.0519

Train time: 11.206992864608765
 * Prec@1 90.730 Prec@5 99.610 Loss 0.3457
Best acc: 90.750
--------------------------------------------------------------------------------
Test time: 13.924149513244629

Epoch: [4][38/196]	LR: 0.01	Loss 0.0712 (0.0494)	Prec@1 96.875 (98.438)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.0545 (0.0512)	Prec@1 98.828 (98.387)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.0458 (0.0520)	Prec@1 98.438 (98.377)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.0624 (0.0512)	Prec@1 97.266 (98.392)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.0598 (0.0524)	Prec@1 97.266 (98.327)	
Total train loss: 0.0525

Train time: 11.314261198043823
 * Prec@1 90.640 Prec@5 99.630 Loss 0.3513
Best acc: 90.750
--------------------------------------------------------------------------------
Test time: 15.256664752960205

Epoch: [5][38/196]	LR: 0.01	Loss 0.0232 (0.0503)	Prec@1 99.609 (98.427)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.0855 (0.0507)	Prec@1 96.875 (98.432)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.0656 (0.0513)	Prec@1 98.438 (98.401)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.0398 (0.0511)	Prec@1 99.609 (98.402)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.0701 (0.0511)	Prec@1 98.438 (98.377)	
Total train loss: 0.0511

Train time: 16.208142280578613
 * Prec@1 90.840 Prec@5 99.610 Loss 0.3467
Best acc: 90.840
--------------------------------------------------------------------------------
Test time: 20.38538646697998

Epoch: [6][38/196]	LR: 0.01	Loss 0.0604 (0.0488)	Prec@1 97.656 (98.488)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.0255 (0.0499)	Prec@1 99.219 (98.463)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.0548 (0.0507)	Prec@1 98.438 (98.414)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.0608 (0.0504)	Prec@1 98.047 (98.427)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.0500 (0.0519)	Prec@1 98.047 (98.345)	
Total train loss: 0.0520

Train time: 14.763015747070312
 * Prec@1 90.770 Prec@5 99.620 Loss 0.3491
Best acc: 90.840
--------------------------------------------------------------------------------
Test time: 19.22707772254944

Epoch: [7][38/196]	LR: 0.01	Loss 0.0482 (0.0494)	Prec@1 98.047 (98.387)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.0482 (0.0486)	Prec@1 98.438 (98.473)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.0558 (0.0476)	Prec@1 98.438 (98.528)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.0538 (0.0486)	Prec@1 97.656 (98.480)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.0806 (0.0504)	Prec@1 97.656 (98.397)	
Total train loss: 0.0504

Train time: 14.424657821655273
 * Prec@1 90.820 Prec@5 99.660 Loss 0.3479
Best acc: 90.840
--------------------------------------------------------------------------------
Test time: 17.51880693435669

Epoch: [8][38/196]	LR: 0.001	Loss 0.0674 (0.0520)	Prec@1 97.656 (98.427)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.0334 (0.0484)	Prec@1 99.219 (98.498)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.0485 (0.0482)	Prec@1 99.219 (98.508)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.0448 (0.0496)	Prec@1 98.828 (98.405)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.0549 (0.0496)	Prec@1 98.047 (98.427)	
Total train loss: 0.0496

Train time: 15.736630916595459
 * Prec@1 90.730 Prec@5 99.590 Loss 0.3521
Best acc: 90.840
--------------------------------------------------------------------------------
Test time: 19.94255542755127

Epoch: [9][38/196]	LR: 0.001	Loss 0.0535 (0.0500)	Prec@1 98.438 (98.488)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.0303 (0.0485)	Prec@1 99.219 (98.528)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.0454 (0.0491)	Prec@1 98.828 (98.544)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.0809 (0.0491)	Prec@1 96.875 (98.500)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.0410 (0.0491)	Prec@1 98.047 (98.496)	
Total train loss: 0.0492

Train time: 17.269953966140747
 * Prec@1 90.850 Prec@5 99.600 Loss 0.3479
Best acc: 90.850
--------------------------------------------------------------------------------
Test time: 21.706238508224487

Epoch: [10][38/196]	LR: 0.001	Loss 0.0547 (0.0517)	Prec@1 98.828 (98.448)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.0487 (0.0510)	Prec@1 98.828 (98.432)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.0396 (0.0495)	Prec@1 98.828 (98.518)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.0438 (0.0499)	Prec@1 98.828 (98.490)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.0314 (0.0494)	Prec@1 100.000 (98.494)	
Total train loss: 0.0493

Train time: 17.344950199127197
 * Prec@1 90.750 Prec@5 99.590 Loss 0.3479
Best acc: 90.850
--------------------------------------------------------------------------------
Test time: 20.857094049453735

Epoch: [11][38/196]	LR: 0.001	Loss 0.0298 (0.0522)	Prec@1 99.219 (98.367)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.0505 (0.0514)	Prec@1 98.438 (98.397)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.0326 (0.0494)	Prec@1 99.609 (98.481)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.0556 (0.0492)	Prec@1 97.656 (98.450)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.0277 (0.0497)	Prec@1 99.609 (98.446)	
Total train loss: 0.0498

Train time: 16.933855533599854
 * Prec@1 90.870 Prec@5 99.650 Loss 0.3486
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 22.28119707107544

Epoch: [12][38/196]	LR: 0.001	Loss 0.0643 (0.0496)	Prec@1 97.266 (98.417)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.0243 (0.0499)	Prec@1 99.609 (98.432)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.0596 (0.0495)	Prec@1 98.047 (98.397)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.0637 (0.0499)	Prec@1 98.047 (98.385)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.0481 (0.0498)	Prec@1 98.828 (98.411)	
Total train loss: 0.0498

Train time: 19.9466769695282
 * Prec@1 90.810 Prec@5 99.640 Loss 0.3479
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 25.65406346321106

Epoch: [13][38/196]	LR: 0.001	Loss 0.0277 (0.0483)	Prec@1 99.219 (98.578)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.0414 (0.0493)	Prec@1 98.438 (98.478)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.0467 (0.0492)	Prec@1 99.219 (98.488)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.0550 (0.0491)	Prec@1 98.828 (98.480)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.0416 (0.0493)	Prec@1 99.609 (98.484)	
Total train loss: 0.0493

Train time: 21.98732566833496
 * Prec@1 90.820 Prec@5 99.630 Loss 0.3459
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 27.220317602157593

Epoch: [14][38/196]	LR: 0.001	Loss 0.0249 (0.0484)	Prec@1 99.609 (98.397)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.0676 (0.0492)	Prec@1 96.875 (98.397)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.0495 (0.0486)	Prec@1 98.828 (98.431)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.0563 (0.0488)	Prec@1 98.438 (98.453)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.0612 (0.0488)	Prec@1 97.656 (98.460)	
Total train loss: 0.0488

Train time: 18.82050371170044
 * Prec@1 90.840 Prec@5 99.650 Loss 0.3479
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 24.24360942840576

Epoch: [15][38/196]	LR: 0.001	Loss 0.0366 (0.0562)	Prec@1 98.828 (98.197)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.0511 (0.0510)	Prec@1 97.656 (98.422)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.0683 (0.0511)	Prec@1 96.875 (98.401)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.0781 (0.0508)	Prec@1 96.484 (98.375)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.0621 (0.0501)	Prec@1 97.266 (98.381)	
Total train loss: 0.0501

Train time: 20.21598172187805
 * Prec@1 90.780 Prec@5 99.620 Loss 0.3491
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 26.02585196495056

Epoch: [16][38/196]	LR: 0.0001	Loss 0.0404 (0.0527)	Prec@1 98.438 (98.357)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.0767 (0.0503)	Prec@1 98.047 (98.407)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.0548 (0.0507)	Prec@1 98.047 (98.387)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.0453 (0.0503)	Prec@1 98.438 (98.395)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.0311 (0.0495)	Prec@1 99.609 (98.444)	
Total train loss: 0.0495

Train time: 20.701865673065186
 * Prec@1 90.830 Prec@5 99.610 Loss 0.3467
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 25.951924562454224

Epoch: [17][38/196]	LR: 0.0001	Loss 0.0544 (0.0465)	Prec@1 98.047 (98.678)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.0439 (0.0466)	Prec@1 98.047 (98.593)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.0475 (0.0476)	Prec@1 98.438 (98.574)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.0850 (0.0482)	Prec@1 98.047 (98.543)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.0326 (0.0489)	Prec@1 99.609 (98.508)	
Total train loss: 0.0490

Train time: 21.351020574569702
 * Prec@1 90.810 Prec@5 99.620 Loss 0.3479
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 26.310014724731445

Epoch: [18][38/196]	LR: 0.0001	Loss 0.0583 (0.0491)	Prec@1 97.656 (98.478)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.0481 (0.0492)	Prec@1 97.656 (98.453)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.0358 (0.0488)	Prec@1 98.828 (98.441)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.0247 (0.0489)	Prec@1 99.609 (98.440)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.0405 (0.0499)	Prec@1 98.828 (98.391)	
Total train loss: 0.0499

Train time: 20.80868411064148
 * Prec@1 90.820 Prec@5 99.590 Loss 0.3506
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 26.427741765975952

Epoch: [19][38/196]	LR: 0.0001	Loss 0.0277 (0.0504)	Prec@1 99.219 (98.227)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.0321 (0.0491)	Prec@1 99.609 (98.372)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.0303 (0.0488)	Prec@1 98.828 (98.387)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.0591 (0.0485)	Prec@1 98.828 (98.417)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.0404 (0.0489)	Prec@1 98.438 (98.435)	
Total train loss: 0.0489

Train time: 18.581786155700684
 * Prec@1 90.770 Prec@5 99.580 Loss 0.3508
Best acc: 90.870
--------------------------------------------------------------------------------
Test time: 23.196451902389526

Epoch: [20][38/196]	LR: 0.0001	Loss 0.0333 (0.0524)	Prec@1 99.609 (98.407)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.0594 (0.0514)	Prec@1 98.047 (98.382)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.0437 (0.0503)	Prec@1 98.438 (98.438)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.0823 (0.0490)	Prec@1 97.656 (98.468)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.0258 (0.0490)	Prec@1 99.609 (98.498)	
Total train loss: 0.0492

Train time: 19.801350593566895
 * Prec@1 90.880 Prec@5 99.590 Loss 0.3474
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.328104734420776

Epoch: [21][38/196]	LR: 0.0001	Loss 0.0457 (0.0520)	Prec@1 98.828 (98.297)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.0615 (0.0516)	Prec@1 97.266 (98.327)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.0371 (0.0508)	Prec@1 98.828 (98.371)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.0520 (0.0486)	Prec@1 98.047 (98.470)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.0382 (0.0486)	Prec@1 98.438 (98.480)	
Total train loss: 0.0487

Train time: 20.451011180877686
 * Prec@1 90.790 Prec@5 99.610 Loss 0.3464
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.986829042434692

Epoch: [22][38/196]	LR: 0.0001	Loss 0.0436 (0.0486)	Prec@1 98.047 (98.488)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.0320 (0.0485)	Prec@1 99.609 (98.513)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.1050 (0.0500)	Prec@1 97.266 (98.468)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.0334 (0.0496)	Prec@1 99.609 (98.453)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.0518 (0.0491)	Prec@1 98.438 (98.476)	
Total train loss: 0.0492

Train time: 21.748374462127686
 * Prec@1 90.840 Prec@5 99.610 Loss 0.3464
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.784230709075928

Epoch: [23][38/196]	LR: 0.0001	Loss 0.0558 (0.0477)	Prec@1 99.219 (98.468)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.0682 (0.0507)	Prec@1 97.266 (98.327)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.0452 (0.0495)	Prec@1 98.828 (98.414)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.0234 (0.0489)	Prec@1 99.609 (98.425)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.0370 (0.0493)	Prec@1 98.438 (98.407)	
Total train loss: 0.0493

Train time: 19.441174030303955
 * Prec@1 90.730 Prec@5 99.580 Loss 0.3467
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 24.185916423797607

Epoch: [24][38/196]	LR: 1e-05	Loss 0.0652 (0.0491)	Prec@1 98.047 (98.407)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.0586 (0.0483)	Prec@1 97.656 (98.503)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.0312 (0.0485)	Prec@1 99.609 (98.471)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.0455 (0.0491)	Prec@1 98.828 (98.455)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.0779 (0.0495)	Prec@1 97.656 (98.458)	
Total train loss: 0.0495

Train time: 19.501648902893066
 * Prec@1 90.780 Prec@5 99.600 Loss 0.3474
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 23.815118074417114

Epoch: [25][38/196]	LR: 1e-05	Loss 0.0650 (0.0491)	Prec@1 97.656 (98.458)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.0526 (0.0490)	Prec@1 98.828 (98.458)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.0286 (0.0487)	Prec@1 99.219 (98.498)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.0817 (0.0496)	Prec@1 98.047 (98.483)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.0363 (0.0498)	Prec@1 99.219 (98.478)	
Total train loss: 0.0499

Train time: 21.850350379943848
 * Prec@1 90.840 Prec@5 99.580 Loss 0.3472
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.390047788619995

Epoch: [26][38/196]	LR: 1e-05	Loss 0.0508 (0.0468)	Prec@1 97.656 (98.488)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.0635 (0.0469)	Prec@1 97.656 (98.543)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.0349 (0.0488)	Prec@1 98.828 (98.454)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.0295 (0.0482)	Prec@1 98.828 (98.432)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.0538 (0.0489)	Prec@1 98.047 (98.429)	
Total train loss: 0.0489

Train time: 21.35963225364685
 * Prec@1 90.780 Prec@5 99.610 Loss 0.3474
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.25110936164856

Epoch: [27][38/196]	LR: 1e-05	Loss 0.0580 (0.0501)	Prec@1 98.047 (98.488)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.0524 (0.0501)	Prec@1 98.047 (98.382)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.0287 (0.0486)	Prec@1 99.609 (98.461)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.0187 (0.0490)	Prec@1 99.609 (98.460)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.0595 (0.0495)	Prec@1 97.266 (98.440)	
Total train loss: 0.0495

Train time: 22.40692663192749
 * Prec@1 90.800 Prec@5 99.620 Loss 0.3491
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 27.12255358695984

Epoch: [28][38/196]	LR: 1e-05	Loss 0.0363 (0.0479)	Prec@1 98.438 (98.618)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.0490 (0.0486)	Prec@1 97.656 (98.538)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.0891 (0.0494)	Prec@1 96.875 (98.521)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.0360 (0.0495)	Prec@1 99.609 (98.465)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.0561 (0.0491)	Prec@1 97.266 (98.454)	
Total train loss: 0.0491

Train time: 22.60990571975708
 * Prec@1 90.760 Prec@5 99.620 Loss 0.3477
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.161616563796997

Epoch: [29][38/196]	LR: 1e-05	Loss 0.0320 (0.0501)	Prec@1 99.219 (98.488)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.0553 (0.0503)	Prec@1 98.047 (98.533)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.0503 (0.0501)	Prec@1 98.047 (98.468)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.0446 (0.0484)	Prec@1 97.656 (98.488)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.0418 (0.0491)	Prec@1 98.828 (98.435)	
Total train loss: 0.0490

Train time: 22.121381998062134
 * Prec@1 90.820 Prec@5 99.640 Loss 0.3499
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.24052929878235

Epoch: [30][38/196]	LR: 1e-05	Loss 0.0541 (0.0534)	Prec@1 98.438 (98.287)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.0419 (0.0507)	Prec@1 99.219 (98.292)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.0397 (0.0495)	Prec@1 98.828 (98.387)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.0337 (0.0488)	Prec@1 99.219 (98.445)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.0527 (0.0494)	Prec@1 98.828 (98.431)	
Total train loss: 0.0495

Train time: 22.83755660057068
 * Prec@1 90.800 Prec@5 99.620 Loss 0.3479
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 27.326839923858643

Epoch: [31][38/196]	LR: 1e-05	Loss 0.0345 (0.0458)	Prec@1 99.219 (98.548)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.0518 (0.0480)	Prec@1 98.047 (98.443)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.0339 (0.0482)	Prec@1 99.219 (98.454)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.0309 (0.0480)	Prec@1 99.219 (98.453)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.0951 (0.0489)	Prec@1 97.266 (98.431)	
Total train loss: 0.0489

Train time: 21.82070016860962
 * Prec@1 90.830 Prec@5 99.580 Loss 0.3477
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.475386142730713

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.0548 (0.0489)	Prec@1 99.219 (98.568)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.0436 (0.0502)	Prec@1 99.219 (98.468)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.0325 (0.0495)	Prec@1 99.219 (98.458)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.0388 (0.0493)	Prec@1 98.828 (98.463)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.0545 (0.0494)	Prec@1 98.828 (98.464)	
Total train loss: 0.0494

Train time: 22.615734577178955
 * Prec@1 90.790 Prec@5 99.580 Loss 0.3477
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.193622827529907

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.0710 (0.0464)	Prec@1 97.266 (98.658)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.0391 (0.0480)	Prec@1 98.828 (98.548)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.0441 (0.0480)	Prec@1 98.438 (98.484)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.0487 (0.0486)	Prec@1 98.047 (98.450)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.0526 (0.0486)	Prec@1 97.656 (98.458)	
Total train loss: 0.0486

Train time: 22.185054540634155
 * Prec@1 90.860 Prec@5 99.640 Loss 0.3501
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.655311346054077

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.0493 (0.0514)	Prec@1 98.828 (98.458)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.0273 (0.0500)	Prec@1 99.609 (98.453)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.0466 (0.0495)	Prec@1 98.828 (98.454)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.0593 (0.0494)	Prec@1 98.438 (98.450)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.0726 (0.0492)	Prec@1 97.656 (98.464)	
Total train loss: 0.0493

Train time: 23.45631217956543
 * Prec@1 90.820 Prec@5 99.620 Loss 0.3469
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 27.43142580986023

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.0500 (0.0469)	Prec@1 98.047 (98.598)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.0580 (0.0486)	Prec@1 98.047 (98.458)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.0438 (0.0484)	Prec@1 98.828 (98.478)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.0322 (0.0484)	Prec@1 98.828 (98.475)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.0612 (0.0490)	Prec@1 97.656 (98.460)	
Total train loss: 0.0490

Train time: 21.767690181732178
 * Prec@1 90.780 Prec@5 99.650 Loss 0.3467
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.14835810661316

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.0377 (0.0477)	Prec@1 98.828 (98.518)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.0468 (0.0472)	Prec@1 98.047 (98.518)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.0643 (0.0489)	Prec@1 98.828 (98.458)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.0302 (0.0499)	Prec@1 99.219 (98.440)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.0330 (0.0495)	Prec@1 98.828 (98.466)	
Total train loss: 0.0495

Train time: 20.450774669647217
 * Prec@1 90.840 Prec@5 99.650 Loss 0.3479
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 26.26728868484497

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.0287 (0.0496)	Prec@1 99.219 (98.518)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.0379 (0.0493)	Prec@1 98.047 (98.538)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.0332 (0.0490)	Prec@1 99.219 (98.504)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.0735 (0.0493)	Prec@1 96.484 (98.440)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.0382 (0.0494)	Prec@1 99.219 (98.456)	
Total train loss: 0.0495

Train time: 21.537805318832397
 * Prec@1 90.790 Prec@5 99.620 Loss 0.3484
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 25.115675449371338

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.0739 (0.0462)	Prec@1 97.266 (98.508)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.0298 (0.0475)	Prec@1 99.609 (98.498)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.0367 (0.0482)	Prec@1 98.828 (98.441)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.1104 (0.0487)	Prec@1 96.484 (98.430)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.0465 (0.0495)	Prec@1 98.438 (98.425)	
Total train loss: 0.0496

Train time: 22.63185954093933
 * Prec@1 90.840 Prec@5 99.610 Loss 0.3494
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 27.111238479614258

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.0474 (0.0487)	Prec@1 98.438 (98.618)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.0438 (0.0501)	Prec@1 99.219 (98.513)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.0577 (0.0491)	Prec@1 98.047 (98.548)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.0442 (0.0492)	Prec@1 98.047 (98.525)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.0453 (0.0489)	Prec@1 98.828 (98.506)	
Total train loss: 0.0489

Train time: 21.72690439224243
 * Prec@1 90.800 Prec@5 99.590 Loss 0.3489
Best acc: 90.880
--------------------------------------------------------------------------------
Test time: 27.077736139297485


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv8): QConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 79.200 Prec@5 98.700 Loss 0.9790
Pre-trained Prec@1 with 7 layers frozen: 79.19999694824219 	 Loss: 0.97900390625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.0960 (0.1072)	Prec@1 96.094 (96.314)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.1396 (0.1068)	Prec@1 95.703 (96.319)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.0875 (0.1028)	Prec@1 97.266 (96.454)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.0760 (0.1012)	Prec@1 96.875 (96.512)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.0687 (0.1002)	Prec@1 98.047 (96.494)	
Total train loss: 0.1001

Train time: 51.81950354576111
 * Prec@1 90.200 Prec@5 99.470 Loss 0.3792
Best acc: 90.200
--------------------------------------------------------------------------------
Test time: 62.35791802406311

Epoch: [1][38/196]	LR: 0.01	Loss 0.0936 (0.0885)	Prec@1 96.875 (96.855)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.1405 (0.0882)	Prec@1 94.922 (96.960)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.1165 (0.0910)	Prec@1 94.531 (96.802)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.1022 (0.0899)	Prec@1 95.312 (96.842)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.0908 (0.0911)	Prec@1 96.484 (96.763)	
Total train loss: 0.0910

Train time: 43.8787407875061
 * Prec@1 90.230 Prec@5 99.460 Loss 0.3721
Best acc: 90.230
--------------------------------------------------------------------------------
Test time: 53.006316900253296

Epoch: [2][38/196]	LR: 0.01	Loss 0.1044 (0.0925)	Prec@1 96.484 (96.785)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.0572 (0.0874)	Prec@1 98.438 (96.935)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.0659 (0.0865)	Prec@1 97.656 (96.972)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.0671 (0.0869)	Prec@1 97.266 (96.955)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.0922 (0.0887)	Prec@1 96.875 (96.927)	
Total train loss: 0.0886

Train time: 21.282807111740112
 * Prec@1 90.120 Prec@5 99.490 Loss 0.3745
Best acc: 90.230
--------------------------------------------------------------------------------
Test time: 27.16229248046875

Epoch: [3][38/196]	LR: 0.01	Loss 0.1134 (0.0844)	Prec@1 97.656 (97.105)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.1538 (0.0842)	Prec@1 95.312 (97.085)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.0624 (0.0871)	Prec@1 98.047 (97.025)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.1063 (0.0877)	Prec@1 95.703 (96.973)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.0598 (0.0871)	Prec@1 97.656 (96.981)	
Total train loss: 0.0871

Train time: 19.710031747817993
 * Prec@1 90.200 Prec@5 99.480 Loss 0.3711
Best acc: 90.230
--------------------------------------------------------------------------------
Test time: 24.831018209457397

Epoch: [4][38/196]	LR: 0.01	Loss 0.1044 (0.0781)	Prec@1 95.703 (97.346)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.0651 (0.0809)	Prec@1 98.438 (97.055)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.0962 (0.0828)	Prec@1 96.094 (97.082)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.0571 (0.0834)	Prec@1 98.438 (97.103)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.1469 (0.0843)	Prec@1 94.531 (97.073)	
Total train loss: 0.0844

Train time: 20.550413370132446
 * Prec@1 90.130 Prec@5 99.500 Loss 0.3740
Best acc: 90.230
--------------------------------------------------------------------------------
Test time: 26.224937915802002

Epoch: [5][38/196]	LR: 0.01	Loss 0.0826 (0.0888)	Prec@1 96.875 (97.015)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.0692 (0.0854)	Prec@1 97.266 (97.040)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.0817 (0.0845)	Prec@1 96.094 (97.059)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.0919 (0.0848)	Prec@1 96.875 (97.013)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.0780 (0.0847)	Prec@1 95.703 (97.031)	
Total train loss: 0.0846

Train time: 21.003303289413452
 * Prec@1 90.270 Prec@5 99.450 Loss 0.3730
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 28.101674795150757

Epoch: [6][38/196]	LR: 0.01	Loss 0.0751 (0.0906)	Prec@1 97.656 (96.695)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.0870 (0.0888)	Prec@1 96.875 (96.775)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.0992 (0.0863)	Prec@1 96.484 (96.908)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.0851 (0.0855)	Prec@1 96.484 (96.968)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.0790 (0.0847)	Prec@1 97.656 (97.009)	
Total train loss: 0.0847

Train time: 21.18132185935974
 * Prec@1 90.220 Prec@5 99.450 Loss 0.3726
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 27.080817461013794

Epoch: [7][38/196]	LR: 0.01	Loss 0.0668 (0.0809)	Prec@1 97.656 (97.185)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.1038 (0.0842)	Prec@1 96.094 (97.045)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.0886 (0.0853)	Prec@1 96.875 (97.009)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.1051 (0.0856)	Prec@1 96.094 (97.008)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.1107 (0.0856)	Prec@1 96.484 (97.031)	
Total train loss: 0.0856

Train time: 19.864876985549927
 * Prec@1 90.200 Prec@5 99.530 Loss 0.3711
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 24.96972346305847

Epoch: [8][38/196]	LR: 0.001	Loss 0.0982 (0.0826)	Prec@1 96.875 (97.005)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.0917 (0.0830)	Prec@1 96.875 (97.095)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.0536 (0.0811)	Prec@1 98.828 (97.172)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.0488 (0.0812)	Prec@1 98.438 (97.128)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.0308 (0.0814)	Prec@1 99.609 (97.119)	
Total train loss: 0.0814

Train time: 20.490363359451294
 * Prec@1 90.230 Prec@5 99.490 Loss 0.3728
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 26.37641954421997

Epoch: [9][38/196]	LR: 0.001	Loss 0.1359 (0.0776)	Prec@1 95.312 (97.246)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.1259 (0.0805)	Prec@1 94.141 (97.055)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.0771 (0.0807)	Prec@1 98.047 (97.069)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.0632 (0.0804)	Prec@1 97.656 (97.120)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.0967 (0.0805)	Prec@1 96.484 (97.149)	
Total train loss: 0.0805

Train time: 14.576822996139526
 * Prec@1 90.180 Prec@5 99.500 Loss 0.3723
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 18.11388874053955

Epoch: [10][38/196]	LR: 0.001	Loss 0.0852 (0.0796)	Prec@1 96.875 (97.316)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.0745 (0.0815)	Prec@1 98.438 (97.266)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.1099 (0.0821)	Prec@1 96.484 (97.185)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.0812 (0.0815)	Prec@1 98.047 (97.251)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.1010 (0.0815)	Prec@1 95.703 (97.218)	
Total train loss: 0.0815

Train time: 11.49832534790039
 * Prec@1 90.210 Prec@5 99.510 Loss 0.3726
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 14.917176723480225

Epoch: [11][38/196]	LR: 0.001	Loss 0.0564 (0.0815)	Prec@1 98.438 (97.185)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.1028 (0.0813)	Prec@1 96.484 (97.201)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.0930 (0.0836)	Prec@1 96.875 (97.122)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.0632 (0.0826)	Prec@1 98.438 (97.145)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.0521 (0.0823)	Prec@1 98.047 (97.157)	
Total train loss: 0.0823

Train time: 11.180782079696655
 * Prec@1 90.090 Prec@5 99.500 Loss 0.3738
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 14.585424900054932

Epoch: [12][38/196]	LR: 0.001	Loss 0.1042 (0.0854)	Prec@1 96.484 (97.005)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.0761 (0.0825)	Prec@1 96.875 (97.140)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.0537 (0.0800)	Prec@1 98.047 (97.272)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.0875 (0.0794)	Prec@1 96.484 (97.281)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.0742 (0.0807)	Prec@1 98.438 (97.220)	
Total train loss: 0.0807

Train time: 12.133372783660889
 * Prec@1 90.180 Prec@5 99.480 Loss 0.3767
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 16.204411268234253

Epoch: [13][38/196]	LR: 0.001	Loss 0.0917 (0.0764)	Prec@1 97.266 (97.406)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.0952 (0.0787)	Prec@1 97.266 (97.366)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.0534 (0.0798)	Prec@1 98.828 (97.339)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.0759 (0.0808)	Prec@1 97.266 (97.286)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.0497 (0.0807)	Prec@1 97.656 (97.260)	
Total train loss: 0.0809

Train time: 14.325231313705444
 * Prec@1 90.200 Prec@5 99.500 Loss 0.3701
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 16.842793464660645

Epoch: [14][38/196]	LR: 0.001	Loss 0.0684 (0.0794)	Prec@1 97.266 (97.236)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.0602 (0.0795)	Prec@1 97.266 (97.140)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.1328 (0.0814)	Prec@1 95.312 (97.172)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.0931 (0.0815)	Prec@1 97.266 (97.188)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.0596 (0.0814)	Prec@1 98.438 (97.159)	
Total train loss: 0.0815

Train time: 16.426945447921753
 * Prec@1 90.200 Prec@5 99.480 Loss 0.3723
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 19.795433282852173

Epoch: [15][38/196]	LR: 0.001	Loss 0.0934 (0.0795)	Prec@1 98.047 (97.256)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.0724 (0.0805)	Prec@1 97.656 (97.201)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.0804 (0.0811)	Prec@1 97.266 (97.206)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.0541 (0.0809)	Prec@1 98.047 (97.196)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.0632 (0.0813)	Prec@1 97.656 (97.179)	
Total train loss: 0.0813

Train time: 17.28923535346985
 * Prec@1 90.170 Prec@5 99.480 Loss 0.3733
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 20.94736623764038

Epoch: [16][38/196]	LR: 0.0001	Loss 0.0817 (0.0785)	Prec@1 97.266 (97.246)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.1372 (0.0814)	Prec@1 95.312 (97.130)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.0824 (0.0802)	Prec@1 98.047 (97.262)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.0578 (0.0823)	Prec@1 97.656 (97.170)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.0549 (0.0809)	Prec@1 98.438 (97.210)	
Total train loss: 0.0811

Train time: 17.276734113693237
 * Prec@1 90.260 Prec@5 99.490 Loss 0.3733
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 22.84332799911499

Epoch: [17][38/196]	LR: 0.0001	Loss 0.0881 (0.0818)	Prec@1 95.703 (97.316)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.0471 (0.0789)	Prec@1 98.438 (97.316)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.0825 (0.0804)	Prec@1 96.484 (97.196)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.0715 (0.0797)	Prec@1 98.047 (97.203)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.0545 (0.0805)	Prec@1 98.438 (97.141)	
Total train loss: 0.0807

Train time: 16.982614278793335
 * Prec@1 90.260 Prec@5 99.470 Loss 0.3713
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 23.425575494766235

Epoch: [18][38/196]	LR: 0.0001	Loss 0.0636 (0.0793)	Prec@1 97.266 (97.306)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.1065 (0.0824)	Prec@1 95.703 (97.256)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.0439 (0.0801)	Prec@1 98.438 (97.292)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.0717 (0.0809)	Prec@1 97.656 (97.198)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.0675 (0.0809)	Prec@1 98.047 (97.240)	
Total train loss: 0.0809

Train time: 17.001495599746704
 * Prec@1 90.080 Prec@5 99.490 Loss 0.3745
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 21.7997727394104

Epoch: [19][38/196]	LR: 0.0001	Loss 0.1534 (0.0834)	Prec@1 94.922 (97.065)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.0652 (0.0816)	Prec@1 97.656 (97.095)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.0573 (0.0821)	Prec@1 98.047 (97.119)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.0953 (0.0828)	Prec@1 95.312 (97.123)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.0676 (0.0825)	Prec@1 96.875 (97.123)	
Total train loss: 0.0824

Train time: 18.35271954536438
 * Prec@1 90.230 Prec@5 99.460 Loss 0.3728
Best acc: 90.270
--------------------------------------------------------------------------------
Test time: 22.911651134490967

Epoch: [20][38/196]	LR: 0.0001	Loss 0.0804 (0.0826)	Prec@1 96.484 (97.145)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.0646 (0.0793)	Prec@1 97.656 (97.286)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.0786 (0.0803)	Prec@1 96.875 (97.282)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.0891 (0.0818)	Prec@1 97.266 (97.208)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.0599 (0.0814)	Prec@1 96.875 (97.202)	
Total train loss: 0.0815

Train time: 20.691651344299316
 * Prec@1 90.300 Prec@5 99.490 Loss 0.3711
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 26.526721477508545

Epoch: [21][38/196]	LR: 0.0001	Loss 0.0638 (0.0783)	Prec@1 98.047 (97.336)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.0456 (0.0792)	Prec@1 98.438 (97.286)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.0870 (0.0808)	Prec@1 96.094 (97.226)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.0380 (0.0806)	Prec@1 98.438 (97.221)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.0728 (0.0807)	Prec@1 98.828 (97.212)	
Total train loss: 0.0806

Train time: 21.761000871658325
 * Prec@1 90.120 Prec@5 99.490 Loss 0.3760
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.8187038898468

Epoch: [22][38/196]	LR: 0.0001	Loss 0.0570 (0.0818)	Prec@1 97.656 (97.105)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.0571 (0.0788)	Prec@1 98.047 (97.231)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.0745 (0.0771)	Prec@1 97.656 (97.302)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.0447 (0.0784)	Prec@1 98.828 (97.278)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.1377 (0.0800)	Prec@1 95.703 (97.242)	
Total train loss: 0.0802

Train time: 20.58228874206543
 * Prec@1 90.210 Prec@5 99.520 Loss 0.3745
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.807753086090088

Epoch: [23][38/196]	LR: 0.0001	Loss 0.0554 (0.0799)	Prec@1 98.828 (97.416)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.0695 (0.0807)	Prec@1 97.266 (97.271)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.0801 (0.0812)	Prec@1 96.484 (97.182)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.0957 (0.0816)	Prec@1 96.875 (97.183)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.1131 (0.0813)	Prec@1 96.484 (97.183)	
Total train loss: 0.0811

Train time: 21.267273902893066
 * Prec@1 90.260 Prec@5 99.480 Loss 0.3706
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 24.83791446685791

Epoch: [24][38/196]	LR: 1e-05	Loss 0.0839 (0.0791)	Prec@1 97.266 (97.326)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.0395 (0.0774)	Prec@1 98.828 (97.276)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.0893 (0.0806)	Prec@1 97.656 (97.172)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.1306 (0.0811)	Prec@1 96.484 (97.178)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.1476 (0.0819)	Prec@1 95.312 (97.151)	
Total train loss: 0.0818

Train time: 21.425191402435303
 * Prec@1 90.120 Prec@5 99.490 Loss 0.3723
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.232483625411987

Epoch: [25][38/196]	LR: 1e-05	Loss 0.0728 (0.0835)	Prec@1 96.875 (97.185)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.0813 (0.0789)	Prec@1 96.484 (97.341)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.0703 (0.0802)	Prec@1 97.266 (97.282)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.0746 (0.0799)	Prec@1 97.656 (97.306)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.1036 (0.0808)	Prec@1 94.922 (97.254)	
Total train loss: 0.0808

Train time: 21.079482555389404
 * Prec@1 90.290 Prec@5 99.490 Loss 0.3721
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 26.924705028533936

Epoch: [26][38/196]	LR: 1e-05	Loss 0.0825 (0.0784)	Prec@1 97.266 (97.226)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.1174 (0.0779)	Prec@1 94.531 (97.251)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.1046 (0.0788)	Prec@1 96.094 (97.229)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.0575 (0.0795)	Prec@1 99.219 (97.191)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.1285 (0.0806)	Prec@1 96.094 (97.143)	
Total train loss: 0.0806

Train time: 23.020759105682373
 * Prec@1 90.170 Prec@5 99.490 Loss 0.3745
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 26.212901830673218

Epoch: [27][38/196]	LR: 1e-05	Loss 0.0919 (0.0778)	Prec@1 97.656 (97.396)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.0724 (0.0800)	Prec@1 97.656 (97.261)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.0556 (0.0793)	Prec@1 98.047 (97.286)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.0679 (0.0824)	Prec@1 97.656 (97.170)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.0602 (0.0814)	Prec@1 97.656 (97.190)	
Total train loss: 0.0814

Train time: 21.167797088623047
 * Prec@1 90.230 Prec@5 99.480 Loss 0.3718
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.230757474899292

Epoch: [28][38/196]	LR: 1e-05	Loss 0.0548 (0.0758)	Prec@1 98.047 (97.406)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.0739 (0.0749)	Prec@1 98.047 (97.506)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.1085 (0.0815)	Prec@1 97.266 (97.232)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.0710 (0.0813)	Prec@1 97.656 (97.213)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.0707 (0.0814)	Prec@1 97.266 (97.236)	
Total train loss: 0.0814

Train time: 20.567732095718384
 * Prec@1 90.150 Prec@5 99.470 Loss 0.3735
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 29.547502040863037

Epoch: [29][38/196]	LR: 1e-05	Loss 0.1113 (0.0854)	Prec@1 95.703 (97.125)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.0732 (0.0850)	Prec@1 96.875 (97.125)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.0729 (0.0824)	Prec@1 97.656 (97.202)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.0844 (0.0813)	Prec@1 96.875 (97.208)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.0821 (0.0807)	Prec@1 96.484 (97.210)	
Total train loss: 0.0807

Train time: 20.54386067390442
 * Prec@1 90.210 Prec@5 99.500 Loss 0.3704
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 24.035592317581177

Epoch: [30][38/196]	LR: 1e-05	Loss 0.1182 (0.0845)	Prec@1 96.094 (96.965)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.0699 (0.0844)	Prec@1 97.656 (97.035)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.0495 (0.0822)	Prec@1 98.828 (97.135)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.1240 (0.0807)	Prec@1 95.312 (97.160)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.1196 (0.0810)	Prec@1 96.484 (97.167)	
Total train loss: 0.0810

Train time: 20.12224268913269
 * Prec@1 90.300 Prec@5 99.480 Loss 0.3726
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 24.790358304977417

Epoch: [31][38/196]	LR: 1e-05	Loss 0.0930 (0.0786)	Prec@1 96.875 (97.336)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.0773 (0.0777)	Prec@1 97.266 (97.396)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.0989 (0.0810)	Prec@1 95.312 (97.222)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.0835 (0.0810)	Prec@1 97.266 (97.223)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.1001 (0.0815)	Prec@1 96.875 (97.228)	
Total train loss: 0.0818

Train time: 21.814833879470825
 * Prec@1 90.250 Prec@5 99.460 Loss 0.3726
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 27.317941665649414

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.0744 (0.0843)	Prec@1 96.484 (96.975)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.0702 (0.0816)	Prec@1 96.094 (97.135)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.0936 (0.0825)	Prec@1 96.875 (97.142)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.0975 (0.0817)	Prec@1 96.094 (97.153)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.1172 (0.0807)	Prec@1 95.312 (97.196)	
Total train loss: 0.0808

Train time: 20.735533237457275
 * Prec@1 90.230 Prec@5 99.500 Loss 0.3713
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.05192542076111

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.0469 (0.0817)	Prec@1 98.828 (97.296)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.0645 (0.0817)	Prec@1 97.656 (97.191)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.0886 (0.0823)	Prec@1 96.875 (97.162)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.0919 (0.0830)	Prec@1 96.094 (97.100)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.0717 (0.0821)	Prec@1 96.875 (97.151)	
Total train loss: 0.0821

Train time: 20.969326972961426
 * Prec@1 90.220 Prec@5 99.490 Loss 0.3701
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.55132794380188

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.0527 (0.0790)	Prec@1 98.438 (97.436)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.0645 (0.0787)	Prec@1 98.047 (97.371)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.0856 (0.0803)	Prec@1 96.484 (97.276)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.0897 (0.0817)	Prec@1 97.266 (97.228)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.0539 (0.0823)	Prec@1 98.438 (97.163)	
Total train loss: 0.0825

Train time: 21.178847789764404
 * Prec@1 90.230 Prec@5 99.490 Loss 0.3728
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 27.20334482192993

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.0709 (0.0806)	Prec@1 97.656 (97.296)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.0558 (0.0800)	Prec@1 98.438 (97.291)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.1329 (0.0806)	Prec@1 95.703 (97.289)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.0837 (0.0801)	Prec@1 96.875 (97.296)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.0261 (0.0809)	Prec@1 99.609 (97.248)	
Total train loss: 0.0813

Train time: 20.745576858520508
 * Prec@1 90.250 Prec@5 99.430 Loss 0.3716
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 24.410433053970337

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.1148 (0.0847)	Prec@1 96.094 (97.145)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.0801 (0.0834)	Prec@1 96.484 (97.211)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.0605 (0.0821)	Prec@1 97.266 (97.212)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.0589 (0.0803)	Prec@1 98.047 (97.253)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.0780 (0.0816)	Prec@1 98.047 (97.167)	
Total train loss: 0.0817

Train time: 20.901036500930786
 * Prec@1 90.150 Prec@5 99.490 Loss 0.3711
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 25.43770456314087

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.0570 (0.0835)	Prec@1 98.438 (97.165)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.0826 (0.0817)	Prec@1 97.266 (97.180)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.0444 (0.0807)	Prec@1 99.219 (97.189)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.0841 (0.0801)	Prec@1 96.875 (97.243)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.0384 (0.0800)	Prec@1 98.828 (97.210)	
Total train loss: 0.0803

Train time: 20.429054498672485
 * Prec@1 90.210 Prec@5 99.490 Loss 0.3745
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 26.697985887527466

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.0649 (0.0807)	Prec@1 97.656 (97.185)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.0695 (0.0781)	Prec@1 97.656 (97.221)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.0737 (0.0804)	Prec@1 96.875 (97.226)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.0780 (0.0803)	Prec@1 96.484 (97.193)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.1181 (0.0806)	Prec@1 95.703 (97.204)	
Total train loss: 0.0806

Train time: 22.216153383255005
 * Prec@1 90.110 Prec@5 99.500 Loss 0.3699
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 28.950916528701782

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.0701 (0.0884)	Prec@1 97.266 (96.955)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.0772 (0.0838)	Prec@1 96.875 (97.070)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.0757 (0.0848)	Prec@1 97.656 (97.019)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.0693 (0.0827)	Prec@1 98.047 (97.123)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.0748 (0.0806)	Prec@1 96.875 (97.206)	
Total train loss: 0.0808

Train time: 21.42004680633545
 * Prec@1 90.210 Prec@5 99.460 Loss 0.3726
Best acc: 90.300
--------------------------------------------------------------------------------
Test time: 27.205465078353882


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode: rram
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24, 32]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv10): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 39.680 Prec@5 87.880 Loss 3.0215
Pre-trained Prec@1 with 9 layers frozen: 39.68000030517578 	 Loss: 3.021484375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.01	Loss 0.3152 (0.3698)	Prec@1 91.797 (88.822)	
Epoch: [0][77/196]	LR: 0.01	Loss 0.5459 (0.3453)	Prec@1 85.156 (89.428)	
Epoch: [0][116/196]	LR: 0.01	Loss 0.1821 (0.3250)	Prec@1 94.531 (89.897)	
Epoch: [0][155/196]	LR: 0.01	Loss 0.2893 (0.3168)	Prec@1 91.016 (90.054)	
Epoch: [0][194/196]	LR: 0.01	Loss 0.1676 (0.3086)	Prec@1 95.312 (90.236)	
Total train loss: 0.3086

Train time: 33.28946352005005
 * Prec@1 87.240 Prec@5 99.270 Loss 0.4917
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 41.332043409347534

Epoch: [1][38/196]	LR: 0.01	Loss 0.2260 (0.2494)	Prec@1 92.578 (91.857)	
Epoch: [1][77/196]	LR: 0.01	Loss 0.2852 (0.2607)	Prec@1 91.406 (91.426)	
Epoch: [1][116/196]	LR: 0.01	Loss 0.2598 (0.2600)	Prec@1 91.797 (91.423)	
Epoch: [1][155/196]	LR: 0.01	Loss 0.2729 (0.2608)	Prec@1 89.844 (91.354)	
Epoch: [1][194/196]	LR: 0.01	Loss 0.2345 (0.2579)	Prec@1 89.453 (91.366)	
Total train loss: 0.2582

Train time: 53.280487060546875
 * Prec@1 87.200 Prec@5 99.370 Loss 0.4695
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 69.06178069114685

Epoch: [2][38/196]	LR: 0.01	Loss 0.3469 (0.2336)	Prec@1 90.234 (92.087)	
Epoch: [2][77/196]	LR: 0.01	Loss 0.2866 (0.2396)	Prec@1 89.062 (91.877)	
Epoch: [2][116/196]	LR: 0.01	Loss 0.2488 (0.2406)	Prec@1 92.188 (91.790)	
Epoch: [2][155/196]	LR: 0.01	Loss 0.2502 (0.2425)	Prec@1 91.406 (91.747)	
Epoch: [2][194/196]	LR: 0.01	Loss 0.2646 (0.2431)	Prec@1 90.625 (91.769)	
Total train loss: 0.2433

Train time: 52.16452693939209
 * Prec@1 87.200 Prec@5 99.350 Loss 0.4629
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 57.68758010864258

Epoch: [3][38/196]	LR: 0.01	Loss 0.2203 (0.2300)	Prec@1 92.188 (92.137)	
Epoch: [3][77/196]	LR: 0.01	Loss 0.3013 (0.2303)	Prec@1 89.844 (92.047)	
Epoch: [3][116/196]	LR: 0.01	Loss 0.1862 (0.2342)	Prec@1 94.141 (91.927)	
Epoch: [3][155/196]	LR: 0.01	Loss 0.1890 (0.2355)	Prec@1 91.406 (91.857)	
Epoch: [3][194/196]	LR: 0.01	Loss 0.1796 (0.2363)	Prec@1 92.578 (91.845)	
Total train loss: 0.2364

Train time: 19.155024766921997
 * Prec@1 86.790 Prec@5 99.370 Loss 0.4707
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 24.46589469909668

Epoch: [4][38/196]	LR: 0.01	Loss 0.2008 (0.2271)	Prec@1 93.359 (92.308)	
Epoch: [4][77/196]	LR: 0.01	Loss 0.2289 (0.2299)	Prec@1 92.578 (92.167)	
Epoch: [4][116/196]	LR: 0.01	Loss 0.1788 (0.2300)	Prec@1 92.578 (92.097)	
Epoch: [4][155/196]	LR: 0.01	Loss 0.2256 (0.2327)	Prec@1 91.016 (91.975)	
Epoch: [4][194/196]	LR: 0.01	Loss 0.2416 (0.2342)	Prec@1 92.969 (91.955)	
Total train loss: 0.2344

Train time: 18.5491783618927
 * Prec@1 87.180 Prec@5 99.430 Loss 0.4568
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 22.54844903945923

Epoch: [5][38/196]	LR: 0.01	Loss 0.1833 (0.2300)	Prec@1 93.750 (91.757)	
Epoch: [5][77/196]	LR: 0.01	Loss 0.2368 (0.2329)	Prec@1 92.578 (91.807)	
Epoch: [5][116/196]	LR: 0.01	Loss 0.2402 (0.2307)	Prec@1 94.141 (91.854)	
Epoch: [5][155/196]	LR: 0.01	Loss 0.1498 (0.2321)	Prec@1 93.750 (91.854)	
Epoch: [5][194/196]	LR: 0.01	Loss 0.2209 (0.2328)	Prec@1 91.797 (91.873)	
Total train loss: 0.2329

Train time: 19.667101860046387
 * Prec@1 86.910 Prec@5 99.380 Loss 0.4661
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 24.837286949157715

Epoch: [6][38/196]	LR: 0.01	Loss 0.2046 (0.2280)	Prec@1 92.578 (92.208)	
Epoch: [6][77/196]	LR: 0.01	Loss 0.2593 (0.2280)	Prec@1 90.234 (92.188)	
Epoch: [6][116/196]	LR: 0.01	Loss 0.2856 (0.2296)	Prec@1 90.234 (92.127)	
Epoch: [6][155/196]	LR: 0.01	Loss 0.2469 (0.2327)	Prec@1 92.188 (92.037)	
Epoch: [6][194/196]	LR: 0.01	Loss 0.2319 (0.2335)	Prec@1 92.188 (91.955)	
Total train loss: 0.2334

Train time: 17.960689783096313
 * Prec@1 87.300 Prec@5 99.340 Loss 0.4529
Best acc: 87.300
--------------------------------------------------------------------------------
Test time: 24.65825605392456

Epoch: [7][38/196]	LR: 0.01	Loss 0.2695 (0.2368)	Prec@1 91.016 (91.797)	
Epoch: [7][77/196]	LR: 0.01	Loss 0.2367 (0.2344)	Prec@1 91.016 (91.862)	
Epoch: [7][116/196]	LR: 0.01	Loss 0.2137 (0.2342)	Prec@1 91.797 (91.860)	
Epoch: [7][155/196]	LR: 0.01	Loss 0.3062 (0.2317)	Prec@1 89.844 (91.930)	
Epoch: [7][194/196]	LR: 0.01	Loss 0.1583 (0.2324)	Prec@1 93.750 (91.873)	
Total train loss: 0.2323

Train time: 19.896266222000122
 * Prec@1 87.330 Prec@5 99.470 Loss 0.4480
Best acc: 87.330
--------------------------------------------------------------------------------
Test time: 24.236521005630493

Epoch: [8][38/196]	LR: 0.001	Loss 0.3018 (0.2287)	Prec@1 90.234 (91.857)	
Epoch: [8][77/196]	LR: 0.001	Loss 0.2013 (0.2206)	Prec@1 93.359 (92.218)	
Epoch: [8][116/196]	LR: 0.001	Loss 0.2148 (0.2221)	Prec@1 91.797 (92.321)	
Epoch: [8][155/196]	LR: 0.001	Loss 0.2769 (0.2246)	Prec@1 90.234 (92.210)	
Epoch: [8][194/196]	LR: 0.001	Loss 0.2150 (0.2260)	Prec@1 92.578 (92.173)	
Total train loss: 0.2260

Train time: 18.09411358833313
 * Prec@1 87.310 Prec@5 99.410 Loss 0.4500
Best acc: 87.330
--------------------------------------------------------------------------------
Test time: 23.953438758850098

Epoch: [9][38/196]	LR: 0.001	Loss 0.2230 (0.2235)	Prec@1 92.188 (92.127)	
Epoch: [9][77/196]	LR: 0.001	Loss 0.2054 (0.2281)	Prec@1 93.359 (92.027)	
Epoch: [9][116/196]	LR: 0.001	Loss 0.2786 (0.2296)	Prec@1 91.016 (92.034)	
Epoch: [9][155/196]	LR: 0.001	Loss 0.1786 (0.2295)	Prec@1 93.359 (92.045)	
Epoch: [9][194/196]	LR: 0.001	Loss 0.1904 (0.2276)	Prec@1 93.359 (92.067)	
Total train loss: 0.2274

Train time: 19.173853635787964
 * Prec@1 87.390 Prec@5 99.450 Loss 0.4487
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.166704893112183

Epoch: [10][38/196]	LR: 0.001	Loss 0.2161 (0.2277)	Prec@1 92.969 (91.977)	
Epoch: [10][77/196]	LR: 0.001	Loss 0.3325 (0.2224)	Prec@1 89.453 (92.198)	
Epoch: [10][116/196]	LR: 0.001	Loss 0.2610 (0.2251)	Prec@1 92.578 (92.094)	
Epoch: [10][155/196]	LR: 0.001	Loss 0.2754 (0.2258)	Prec@1 90.234 (92.125)	
Epoch: [10][194/196]	LR: 0.001	Loss 0.2084 (0.2259)	Prec@1 92.969 (92.105)	
Total train loss: 0.2257

Train time: 19.329304218292236
 * Prec@1 87.260 Prec@5 99.410 Loss 0.4512
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.13778591156006

Epoch: [11][38/196]	LR: 0.001	Loss 0.1719 (0.2240)	Prec@1 93.359 (92.358)	
Epoch: [11][77/196]	LR: 0.001	Loss 0.2588 (0.2242)	Prec@1 91.797 (92.303)	
Epoch: [11][116/196]	LR: 0.001	Loss 0.2218 (0.2262)	Prec@1 93.750 (92.241)	
Epoch: [11][155/196]	LR: 0.001	Loss 0.1952 (0.2268)	Prec@1 92.578 (92.188)	
Epoch: [11][194/196]	LR: 0.001	Loss 0.2144 (0.2267)	Prec@1 92.969 (92.135)	
Total train loss: 0.2266

Train time: 20.972604513168335
 * Prec@1 87.170 Prec@5 99.450 Loss 0.4485
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 25.804504871368408

Epoch: [12][38/196]	LR: 0.001	Loss 0.2944 (0.2260)	Prec@1 89.453 (92.067)	
Epoch: [12][77/196]	LR: 0.001	Loss 0.2450 (0.2252)	Prec@1 91.406 (92.102)	
Epoch: [12][116/196]	LR: 0.001	Loss 0.2361 (0.2260)	Prec@1 90.625 (92.124)	
Epoch: [12][155/196]	LR: 0.001	Loss 0.1991 (0.2249)	Prec@1 91.797 (92.142)	
Epoch: [12][194/196]	LR: 0.001	Loss 0.2534 (0.2252)	Prec@1 90.625 (92.155)	
Total train loss: 0.2251

Train time: 19.468773126602173
 * Prec@1 87.170 Prec@5 99.430 Loss 0.4500
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.891554594039917

Epoch: [13][38/196]	LR: 0.001	Loss 0.2546 (0.2249)	Prec@1 91.797 (92.167)	
Epoch: [13][77/196]	LR: 0.001	Loss 0.3264 (0.2234)	Prec@1 88.281 (92.132)	
Epoch: [13][116/196]	LR: 0.001	Loss 0.3040 (0.2263)	Prec@1 88.672 (92.104)	
Epoch: [13][155/196]	LR: 0.001	Loss 0.2573 (0.2252)	Prec@1 91.016 (92.122)	
Epoch: [13][194/196]	LR: 0.001	Loss 0.1256 (0.2265)	Prec@1 95.703 (92.121)	
Total train loss: 0.2264

Train time: 20.3631112575531
 * Prec@1 87.290 Prec@5 99.420 Loss 0.4524
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.461570024490356

Epoch: [14][38/196]	LR: 0.001	Loss 0.1808 (0.2202)	Prec@1 93.750 (92.258)	
Epoch: [14][77/196]	LR: 0.001	Loss 0.2098 (0.2274)	Prec@1 93.750 (92.157)	
Epoch: [14][116/196]	LR: 0.001	Loss 0.2322 (0.2291)	Prec@1 93.750 (92.101)	
Epoch: [14][155/196]	LR: 0.001	Loss 0.2803 (0.2275)	Prec@1 91.016 (92.142)	
Epoch: [14][194/196]	LR: 0.001	Loss 0.2235 (0.2272)	Prec@1 91.016 (92.083)	
Total train loss: 0.2271

Train time: 19.900938749313354
 * Prec@1 87.290 Prec@5 99.430 Loss 0.4519
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.226781368255615

Epoch: [15][38/196]	LR: 0.001	Loss 0.2130 (0.2223)	Prec@1 91.406 (92.238)	
Epoch: [15][77/196]	LR: 0.001	Loss 0.2129 (0.2250)	Prec@1 92.969 (92.303)	
Epoch: [15][116/196]	LR: 0.001	Loss 0.2087 (0.2260)	Prec@1 93.750 (92.254)	
Epoch: [15][155/196]	LR: 0.001	Loss 0.2754 (0.2254)	Prec@1 91.016 (92.208)	
Epoch: [15][194/196]	LR: 0.001	Loss 0.2253 (0.2251)	Prec@1 89.453 (92.173)	
Total train loss: 0.2249

Train time: 19.171522617340088
 * Prec@1 87.260 Prec@5 99.410 Loss 0.4517
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.486574172973633

Epoch: [16][38/196]	LR: 0.0001	Loss 0.2257 (0.2183)	Prec@1 91.797 (92.198)	
Epoch: [16][77/196]	LR: 0.0001	Loss 0.2769 (0.2213)	Prec@1 91.406 (92.208)	
Epoch: [16][116/196]	LR: 0.0001	Loss 0.2661 (0.2240)	Prec@1 90.625 (92.091)	
Epoch: [16][155/196]	LR: 0.0001	Loss 0.2754 (0.2270)	Prec@1 92.969 (92.027)	
Epoch: [16][194/196]	LR: 0.0001	Loss 0.2783 (0.2276)	Prec@1 90.625 (92.061)	
Total train loss: 0.2275

Train time: 19.70098376274109
 * Prec@1 87.250 Prec@5 99.420 Loss 0.4500
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.310070991516113

Epoch: [17][38/196]	LR: 0.0001	Loss 0.2144 (0.2207)	Prec@1 91.406 (92.408)	
Epoch: [17][77/196]	LR: 0.0001	Loss 0.2072 (0.2203)	Prec@1 94.141 (92.383)	
Epoch: [17][116/196]	LR: 0.0001	Loss 0.2084 (0.2225)	Prec@1 92.578 (92.254)	
Epoch: [17][155/196]	LR: 0.0001	Loss 0.2238 (0.2236)	Prec@1 92.578 (92.233)	
Epoch: [17][194/196]	LR: 0.0001	Loss 0.1754 (0.2246)	Prec@1 94.141 (92.204)	
Total train loss: 0.2249

Train time: 16.41965413093567
 * Prec@1 87.270 Prec@5 99.420 Loss 0.4480
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 19.42029595375061

Epoch: [18][38/196]	LR: 0.0001	Loss 0.2236 (0.2295)	Prec@1 94.141 (91.887)	
Epoch: [18][77/196]	LR: 0.0001	Loss 0.2356 (0.2225)	Prec@1 91.016 (92.213)	
Epoch: [18][116/196]	LR: 0.0001	Loss 0.2402 (0.2243)	Prec@1 92.188 (92.171)	
Epoch: [18][155/196]	LR: 0.0001	Loss 0.2593 (0.2259)	Prec@1 91.016 (92.127)	
Epoch: [18][194/196]	LR: 0.0001	Loss 0.2959 (0.2269)	Prec@1 91.406 (92.053)	
Total train loss: 0.2268

Train time: 10.087660074234009
 * Prec@1 87.330 Prec@5 99.410 Loss 0.4485
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 12.883642196655273

Epoch: [19][38/196]	LR: 0.0001	Loss 0.2690 (0.2234)	Prec@1 92.969 (92.208)	
Epoch: [19][77/196]	LR: 0.0001	Loss 0.2230 (0.2293)	Prec@1 91.016 (91.982)	
Epoch: [19][116/196]	LR: 0.0001	Loss 0.3032 (0.2284)	Prec@1 89.453 (92.061)	
Epoch: [19][155/196]	LR: 0.0001	Loss 0.1765 (0.2294)	Prec@1 93.750 (92.005)	
Epoch: [19][194/196]	LR: 0.0001	Loss 0.1797 (0.2282)	Prec@1 93.359 (92.065)	
Total train loss: 0.2281

Train time: 12.792092084884644
 * Prec@1 87.240 Prec@5 99.390 Loss 0.4485
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 15.938596487045288

Epoch: [20][38/196]	LR: 0.0001	Loss 0.2546 (0.2238)	Prec@1 91.016 (92.167)	
Epoch: [20][77/196]	LR: 0.0001	Loss 0.1882 (0.2222)	Prec@1 93.359 (92.223)	
Epoch: [20][116/196]	LR: 0.0001	Loss 0.1548 (0.2254)	Prec@1 94.922 (92.111)	
Epoch: [20][155/196]	LR: 0.0001	Loss 0.3467 (0.2264)	Prec@1 89.453 (91.970)	
Epoch: [20][194/196]	LR: 0.0001	Loss 0.1763 (0.2257)	Prec@1 94.141 (91.995)	
Total train loss: 0.2257

Train time: 11.99456000328064
 * Prec@1 87.190 Prec@5 99.460 Loss 0.4512
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 14.640204668045044

Epoch: [21][38/196]	LR: 0.0001	Loss 0.2229 (0.2227)	Prec@1 92.188 (92.208)	
Epoch: [21][77/196]	LR: 0.0001	Loss 0.2391 (0.2212)	Prec@1 92.188 (92.383)	
Epoch: [21][116/196]	LR: 0.0001	Loss 0.2084 (0.2254)	Prec@1 93.750 (92.248)	
Epoch: [21][155/196]	LR: 0.0001	Loss 0.2103 (0.2242)	Prec@1 92.578 (92.243)	
Epoch: [21][194/196]	LR: 0.0001	Loss 0.1764 (0.2255)	Prec@1 94.141 (92.183)	
Total train loss: 0.2259

Train time: 11.046813488006592
 * Prec@1 87.220 Prec@5 99.450 Loss 0.4487
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 13.5003080368042

Epoch: [22][38/196]	LR: 0.0001	Loss 0.2417 (0.2192)	Prec@1 91.406 (92.368)	
Epoch: [22][77/196]	LR: 0.0001	Loss 0.2046 (0.2232)	Prec@1 92.188 (92.182)	
Epoch: [22][116/196]	LR: 0.0001	Loss 0.2255 (0.2247)	Prec@1 92.578 (92.174)	
Epoch: [22][155/196]	LR: 0.0001	Loss 0.1882 (0.2264)	Prec@1 93.359 (92.200)	
Epoch: [22][194/196]	LR: 0.0001	Loss 0.2937 (0.2265)	Prec@1 90.625 (92.165)	
Total train loss: 0.2264

Train time: 14.033539533615112
 * Prec@1 87.260 Prec@5 99.440 Loss 0.4500
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 18.21548581123352

Epoch: [23][38/196]	LR: 0.0001	Loss 0.2556 (0.2194)	Prec@1 90.234 (92.298)	
Epoch: [23][77/196]	LR: 0.0001	Loss 0.2104 (0.2250)	Prec@1 93.750 (92.067)	
Epoch: [23][116/196]	LR: 0.0001	Loss 0.2491 (0.2257)	Prec@1 92.578 (92.094)	
Epoch: [23][155/196]	LR: 0.0001	Loss 0.1829 (0.2272)	Prec@1 93.359 (92.075)	
Epoch: [23][194/196]	LR: 0.0001	Loss 0.3042 (0.2274)	Prec@1 90.625 (92.003)	
Total train loss: 0.2273

Train time: 14.660534858703613
 * Prec@1 87.230 Prec@5 99.440 Loss 0.4497
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 19.34494686126709

Epoch: [24][38/196]	LR: 1e-05	Loss 0.1581 (0.2230)	Prec@1 94.141 (92.167)	
Epoch: [24][77/196]	LR: 1e-05	Loss 0.2197 (0.2219)	Prec@1 94.141 (92.188)	
Epoch: [24][116/196]	LR: 1e-05	Loss 0.2266 (0.2223)	Prec@1 93.359 (92.268)	
Epoch: [24][155/196]	LR: 1e-05	Loss 0.2788 (0.2261)	Prec@1 86.719 (92.122)	
Epoch: [24][194/196]	LR: 1e-05	Loss 0.2717 (0.2261)	Prec@1 91.797 (92.185)	
Total train loss: 0.2263

Train time: 14.10797905921936
 * Prec@1 87.280 Prec@5 99.410 Loss 0.4529
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 17.156667470932007

Epoch: [25][38/196]	LR: 1e-05	Loss 0.2778 (0.2266)	Prec@1 91.797 (92.007)	
Epoch: [25][77/196]	LR: 1e-05	Loss 0.2054 (0.2243)	Prec@1 93.359 (92.238)	
Epoch: [25][116/196]	LR: 1e-05	Loss 0.1902 (0.2268)	Prec@1 92.188 (92.171)	
Epoch: [25][155/196]	LR: 1e-05	Loss 0.2004 (0.2242)	Prec@1 92.969 (92.210)	
Epoch: [25][194/196]	LR: 1e-05	Loss 0.2468 (0.2241)	Prec@1 90.625 (92.175)	
Total train loss: 0.2241

Train time: 13.84594202041626
 * Prec@1 87.270 Prec@5 99.430 Loss 0.4517
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 17.124879360198975

Epoch: [26][38/196]	LR: 1e-05	Loss 0.2217 (0.2375)	Prec@1 92.969 (91.907)	
Epoch: [26][77/196]	LR: 1e-05	Loss 0.1393 (0.2303)	Prec@1 94.531 (91.957)	
Epoch: [26][116/196]	LR: 1e-05	Loss 0.1946 (0.2244)	Prec@1 92.578 (92.174)	
Epoch: [26][155/196]	LR: 1e-05	Loss 0.1749 (0.2256)	Prec@1 94.141 (92.140)	
Epoch: [26][194/196]	LR: 1e-05	Loss 0.1614 (0.2263)	Prec@1 94.141 (92.125)	
Total train loss: 0.2264

Train time: 14.443870544433594
 * Prec@1 87.340 Prec@5 99.430 Loss 0.4485
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 19.218300819396973

Epoch: [27][38/196]	LR: 1e-05	Loss 0.2448 (0.2270)	Prec@1 90.234 (92.218)	
Epoch: [27][77/196]	LR: 1e-05	Loss 0.2539 (0.2266)	Prec@1 91.016 (92.248)	
Epoch: [27][116/196]	LR: 1e-05	Loss 0.1899 (0.2266)	Prec@1 92.188 (92.234)	
Epoch: [27][155/196]	LR: 1e-05	Loss 0.2749 (0.2286)	Prec@1 90.625 (92.130)	
Epoch: [27][194/196]	LR: 1e-05	Loss 0.1860 (0.2264)	Prec@1 92.578 (92.171)	
Total train loss: 0.2267

Train time: 18.088874578475952
 * Prec@1 87.220 Prec@5 99.420 Loss 0.4500
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 22.022242546081543

Epoch: [28][38/196]	LR: 1e-05	Loss 0.2246 (0.2324)	Prec@1 91.016 (92.047)	
Epoch: [28][77/196]	LR: 1e-05	Loss 0.1975 (0.2316)	Prec@1 92.578 (91.957)	
Epoch: [28][116/196]	LR: 1e-05	Loss 0.1439 (0.2276)	Prec@1 93.359 (92.017)	
Epoch: [28][155/196]	LR: 1e-05	Loss 0.2488 (0.2285)	Prec@1 91.406 (92.027)	
Epoch: [28][194/196]	LR: 1e-05	Loss 0.2296 (0.2262)	Prec@1 92.969 (92.079)	
Total train loss: 0.2263

Train time: 18.934572219848633
 * Prec@1 87.310 Prec@5 99.400 Loss 0.4536
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 22.29807448387146

Epoch: [29][38/196]	LR: 1e-05	Loss 0.2683 (0.2315)	Prec@1 89.453 (91.777)	
Epoch: [29][77/196]	LR: 1e-05	Loss 0.1692 (0.2315)	Prec@1 91.406 (91.667)	
Epoch: [29][116/196]	LR: 1e-05	Loss 0.2411 (0.2293)	Prec@1 91.016 (91.850)	
Epoch: [29][155/196]	LR: 1e-05	Loss 0.1587 (0.2279)	Prec@1 95.703 (92.005)	
Epoch: [29][194/196]	LR: 1e-05	Loss 0.1610 (0.2258)	Prec@1 95.312 (92.143)	
Total train loss: 0.2257

Train time: 19.401678562164307
 * Prec@1 87.310 Prec@5 99.430 Loss 0.4480
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.600111961364746

Epoch: [30][38/196]	LR: 1e-05	Loss 0.2494 (0.2263)	Prec@1 92.578 (91.927)	
Epoch: [30][77/196]	LR: 1e-05	Loss 0.2664 (0.2246)	Prec@1 89.844 (92.127)	
Epoch: [30][116/196]	LR: 1e-05	Loss 0.2957 (0.2272)	Prec@1 90.234 (92.097)	
Epoch: [30][155/196]	LR: 1e-05	Loss 0.2179 (0.2256)	Prec@1 92.578 (92.125)	
Epoch: [30][194/196]	LR: 1e-05	Loss 0.2028 (0.2247)	Prec@1 91.797 (92.175)	
Total train loss: 0.2248

Train time: 18.07448387145996
 * Prec@1 87.240 Prec@5 99.420 Loss 0.4507
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.971891403198242

Epoch: [31][38/196]	LR: 1e-05	Loss 0.2008 (0.2384)	Prec@1 93.359 (91.847)	
Epoch: [31][77/196]	LR: 1e-05	Loss 0.2644 (0.2339)	Prec@1 90.625 (91.927)	
Epoch: [31][116/196]	LR: 1e-05	Loss 0.2266 (0.2289)	Prec@1 92.188 (92.071)	
Epoch: [31][155/196]	LR: 1e-05	Loss 0.1866 (0.2272)	Prec@1 92.578 (92.117)	
Epoch: [31][194/196]	LR: 1e-05	Loss 0.3010 (0.2256)	Prec@1 90.234 (92.165)	
Total train loss: 0.2259

Train time: 20.24561333656311
 * Prec@1 87.180 Prec@5 99.440 Loss 0.4517
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.831807374954224

Epoch: [32][38/196]	LR: 1.0000000000000002e-06	Loss 0.2073 (0.2469)	Prec@1 92.188 (91.396)	
Epoch: [32][77/196]	LR: 1.0000000000000002e-06	Loss 0.2122 (0.2288)	Prec@1 93.359 (92.022)	
Epoch: [32][116/196]	LR: 1.0000000000000002e-06	Loss 0.1819 (0.2302)	Prec@1 92.969 (92.054)	
Epoch: [32][155/196]	LR: 1.0000000000000002e-06	Loss 0.1870 (0.2270)	Prec@1 92.969 (92.130)	
Epoch: [32][194/196]	LR: 1.0000000000000002e-06	Loss 0.2086 (0.2255)	Prec@1 91.406 (92.175)	
Total train loss: 0.2254

Train time: 20.923826456069946
 * Prec@1 87.180 Prec@5 99.440 Loss 0.4475
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 25.262649536132812

Epoch: [33][38/196]	LR: 1.0000000000000002e-06	Loss 0.1747 (0.2244)	Prec@1 93.359 (92.278)	
Epoch: [33][77/196]	LR: 1.0000000000000002e-06	Loss 0.2209 (0.2220)	Prec@1 91.406 (92.228)	
Epoch: [33][116/196]	LR: 1.0000000000000002e-06	Loss 0.2156 (0.2255)	Prec@1 92.578 (92.131)	
Epoch: [33][155/196]	LR: 1.0000000000000002e-06	Loss 0.1980 (0.2242)	Prec@1 91.797 (92.172)	
Epoch: [33][194/196]	LR: 1.0000000000000002e-06	Loss 0.2469 (0.2268)	Prec@1 92.578 (92.067)	
Total train loss: 0.2266

Train time: 20.12466263771057
 * Prec@1 87.180 Prec@5 99.420 Loss 0.4497
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 27.150100469589233

Epoch: [34][38/196]	LR: 1.0000000000000002e-06	Loss 0.1976 (0.2225)	Prec@1 93.359 (92.368)	
Epoch: [34][77/196]	LR: 1.0000000000000002e-06	Loss 0.1669 (0.2190)	Prec@1 94.531 (92.493)	
Epoch: [34][116/196]	LR: 1.0000000000000002e-06	Loss 0.1648 (0.2206)	Prec@1 92.969 (92.331)	
Epoch: [34][155/196]	LR: 1.0000000000000002e-06	Loss 0.2482 (0.2244)	Prec@1 91.797 (92.205)	
Epoch: [34][194/196]	LR: 1.0000000000000002e-06	Loss 0.3093 (0.2236)	Prec@1 88.281 (92.179)	
Total train loss: 0.2239

Train time: 19.080421924591064
 * Prec@1 87.220 Prec@5 99.430 Loss 0.4497
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.915952444076538

Epoch: [35][38/196]	LR: 1.0000000000000002e-06	Loss 0.2498 (0.2239)	Prec@1 91.016 (92.268)	
Epoch: [35][77/196]	LR: 1.0000000000000002e-06	Loss 0.2313 (0.2222)	Prec@1 91.406 (92.283)	
Epoch: [35][116/196]	LR: 1.0000000000000002e-06	Loss 0.3020 (0.2242)	Prec@1 89.844 (92.194)	
Epoch: [35][155/196]	LR: 1.0000000000000002e-06	Loss 0.1703 (0.2226)	Prec@1 92.969 (92.275)	
Epoch: [35][194/196]	LR: 1.0000000000000002e-06	Loss 0.2002 (0.2272)	Prec@1 91.797 (92.117)	
Total train loss: 0.2273

Train time: 19.032859563827515
 * Prec@1 87.230 Prec@5 99.460 Loss 0.4460
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.91223430633545

Epoch: [36][38/196]	LR: 1.0000000000000002e-06	Loss 0.1823 (0.2281)	Prec@1 92.969 (92.157)	
Epoch: [36][77/196]	LR: 1.0000000000000002e-06	Loss 0.2700 (0.2261)	Prec@1 91.406 (92.052)	
Epoch: [36][116/196]	LR: 1.0000000000000002e-06	Loss 0.2700 (0.2248)	Prec@1 90.625 (92.061)	
Epoch: [36][155/196]	LR: 1.0000000000000002e-06	Loss 0.2432 (0.2262)	Prec@1 92.578 (92.047)	
Epoch: [36][194/196]	LR: 1.0000000000000002e-06	Loss 0.2786 (0.2261)	Prec@1 92.969 (92.099)	
Total train loss: 0.2260

Train time: 18.973531007766724
 * Prec@1 87.320 Prec@5 99.430 Loss 0.4524
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.680476665496826

Epoch: [37][38/196]	LR: 1.0000000000000002e-06	Loss 0.2866 (0.2191)	Prec@1 91.016 (92.388)	
Epoch: [37][77/196]	LR: 1.0000000000000002e-06	Loss 0.3054 (0.2206)	Prec@1 91.406 (92.433)	
Epoch: [37][116/196]	LR: 1.0000000000000002e-06	Loss 0.3115 (0.2232)	Prec@1 89.062 (92.341)	
Epoch: [37][155/196]	LR: 1.0000000000000002e-06	Loss 0.1819 (0.2245)	Prec@1 92.188 (92.275)	
Epoch: [37][194/196]	LR: 1.0000000000000002e-06	Loss 0.1553 (0.2267)	Prec@1 94.141 (92.159)	
Total train loss: 0.2267

Train time: 20.46935200691223
 * Prec@1 87.360 Prec@5 99.430 Loss 0.4524
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.969234228134155

Epoch: [38][38/196]	LR: 1.0000000000000002e-06	Loss 0.2471 (0.2394)	Prec@1 91.016 (91.687)	
Epoch: [38][77/196]	LR: 1.0000000000000002e-06	Loss 0.2439 (0.2306)	Prec@1 92.188 (91.917)	
Epoch: [38][116/196]	LR: 1.0000000000000002e-06	Loss 0.2593 (0.2296)	Prec@1 90.625 (91.964)	
Epoch: [38][155/196]	LR: 1.0000000000000002e-06	Loss 0.2303 (0.2274)	Prec@1 91.016 (92.005)	
Epoch: [38][194/196]	LR: 1.0000000000000002e-06	Loss 0.1445 (0.2255)	Prec@1 95.312 (92.075)	
Total train loss: 0.2254

Train time: 19.959226608276367
 * Prec@1 87.290 Prec@5 99.420 Loss 0.4497
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 24.949839115142822

Epoch: [39][38/196]	LR: 1.0000000000000002e-06	Loss 0.1946 (0.2206)	Prec@1 92.969 (92.538)	
Epoch: [39][77/196]	LR: 1.0000000000000002e-06	Loss 0.1848 (0.2220)	Prec@1 92.578 (92.333)	
Epoch: [39][116/196]	LR: 1.0000000000000002e-06	Loss 0.2903 (0.2235)	Prec@1 91.797 (92.228)	
Epoch: [39][155/196]	LR: 1.0000000000000002e-06	Loss 0.2615 (0.2250)	Prec@1 91.016 (92.182)	
Epoch: [39][194/196]	LR: 1.0000000000000002e-06	Loss 0.2242 (0.2263)	Prec@1 92.188 (92.163)	
Total train loss: 0.2262

Train time: 18.870893001556396
 * Prec@1 87.350 Prec@5 99.440 Loss 0.4504
Best acc: 87.390
--------------------------------------------------------------------------------
Test time: 23.204421997070312

