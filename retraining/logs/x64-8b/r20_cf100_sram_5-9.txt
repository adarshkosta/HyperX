
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 68.380 Prec@5 89.980 Loss 1.2217
Pre-trained Prec@1 with 5 layers frozen: 68.37999725341797 	 Loss: 1.2216796875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6348 (1.3381)	Prec@1 57.812 (62.680)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.6436 (1.4713)	Prec@1 55.469 (58.899)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.1924 (1.4779)	Prec@1 67.188 (58.804)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.4600 (1.4548)	Prec@1 59.375 (59.403)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.3252 (1.4351)	Prec@1 61.719 (59.906)	
Total train loss: 1.4347

Train time: 197.3390634059906
 * Prec@1 43.440 Prec@5 72.850 Loss 2.4980
Best acc: 43.440
--------------------------------------------------------------------------------
Test time: 202.5656669139862

Epoch: [1][38/196]	LR: 0.1	Loss 1.1855 (1.1203)	Prec@1 67.578 (68.299)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.2451 (1.1406)	Prec@1 62.891 (67.543)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1377 (1.1537)	Prec@1 66.797 (67.111)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2646 (1.1648)	Prec@1 67.969 (66.744)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2549 (1.1686)	Prec@1 64.453 (66.556)	
Total train loss: 1.1691

Train time: 23.19238519668579
 * Prec@1 51.650 Prec@5 80.580 Loss 1.9746
Best acc: 51.650
--------------------------------------------------------------------------------
Test time: 27.783862113952637

Epoch: [2][38/196]	LR: 0.1	Loss 0.8774 (0.9507)	Prec@1 74.609 (72.336)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0518 (0.9829)	Prec@1 67.578 (71.324)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9854 (0.9941)	Prec@1 72.656 (71.014)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0459 (1.0065)	Prec@1 71.094 (70.648)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.0840 (1.0208)	Prec@1 67.969 (70.274)	
Total train loss: 1.0213

Train time: 22.66780138015747
 * Prec@1 51.290 Prec@5 80.000 Loss 1.9805
Best acc: 51.650
--------------------------------------------------------------------------------
Test time: 27.536593914031982

Epoch: [3][38/196]	LR: 0.1	Loss 0.8960 (0.8471)	Prec@1 73.438 (75.280)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.7676 (0.8661)	Prec@1 77.344 (74.594)	
Epoch: [3][116/196]	LR: 0.1	Loss 1.0498 (0.8797)	Prec@1 67.578 (74.075)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.1084 (0.9043)	Prec@1 67.188 (73.312)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9175 (0.9173)	Prec@1 71.484 (73.041)	
Total train loss: 0.9177

Train time: 21.72080659866333
 * Prec@1 56.970 Prec@5 84.610 Loss 1.6592
Best acc: 56.970
--------------------------------------------------------------------------------
Test time: 26.677456378936768

Epoch: [4][38/196]	LR: 0.1	Loss 0.7451 (0.7670)	Prec@1 78.516 (77.284)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8052 (0.7750)	Prec@1 76.953 (77.073)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7051 (0.7956)	Prec@1 81.641 (76.449)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.7847 (0.8136)	Prec@1 78.516 (75.889)	
Epoch: [4][194/196]	LR: 0.1	Loss 1.0039 (0.8314)	Prec@1 74.219 (75.387)	
Total train loss: 0.8320

Train time: 21.212441205978394
 * Prec@1 54.920 Prec@5 83.030 Loss 1.8779
Best acc: 56.970
--------------------------------------------------------------------------------
Test time: 26.257553577423096

Epoch: [5][38/196]	LR: 0.1	Loss 0.6724 (0.6966)	Prec@1 77.734 (79.157)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6953 (0.7062)	Prec@1 78.516 (78.926)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.8750 (0.7266)	Prec@1 78.125 (78.332)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.8613 (0.7423)	Prec@1 73.438 (77.807)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.7998 (0.7602)	Prec@1 76.172 (77.296)	
Total train loss: 0.7605

Train time: 21.838152170181274
 * Prec@1 54.860 Prec@5 82.810 Loss 1.9395
Best acc: 56.970
--------------------------------------------------------------------------------
Test time: 25.971628189086914

Epoch: [6][38/196]	LR: 0.1	Loss 0.6240 (0.6054)	Prec@1 81.641 (82.292)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.8140 (0.6245)	Prec@1 75.000 (81.385)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6846 (0.6370)	Prec@1 78.906 (80.966)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6411 (0.6620)	Prec@1 79.297 (80.138)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.6938 (0.6864)	Prec@1 79.297 (79.313)	
Total train loss: 0.6870

Train time: 20.31255793571472
 * Prec@1 55.490 Prec@5 82.400 Loss 1.9043
Best acc: 56.970
--------------------------------------------------------------------------------
Test time: 25.140302896499634

Epoch: [7][38/196]	LR: 0.1	Loss 0.5840 (0.5671)	Prec@1 80.469 (83.013)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5317 (0.5731)	Prec@1 84.766 (82.682)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6626 (0.5924)	Prec@1 79.297 (81.985)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.7778 (0.6168)	Prec@1 75.781 (81.167)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.7129 (0.6358)	Prec@1 80.469 (80.585)	
Total train loss: 0.6359

Train time: 21.03899621963501
 * Prec@1 57.040 Prec@5 83.280 Loss 1.8096
Best acc: 57.040
--------------------------------------------------------------------------------
Test time: 25.07535195350647

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3711 (0.4410)	Prec@1 89.453 (87.069)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3513 (0.4037)	Prec@1 89.844 (88.517)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3584 (0.3889)	Prec@1 88.672 (89.036)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3489 (0.3738)	Prec@1 89.062 (89.623)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2600 (0.3624)	Prec@1 93.750 (90.016)	
Total train loss: 0.3623

Train time: 21.219056844711304
 * Prec@1 64.650 Prec@5 88.230 Loss 1.4092
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.130319595336914

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2708 (0.2696)	Prec@1 94.141 (93.510)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.1816 (0.2658)	Prec@1 96.484 (93.685)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2837 (0.2666)	Prec@1 92.969 (93.583)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2874 (0.2654)	Prec@1 91.016 (93.547)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2354 (0.2642)	Prec@1 95.312 (93.588)	
Total train loss: 0.2643

Train time: 20.009018898010254
 * Prec@1 64.550 Prec@5 88.060 Loss 1.4307
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 24.74367642402649

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2100 (0.2187)	Prec@1 94.922 (95.423)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2047 (0.2234)	Prec@1 94.922 (95.167)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.3142 (0.2277)	Prec@1 92.969 (95.035)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2537 (0.2259)	Prec@1 93.750 (95.075)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2556 (0.2263)	Prec@1 93.359 (94.996)	
Total train loss: 0.2267

Train time: 21.933385372161865
 * Prec@1 64.320 Prec@5 87.900 Loss 1.4668
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.910573720932007

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2003 (0.1981)	Prec@1 96.094 (96.314)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1707 (0.1996)	Prec@1 97.656 (96.169)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2087 (0.1999)	Prec@1 94.922 (96.100)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2350 (0.2016)	Prec@1 93.750 (95.966)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1940 (0.2021)	Prec@1 95.703 (95.976)	
Total train loss: 0.2024

Train time: 20.736807107925415
 * Prec@1 64.230 Prec@5 87.850 Loss 1.4756
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 24.81501603126526

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1924 (0.1754)	Prec@1 96.875 (97.025)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1960 (0.1806)	Prec@1 98.047 (96.830)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2125 (0.1790)	Prec@1 95.703 (96.882)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1543 (0.1795)	Prec@1 98.047 (96.820)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1689 (0.1820)	Prec@1 98.047 (96.707)	
Total train loss: 0.1822

Train time: 20.460870504379272
 * Prec@1 64.310 Prec@5 87.600 Loss 1.4844
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.919615745544434

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1459 (0.1652)	Prec@1 98.047 (97.105)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1576 (0.1653)	Prec@1 97.266 (97.150)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1348 (0.1652)	Prec@1 97.656 (97.162)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.2042 (0.1663)	Prec@1 96.094 (97.140)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1808 (0.1692)	Prec@1 96.875 (97.095)	
Total train loss: 0.1696

Train time: 22.009220838546753
 * Prec@1 64.120 Prec@5 87.550 Loss 1.5068
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.0640652179718

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1455 (0.1552)	Prec@1 98.047 (97.746)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1688 (0.1563)	Prec@1 96.875 (97.691)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1692 (0.1578)	Prec@1 97.266 (97.616)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1641 (0.1605)	Prec@1 98.047 (97.498)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1910 (0.1603)	Prec@1 96.484 (97.510)	
Total train loss: 0.1605

Train time: 20.373410940170288
 * Prec@1 63.780 Prec@5 87.230 Loss 1.5254
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 24.955909490585327

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1528 (0.1445)	Prec@1 98.047 (97.937)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1785 (0.1466)	Prec@1 96.484 (98.002)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1442 (0.1477)	Prec@1 98.828 (97.950)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1428 (0.1471)	Prec@1 98.047 (97.954)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1348 (0.1481)	Prec@1 98.438 (97.911)	
Total train loss: 0.1482

Train time: 21.326017141342163
 * Prec@1 63.770 Prec@5 87.050 Loss 1.5312
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.806057453155518

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1398 (0.1370)	Prec@1 97.656 (98.247)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1232 (0.1345)	Prec@1 98.828 (98.357)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1177 (0.1346)	Prec@1 100.000 (98.424)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1498 (0.1335)	Prec@1 98.438 (98.458)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1266 (0.1337)	Prec@1 98.438 (98.425)	
Total train loss: 0.1340

Train time: 21.959430694580078
 * Prec@1 63.830 Prec@5 87.040 Loss 1.5361
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.795149087905884

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1587 (0.1308)	Prec@1 98.047 (98.488)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1163 (0.1316)	Prec@1 99.609 (98.628)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1515 (0.1324)	Prec@1 97.656 (98.581)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1188 (0.1326)	Prec@1 99.219 (98.568)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1229 (0.1331)	Prec@1 98.828 (98.512)	
Total train loss: 0.1334

Train time: 21.708834409713745
 * Prec@1 64.150 Prec@5 87.110 Loss 1.5244
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.929091215133667

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0988 (0.1260)	Prec@1 98.438 (98.658)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1116 (0.1297)	Prec@1 98.438 (98.593)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1676 (0.1317)	Prec@1 98.047 (98.538)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1364 (0.1322)	Prec@1 98.438 (98.505)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1114 (0.1324)	Prec@1 99.219 (98.472)	
Total train loss: 0.1326

Train time: 22.003618001937866
 * Prec@1 63.870 Prec@5 87.020 Loss 1.5391
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 27.41464877128601

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1111 (0.1301)	Prec@1 98.828 (98.568)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1052 (0.1295)	Prec@1 99.219 (98.618)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1439 (0.1302)	Prec@1 98.047 (98.608)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1283 (0.1314)	Prec@1 98.438 (98.588)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1096 (0.1312)	Prec@1 99.609 (98.586)	
Total train loss: 0.1315

Train time: 22.356818675994873
 * Prec@1 63.870 Prec@5 87.180 Loss 1.5342
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.547135829925537

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1478 (0.1311)	Prec@1 97.266 (98.478)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1434 (0.1313)	Prec@1 98.047 (98.508)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1469 (0.1310)	Prec@1 98.047 (98.531)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1127 (0.1321)	Prec@1 98.047 (98.465)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1301 (0.1320)	Prec@1 98.828 (98.482)	
Total train loss: 0.1322

Train time: 20.63061237335205
 * Prec@1 63.880 Prec@5 87.300 Loss 1.5273
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.173887252807617

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1401 (0.1378)	Prec@1 98.828 (98.187)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1061 (0.1334)	Prec@1 99.609 (98.377)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1432 (0.1323)	Prec@1 98.438 (98.401)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1351 (0.1329)	Prec@1 97.266 (98.430)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1648 (0.1325)	Prec@1 96.875 (98.448)	
Total train loss: 0.1326

Train time: 21.062596559524536
 * Prec@1 63.860 Prec@5 87.120 Loss 1.5254
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.65598773956299

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1161 (0.1275)	Prec@1 99.219 (98.708)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1181 (0.1307)	Prec@1 98.828 (98.538)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1285 (0.1293)	Prec@1 98.438 (98.591)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1357 (0.1306)	Prec@1 98.438 (98.555)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1108 (0.1303)	Prec@1 100.000 (98.562)	
Total train loss: 0.1304

Train time: 21.12661576271057
 * Prec@1 63.850 Prec@5 87.160 Loss 1.5332
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.153663158416748

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1321 (0.1310)	Prec@1 98.828 (98.528)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1046 (0.1302)	Prec@1 99.219 (98.573)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1320 (0.1309)	Prec@1 98.047 (98.571)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1478 (0.1312)	Prec@1 98.047 (98.560)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1489 (0.1310)	Prec@1 98.047 (98.558)	
Total train loss: 0.1312

Train time: 22.649192333221436
 * Prec@1 63.950 Prec@5 87.050 Loss 1.5371
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.77044439315796

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1315 (0.1321)	Prec@1 98.828 (98.518)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1128 (0.1326)	Prec@1 99.609 (98.473)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1274 (0.1331)	Prec@1 98.047 (98.454)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1010 (0.1321)	Prec@1 98.438 (98.483)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1387 (0.1311)	Prec@1 98.828 (98.520)	
Total train loss: 0.1314

Train time: 21.434155225753784
 * Prec@1 63.800 Prec@5 86.950 Loss 1.5420
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.609591960906982

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1272 (0.1287)	Prec@1 98.438 (98.558)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1390 (0.1304)	Prec@1 98.047 (98.498)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1477 (0.1307)	Prec@1 97.266 (98.491)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1069 (0.1297)	Prec@1 99.219 (98.543)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1390 (0.1303)	Prec@1 99.219 (98.518)	
Total train loss: 0.1305

Train time: 21.860729455947876
 * Prec@1 63.900 Prec@5 87.050 Loss 1.5332
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.961264610290527

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1400 (0.1308)	Prec@1 96.484 (98.448)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1465 (0.1296)	Prec@1 97.266 (98.488)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1083 (0.1307)	Prec@1 99.609 (98.508)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1477 (0.1301)	Prec@1 98.047 (98.558)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1604 (0.1302)	Prec@1 97.656 (98.566)	
Total train loss: 0.1305

Train time: 21.24267601966858
 * Prec@1 64.020 Prec@5 87.050 Loss 1.5361
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 26.293198347091675

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1326 (0.1311)	Prec@1 98.047 (98.618)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1227 (0.1312)	Prec@1 98.047 (98.483)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1193 (0.1310)	Prec@1 98.438 (98.474)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1273 (0.1311)	Prec@1 99.219 (98.493)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1159 (0.1303)	Prec@1 99.219 (98.536)	
Total train loss: 0.1304

Train time: 22.603156328201294
 * Prec@1 64.090 Prec@5 86.920 Loss 1.5264
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 27.300793886184692

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1257 (0.1255)	Prec@1 98.438 (98.728)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1163 (0.1277)	Prec@1 99.219 (98.633)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1006 (0.1290)	Prec@1 100.000 (98.584)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1526 (0.1292)	Prec@1 98.047 (98.620)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1256 (0.1301)	Prec@1 98.438 (98.606)	
Total train loss: 0.1302

Train time: 21.009380340576172
 * Prec@1 64.150 Prec@5 86.970 Loss 1.5332
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 25.766164541244507

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1205 (0.1279)	Prec@1 99.609 (98.668)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1236 (0.1304)	Prec@1 100.000 (98.588)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1307 (0.1291)	Prec@1 98.047 (98.578)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1133 (0.1292)	Prec@1 98.828 (98.575)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1086 (0.1299)	Prec@1 98.828 (98.542)	
Total train loss: 0.1301

Train time: 25.234723329544067
 * Prec@1 63.970 Prec@5 87.180 Loss 1.5273
Best acc: 64.650
--------------------------------------------------------------------------------
Test time: 38.20943474769592


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 67.360 Prec@5 89.480 Loss 1.2588
Pre-trained Prec@1 with 7 layers frozen: 67.36000061035156 	 Loss: 1.2587890625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.5771 (1.2925)	Prec@1 56.641 (63.942)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.4600 (1.4273)	Prec@1 60.156 (60.452)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.6104 (1.4408)	Prec@1 55.469 (59.836)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.3740 (1.4223)	Prec@1 60.938 (60.179)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.4473 (1.4082)	Prec@1 58.594 (60.485)	
Total train loss: 1.4081

Train time: 169.11051845550537
 * Prec@1 48.200 Prec@5 78.200 Loss 2.0840
Best acc: 48.200
--------------------------------------------------------------------------------
Test time: 175.12662601470947

Epoch: [1][38/196]	LR: 0.1	Loss 1.1084 (1.1319)	Prec@1 69.141 (67.668)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.2344 (1.1283)	Prec@1 62.891 (67.528)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.3818 (1.1542)	Prec@1 60.156 (66.713)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1846 (1.1579)	Prec@1 70.703 (66.709)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2119 (1.1587)	Prec@1 68.750 (66.677)	
Total train loss: 1.1589

Train time: 22.188588619232178
 * Prec@1 55.700 Prec@5 83.050 Loss 1.7822
Best acc: 55.700
--------------------------------------------------------------------------------
Test time: 26.630244731903076

Epoch: [2][38/196]	LR: 0.1	Loss 1.0518 (0.9724)	Prec@1 68.750 (71.945)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0732 (0.9819)	Prec@1 69.141 (71.444)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9526 (0.9963)	Prec@1 71.484 (71.047)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.9878 (1.0046)	Prec@1 69.922 (70.881)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.1260 (1.0128)	Prec@1 68.750 (70.551)	
Total train loss: 1.0129

Train time: 21.81039023399353
 * Prec@1 57.500 Prec@5 85.050 Loss 1.5996
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 26.371068954467773

Epoch: [3][38/196]	LR: 0.1	Loss 0.8477 (0.8303)	Prec@1 75.000 (75.861)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8740 (0.8503)	Prec@1 72.656 (74.715)	
Epoch: [3][116/196]	LR: 0.1	Loss 1.0410 (0.8659)	Prec@1 69.531 (74.329)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9023 (0.8781)	Prec@1 74.219 (73.843)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9839 (0.8943)	Prec@1 71.484 (73.371)	
Total train loss: 0.8944

Train time: 20.699249505996704
 * Prec@1 55.530 Prec@5 83.470 Loss 1.8027
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 25.363070249557495

Epoch: [4][38/196]	LR: 0.1	Loss 0.6587 (0.7471)	Prec@1 80.859 (78.025)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7153 (0.7694)	Prec@1 77.734 (77.344)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8545 (0.7864)	Prec@1 73.047 (76.686)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8550 (0.8128)	Prec@1 75.391 (75.974)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.9023 (0.8255)	Prec@1 75.000 (75.567)	
Total train loss: 0.8259

Train time: 21.51584267616272
 * Prec@1 57.310 Prec@5 84.860 Loss 1.6973
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 26.520873546600342

Epoch: [5][38/196]	LR: 0.1	Loss 0.6089 (0.6689)	Prec@1 83.203 (80.278)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6621 (0.6863)	Prec@1 82.031 (79.577)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7744 (0.7037)	Prec@1 78.125 (78.920)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7461 (0.7191)	Prec@1 78.906 (78.428)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.9556 (0.7396)	Prec@1 68.359 (77.712)	
Total train loss: 0.7399

Train time: 21.867709636688232
 * Prec@1 57.220 Prec@5 84.960 Loss 1.7266
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 25.836496829986572

Epoch: [6][38/196]	LR: 0.1	Loss 0.5044 (0.6179)	Prec@1 84.766 (81.450)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5059 (0.6188)	Prec@1 84.766 (81.365)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6719 (0.6375)	Prec@1 79.297 (80.706)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7827 (0.6579)	Prec@1 76.562 (80.006)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.6523 (0.6757)	Prec@1 80.469 (79.419)	
Total train loss: 0.6760

Train time: 19.852285861968994
 * Prec@1 53.580 Prec@5 82.020 Loss 1.9893
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 24.953125

Epoch: [7][38/196]	LR: 0.1	Loss 0.6875 (0.5626)	Prec@1 79.297 (82.772)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.6416 (0.5738)	Prec@1 79.688 (82.417)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6313 (0.5910)	Prec@1 80.859 (81.988)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6943 (0.6020)	Prec@1 76.172 (81.668)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.7148 (0.6195)	Prec@1 76.953 (81.112)	
Total train loss: 0.6200

Train time: 20.76002264022827
 * Prec@1 55.650 Prec@5 83.140 Loss 1.8877
Best acc: 57.500
--------------------------------------------------------------------------------
Test time: 24.604453086853027

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.4375 (0.4385)	Prec@1 85.156 (87.159)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3235 (0.3959)	Prec@1 91.406 (88.847)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2590 (0.3755)	Prec@1 93.750 (89.640)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3210 (0.3639)	Prec@1 92.578 (89.979)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2920 (0.3552)	Prec@1 92.188 (90.236)	
Total train loss: 0.3552

Train time: 19.327234268188477
 * Prec@1 65.140 Prec@5 88.460 Loss 1.3965
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 24.251437664031982

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2815 (0.2535)	Prec@1 92.969 (93.970)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2344 (0.2538)	Prec@1 94.141 (94.010)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2424 (0.2552)	Prec@1 92.969 (93.990)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2676 (0.2553)	Prec@1 92.969 (93.958)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2040 (0.2528)	Prec@1 95.312 (93.952)	
Total train loss: 0.2529

Train time: 19.3437442779541
 * Prec@1 64.690 Prec@5 88.460 Loss 1.4229
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 24.021684885025024

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1819 (0.2157)	Prec@1 98.047 (95.383)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2334 (0.2157)	Prec@1 95.312 (95.393)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1663 (0.2177)	Prec@1 95.312 (95.329)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2382 (0.2172)	Prec@1 96.484 (95.378)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2299 (0.2167)	Prec@1 94.922 (95.391)	
Total train loss: 0.2169

Train time: 20.871155977249146
 * Prec@1 64.630 Prec@5 88.090 Loss 1.4443
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.747840881347656

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1967 (0.1980)	Prec@1 97.266 (96.154)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1880 (0.1921)	Prec@1 97.656 (96.384)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1857 (0.1947)	Prec@1 96.094 (96.241)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1743 (0.1925)	Prec@1 97.266 (96.289)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1703 (0.1949)	Prec@1 96.484 (96.222)	
Total train loss: 0.1950

Train time: 20.966347217559814
 * Prec@1 64.610 Prec@5 88.040 Loss 1.4600
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 24.76926612854004

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1494 (0.1728)	Prec@1 97.656 (96.865)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1479 (0.1707)	Prec@1 97.656 (97.020)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1663 (0.1728)	Prec@1 97.656 (97.045)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1549 (0.1746)	Prec@1 97.266 (96.945)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1528 (0.1769)	Prec@1 97.656 (96.843)	
Total train loss: 0.1771

Train time: 20.95448398590088
 * Prec@1 64.220 Prec@5 87.970 Loss 1.4844
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.962947607040405

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1761 (0.1588)	Prec@1 96.875 (97.436)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1523 (0.1572)	Prec@1 97.656 (97.531)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1805 (0.1606)	Prec@1 96.875 (97.459)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1550 (0.1621)	Prec@1 97.656 (97.393)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1272 (0.1624)	Prec@1 98.438 (97.306)	
Total train loss: 0.1626

Train time: 19.938812494277954
 * Prec@1 64.140 Prec@5 87.700 Loss 1.4971
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 23.762208700180054

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1528 (0.1457)	Prec@1 96.094 (97.887)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1555 (0.1496)	Prec@1 98.047 (97.811)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1428 (0.1517)	Prec@1 98.438 (97.723)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1530 (0.1515)	Prec@1 96.484 (97.761)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1338 (0.1517)	Prec@1 96.875 (97.720)	
Total train loss: 0.1519

Train time: 20.40068006515503
 * Prec@1 63.810 Prec@5 87.680 Loss 1.5078
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.187140464782715

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1398 (0.1371)	Prec@1 98.438 (98.377)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1801 (0.1377)	Prec@1 97.266 (98.367)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1469 (0.1393)	Prec@1 97.656 (98.291)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1707 (0.1398)	Prec@1 97.266 (98.200)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1405 (0.1410)	Prec@1 98.047 (98.121)	
Total train loss: 0.1411

Train time: 20.713658571243286
 * Prec@1 63.860 Prec@5 87.640 Loss 1.5186
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 24.974759817123413

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1221 (0.1277)	Prec@1 99.219 (98.548)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1456 (0.1298)	Prec@1 96.484 (98.523)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1296 (0.1293)	Prec@1 99.219 (98.524)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1240 (0.1293)	Prec@1 98.828 (98.580)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1223 (0.1279)	Prec@1 98.438 (98.602)	
Total train loss: 0.1282

Train time: 24.86165428161621
 * Prec@1 63.680 Prec@5 87.670 Loss 1.5225
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 29.978180408477783

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1047 (0.1268)	Prec@1 99.609 (98.448)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1443 (0.1298)	Prec@1 98.047 (98.407)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1263 (0.1297)	Prec@1 99.609 (98.458)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1003 (0.1287)	Prec@1 99.609 (98.495)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1054 (0.1295)	Prec@1 99.219 (98.504)	
Total train loss: 0.1297

Train time: 24.847960233688354
 * Prec@1 63.470 Prec@5 87.510 Loss 1.5312
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 29.071434259414673

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1396 (0.1254)	Prec@1 98.438 (98.648)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1193 (0.1267)	Prec@1 99.219 (98.643)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1534 (0.1257)	Prec@1 97.266 (98.594)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1550 (0.1272)	Prec@1 98.828 (98.590)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1186 (0.1273)	Prec@1 99.219 (98.578)	
Total train loss: 0.1274

Train time: 23.69870615005493
 * Prec@1 63.900 Prec@5 87.520 Loss 1.5205
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 29.062735319137573

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1230 (0.1248)	Prec@1 98.047 (98.758)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1309 (0.1279)	Prec@1 98.438 (98.613)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1225 (0.1290)	Prec@1 99.219 (98.601)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1044 (0.1285)	Prec@1 98.438 (98.605)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1486 (0.1279)	Prec@1 98.828 (98.614)	
Total train loss: 0.1279

Train time: 21.312913179397583
 * Prec@1 63.590 Prec@5 87.590 Loss 1.5127
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.51193070411682

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1033 (0.1211)	Prec@1 99.609 (98.768)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1211 (0.1231)	Prec@1 98.438 (98.773)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1416 (0.1254)	Prec@1 97.656 (98.755)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1373 (0.1259)	Prec@1 98.047 (98.705)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1241 (0.1260)	Prec@1 98.828 (98.672)	
Total train loss: 0.1262

Train time: 21.2507266998291
 * Prec@1 63.700 Prec@5 87.510 Loss 1.5244
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.85836172103882

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1243 (0.1277)	Prec@1 99.609 (98.498)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1445 (0.1261)	Prec@1 100.000 (98.618)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1194 (0.1271)	Prec@1 98.828 (98.624)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1322 (0.1277)	Prec@1 98.828 (98.605)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1229 (0.1272)	Prec@1 99.219 (98.628)	
Total train loss: 0.1273

Train time: 21.464857578277588
 * Prec@1 63.770 Prec@5 87.520 Loss 1.5244
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 26.322354555130005

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1093 (0.1248)	Prec@1 98.438 (98.768)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1459 (0.1245)	Prec@1 98.438 (98.763)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1025 (0.1252)	Prec@1 99.219 (98.685)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1306 (0.1258)	Prec@1 98.047 (98.670)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1294 (0.1269)	Prec@1 98.438 (98.638)	
Total train loss: 0.1271

Train time: 21.203295946121216
 * Prec@1 63.640 Prec@5 87.600 Loss 1.5225
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.5903000831604

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1521 (0.1266)	Prec@1 98.438 (98.618)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1724 (0.1268)	Prec@1 97.656 (98.628)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1317 (0.1271)	Prec@1 97.656 (98.624)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1088 (0.1273)	Prec@1 99.219 (98.650)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1159 (0.1275)	Prec@1 98.047 (98.610)	
Total train loss: 0.1279

Train time: 21.46846866607666
 * Prec@1 63.790 Prec@5 87.470 Loss 1.5195
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.19136357307434

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1458 (0.1257)	Prec@1 98.047 (98.698)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1367 (0.1272)	Prec@1 97.656 (98.608)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1096 (0.1264)	Prec@1 99.219 (98.641)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1272 (0.1259)	Prec@1 98.438 (98.653)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1305 (0.1263)	Prec@1 99.609 (98.640)	
Total train loss: 0.1266

Train time: 20.544862747192383
 * Prec@1 63.610 Prec@5 87.590 Loss 1.5264
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.88922667503357

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1508 (0.1245)	Prec@1 98.438 (98.718)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1238 (0.1261)	Prec@1 99.609 (98.728)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1252 (0.1268)	Prec@1 98.828 (98.668)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1097 (0.1260)	Prec@1 99.219 (98.688)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1311 (0.1260)	Prec@1 98.438 (98.682)	
Total train loss: 0.1260

Train time: 21.15249276161194
 * Prec@1 63.590 Prec@5 87.600 Loss 1.5205
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.268449068069458

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1804 (0.1241)	Prec@1 97.266 (98.658)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1368 (0.1270)	Prec@1 98.828 (98.593)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1383 (0.1268)	Prec@1 98.828 (98.548)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1236 (0.1265)	Prec@1 98.047 (98.528)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1416 (0.1267)	Prec@1 97.656 (98.528)	
Total train loss: 0.1268

Train time: 21.455334424972534
 * Prec@1 63.820 Prec@5 87.570 Loss 1.5225
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 26.182116270065308

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1340 (0.1225)	Prec@1 98.047 (98.798)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1066 (0.1242)	Prec@1 99.609 (98.703)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1109 (0.1250)	Prec@1 98.438 (98.631)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1334 (0.1253)	Prec@1 98.438 (98.640)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0898 (0.1256)	Prec@1 99.609 (98.622)	
Total train loss: 0.1258

Train time: 21.73282027244568
 * Prec@1 63.620 Prec@5 87.690 Loss 1.5215
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 26.36986804008484

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1208 (0.1286)	Prec@1 99.219 (98.568)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1245 (0.1271)	Prec@1 98.828 (98.613)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1142 (0.1280)	Prec@1 98.828 (98.574)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1718 (0.1280)	Prec@1 96.484 (98.548)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1422 (0.1277)	Prec@1 98.828 (98.568)	
Total train loss: 0.1278

Train time: 20.59566640853882
 * Prec@1 63.660 Prec@5 87.560 Loss 1.5234
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 25.4593448638916

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1014 (0.1275)	Prec@1 98.828 (98.568)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1003 (0.1257)	Prec@1 99.219 (98.663)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1285 (0.1257)	Prec@1 98.828 (98.698)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1409 (0.1262)	Prec@1 98.828 (98.645)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1208 (0.1264)	Prec@1 98.828 (98.608)	
Total train loss: 0.1267

Train time: 18.404545307159424
 * Prec@1 63.710 Prec@5 87.610 Loss 1.5244
Best acc: 65.140
--------------------------------------------------------------------------------
Test time: 20.658719539642334


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 59.110 Prec@5 84.670 Loss 1.6621
Pre-trained Prec@1 with 9 layers frozen: 59.1099967956543 	 Loss: 1.662109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6865 (1.3586)	Prec@1 55.078 (62.089)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3711 (1.4165)	Prec@1 62.500 (60.171)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.4219 (1.4099)	Prec@1 59.766 (60.383)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.3428 (1.3996)	Prec@1 62.891 (60.682)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.4551 (1.3845)	Prec@1 60.938 (61.102)	
Total train loss: 1.3844

Train time: 160.22926950454712
 * Prec@1 50.210 Prec@5 79.980 Loss 1.9951
Best acc: 50.210
--------------------------------------------------------------------------------
Test time: 164.14603757858276

Epoch: [1][38/196]	LR: 0.1	Loss 1.1055 (1.1104)	Prec@1 71.484 (68.560)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0410 (1.1107)	Prec@1 70.312 (68.284)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1953 (1.1208)	Prec@1 65.234 (67.788)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2119 (1.1228)	Prec@1 67.578 (67.656)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0654 (1.1315)	Prec@1 67.578 (67.450)	
Total train loss: 1.1323

Train time: 20.785730123519897
 * Prec@1 51.780 Prec@5 80.340 Loss 2.0078
Best acc: 51.780
--------------------------------------------------------------------------------
Test time: 24.423185110092163

Epoch: [2][38/196]	LR: 0.1	Loss 0.9751 (0.9408)	Prec@1 71.875 (72.746)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0391 (0.9589)	Prec@1 69.531 (72.291)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.0498 (0.9757)	Prec@1 71.094 (71.738)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0664 (0.9920)	Prec@1 68.359 (71.149)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.0791 (0.9995)	Prec@1 68.750 (70.950)	
Total train loss: 1.0003

Train time: 18.846153259277344
 * Prec@1 56.140 Prec@5 84.570 Loss 1.7314
Best acc: 56.140
--------------------------------------------------------------------------------
Test time: 22.332000732421875

Epoch: [3][38/196]	LR: 0.1	Loss 0.8364 (0.8194)	Prec@1 76.172 (76.032)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8271 (0.8336)	Prec@1 74.219 (75.451)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9316 (0.8552)	Prec@1 72.656 (74.703)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9697 (0.8718)	Prec@1 68.359 (74.189)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9902 (0.8858)	Prec@1 71.484 (73.784)	
Total train loss: 0.8866

Train time: 19.914549112319946
 * Prec@1 57.680 Prec@5 85.450 Loss 1.6514
Best acc: 57.680
--------------------------------------------------------------------------------
Test time: 24.000885725021362

Epoch: [4][38/196]	LR: 0.1	Loss 0.7339 (0.7589)	Prec@1 78.516 (77.835)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7412 (0.7667)	Prec@1 78.125 (77.269)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8560 (0.7855)	Prec@1 76.172 (76.686)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.9194 (0.7998)	Prec@1 73.438 (76.270)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8081 (0.8153)	Prec@1 74.609 (75.765)	
Total train loss: 0.8159

Train time: 20.394848108291626
 * Prec@1 57.240 Prec@5 85.020 Loss 1.7393
Best acc: 57.680
--------------------------------------------------------------------------------
Test time: 23.99871015548706

Epoch: [5][38/196]	LR: 0.1	Loss 0.6030 (0.6645)	Prec@1 80.859 (80.659)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.6953 (0.6877)	Prec@1 78.125 (79.667)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7900 (0.7072)	Prec@1 77.344 (78.943)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7886 (0.7238)	Prec@1 75.781 (78.375)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.7222 (0.7407)	Prec@1 79.297 (77.869)	
Total train loss: 0.7411

Train time: 19.846847534179688
 * Prec@1 58.410 Prec@5 85.320 Loss 1.6895
Best acc: 58.410
--------------------------------------------------------------------------------
Test time: 23.32678484916687

Epoch: [6][38/196]	LR: 0.1	Loss 0.6328 (0.6065)	Prec@1 79.297 (82.212)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5879 (0.6200)	Prec@1 82.812 (81.505)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6802 (0.6452)	Prec@1 78.125 (80.482)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7324 (0.6627)	Prec@1 78.516 (79.875)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.8267 (0.6831)	Prec@1 75.781 (79.225)	
Total train loss: 0.6832

Train time: 20.362971544265747
 * Prec@1 56.850 Prec@5 84.630 Loss 1.7676
Best acc: 58.410
--------------------------------------------------------------------------------
Test time: 24.91127109527588

Epoch: [7][38/196]	LR: 0.1	Loss 0.5767 (0.5744)	Prec@1 80.469 (82.612)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5771 (0.5829)	Prec@1 82.031 (82.437)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6553 (0.5915)	Prec@1 82.812 (82.095)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.7529 (0.6089)	Prec@1 76.562 (81.543)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6582 (0.6236)	Prec@1 78.906 (81.000)	
Total train loss: 0.6239

Train time: 21.447901487350464
 * Prec@1 56.040 Prec@5 83.820 Loss 1.9043
Best acc: 58.410
--------------------------------------------------------------------------------
Test time: 24.891216278076172

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3843 (0.4340)	Prec@1 89.844 (87.540)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3665 (0.4080)	Prec@1 87.891 (88.366)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4338 (0.3891)	Prec@1 88.672 (89.079)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2715 (0.3740)	Prec@1 91.797 (89.583)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2461 (0.3615)	Prec@1 95.312 (90.024)	
Total train loss: 0.3617

Train time: 18.66374683380127
 * Prec@1 64.570 Prec@5 88.310 Loss 1.4434
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 22.611409902572632

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2861 (0.2695)	Prec@1 91.406 (93.590)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2693 (0.2698)	Prec@1 94.531 (93.419)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2832 (0.2683)	Prec@1 90.234 (93.480)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2871 (0.2671)	Prec@1 92.188 (93.500)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2279 (0.2642)	Prec@1 94.531 (93.630)	
Total train loss: 0.2642

Train time: 19.451679229736328
 * Prec@1 64.400 Prec@5 88.170 Loss 1.4570
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.436468839645386

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.1982 (0.2208)	Prec@1 96.094 (95.353)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2229 (0.2237)	Prec@1 94.141 (95.157)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2141 (0.2273)	Prec@1 96.484 (95.045)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2255 (0.2287)	Prec@1 96.875 (94.947)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2246 (0.2290)	Prec@1 95.312 (94.942)	
Total train loss: 0.2292

Train time: 18.87786626815796
 * Prec@1 63.720 Prec@5 88.000 Loss 1.4902
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 22.20597815513611

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2020 (0.2134)	Prec@1 95.312 (95.493)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1887 (0.2087)	Prec@1 95.703 (95.723)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1611 (0.2070)	Prec@1 98.047 (95.753)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2299 (0.2072)	Prec@1 95.312 (95.751)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1926 (0.2082)	Prec@1 96.094 (95.743)	
Total train loss: 0.2085

Train time: 26.428447008132935
 * Prec@1 64.220 Prec@5 87.930 Loss 1.4971
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 30.15619397163391

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1993 (0.1843)	Prec@1 96.094 (96.534)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1526 (0.1838)	Prec@1 97.656 (96.650)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1936 (0.1871)	Prec@1 97.266 (96.625)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1965 (0.1888)	Prec@1 98.438 (96.552)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1721 (0.1892)	Prec@1 97.266 (96.520)	
Total train loss: 0.1893

Train time: 20.37264633178711
 * Prec@1 63.670 Prec@5 87.480 Loss 1.5195
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.878014087677002

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1769 (0.1634)	Prec@1 96.484 (97.476)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1958 (0.1695)	Prec@1 95.312 (97.196)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1904 (0.1712)	Prec@1 95.703 (97.175)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1478 (0.1735)	Prec@1 97.656 (97.078)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1796 (0.1739)	Prec@1 97.266 (97.085)	
Total train loss: 0.1741

Train time: 18.543875217437744
 * Prec@1 63.530 Prec@5 87.330 Loss 1.5430
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 21.783308029174805

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1664 (0.1562)	Prec@1 96.875 (97.766)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1558 (0.1596)	Prec@1 98.047 (97.591)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1698 (0.1621)	Prec@1 96.484 (97.579)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1685 (0.1613)	Prec@1 97.656 (97.591)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1565 (0.1637)	Prec@1 98.047 (97.486)	
Total train loss: 0.1640

Train time: 20.685423612594604
 * Prec@1 63.360 Prec@5 87.400 Loss 1.5596
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 24.734933614730835

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1357 (0.1457)	Prec@1 99.219 (98.197)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1522 (0.1484)	Prec@1 97.266 (98.082)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1428 (0.1501)	Prec@1 98.047 (97.937)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1215 (0.1512)	Prec@1 98.438 (97.902)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1188 (0.1531)	Prec@1 98.047 (97.833)	
Total train loss: 0.1532

Train time: 18.690632820129395
 * Prec@1 63.160 Prec@5 87.270 Loss 1.5693
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 22.431253910064697

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1533 (0.1428)	Prec@1 96.875 (98.177)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1209 (0.1393)	Prec@1 98.047 (98.257)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1353 (0.1377)	Prec@1 96.875 (98.304)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1230 (0.1387)	Prec@1 98.438 (98.317)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1445 (0.1380)	Prec@1 98.438 (98.303)	
Total train loss: 0.1383

Train time: 20.438488483428955
 * Prec@1 63.110 Prec@5 87.070 Loss 1.5674
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.81608009338379

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1309 (0.1335)	Prec@1 99.219 (98.448)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1146 (0.1341)	Prec@1 99.219 (98.407)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1254 (0.1362)	Prec@1 97.656 (98.387)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1545 (0.1373)	Prec@1 98.438 (98.382)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1481 (0.1375)	Prec@1 98.438 (98.387)	
Total train loss: 0.1378

Train time: 19.484973907470703
 * Prec@1 63.120 Prec@5 87.190 Loss 1.5654
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.379979372024536

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1282 (0.1358)	Prec@1 99.219 (98.407)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1338 (0.1389)	Prec@1 98.047 (98.382)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1332 (0.1377)	Prec@1 99.219 (98.404)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1187 (0.1373)	Prec@1 99.609 (98.430)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1350 (0.1368)	Prec@1 98.047 (98.429)	
Total train loss: 0.1370

Train time: 19.456218004226685
 * Prec@1 63.190 Prec@5 87.230 Loss 1.5713
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.214349269866943

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1639 (0.1334)	Prec@1 97.266 (98.608)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1338 (0.1343)	Prec@1 99.609 (98.538)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1417 (0.1368)	Prec@1 97.266 (98.431)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1221 (0.1357)	Prec@1 98.047 (98.445)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1309 (0.1368)	Prec@1 97.656 (98.397)	
Total train loss: 0.1371

Train time: 20.580981016159058
 * Prec@1 63.380 Prec@5 87.170 Loss 1.5693
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.88085699081421

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1376 (0.1350)	Prec@1 98.828 (98.688)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1307 (0.1347)	Prec@1 98.828 (98.618)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1840 (0.1363)	Prec@1 96.484 (98.551)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1340 (0.1366)	Prec@1 98.438 (98.528)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1274 (0.1368)	Prec@1 99.219 (98.516)	
Total train loss: 0.1371

Train time: 20.09147572517395
 * Prec@1 63.210 Prec@5 87.310 Loss 1.5703
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 24.06168270111084

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1471 (0.1360)	Prec@1 98.828 (98.317)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1186 (0.1386)	Prec@1 98.828 (98.277)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1183 (0.1390)	Prec@1 98.047 (98.304)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1179 (0.1384)	Prec@1 99.219 (98.287)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1573 (0.1380)	Prec@1 97.656 (98.313)	
Total train loss: 0.1380

Train time: 20.63225746154785
 * Prec@1 63.210 Prec@5 87.290 Loss 1.5693
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 24.37973690032959

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1300 (0.1350)	Prec@1 98.438 (98.347)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1213 (0.1369)	Prec@1 100.000 (98.352)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1368 (0.1374)	Prec@1 99.609 (98.324)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1230 (0.1367)	Prec@1 98.828 (98.377)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1411 (0.1363)	Prec@1 99.219 (98.379)	
Total train loss: 0.1363

Train time: 20.63842225074768
 * Prec@1 63.080 Prec@5 87.160 Loss 1.5732
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 24.706735372543335

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1204 (0.1339)	Prec@1 98.438 (98.427)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1625 (0.1362)	Prec@1 96.875 (98.257)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1499 (0.1368)	Prec@1 97.266 (98.267)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1390 (0.1365)	Prec@1 97.656 (98.310)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1450 (0.1365)	Prec@1 98.828 (98.369)	
Total train loss: 0.1368

Train time: 20.104975700378418
 * Prec@1 63.220 Prec@5 87.220 Loss 1.5674
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.41905927658081

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1599 (0.1377)	Prec@1 98.828 (98.247)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1350 (0.1354)	Prec@1 98.438 (98.377)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1364 (0.1358)	Prec@1 98.047 (98.377)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1172 (0.1351)	Prec@1 99.609 (98.387)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1448 (0.1365)	Prec@1 98.438 (98.369)	
Total train loss: 0.1366

Train time: 19.370110034942627
 * Prec@1 63.160 Prec@5 87.290 Loss 1.5723
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.035417079925537

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1425 (0.1368)	Prec@1 98.828 (98.468)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1223 (0.1351)	Prec@1 98.828 (98.382)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1301 (0.1354)	Prec@1 98.438 (98.387)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1450 (0.1357)	Prec@1 99.219 (98.395)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1305 (0.1359)	Prec@1 98.828 (98.381)	
Total train loss: 0.1361

Train time: 20.43739652633667
 * Prec@1 63.350 Prec@5 87.220 Loss 1.5723
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 24.327614068984985

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1209 (0.1343)	Prec@1 99.609 (98.538)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1383 (0.1366)	Prec@1 98.047 (98.382)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1315 (0.1361)	Prec@1 99.219 (98.427)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1097 (0.1348)	Prec@1 100.000 (98.468)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1370 (0.1363)	Prec@1 97.656 (98.438)	
Total train loss: 0.1365

Train time: 19.72362756729126
 * Prec@1 63.120 Prec@5 87.220 Loss 1.5732
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.10495948791504

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1500 (0.1373)	Prec@1 98.438 (98.427)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1244 (0.1347)	Prec@1 98.828 (98.397)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1720 (0.1360)	Prec@1 98.047 (98.394)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1224 (0.1354)	Prec@1 98.438 (98.427)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1226 (0.1361)	Prec@1 98.438 (98.385)	
Total train loss: 0.1362

Train time: 20.2385036945343
 * Prec@1 63.130 Prec@5 87.310 Loss 1.5664
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.885215282440186

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1168 (0.1342)	Prec@1 99.219 (98.528)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1465 (0.1360)	Prec@1 98.438 (98.417)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1729 (0.1363)	Prec@1 97.266 (98.448)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1170 (0.1370)	Prec@1 98.828 (98.427)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1512 (0.1358)	Prec@1 98.047 (98.464)	
Total train loss: 0.1360

Train time: 19.225959062576294
 * Prec@1 63.130 Prec@5 87.230 Loss 1.5762
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.48206377029419

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1437 (0.1341)	Prec@1 98.828 (98.558)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1462 (0.1355)	Prec@1 97.656 (98.533)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1481 (0.1355)	Prec@1 97.656 (98.514)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1107 (0.1353)	Prec@1 99.219 (98.515)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1173 (0.1350)	Prec@1 98.047 (98.474)	
Total train loss: 0.1353

Train time: 20.499692678451538
 * Prec@1 62.990 Prec@5 87.220 Loss 1.5723
Best acc: 64.570
--------------------------------------------------------------------------------
Test time: 23.90439248085022

