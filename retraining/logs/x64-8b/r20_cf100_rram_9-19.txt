
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 54.600 Prec@5 81.310 Loss 1.8965
Pre-trained Prec@1 with 9 layers frozen: 54.599998474121094 	 Loss: 1.896484375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6953 (1.3950)	Prec@1 51.953 (60.837)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.4629 (1.4397)	Prec@1 60.938 (59.510)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.4600 (1.4337)	Prec@1 59.766 (59.535)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.3008 (1.4171)	Prec@1 62.500 (59.976)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.5596 (1.4025)	Prec@1 57.812 (60.359)	
Total train loss: 1.4026

Train time: 967.3291478157043
 * Prec@1 51.970 Prec@5 81.110 Loss 1.9297
Best acc: 51.970
--------------------------------------------------------------------------------
Test time: 994.226820230484

Epoch: [1][38/196]	LR: 0.1	Loss 1.0732 (1.1056)	Prec@1 69.141 (67.969)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0840 (1.1150)	Prec@1 69.531 (67.708)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1660 (1.1319)	Prec@1 65.234 (67.234)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2539 (1.1370)	Prec@1 63.672 (67.050)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.1104 (1.1477)	Prec@1 70.312 (66.767)	
Total train loss: 1.1485

Train time: 98.73568868637085
 * Prec@1 52.870 Prec@5 81.720 Loss 1.8916
Best acc: 52.870
--------------------------------------------------------------------------------
Test time: 122.34703016281128

Epoch: [2][38/196]	LR: 0.1	Loss 0.9146 (0.9579)	Prec@1 74.609 (72.216)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0527 (0.9697)	Prec@1 67.969 (71.795)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.0049 (0.9862)	Prec@1 70.703 (71.361)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.1494 (1.0032)	Prec@1 67.578 (70.703)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.0898 (1.0127)	Prec@1 69.531 (70.457)	
Total train loss: 1.0135

Train time: 97.45466756820679
 * Prec@1 53.500 Prec@5 81.810 Loss 1.9355
Best acc: 53.500
--------------------------------------------------------------------------------
Test time: 118.06559562683105

Epoch: [3][38/196]	LR: 0.1	Loss 0.7979 (0.8567)	Prec@1 80.859 (75.140)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.7334 (0.8654)	Prec@1 77.344 (74.579)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.9092 (0.8791)	Prec@1 71.875 (74.062)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9790 (0.8910)	Prec@1 70.703 (73.700)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9751 (0.9023)	Prec@1 69.922 (73.381)	
Total train loss: 0.9033

Train time: 96.57942271232605
 * Prec@1 55.200 Prec@5 83.650 Loss 1.7812
Best acc: 55.200
--------------------------------------------------------------------------------
Test time: 118.4991557598114

Epoch: [4][38/196]	LR: 0.1	Loss 0.7104 (0.7770)	Prec@1 77.734 (77.053)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7139 (0.7841)	Prec@1 76.172 (76.673)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8950 (0.8036)	Prec@1 70.703 (76.225)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8242 (0.8178)	Prec@1 77.344 (75.851)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8618 (0.8294)	Prec@1 74.609 (75.405)	
Total train loss: 0.8297

Train time: 101.0374743938446
 * Prec@1 57.300 Prec@5 84.490 Loss 1.7363
Best acc: 57.300
--------------------------------------------------------------------------------
Test time: 122.1614077091217

Epoch: [5][38/196]	LR: 0.1	Loss 0.5747 (0.6759)	Prec@1 83.594 (79.567)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.7432 (0.6985)	Prec@1 79.297 (79.107)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.9424 (0.7181)	Prec@1 72.266 (78.466)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.8438 (0.7409)	Prec@1 74.609 (77.789)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.6660 (0.7558)	Prec@1 79.688 (77.430)	
Total train loss: 0.7561

Train time: 141.69811463356018
 * Prec@1 57.150 Prec@5 84.730 Loss 1.7197
Best acc: 57.300
--------------------------------------------------------------------------------
Test time: 166.67762112617493

Epoch: [6][38/196]	LR: 0.1	Loss 0.6572 (0.6118)	Prec@1 79.297 (81.691)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5986 (0.6228)	Prec@1 83.203 (81.475)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.7075 (0.6495)	Prec@1 79.297 (80.379)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.8003 (0.6724)	Prec@1 73.047 (79.585)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.7256 (0.6913)	Prec@1 80.078 (79.103)	
Total train loss: 0.6916

Train time: 116.51195573806763
 * Prec@1 57.960 Prec@5 84.530 Loss 1.7539
Best acc: 57.960
--------------------------------------------------------------------------------
Test time: 139.4781367778778

Epoch: [7][38/196]	LR: 0.1	Loss 0.5767 (0.5757)	Prec@1 81.641 (82.893)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5649 (0.5851)	Prec@1 83.594 (82.392)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6094 (0.5908)	Prec@1 81.641 (82.255)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6245 (0.6102)	Prec@1 80.859 (81.550)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6890 (0.6274)	Prec@1 77.734 (80.992)	
Total train loss: 0.6278

Train time: 104.63559174537659
 * Prec@1 57.470 Prec@5 83.910 Loss 1.8125
Best acc: 57.960
--------------------------------------------------------------------------------
Test time: 123.87993359565735

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3799 (0.4501)	Prec@1 88.281 (86.919)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3516 (0.4201)	Prec@1 89.844 (87.976)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4006 (0.3956)	Prec@1 86.328 (88.792)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2976 (0.3792)	Prec@1 92.188 (89.401)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2686 (0.3664)	Prec@1 93.359 (89.866)	
Total train loss: 0.3666

Train time: 101.29005694389343
 * Prec@1 64.670 Prec@5 88.370 Loss 1.4375
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 122.14165616035461

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2668 (0.2716)	Prec@1 94.531 (93.289)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2795 (0.2728)	Prec@1 93.750 (93.299)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2299 (0.2698)	Prec@1 94.531 (93.466)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2732 (0.2691)	Prec@1 93.359 (93.490)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2208 (0.2682)	Prec@1 95.703 (93.504)	
Total train loss: 0.2683

Train time: 104.58309626579285
 * Prec@1 64.580 Prec@5 88.340 Loss 1.4551
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 123.632404088974

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2075 (0.2257)	Prec@1 96.484 (95.333)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.2448 (0.2272)	Prec@1 93.359 (95.157)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2362 (0.2313)	Prec@1 95.703 (95.092)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2192 (0.2322)	Prec@1 96.484 (94.959)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2424 (0.2317)	Prec@1 92.969 (94.956)	
Total train loss: 0.2318

Train time: 96.40377378463745
 * Prec@1 63.990 Prec@5 88.060 Loss 1.4854
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 119.65558290481567

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2034 (0.2113)	Prec@1 96.875 (95.994)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1927 (0.2085)	Prec@1 97.656 (95.944)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1665 (0.2065)	Prec@1 97.266 (96.004)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.1897 (0.2074)	Prec@1 96.484 (95.873)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2063 (0.2093)	Prec@1 95.312 (95.829)	
Total train loss: 0.2096

Train time: 103.79852199554443
 * Prec@1 64.380 Prec@5 88.050 Loss 1.4824
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 115.62442350387573

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1891 (0.1855)	Prec@1 96.875 (96.795)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1873 (0.1856)	Prec@1 96.094 (96.750)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1753 (0.1876)	Prec@1 97.656 (96.628)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2205 (0.1903)	Prec@1 95.312 (96.534)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1797 (0.1910)	Prec@1 95.703 (96.498)	
Total train loss: 0.1911

Train time: 37.39246439933777
 * Prec@1 63.790 Prec@5 87.730 Loss 1.5039
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 45.87737989425659

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1530 (0.1691)	Prec@1 98.438 (97.376)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1862 (0.1735)	Prec@1 96.875 (97.140)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2039 (0.1754)	Prec@1 95.312 (97.105)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1284 (0.1764)	Prec@1 99.609 (97.080)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.1925 (0.1769)	Prec@1 95.703 (97.065)	
Total train loss: 0.1770

Train time: 46.2988121509552
 * Prec@1 63.870 Prec@5 87.540 Loss 1.5264
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 57.60639691352844

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1543 (0.1558)	Prec@1 98.438 (97.827)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1542 (0.1618)	Prec@1 98.047 (97.681)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1571 (0.1654)	Prec@1 97.266 (97.506)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.2117 (0.1642)	Prec@1 94.922 (97.504)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1489 (0.1658)	Prec@1 98.438 (97.438)	
Total train loss: 0.1662

Train time: 44.54655647277832
 * Prec@1 63.440 Prec@5 87.360 Loss 1.5469
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 51.76353979110718

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1434 (0.1529)	Prec@1 98.438 (97.867)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1592 (0.1519)	Prec@1 96.875 (97.927)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1493 (0.1531)	Prec@1 97.266 (97.870)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1086 (0.1542)	Prec@1 98.828 (97.819)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1150 (0.1550)	Prec@1 99.219 (97.804)	
Total train loss: 0.1551

Train time: 44.891446590423584
 * Prec@1 63.490 Prec@5 87.410 Loss 1.5488
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 61.34999084472656

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1342 (0.1429)	Prec@1 98.047 (98.377)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1331 (0.1407)	Prec@1 98.438 (98.367)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1304 (0.1391)	Prec@1 98.438 (98.367)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1406 (0.1407)	Prec@1 98.047 (98.345)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1560 (0.1396)	Prec@1 98.438 (98.359)	
Total train loss: 0.1399

Train time: 45.877471685409546
 * Prec@1 63.590 Prec@5 87.360 Loss 1.5527
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 56.01186752319336

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1276 (0.1357)	Prec@1 98.438 (98.478)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1215 (0.1360)	Prec@1 98.438 (98.438)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1251 (0.1385)	Prec@1 98.438 (98.327)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1337 (0.1386)	Prec@1 98.828 (98.375)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1641 (0.1394)	Prec@1 98.438 (98.379)	
Total train loss: 0.1397

Train time: 49.57153081893921
 * Prec@1 63.660 Prec@5 87.140 Loss 1.5527
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 66.06826043128967

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1296 (0.1349)	Prec@1 100.000 (98.678)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1327 (0.1379)	Prec@1 97.656 (98.458)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1488 (0.1383)	Prec@1 98.047 (98.394)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1158 (0.1392)	Prec@1 99.609 (98.322)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1231 (0.1391)	Prec@1 98.438 (98.321)	
Total train loss: 0.1393

Train time: 80.58866548538208
 * Prec@1 63.390 Prec@5 87.350 Loss 1.5518
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 101.20601296424866

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1757 (0.1353)	Prec@1 98.828 (98.638)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1514 (0.1360)	Prec@1 98.047 (98.603)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1573 (0.1383)	Prec@1 98.047 (98.481)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1292 (0.1377)	Prec@1 98.438 (98.488)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1252 (0.1382)	Prec@1 99.219 (98.444)	
Total train loss: 0.1384

Train time: 95.35456705093384
 * Prec@1 63.770 Prec@5 87.250 Loss 1.5498
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 114.85960555076599

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1418 (0.1385)	Prec@1 98.828 (98.498)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1426 (0.1373)	Prec@1 97.656 (98.387)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1566 (0.1383)	Prec@1 98.047 (98.387)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1285 (0.1380)	Prec@1 98.828 (98.420)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1169 (0.1380)	Prec@1 99.609 (98.419)	
Total train loss: 0.1383

Train time: 156.40356922149658
 * Prec@1 63.760 Prec@5 87.250 Loss 1.5547
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 179.67506861686707

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1572 (0.1382)	Prec@1 98.828 (98.327)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1293 (0.1406)	Prec@1 97.656 (98.252)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1216 (0.1409)	Prec@1 98.438 (98.287)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1256 (0.1404)	Prec@1 99.219 (98.287)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1533 (0.1395)	Prec@1 98.828 (98.319)	
Total train loss: 0.1395

Train time: 129.9170114994049
 * Prec@1 63.450 Prec@5 87.360 Loss 1.5547
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 150.21649551391602

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1288 (0.1371)	Prec@1 99.219 (98.427)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1327 (0.1383)	Prec@1 98.828 (98.432)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1256 (0.1390)	Prec@1 99.609 (98.404)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1064 (0.1382)	Prec@1 99.219 (98.420)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1377 (0.1377)	Prec@1 99.219 (98.405)	
Total train loss: 0.1378

Train time: 95.55976557731628
 * Prec@1 63.480 Prec@5 87.300 Loss 1.5566
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 114.88050198554993

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1196 (0.1369)	Prec@1 99.219 (98.417)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1714 (0.1374)	Prec@1 96.875 (98.372)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1470 (0.1379)	Prec@1 98.047 (98.337)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1384 (0.1381)	Prec@1 98.828 (98.340)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1638 (0.1381)	Prec@1 98.438 (98.345)	
Total train loss: 0.1384

Train time: 103.97610092163086
 * Prec@1 63.730 Prec@5 87.290 Loss 1.5566
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 122.58015394210815

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1497 (0.1386)	Prec@1 98.828 (98.438)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1434 (0.1377)	Prec@1 98.828 (98.377)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1455 (0.1383)	Prec@1 98.438 (98.444)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1193 (0.1374)	Prec@1 99.609 (98.480)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1642 (0.1387)	Prec@1 96.875 (98.433)	
Total train loss: 0.1390

Train time: 97.81909680366516
 * Prec@1 63.350 Prec@5 87.160 Loss 1.5596
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 118.52444863319397

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1500 (0.1381)	Prec@1 98.438 (98.548)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1266 (0.1361)	Prec@1 98.828 (98.543)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1414 (0.1371)	Prec@1 98.047 (98.474)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1508 (0.1379)	Prec@1 96.875 (98.488)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1327 (0.1377)	Prec@1 98.438 (98.454)	
Total train loss: 0.1378

Train time: 97.48281216621399
 * Prec@1 63.620 Prec@5 87.460 Loss 1.5557
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 120.47612977027893

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1246 (0.1358)	Prec@1 99.609 (98.508)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1467 (0.1378)	Prec@1 97.266 (98.362)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1382 (0.1371)	Prec@1 98.438 (98.444)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1083 (0.1358)	Prec@1 99.609 (98.500)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1506 (0.1375)	Prec@1 98.047 (98.454)	
Total train loss: 0.1377

Train time: 131.72823429107666
 * Prec@1 63.650 Prec@5 87.340 Loss 1.5557
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 155.32605576515198

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1656 (0.1383)	Prec@1 97.656 (98.307)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1158 (0.1368)	Prec@1 98.438 (98.352)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1851 (0.1381)	Prec@1 98.438 (98.407)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1232 (0.1375)	Prec@1 98.828 (98.455)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1278 (0.1383)	Prec@1 98.438 (98.417)	
Total train loss: 0.1384

Train time: 148.58104300498962
 * Prec@1 63.620 Prec@5 87.390 Loss 1.5508
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 170.1841447353363

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1284 (0.1338)	Prec@1 98.828 (98.598)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1503 (0.1354)	Prec@1 98.828 (98.523)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1652 (0.1374)	Prec@1 97.656 (98.444)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1031 (0.1392)	Prec@1 100.000 (98.325)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1588 (0.1380)	Prec@1 98.438 (98.401)	
Total train loss: 0.1381

Train time: 121.22829699516296
 * Prec@1 63.640 Prec@5 87.330 Loss 1.5596
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 143.67300391197205

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1676 (0.1357)	Prec@1 97.266 (98.417)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1406 (0.1372)	Prec@1 99.219 (98.538)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1484 (0.1371)	Prec@1 98.828 (98.504)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1365 (0.1376)	Prec@1 98.828 (98.455)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1009 (0.1362)	Prec@1 99.219 (98.464)	
Total train loss: 0.1364

Train time: 142.6388750076294
 * Prec@1 63.670 Prec@5 87.310 Loss 1.5527
Best acc: 64.670
--------------------------------------------------------------------------------
Test time: 146.24281001091003


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 49.390 Prec@5 77.100 Loss 2.1816
Pre-trained Prec@1 with 11 layers frozen: 49.38999938964844 	 Loss: 2.181640625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.6025 (1.3590)	Prec@1 55.078 (61.819)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.4512 (1.4221)	Prec@1 60.938 (60.256)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.3604 (1.4116)	Prec@1 60.938 (60.503)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.4131 (1.3908)	Prec@1 63.672 (61.065)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.1533 (1.3672)	Prec@1 68.750 (61.699)	
Total train loss: 1.3669

Train time: 518.104977607727
 * Prec@1 55.480 Prec@5 83.220 Loss 1.6660
Best acc: 55.480
--------------------------------------------------------------------------------
Test time: 545.4321899414062

Epoch: [1][38/196]	LR: 0.1	Loss 1.0654 (1.0891)	Prec@1 67.578 (68.770)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0947 (1.0919)	Prec@1 67.188 (68.650)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.2002 (1.1001)	Prec@1 67.969 (68.146)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1885 (1.1109)	Prec@1 68.359 (67.926)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2324 (1.1124)	Prec@1 62.500 (67.937)	
Total train loss: 1.1126

Train time: 109.40946841239929
 * Prec@1 56.180 Prec@5 82.390 Loss 1.7822
Best acc: 56.180
--------------------------------------------------------------------------------
Test time: 130.00328016281128

Epoch: [2][38/196]	LR: 0.1	Loss 0.9170 (0.9278)	Prec@1 75.000 (73.468)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.1143 (0.9342)	Prec@1 66.016 (72.862)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.9878 (0.9499)	Prec@1 69.141 (72.346)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.0879 (0.9666)	Prec@1 65.625 (71.705)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.8843 (0.9790)	Prec@1 73.438 (71.274)	
Total train loss: 0.9789

Train time: 101.36773157119751
 * Prec@1 58.330 Prec@5 84.770 Loss 1.6240
Best acc: 58.330
--------------------------------------------------------------------------------
Test time: 121.30516600608826

Epoch: [3][38/196]	LR: 0.1	Loss 0.7417 (0.8044)	Prec@1 77.734 (75.962)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8682 (0.8197)	Prec@1 75.391 (75.671)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8994 (0.8365)	Prec@1 76.562 (75.257)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.0010 (0.8556)	Prec@1 70.312 (74.604)	
Epoch: [3][194/196]	LR: 0.1	Loss 1.0020 (0.8702)	Prec@1 68.359 (74.157)	
Total train loss: 0.8703

Train time: 147.84527921676636
 * Prec@1 54.200 Prec@5 81.650 Loss 1.9277
Best acc: 58.330
--------------------------------------------------------------------------------
Test time: 169.17524218559265

Epoch: [4][38/196]	LR: 0.1	Loss 0.6616 (0.7491)	Prec@1 80.469 (78.055)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.7646 (0.7465)	Prec@1 78.125 (77.920)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8242 (0.7641)	Prec@1 76.562 (77.227)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8047 (0.7788)	Prec@1 78.125 (76.693)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.8940 (0.7937)	Prec@1 77.734 (76.196)	
Total train loss: 0.7940

Train time: 104.31597542762756
 * Prec@1 56.510 Prec@5 83.700 Loss 1.7939
Best acc: 58.330
--------------------------------------------------------------------------------
Test time: 122.41004347801208

Epoch: [5][38/196]	LR: 0.1	Loss 0.6709 (0.6652)	Prec@1 78.516 (80.329)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.7510 (0.6636)	Prec@1 79.688 (80.319)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.7793 (0.6846)	Prec@1 77.344 (79.587)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7783 (0.7054)	Prec@1 74.609 (78.894)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.7930 (0.7216)	Prec@1 74.609 (78.293)	
Total train loss: 0.7220

Train time: 89.75257730484009
 * Prec@1 56.970 Prec@5 83.830 Loss 1.8242
Best acc: 58.330
--------------------------------------------------------------------------------
Test time: 108.07276177406311

Epoch: [6][38/196]	LR: 0.1	Loss 0.6396 (0.6118)	Prec@1 80.469 (81.741)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.6851 (0.6191)	Prec@1 78.906 (81.470)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6812 (0.6268)	Prec@1 80.078 (81.183)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6973 (0.6411)	Prec@1 80.859 (80.684)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.6431 (0.6562)	Prec@1 82.812 (80.264)	
Total train loss: 0.6565

Train time: 88.80404162406921
 * Prec@1 58.520 Prec@5 84.950 Loss 1.7197
Best acc: 58.520
--------------------------------------------------------------------------------
Test time: 110.80693435668945

Epoch: [7][38/196]	LR: 0.1	Loss 0.4368 (0.5365)	Prec@1 85.938 (84.395)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5205 (0.5415)	Prec@1 84.375 (84.014)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6216 (0.5575)	Prec@1 80.078 (83.146)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6782 (0.5733)	Prec@1 80.078 (82.582)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6890 (0.5941)	Prec@1 79.297 (81.883)	
Total train loss: 0.5949

Train time: 101.53007936477661
 * Prec@1 56.600 Prec@5 83.870 Loss 1.9102
Best acc: 58.520
--------------------------------------------------------------------------------
Test time: 122.12857246398926

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3398 (0.4301)	Prec@1 91.797 (87.851)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3091 (0.4016)	Prec@1 91.797 (88.772)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2717 (0.3819)	Prec@1 92.188 (89.446)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2822 (0.3644)	Prec@1 93.359 (90.012)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.2566 (0.3517)	Prec@1 94.531 (90.433)	
Total train loss: 0.3520

Train time: 92.4837417602539
 * Prec@1 64.240 Prec@5 87.870 Loss 1.4766
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 113.58694195747375

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2471 (0.2583)	Prec@1 95.703 (93.930)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3281 (0.2530)	Prec@1 91.016 (94.241)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2445 (0.2541)	Prec@1 94.922 (94.131)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2510 (0.2539)	Prec@1 93.359 (94.066)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2815 (0.2530)	Prec@1 91.797 (94.087)	
Total train loss: 0.2533

Train time: 109.13828134536743
 * Prec@1 63.920 Prec@5 87.950 Loss 1.4883
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 127.81045055389404

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2271 (0.2168)	Prec@1 94.531 (95.573)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3130 (0.2167)	Prec@1 92.188 (95.533)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.1989 (0.2175)	Prec@1 96.484 (95.519)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2473 (0.2186)	Prec@1 94.141 (95.438)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2849 (0.2203)	Prec@1 92.578 (95.304)	
Total train loss: 0.2205

Train time: 97.72129487991333
 * Prec@1 63.920 Prec@5 87.960 Loss 1.5156
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 117.68454718589783

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.1636 (0.1914)	Prec@1 97.266 (96.444)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.2085 (0.1928)	Prec@1 95.703 (96.434)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1783 (0.1942)	Prec@1 97.656 (96.314)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2405 (0.1964)	Prec@1 93.750 (96.254)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1692 (0.1992)	Prec@1 98.438 (96.166)	
Total train loss: 0.1994

Train time: 108.0996961593628
 * Prec@1 63.620 Prec@5 87.540 Loss 1.5381
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 124.77365946769714

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2042 (0.1781)	Prec@1 95.312 (96.955)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.2145 (0.1772)	Prec@1 96.484 (97.040)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.1952 (0.1781)	Prec@1 96.875 (97.029)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1285 (0.1808)	Prec@1 98.828 (96.913)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.2341 (0.1823)	Prec@1 94.922 (96.811)	
Total train loss: 0.1825

Train time: 35.08133864402771
 * Prec@1 63.500 Prec@5 87.350 Loss 1.5547
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 43.32183218002319

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1680 (0.1683)	Prec@1 97.656 (97.296)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1808 (0.1673)	Prec@1 96.875 (97.346)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1633 (0.1694)	Prec@1 97.266 (97.269)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1945 (0.1704)	Prec@1 95.312 (97.155)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2253 (0.1714)	Prec@1 95.312 (97.149)	
Total train loss: 0.1717

Train time: 49.969584226608276
 * Prec@1 63.420 Prec@5 87.260 Loss 1.5557
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 59.702571868896484

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1680 (0.1565)	Prec@1 97.656 (97.626)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1877 (0.1561)	Prec@1 96.094 (97.626)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1421 (0.1584)	Prec@1 98.047 (97.599)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1725 (0.1592)	Prec@1 98.438 (97.601)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1521 (0.1592)	Prec@1 98.438 (97.592)	
Total train loss: 0.1594

Train time: 49.53773355484009
 * Prec@1 63.340 Prec@5 87.190 Loss 1.5684
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 56.69228196144104

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2094 (0.1461)	Prec@1 96.484 (98.027)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1561 (0.1465)	Prec@1 98.047 (98.062)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1373 (0.1485)	Prec@1 98.828 (97.953)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1993 (0.1503)	Prec@1 96.875 (97.934)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1506 (0.1513)	Prec@1 97.656 (97.907)	
Total train loss: 0.1514

Train time: 40.19573187828064
 * Prec@1 63.220 Prec@5 86.650 Loss 1.5879
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 51.324382066726685

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1490 (0.1358)	Prec@1 97.266 (98.367)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1350 (0.1351)	Prec@1 98.828 (98.458)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1388 (0.1364)	Prec@1 98.047 (98.374)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1399 (0.1361)	Prec@1 98.828 (98.387)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1584 (0.1366)	Prec@1 96.484 (98.399)	
Total train loss: 0.1368

Train time: 50.66319298744202
 * Prec@1 63.180 Prec@5 86.750 Loss 1.5850
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 58.74794793128967

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1272 (0.1380)	Prec@1 98.828 (98.427)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1175 (0.1363)	Prec@1 98.828 (98.377)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1235 (0.1365)	Prec@1 97.656 (98.407)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1366 (0.1359)	Prec@1 98.438 (98.392)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1484 (0.1363)	Prec@1 99.219 (98.405)	
Total train loss: 0.1364

Train time: 45.18207311630249
 * Prec@1 63.210 Prec@5 86.760 Loss 1.5889
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 51.19026303291321

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1273 (0.1354)	Prec@1 98.438 (98.458)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1288 (0.1351)	Prec@1 98.438 (98.407)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1278 (0.1348)	Prec@1 98.828 (98.424)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1166 (0.1348)	Prec@1 99.609 (98.432)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1566 (0.1348)	Prec@1 97.266 (98.415)	
Total train loss: 0.1350

Train time: 41.94538736343384
 * Prec@1 63.360 Prec@5 86.710 Loss 1.5830
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 57.40172719955444

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1453 (0.1316)	Prec@1 98.438 (98.578)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1445 (0.1320)	Prec@1 97.266 (98.543)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1387 (0.1346)	Prec@1 99.219 (98.508)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1234 (0.1340)	Prec@1 98.828 (98.503)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1101 (0.1329)	Prec@1 99.609 (98.528)	
Total train loss: 0.1330

Train time: 77.10618042945862
 * Prec@1 63.380 Prec@5 86.730 Loss 1.5820
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 97.05507326126099

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1631 (0.1371)	Prec@1 98.438 (98.367)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1406 (0.1357)	Prec@1 98.047 (98.417)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1079 (0.1341)	Prec@1 99.219 (98.474)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1243 (0.1346)	Prec@1 99.609 (98.465)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1348 (0.1348)	Prec@1 98.438 (98.474)	
Total train loss: 0.1349

Train time: 89.77361869812012
 * Prec@1 63.090 Prec@5 86.760 Loss 1.5869
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 113.32870411872864

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1433 (0.1352)	Prec@1 97.266 (98.237)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1499 (0.1366)	Prec@1 97.266 (98.267)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1128 (0.1358)	Prec@1 98.828 (98.321)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1633 (0.1351)	Prec@1 96.875 (98.342)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1447 (0.1355)	Prec@1 98.828 (98.359)	
Total train loss: 0.1356

Train time: 158.62632131576538
 * Prec@1 63.180 Prec@5 86.630 Loss 1.5840
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 181.78619742393494

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1444 (0.1353)	Prec@1 98.438 (98.548)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1501 (0.1364)	Prec@1 99.219 (98.508)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1375 (0.1342)	Prec@1 99.609 (98.574)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1370 (0.1330)	Prec@1 98.828 (98.608)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1300 (0.1334)	Prec@1 99.609 (98.600)	
Total train loss: 0.1335

Train time: 122.9512071609497
 * Prec@1 63.220 Prec@5 86.740 Loss 1.5869
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 198.49815440177917

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1447 (0.1343)	Prec@1 98.047 (98.438)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1211 (0.1337)	Prec@1 99.609 (98.518)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1469 (0.1355)	Prec@1 97.656 (98.474)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1105 (0.1353)	Prec@1 99.219 (98.488)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1385 (0.1348)	Prec@1 97.656 (98.458)	
Total train loss: 0.1348

Train time: 157.24948859214783
 * Prec@1 63.180 Prec@5 86.660 Loss 1.5859
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 180.34338903427124

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1315 (0.1318)	Prec@1 98.047 (98.337)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1378 (0.1331)	Prec@1 98.828 (98.493)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1570 (0.1330)	Prec@1 98.828 (98.511)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1459 (0.1328)	Prec@1 99.219 (98.530)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1371 (0.1332)	Prec@1 96.875 (98.508)	
Total train loss: 0.1333

Train time: 97.69013094902039
 * Prec@1 63.190 Prec@5 86.680 Loss 1.5947
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 118.09334349632263

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1292 (0.1364)	Prec@1 98.828 (98.357)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1205 (0.1347)	Prec@1 98.828 (98.553)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1254 (0.1340)	Prec@1 99.609 (98.544)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1631 (0.1343)	Prec@1 95.703 (98.510)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1366 (0.1342)	Prec@1 97.266 (98.500)	
Total train loss: 0.1343

Train time: 103.2822253704071
 * Prec@1 63.430 Prec@5 86.730 Loss 1.5908
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 128.01537585258484

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1373 (0.1363)	Prec@1 98.438 (98.367)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1343 (0.1325)	Prec@1 98.047 (98.422)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1383 (0.1343)	Prec@1 97.266 (98.391)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1241 (0.1341)	Prec@1 98.438 (98.450)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1132 (0.1341)	Prec@1 98.828 (98.472)	
Total train loss: 0.1343

Train time: 115.53054404258728
 * Prec@1 62.980 Prec@5 86.800 Loss 1.5820
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 140.30584859848022

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1139 (0.1303)	Prec@1 99.219 (98.678)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1334 (0.1320)	Prec@1 98.828 (98.633)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1190 (0.1330)	Prec@1 98.828 (98.611)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1207 (0.1338)	Prec@1 99.219 (98.528)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1288 (0.1338)	Prec@1 98.438 (98.534)	
Total train loss: 0.1340

Train time: 111.77837038040161
 * Prec@1 63.240 Prec@5 86.710 Loss 1.5830
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 136.77225542068481

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1500 (0.1301)	Prec@1 97.656 (98.688)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1481 (0.1314)	Prec@1 96.484 (98.623)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1534 (0.1325)	Prec@1 97.266 (98.561)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1498 (0.1331)	Prec@1 97.266 (98.508)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1064 (0.1332)	Prec@1 99.219 (98.508)	
Total train loss: 0.1335

Train time: 146.07617902755737
 * Prec@1 63.270 Prec@5 86.970 Loss 1.5840
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 189.5175495147705

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1411 (0.1336)	Prec@1 97.656 (98.327)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1223 (0.1337)	Prec@1 98.828 (98.347)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1185 (0.1344)	Prec@1 98.828 (98.347)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1138 (0.1338)	Prec@1 98.828 (98.415)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1458 (0.1337)	Prec@1 98.047 (98.446)	
Total train loss: 0.1338

Train time: 152.36796498298645
 * Prec@1 63.180 Prec@5 86.870 Loss 1.5840
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 174.60311579704285


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 40.950 Prec@5 68.560 Loss 2.7598
Pre-trained Prec@1 with 13 layers frozen: 40.95000076293945 	 Loss: 2.759765625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.4551 (1.3028)	Prec@1 55.469 (63.211)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3594 (1.3516)	Prec@1 60.938 (61.734)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.3975 (1.3484)	Prec@1 62.109 (61.932)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.4053 (1.3392)	Prec@1 63.281 (62.215)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.4219 (1.3251)	Prec@1 57.031 (62.416)	
Total train loss: 1.3257

Train time: 544.095249414444
 * Prec@1 52.540 Prec@5 80.520 Loss 1.9180
Best acc: 52.540
--------------------------------------------------------------------------------
Test time: 650.8832845687866

Epoch: [1][38/196]	LR: 0.1	Loss 0.8867 (1.0586)	Prec@1 73.047 (69.451)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.1797 (1.0678)	Prec@1 64.062 (69.066)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1855 (1.0791)	Prec@1 66.406 (68.503)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.0820 (1.0850)	Prec@1 67.578 (68.374)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2412 (1.0860)	Prec@1 65.234 (68.405)	
Total train loss: 1.0861

Train time: 99.7244484424591
 * Prec@1 55.900 Prec@5 83.580 Loss 1.7598
Best acc: 55.900
--------------------------------------------------------------------------------
Test time: 119.01368427276611

Epoch: [2][38/196]	LR: 0.1	Loss 0.9282 (0.9079)	Prec@1 72.656 (73.458)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.0811 (0.9144)	Prec@1 70.703 (73.387)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.0068 (0.9308)	Prec@1 67.578 (72.730)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.8193 (0.9415)	Prec@1 74.609 (72.559)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9893 (0.9478)	Prec@1 70.703 (72.362)	
Total train loss: 0.9482

Train time: 95.95130276679993
 * Prec@1 58.580 Prec@5 85.920 Loss 1.5938
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 115.79440879821777

Epoch: [3][38/196]	LR: 0.1	Loss 0.8882 (0.7917)	Prec@1 75.000 (76.522)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.8818 (0.7983)	Prec@1 74.219 (76.262)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8101 (0.8190)	Prec@1 75.781 (75.755)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.8555 (0.8365)	Prec@1 78.516 (75.275)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.9424 (0.8451)	Prec@1 71.875 (75.018)	
Total train loss: 0.8453

Train time: 93.37741279602051
 * Prec@1 58.160 Prec@5 85.150 Loss 1.6660
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 111.91840958595276

Epoch: [4][38/196]	LR: 0.1	Loss 0.6738 (0.6887)	Prec@1 81.250 (79.708)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.8398 (0.7190)	Prec@1 73.828 (78.556)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.7749 (0.7392)	Prec@1 79.688 (78.065)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8452 (0.7540)	Prec@1 73.438 (77.624)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.9180 (0.7688)	Prec@1 74.609 (77.127)	
Total train loss: 0.7691

Train time: 94.96454572677612
 * Prec@1 52.490 Prec@5 79.730 Loss 2.2363
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 114.69988465309143

Epoch: [5][38/196]	LR: 0.1	Loss 0.6929 (0.6323)	Prec@1 79.688 (81.510)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.5913 (0.6449)	Prec@1 83.594 (80.834)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.6787 (0.6640)	Prec@1 79.297 (79.938)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.8140 (0.6837)	Prec@1 73.828 (79.397)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8179 (0.6991)	Prec@1 76.562 (78.910)	
Total train loss: 0.6993

Train time: 102.53407430648804
 * Prec@1 58.520 Prec@5 83.740 Loss 1.7520
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 120.42539238929749

Epoch: [6][38/196]	LR: 0.1	Loss 0.7007 (0.5709)	Prec@1 78.125 (83.644)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5786 (0.5781)	Prec@1 80.078 (82.873)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.6704 (0.5870)	Prec@1 76.172 (82.472)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.6655 (0.6087)	Prec@1 80.469 (81.713)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.8442 (0.6253)	Prec@1 75.000 (81.200)	
Total train loss: 0.6256

Train time: 89.09735798835754
 * Prec@1 56.970 Prec@5 84.140 Loss 1.7988
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 108.02890467643738

Epoch: [7][38/196]	LR: 0.1	Loss 0.4890 (0.5255)	Prec@1 84.375 (84.135)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.6582 (0.5251)	Prec@1 79.688 (83.924)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.5342 (0.5386)	Prec@1 80.859 (83.520)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.6006 (0.5542)	Prec@1 80.469 (83.023)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.6953 (0.5703)	Prec@1 76.953 (82.538)	
Total train loss: 0.5709

Train time: 93.1822862625122
 * Prec@1 58.070 Prec@5 84.290 Loss 1.8398
Best acc: 58.580
--------------------------------------------------------------------------------
Test time: 113.93304896354675

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.4065 (0.4160)	Prec@1 88.672 (87.941)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.2957 (0.3846)	Prec@1 94.141 (89.168)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.2632 (0.3637)	Prec@1 92.969 (89.887)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.2705 (0.3493)	Prec@1 92.969 (90.452)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.3149 (0.3409)	Prec@1 91.016 (90.793)	
Total train loss: 0.3410

Train time: 90.28971314430237
 * Prec@1 64.150 Prec@5 87.570 Loss 1.4717
Best acc: 64.150
--------------------------------------------------------------------------------
Test time: 128.65341925621033

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.2083 (0.2517)	Prec@1 95.312 (94.151)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.2255 (0.2513)	Prec@1 94.922 (94.206)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2325 (0.2526)	Prec@1 94.531 (94.111)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2429 (0.2528)	Prec@1 94.531 (94.118)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.2478 (0.2518)	Prec@1 95.703 (94.099)	
Total train loss: 0.2519

Train time: 194.40480279922485
 * Prec@1 64.390 Prec@5 87.580 Loss 1.4834
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 217.86891913414001

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.2686 (0.2141)	Prec@1 91.797 (95.743)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.1876 (0.2131)	Prec@1 95.703 (95.608)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2034 (0.2149)	Prec@1 94.922 (95.523)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.2151 (0.2163)	Prec@1 94.922 (95.423)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2404 (0.2182)	Prec@1 93.750 (95.371)	
Total train loss: 0.2184

Train time: 92.70271587371826
 * Prec@1 63.650 Prec@5 87.350 Loss 1.5137
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 111.68124079704285

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2067 (0.1951)	Prec@1 96.094 (96.244)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.1796 (0.1963)	Prec@1 94.922 (96.264)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.1866 (0.1988)	Prec@1 96.484 (96.201)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.2113 (0.1976)	Prec@1 94.922 (96.229)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.1305 (0.1978)	Prec@1 98.047 (96.230)	
Total train loss: 0.1982

Train time: 77.16417694091797
 * Prec@1 63.450 Prec@5 87.290 Loss 1.5303
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 86.08001899719238

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.1586 (0.1716)	Prec@1 97.266 (97.196)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.1766 (0.1746)	Prec@1 96.484 (97.105)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2375 (0.1792)	Prec@1 94.141 (96.928)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.1882 (0.1819)	Prec@1 97.266 (96.867)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.1782 (0.1836)	Prec@1 97.266 (96.773)	
Total train loss: 0.1837

Train time: 33.51746416091919
 * Prec@1 63.310 Prec@5 87.090 Loss 1.5400
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 41.60019302368164

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.1647 (0.1684)	Prec@1 98.047 (97.175)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.1685 (0.1670)	Prec@1 96.875 (97.361)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.1689 (0.1667)	Prec@1 96.484 (97.349)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.1812 (0.1675)	Prec@1 96.484 (97.341)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2004 (0.1693)	Prec@1 96.094 (97.254)	
Total train loss: 0.1694

Train time: 50.92320990562439
 * Prec@1 63.320 Prec@5 87.050 Loss 1.5635
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 62.92369747161865

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1349 (0.1599)	Prec@1 97.266 (97.626)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.1342 (0.1594)	Prec@1 98.047 (97.591)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.1257 (0.1582)	Prec@1 98.828 (97.633)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.1565 (0.1593)	Prec@1 98.047 (97.614)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.1848 (0.1597)	Prec@1 96.875 (97.590)	
Total train loss: 0.1599

Train time: 47.48905348777771
 * Prec@1 63.490 Prec@5 87.040 Loss 1.5605
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 54.02478742599487

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.1660 (0.1473)	Prec@1 97.656 (98.167)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.1139 (0.1493)	Prec@1 98.047 (97.992)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.1390 (0.1520)	Prec@1 98.047 (97.880)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.1627 (0.1519)	Prec@1 96.875 (97.879)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.1512 (0.1535)	Prec@1 97.656 (97.815)	
Total train loss: 0.1536

Train time: 37.2007098197937
 * Prec@1 63.280 Prec@5 86.860 Loss 1.5742
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 48.282111406326294

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.1235 (0.1324)	Prec@1 99.219 (98.578)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.1304 (0.1340)	Prec@1 98.828 (98.568)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.1588 (0.1336)	Prec@1 99.219 (98.538)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1420 (0.1345)	Prec@1 97.266 (98.548)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.1017 (0.1359)	Prec@1 99.609 (98.500)	
Total train loss: 0.1362

Train time: 46.02884292602539
 * Prec@1 63.100 Prec@5 86.720 Loss 1.5859
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 53.62761092185974

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.1500 (0.1399)	Prec@1 96.094 (98.307)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.1493 (0.1394)	Prec@1 98.047 (98.272)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1190 (0.1389)	Prec@1 98.828 (98.281)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1655 (0.1384)	Prec@1 98.438 (98.322)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.1309 (0.1385)	Prec@1 98.047 (98.365)	
Total train loss: 0.1387

Train time: 446.5466339588165
 * Prec@1 63.230 Prec@5 86.720 Loss 1.5859
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 545.0059752464294

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.1334 (0.1353)	Prec@1 98.438 (98.367)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1581 (0.1354)	Prec@1 97.266 (98.432)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.1251 (0.1377)	Prec@1 98.828 (98.384)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.1177 (0.1365)	Prec@1 98.828 (98.420)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1263 (0.1364)	Prec@1 98.828 (98.440)	
Total train loss: 0.1367

Train time: 120.45752310752869
 * Prec@1 63.120 Prec@5 86.840 Loss 1.5908
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 139.81260895729065

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.1201 (0.1340)	Prec@1 98.828 (98.427)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.1384 (0.1354)	Prec@1 98.438 (98.382)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.1350 (0.1363)	Prec@1 97.656 (98.421)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.1197 (0.1372)	Prec@1 98.828 (98.412)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1317 (0.1366)	Prec@1 99.219 (98.425)	
Total train loss: 0.1366

Train time: 90.7974464893341
 * Prec@1 63.340 Prec@5 86.640 Loss 1.5879
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 113.68747520446777

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.1223 (0.1352)	Prec@1 98.828 (98.548)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1252 (0.1359)	Prec@1 99.219 (98.438)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.1456 (0.1364)	Prec@1 98.047 (98.444)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.1111 (0.1383)	Prec@1 98.438 (98.380)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1569 (0.1367)	Prec@1 97.656 (98.442)	
Total train loss: 0.1368

Train time: 97.36545085906982
 * Prec@1 62.970 Prec@5 86.780 Loss 1.5859
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 115.85375261306763

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1541 (0.1353)	Prec@1 98.828 (98.327)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.1133 (0.1352)	Prec@1 99.219 (98.397)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1259 (0.1361)	Prec@1 98.438 (98.407)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.1376 (0.1359)	Prec@1 98.438 (98.427)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1404 (0.1362)	Prec@1 98.047 (98.425)	
Total train loss: 0.1362

Train time: 96.43269276618958
 * Prec@1 63.130 Prec@5 86.770 Loss 1.5830
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 117.40251803398132

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.1223 (0.1378)	Prec@1 99.219 (98.438)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.1158 (0.1344)	Prec@1 98.438 (98.553)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.1196 (0.1355)	Prec@1 98.047 (98.531)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.1464 (0.1363)	Prec@1 97.656 (98.448)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.1487 (0.1362)	Prec@1 97.656 (98.480)	
Total train loss: 0.1363

Train time: 95.25167465209961
 * Prec@1 62.920 Prec@5 86.900 Loss 1.5771
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 113.3814127445221

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1218 (0.1394)	Prec@1 98.828 (98.227)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1431 (0.1391)	Prec@1 97.656 (98.317)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1312 (0.1371)	Prec@1 98.047 (98.397)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.1207 (0.1371)	Prec@1 98.438 (98.382)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.1355 (0.1362)	Prec@1 98.047 (98.417)	
Total train loss: 0.1363

Train time: 91.74399328231812
 * Prec@1 63.070 Prec@5 86.840 Loss 1.5908
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 113.88560652732849

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.1233 (0.1355)	Prec@1 98.828 (98.518)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.1656 (0.1369)	Prec@1 97.266 (98.438)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.1086 (0.1357)	Prec@1 100.000 (98.464)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.1521 (0.1352)	Prec@1 97.656 (98.505)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1508 (0.1358)	Prec@1 96.875 (98.484)	
Total train loss: 0.1359

Train time: 100.61424350738525
 * Prec@1 63.260 Prec@5 86.890 Loss 1.5859
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 120.63035440444946

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.1353 (0.1381)	Prec@1 97.656 (98.397)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.1462 (0.1382)	Prec@1 98.438 (98.342)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.1309 (0.1375)	Prec@1 98.828 (98.411)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.1327 (0.1371)	Prec@1 98.828 (98.420)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.1493 (0.1366)	Prec@1 97.656 (98.444)	
Total train loss: 0.1367

Train time: 87.42265033721924
 * Prec@1 63.050 Prec@5 86.710 Loss 1.5918
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 109.53608989715576

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1438 (0.1365)	Prec@1 98.047 (98.518)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1351 (0.1346)	Prec@1 98.438 (98.603)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.1199 (0.1347)	Prec@1 99.219 (98.514)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.1283 (0.1354)	Prec@1 99.219 (98.505)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.1221 (0.1353)	Prec@1 98.828 (98.518)	
Total train loss: 0.1355

Train time: 107.73600745201111
 * Prec@1 63.130 Prec@5 86.930 Loss 1.5898
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 129.15188789367676

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1278 (0.1333)	Prec@1 99.219 (98.558)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1292 (0.1355)	Prec@1 99.609 (98.508)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.1464 (0.1356)	Prec@1 98.438 (98.501)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.1606 (0.1354)	Prec@1 97.656 (98.473)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1388 (0.1350)	Prec@1 97.266 (98.478)	
Total train loss: 0.1352

Train time: 94.35996055603027
 * Prec@1 63.200 Prec@5 86.810 Loss 1.5840
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 117.02585649490356

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.1364 (0.1332)	Prec@1 98.438 (98.347)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.1355 (0.1345)	Prec@1 97.266 (98.412)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.1559 (0.1359)	Prec@1 97.656 (98.404)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.1201 (0.1367)	Prec@1 99.219 (98.382)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.1459 (0.1372)	Prec@1 98.047 (98.365)	
Total train loss: 0.1375

Train time: 97.21736192703247
 * Prec@1 62.900 Prec@5 86.600 Loss 1.5947
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 118.6614682674408

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1510 (0.1295)	Prec@1 96.484 (98.578)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.1071 (0.1333)	Prec@1 99.609 (98.578)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1403 (0.1352)	Prec@1 98.438 (98.458)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.1137 (0.1349)	Prec@1 98.828 (98.490)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1298 (0.1347)	Prec@1 98.047 (98.536)	
Total train loss: 0.1350

Train time: 40.2695255279541
 * Prec@1 63.150 Prec@5 86.940 Loss 1.5811
Best acc: 64.390
--------------------------------------------------------------------------------
Test time: 46.911245346069336


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 38.410 Prec@5 66.830 Loss 2.9219
Pre-trained Prec@1 with 15 layers frozen: 38.40999984741211 	 Loss: 2.921875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.4307 (1.4894)	Prec@1 57.812 (58.283)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3906 (1.4586)	Prec@1 59.375 (58.804)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.1318 (1.4229)	Prec@1 65.625 (59.749)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.2568 (1.3906)	Prec@1 64.062 (60.557)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.3418 (1.3746)	Prec@1 64.844 (60.972)	
Total train loss: 1.3739

Train time: 622.7864291667938
 * Prec@1 54.390 Prec@5 82.370 Loss 1.7441
Best acc: 54.390
--------------------------------------------------------------------------------
Test time: 647.35125041008

Epoch: [1][38/196]	LR: 0.1	Loss 0.9888 (1.0981)	Prec@1 73.828 (67.999)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0859 (1.1140)	Prec@1 68.359 (67.869)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1113 (1.1229)	Prec@1 69.141 (67.511)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2598 (1.1268)	Prec@1 60.938 (67.413)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.2812 (1.1273)	Prec@1 63.281 (67.468)	
Total train loss: 1.1273

Train time: 88.65756273269653
 * Prec@1 57.900 Prec@5 84.570 Loss 1.6592
Best acc: 57.900
--------------------------------------------------------------------------------
Test time: 105.93526697158813

Epoch: [2][38/196]	LR: 0.1	Loss 0.9634 (0.9612)	Prec@1 71.875 (71.655)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.1201 (0.9873)	Prec@1 68.359 (71.229)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.0645 (1.0005)	Prec@1 66.406 (70.837)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.1895 (1.0018)	Prec@1 62.109 (70.733)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.9966 (1.0116)	Prec@1 68.750 (70.459)	
Total train loss: 1.0118

Train time: 85.59224462509155
 * Prec@1 55.290 Prec@5 83.220 Loss 1.7744
Best acc: 57.900
--------------------------------------------------------------------------------
Test time: 103.03550028800964

Epoch: [3][38/196]	LR: 0.1	Loss 0.8652 (0.8809)	Prec@1 75.000 (74.058)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.1143 (0.9056)	Prec@1 70.312 (73.443)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.8359 (0.9197)	Prec@1 74.219 (72.930)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.0283 (0.9312)	Prec@1 69.531 (72.544)	
Epoch: [3][194/196]	LR: 0.1	Loss 1.0361 (0.9406)	Prec@1 66.797 (72.322)	
Total train loss: 0.9404

Train time: 99.9831051826477
 * Prec@1 58.220 Prec@5 84.970 Loss 1.6289
Best acc: 58.220
--------------------------------------------------------------------------------
Test time: 124.46295380592346

Epoch: [4][38/196]	LR: 0.1	Loss 0.9097 (0.8068)	Prec@1 75.391 (76.593)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.9004 (0.8253)	Prec@1 75.000 (75.751)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.9087 (0.8438)	Prec@1 73.438 (75.120)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.8911 (0.8528)	Prec@1 71.875 (74.880)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.9609 (0.8670)	Prec@1 67.578 (74.427)	
Total train loss: 0.8671

Train time: 111.77561521530151
 * Prec@1 57.720 Prec@5 84.020 Loss 1.6943
Best acc: 58.220
--------------------------------------------------------------------------------
Test time: 133.8093843460083

Epoch: [5][38/196]	LR: 0.1	Loss 0.7070 (0.7434)	Prec@1 79.297 (77.945)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.8164 (0.7737)	Prec@1 76.953 (76.858)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.8008 (0.7787)	Prec@1 74.609 (76.609)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.7271 (0.7981)	Prec@1 79.688 (76.084)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.8613 (0.8105)	Prec@1 73.828 (75.767)	
Total train loss: 0.8107

Train time: 105.26924538612366
 * Prec@1 55.660 Prec@5 83.340 Loss 1.8340
Best acc: 58.220
--------------------------------------------------------------------------------
Test time: 125.84624910354614

Epoch: [6][38/196]	LR: 0.1	Loss 0.6333 (0.7001)	Prec@1 82.422 (79.537)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.8369 (0.7125)	Prec@1 73.828 (78.856)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.9399 (0.7325)	Prec@1 71.094 (78.125)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.7583 (0.7497)	Prec@1 77.734 (77.547)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.8496 (0.7608)	Prec@1 71.875 (77.169)	
Total train loss: 0.7612

Train time: 100.62676334381104
 * Prec@1 57.880 Prec@5 83.780 Loss 1.7646
Best acc: 58.220
--------------------------------------------------------------------------------
Test time: 123.44458198547363

Epoch: [7][38/196]	LR: 0.1	Loss 0.6377 (0.6336)	Prec@1 83.203 (81.340)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.5762 (0.6559)	Prec@1 83.984 (80.394)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.6592 (0.6742)	Prec@1 81.250 (79.694)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.7925 (0.6978)	Prec@1 78.125 (78.899)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.8555 (0.7155)	Prec@1 74.609 (78.343)	
Total train loss: 0.7155

Train time: 106.60207152366638
 * Prec@1 58.270 Prec@5 83.950 Loss 1.7539
Best acc: 58.270
--------------------------------------------------------------------------------
Test time: 129.92911744117737

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.4751 (0.5347)	Prec@1 85.156 (84.375)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.4380 (0.5133)	Prec@1 87.500 (85.171)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.4890 (0.4915)	Prec@1 88.672 (85.867)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3740 (0.4809)	Prec@1 88.281 (86.245)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.4617 (0.4733)	Prec@1 87.891 (86.520)	
Total train loss: 0.4735

Train time: 101.72007322311401
 * Prec@1 64.240 Prec@5 87.880 Loss 1.4346
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 122.55832123756409

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.5127 (0.3966)	Prec@1 83.594 (89.533)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3691 (0.3862)	Prec@1 88.672 (89.904)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.4087 (0.3886)	Prec@1 88.281 (89.727)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.3748 (0.3895)	Prec@1 90.625 (89.626)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.4817 (0.3913)	Prec@1 85.938 (89.553)	
Total train loss: 0.3913

Train time: 99.76984095573425
 * Prec@1 63.780 Prec@5 87.530 Loss 1.4619
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 121.04063892364502

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.3059 (0.3518)	Prec@1 91.797 (90.946)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3594 (0.3575)	Prec@1 91.797 (90.705)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2703 (0.3600)	Prec@1 93.359 (90.658)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.3821 (0.3592)	Prec@1 88.672 (90.640)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.2781 (0.3606)	Prec@1 94.531 (90.643)	
Total train loss: 0.3608

Train time: 111.91091895103455
 * Prec@1 63.400 Prec@5 87.410 Loss 1.4824
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 132.7303764820099

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2974 (0.3312)	Prec@1 93.359 (92.107)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.4214 (0.3330)	Prec@1 88.281 (91.937)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.3135 (0.3351)	Prec@1 92.188 (91.834)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.3188 (0.3352)	Prec@1 91.797 (91.827)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.3752 (0.3365)	Prec@1 89.844 (91.709)	
Total train loss: 0.3367

Train time: 87.52372574806213
 * Prec@1 63.560 Prec@5 87.150 Loss 1.4912
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 92.02157020568848

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2252 (0.3102)	Prec@1 95.703 (93.039)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.3191 (0.3093)	Prec@1 92.578 (93.024)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.3047 (0.3132)	Prec@1 92.969 (92.758)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.3435 (0.3148)	Prec@1 90.625 (92.611)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.3210 (0.3189)	Prec@1 91.406 (92.442)	
Total train loss: 0.3194

Train time: 40.72981786727905
 * Prec@1 63.220 Prec@5 86.980 Loss 1.5107
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 47.75322771072388

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2639 (0.2959)	Prec@1 95.703 (93.480)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2729 (0.2974)	Prec@1 96.094 (93.404)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2729 (0.3012)	Prec@1 94.922 (93.256)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.3223 (0.3046)	Prec@1 92.188 (93.069)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.3696 (0.3071)	Prec@1 91.406 (92.921)	
Total train loss: 0.3072

Train time: 55.31492877006531
 * Prec@1 63.030 Prec@5 86.540 Loss 1.5352
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 69.13294887542725

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.3311 (0.2812)	Prec@1 90.234 (93.880)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2629 (0.2884)	Prec@1 95.312 (93.645)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2703 (0.2895)	Prec@1 96.094 (93.576)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3325 (0.2922)	Prec@1 90.234 (93.472)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2695 (0.2954)	Prec@1 95.703 (93.335)	
Total train loss: 0.2960

Train time: 226.03081798553467
 * Prec@1 63.060 Prec@5 86.300 Loss 1.5508
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 235.89071488380432

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2076 (0.2729)	Prec@1 96.094 (94.611)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.3267 (0.2770)	Prec@1 92.578 (94.286)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.3472 (0.2759)	Prec@1 91.016 (94.274)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2871 (0.2783)	Prec@1 93.359 (94.113)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.3210 (0.2817)	Prec@1 92.578 (94.010)	
Total train loss: 0.2818

Train time: 60.01322793960571
 * Prec@1 62.660 Prec@5 86.280 Loss 1.5537
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 81.01854753494263

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.2115 (0.2664)	Prec@1 96.484 (94.571)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.2448 (0.2609)	Prec@1 94.922 (94.752)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2134 (0.2597)	Prec@1 95.703 (94.832)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.2419 (0.2578)	Prec@1 96.484 (94.894)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2976 (0.2575)	Prec@1 93.359 (94.904)	
Total train loss: 0.2576

Train time: 91.5727231502533
 * Prec@1 62.660 Prec@5 86.160 Loss 1.5537
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 111.74887275695801

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2053 (0.2597)	Prec@1 96.875 (94.641)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2266 (0.2578)	Prec@1 95.312 (94.877)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.2169 (0.2570)	Prec@1 95.703 (94.915)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.2930 (0.2600)	Prec@1 93.750 (94.842)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2410 (0.2576)	Prec@1 94.922 (94.882)	
Total train loss: 0.2577

Train time: 92.3684504032135
 * Prec@1 62.760 Prec@5 86.160 Loss 1.5586
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 114.82248711585999

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2100 (0.2553)	Prec@1 97.656 (95.262)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.2515 (0.2535)	Prec@1 94.531 (95.297)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2384 (0.2541)	Prec@1 96.094 (95.229)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2964 (0.2548)	Prec@1 93.359 (95.162)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.2668 (0.2549)	Prec@1 93.750 (95.134)	
Total train loss: 0.2551

Train time: 103.79905104637146
 * Prec@1 62.600 Prec@5 86.290 Loss 1.5635
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 126.60869073867798

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2869 (0.2557)	Prec@1 93.750 (94.972)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2227 (0.2565)	Prec@1 95.312 (94.947)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.3110 (0.2531)	Prec@1 92.969 (95.079)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2494 (0.2535)	Prec@1 95.703 (95.087)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.2576 (0.2540)	Prec@1 94.531 (95.092)	
Total train loss: 0.2542

Train time: 95.27151083946228
 * Prec@1 62.660 Prec@5 86.250 Loss 1.5596
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 114.21208047866821

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2856 (0.2586)	Prec@1 94.141 (95.142)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.2207 (0.2557)	Prec@1 95.703 (95.047)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2627 (0.2565)	Prec@1 94.531 (94.992)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2457 (0.2559)	Prec@1 96.484 (95.015)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.2174 (0.2541)	Prec@1 97.266 (95.096)	
Total train loss: 0.2542

Train time: 88.04564785957336
 * Prec@1 62.570 Prec@5 86.190 Loss 1.5654
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 106.82063364982605

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.2603 (0.2581)	Prec@1 93.750 (94.982)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2603 (0.2544)	Prec@1 94.531 (95.102)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.2683 (0.2554)	Prec@1 95.312 (95.069)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2369 (0.2539)	Prec@1 94.531 (95.077)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.2515 (0.2538)	Prec@1 95.312 (95.104)	
Total train loss: 0.2538

Train time: 100.09678912162781
 * Prec@1 62.870 Prec@5 86.150 Loss 1.5596
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 121.89993858337402

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2637 (0.2491)	Prec@1 95.312 (95.383)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2717 (0.2558)	Prec@1 92.578 (95.022)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2524 (0.2554)	Prec@1 95.312 (95.069)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2756 (0.2558)	Prec@1 92.969 (94.994)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2164 (0.2547)	Prec@1 97.266 (95.030)	
Total train loss: 0.2549

Train time: 101.27333450317383
 * Prec@1 62.730 Prec@5 86.170 Loss 1.5625
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 120.75921821594238

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.2844 (0.2578)	Prec@1 94.531 (94.902)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.3386 (0.2575)	Prec@1 92.188 (94.977)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.2352 (0.2563)	Prec@1 96.484 (95.062)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2441 (0.2533)	Prec@1 94.141 (95.127)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2524 (0.2528)	Prec@1 96.094 (95.130)	
Total train loss: 0.2529

Train time: 104.4221613407135
 * Prec@1 62.860 Prec@5 86.200 Loss 1.5605
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 124.71543622016907

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.2915 (0.2526)	Prec@1 93.359 (95.252)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2800 (0.2562)	Prec@1 94.531 (95.167)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2435 (0.2532)	Prec@1 95.703 (95.192)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2190 (0.2526)	Prec@1 96.094 (95.202)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.2412 (0.2531)	Prec@1 96.875 (95.164)	
Total train loss: 0.2535

Train time: 274.8965859413147
 * Prec@1 62.960 Prec@5 86.290 Loss 1.5566
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 299.9240970611572

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2686 (0.2634)	Prec@1 92.969 (94.641)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2354 (0.2592)	Prec@1 96.094 (94.972)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2183 (0.2590)	Prec@1 98.438 (95.025)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2013 (0.2560)	Prec@1 96.875 (95.097)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.2418 (0.2536)	Prec@1 94.531 (95.134)	
Total train loss: 0.2539

Train time: 90.75084590911865
 * Prec@1 62.760 Prec@5 86.310 Loss 1.5576
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 112.39295053482056

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.3174 (0.2584)	Prec@1 93.359 (95.032)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.2012 (0.2508)	Prec@1 96.875 (95.262)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2427 (0.2532)	Prec@1 93.750 (95.146)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.2544 (0.2517)	Prec@1 96.484 (95.205)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.2444 (0.2516)	Prec@1 96.484 (95.166)	
Total train loss: 0.2516

Train time: 102.88304305076599
 * Prec@1 62.640 Prec@5 86.180 Loss 1.5557
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 123.5768141746521

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.2211 (0.2430)	Prec@1 95.312 (95.333)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.3130 (0.2508)	Prec@1 92.969 (95.297)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2260 (0.2533)	Prec@1 95.312 (95.159)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2725 (0.2522)	Prec@1 93.750 (95.202)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.3169 (0.2523)	Prec@1 93.750 (95.190)	
Total train loss: 0.2525

Train time: 104.86191296577454
 * Prec@1 62.690 Prec@5 86.140 Loss 1.5664
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 126.86104202270508

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2898 (0.2511)	Prec@1 93.750 (95.353)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2454 (0.2528)	Prec@1 95.312 (95.257)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2385 (0.2510)	Prec@1 95.703 (95.239)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2042 (0.2520)	Prec@1 95.703 (95.160)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.2039 (0.2522)	Prec@1 98.047 (95.194)	
Total train loss: 0.2523

Train time: 36.41599655151367
 * Prec@1 62.630 Prec@5 86.350 Loss 1.5605
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 39.77935290336609

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.1910 (0.2503)	Prec@1 96.875 (95.182)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.3167 (0.2525)	Prec@1 91.406 (95.002)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.1964 (0.2524)	Prec@1 97.266 (95.059)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2050 (0.2529)	Prec@1 97.266 (95.102)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.2484 (0.2532)	Prec@1 94.531 (95.060)	
Total train loss: 0.2534

Train time: 37.09703183174133
 * Prec@1 62.670 Prec@5 86.350 Loss 1.5596
Best acc: 64.240
--------------------------------------------------------------------------------
Test time: 46.46756434440613


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 32.430 Prec@5 62.340 Loss 3.4648
Pre-trained Prec@1 with 17 layers frozen: 32.43000030517578 	 Loss: 3.46484375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.4912 (1.5463)	Prec@1 58.984 (57.692)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3564 (1.4637)	Prec@1 62.500 (59.390)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.2373 (1.4182)	Prec@1 66.406 (60.450)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.1689 (1.3952)	Prec@1 67.578 (61.023)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.2295 (1.3693)	Prec@1 62.891 (61.675)	
Total train loss: 1.3692

Train time: 595.8440124988556
 * Prec@1 57.270 Prec@5 84.460 Loss 1.6621
Best acc: 57.270
--------------------------------------------------------------------------------
Test time: 625.0189490318298

Epoch: [1][38/196]	LR: 0.1	Loss 1.0088 (1.1386)	Prec@1 70.703 (67.668)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0957 (1.1410)	Prec@1 71.094 (67.318)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.1895 (1.1505)	Prec@1 64.844 (67.054)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2490 (1.1528)	Prec@1 63.672 (66.987)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.1553 (1.1573)	Prec@1 66.406 (66.819)	
Total train loss: 1.1574

Train time: 106.9068055152893
 * Prec@1 57.180 Prec@5 84.320 Loss 1.6416
Best acc: 57.270
--------------------------------------------------------------------------------
Test time: 127.07408928871155

Epoch: [2][38/196]	LR: 0.1	Loss 1.1025 (1.0372)	Prec@1 67.188 (69.832)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.1855 (1.0385)	Prec@1 67.578 (69.977)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.1064 (1.0571)	Prec@1 68.359 (69.411)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.9937 (1.0697)	Prec@1 69.141 (68.998)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.1045 (1.0743)	Prec@1 67.188 (68.858)	
Total train loss: 1.0746

Train time: 100.87453293800354
 * Prec@1 59.490 Prec@5 85.680 Loss 1.5654
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 121.37561440467834

Epoch: [3][38/196]	LR: 0.1	Loss 0.8823 (0.9552)	Prec@1 71.875 (71.965)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.9893 (0.9829)	Prec@1 70.312 (70.903)	
Epoch: [3][116/196]	LR: 0.1	Loss 1.1182 (0.9989)	Prec@1 70.312 (70.563)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.1426 (1.0095)	Prec@1 67.969 (70.295)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.8682 (1.0173)	Prec@1 72.656 (70.174)	
Total train loss: 1.0175

Train time: 104.11688232421875
 * Prec@1 58.170 Prec@5 84.910 Loss 1.6562
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 126.84598207473755

Epoch: [4][38/196]	LR: 0.1	Loss 0.8418 (0.9174)	Prec@1 75.391 (73.397)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.9839 (0.9367)	Prec@1 71.094 (72.626)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.8911 (0.9496)	Prec@1 76.172 (72.185)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.9092 (0.9534)	Prec@1 72.656 (72.000)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.9287 (0.9641)	Prec@1 71.875 (71.681)	
Total train loss: 0.9644

Train time: 108.18524193763733
 * Prec@1 58.550 Prec@5 85.420 Loss 1.6387
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 127.3277177810669

Epoch: [5][38/196]	LR: 0.1	Loss 0.8237 (0.8727)	Prec@1 77.734 (74.379)	
Epoch: [5][77/196]	LR: 0.1	Loss 1.0293 (0.8880)	Prec@1 71.875 (73.683)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.8545 (0.9053)	Prec@1 75.781 (73.257)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.9551 (0.9178)	Prec@1 74.609 (72.899)	
Epoch: [5][194/196]	LR: 0.1	Loss 1.0332 (0.9274)	Prec@1 70.703 (72.600)	
Total train loss: 0.9278

Train time: 103.14656329154968
 * Prec@1 57.800 Prec@5 84.890 Loss 1.7041
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 126.25446343421936

Epoch: [6][38/196]	LR: 0.1	Loss 0.7896 (0.8271)	Prec@1 74.609 (75.731)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.7993 (0.8463)	Prec@1 75.391 (75.075)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.9902 (0.8660)	Prec@1 72.266 (74.406)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.9829 (0.8792)	Prec@1 66.797 (73.866)	
Epoch: [6][194/196]	LR: 0.1	Loss 1.0537 (0.8841)	Prec@1 71.484 (73.748)	
Total train loss: 0.8851

Train time: 106.07707738876343
 * Prec@1 58.060 Prec@5 84.160 Loss 1.7568
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 127.3719334602356

Epoch: [7][38/196]	LR: 0.1	Loss 0.8208 (0.7997)	Prec@1 75.391 (76.532)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.8931 (0.8129)	Prec@1 75.000 (75.972)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.7695 (0.8259)	Prec@1 78.516 (75.624)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.9199 (0.8409)	Prec@1 71.875 (75.128)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.9746 (0.8536)	Prec@1 72.266 (74.738)	
Total train loss: 0.8538

Train time: 100.21948003768921
 * Prec@1 58.620 Prec@5 84.880 Loss 1.6992
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 120.48077249526978

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.6069 (0.7084)	Prec@1 83.203 (78.836)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.7598 (0.6904)	Prec@1 76.172 (79.482)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.5508 (0.6771)	Prec@1 85.156 (80.075)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.6094 (0.6686)	Prec@1 83.594 (80.404)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.5391 (0.6678)	Prec@1 82.422 (80.495)	
Total train loss: 0.6680

Train time: 104.90456128120422
 * Prec@1 62.790 Prec@5 87.450 Loss 1.4561
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 126.37791132926941

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.5981 (0.6101)	Prec@1 84.375 (82.382)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.5854 (0.6112)	Prec@1 83.984 (82.582)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.6250 (0.6162)	Prec@1 79.297 (82.388)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.7124 (0.6186)	Prec@1 82.031 (82.342)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.5405 (0.6192)	Prec@1 83.594 (82.260)	
Total train loss: 0.6193

Train time: 102.901695728302
 * Prec@1 62.710 Prec@5 87.340 Loss 1.4824
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 123.12006711959839

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.5371 (0.5714)	Prec@1 84.375 (84.075)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.6099 (0.5865)	Prec@1 80.469 (83.233)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.6353 (0.5871)	Prec@1 81.641 (83.317)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.6626 (0.5957)	Prec@1 80.859 (83.100)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.6997 (0.5982)	Prec@1 79.297 (82.989)	
Total train loss: 0.5985

Train time: 104.94658875465393
 * Prec@1 62.250 Prec@5 87.330 Loss 1.4844
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 123.40847945213318

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.4714 (0.5846)	Prec@1 87.500 (83.183)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.7061 (0.5714)	Prec@1 80.469 (83.914)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.5776 (0.5796)	Prec@1 82.812 (83.651)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.5259 (0.5808)	Prec@1 85.156 (83.626)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.5029 (0.5818)	Prec@1 85.156 (83.526)	
Total train loss: 0.5818

Train time: 54.064653635025024
 * Prec@1 62.260 Prec@5 87.230 Loss 1.4980
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 58.82538032531738

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.5039 (0.5312)	Prec@1 87.109 (85.407)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.5596 (0.5548)	Prec@1 82.422 (84.620)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.5444 (0.5645)	Prec@1 85.156 (84.258)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.5181 (0.5680)	Prec@1 83.984 (84.160)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.6260 (0.5689)	Prec@1 83.984 (84.147)	
Total train loss: 0.5692

Train time: 44.50797152519226
 * Prec@1 62.190 Prec@5 87.050 Loss 1.5107
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 52.10893416404724

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.6670 (0.5440)	Prec@1 79.688 (85.236)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.5518 (0.5439)	Prec@1 83.594 (85.191)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.6333 (0.5498)	Prec@1 83.203 (84.919)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.5674 (0.5576)	Prec@1 82.812 (84.653)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.5156 (0.5581)	Prec@1 84.766 (84.567)	
Total train loss: 0.5584

Train time: 52.410258293151855
 * Prec@1 61.900 Prec@5 87.080 Loss 1.5225
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 63.30905270576477

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.6567 (0.5386)	Prec@1 82.812 (84.966)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.5781 (0.5414)	Prec@1 83.203 (84.956)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.5483 (0.5444)	Prec@1 82.422 (84.829)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.4893 (0.5471)	Prec@1 85.547 (84.751)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.5942 (0.5509)	Prec@1 83.203 (84.653)	
Total train loss: 0.5514

Train time: 47.16306757926941
 * Prec@1 61.960 Prec@5 87.010 Loss 1.5293
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 53.86829733848572

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.5327 (0.5419)	Prec@1 85.547 (85.186)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.4600 (0.5433)	Prec@1 86.719 (85.106)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.5859 (0.5419)	Prec@1 83.594 (84.993)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.6460 (0.5447)	Prec@1 80.469 (84.848)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.5640 (0.5457)	Prec@1 82.031 (84.812)	
Total train loss: 0.5458

Train time: 51.161925315856934
 * Prec@1 62.000 Prec@5 86.790 Loss 1.5322
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 62.01292061805725

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.5083 (0.5225)	Prec@1 85.938 (86.318)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.5015 (0.5202)	Prec@1 85.938 (86.123)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.5293 (0.5160)	Prec@1 87.109 (86.248)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.5620 (0.5193)	Prec@1 87.500 (86.150)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.4766 (0.5180)	Prec@1 86.328 (86.108)	
Total train loss: 0.5180

Train time: 48.48696422576904
 * Prec@1 62.010 Prec@5 86.850 Loss 1.5400
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 52.93567419052124

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.5386 (0.5231)	Prec@1 83.594 (85.727)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.4670 (0.5168)	Prec@1 87.500 (86.023)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.5420 (0.5170)	Prec@1 85.156 (86.108)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.5322 (0.5178)	Prec@1 84.375 (86.078)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.4521 (0.5174)	Prec@1 87.891 (86.112)	
Total train loss: 0.5176

Train time: 30.683013200759888
 * Prec@1 62.170 Prec@5 86.850 Loss 1.5244
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 45.196144819259644

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.4524 (0.5152)	Prec@1 87.500 (86.008)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.5796 (0.5202)	Prec@1 83.203 (85.968)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.4922 (0.5167)	Prec@1 89.062 (86.041)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.5044 (0.5162)	Prec@1 89.453 (86.120)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.5435 (0.5172)	Prec@1 85.156 (86.052)	
Total train loss: 0.5176

Train time: 67.26848149299622
 * Prec@1 62.050 Prec@5 86.830 Loss 1.5283
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 83.80158424377441

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.5596 (0.5080)	Prec@1 84.375 (86.599)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.5200 (0.5109)	Prec@1 87.500 (86.388)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.5210 (0.5141)	Prec@1 84.766 (86.218)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.4028 (0.5164)	Prec@1 91.016 (86.150)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.5312 (0.5162)	Prec@1 83.203 (86.104)	
Total train loss: 0.5164

Train time: 117.1444354057312
 * Prec@1 61.660 Prec@5 86.890 Loss 1.5342
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 138.37218022346497

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.5059 (0.5159)	Prec@1 89.062 (85.978)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.5347 (0.5229)	Prec@1 86.719 (85.948)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.5166 (0.5150)	Prec@1 86.328 (86.195)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.5024 (0.5115)	Prec@1 89.062 (86.363)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.5195 (0.5155)	Prec@1 87.500 (86.242)	
Total train loss: 0.5155

Train time: 131.04797506332397
 * Prec@1 62.030 Prec@5 86.970 Loss 1.5273
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 155.95767331123352

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.5464 (0.5079)	Prec@1 85.938 (86.538)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.5093 (0.5081)	Prec@1 87.109 (86.584)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.4954 (0.5122)	Prec@1 87.109 (86.335)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.5161 (0.5140)	Prec@1 87.891 (86.298)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.6270 (0.5132)	Prec@1 81.250 (86.306)	
Total train loss: 0.5136

Train time: 138.53266763687134
 * Prec@1 61.890 Prec@5 86.890 Loss 1.5352
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 189.02863478660583

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.4705 (0.5284)	Prec@1 87.891 (85.897)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.5337 (0.5175)	Prec@1 85.156 (85.917)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.5024 (0.5155)	Prec@1 85.938 (86.081)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.5244 (0.5156)	Prec@1 84.766 (86.068)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.5962 (0.5171)	Prec@1 83.984 (86.052)	
Total train loss: 0.5171

Train time: 109.39061832427979
 * Prec@1 61.860 Prec@5 86.970 Loss 1.5312
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 132.89945793151855

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.4419 (0.5176)	Prec@1 89.062 (86.118)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.5000 (0.5184)	Prec@1 84.766 (86.098)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.5000 (0.5128)	Prec@1 89.062 (86.382)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.4705 (0.5148)	Prec@1 89.062 (86.253)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.4780 (0.5139)	Prec@1 85.938 (86.334)	
Total train loss: 0.5143

Train time: 94.23657393455505
 * Prec@1 61.960 Prec@5 86.930 Loss 1.5283
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 121.51330709457397

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.5542 (0.5088)	Prec@1 85.547 (86.448)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.5288 (0.5068)	Prec@1 84.375 (86.533)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.5620 (0.5097)	Prec@1 85.547 (86.452)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.4907 (0.5101)	Prec@1 85.547 (86.386)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.5552 (0.5099)	Prec@1 85.156 (86.340)	
Total train loss: 0.5101

Train time: 104.82310581207275
 * Prec@1 61.870 Prec@5 86.960 Loss 1.5342
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 122.96419215202332

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.4368 (0.5066)	Prec@1 87.891 (86.428)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.4802 (0.5108)	Prec@1 87.891 (86.318)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.5791 (0.5148)	Prec@1 84.766 (86.281)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.4614 (0.5136)	Prec@1 85.938 (86.263)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.5151 (0.5131)	Prec@1 85.938 (86.332)	
Total train loss: 0.5130

Train time: 91.20744585990906
 * Prec@1 62.220 Prec@5 86.710 Loss 1.5381
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 108.75200414657593

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.5444 (0.5262)	Prec@1 85.156 (85.727)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.5298 (0.5155)	Prec@1 86.719 (86.148)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.4973 (0.5156)	Prec@1 85.547 (86.118)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.4778 (0.5148)	Prec@1 86.719 (86.233)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.5205 (0.5133)	Prec@1 87.500 (86.282)	
Total train loss: 0.5135

Train time: 89.19109845161438
 * Prec@1 61.900 Prec@5 86.930 Loss 1.5371
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 108.89649820327759

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.4846 (0.5114)	Prec@1 87.500 (86.248)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.4932 (0.5111)	Prec@1 87.109 (86.393)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.5225 (0.5141)	Prec@1 87.891 (86.181)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.4929 (0.5140)	Prec@1 85.938 (86.218)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.4475 (0.5122)	Prec@1 89.453 (86.286)	
Total train loss: 0.5126

Train time: 100.57855081558228
 * Prec@1 62.020 Prec@5 86.890 Loss 1.5332
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 119.92280793190002

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.4727 (0.5093)	Prec@1 87.891 (86.498)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.5859 (0.5084)	Prec@1 84.375 (86.503)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.4619 (0.5123)	Prec@1 88.672 (86.388)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.4556 (0.5132)	Prec@1 88.672 (86.341)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.4629 (0.5110)	Prec@1 87.891 (86.382)	
Total train loss: 0.5114

Train time: 107.59690856933594
 * Prec@1 61.830 Prec@5 86.780 Loss 1.5332
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 127.88260054588318

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.5190 (0.5158)	Prec@1 85.547 (86.428)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.4893 (0.5092)	Prec@1 88.672 (86.589)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.4146 (0.5107)	Prec@1 90.234 (86.455)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.4919 (0.5105)	Prec@1 87.891 (86.531)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.4968 (0.5130)	Prec@1 84.375 (86.380)	
Total train loss: 0.5130

Train time: 114.7989673614502
 * Prec@1 62.070 Prec@5 86.860 Loss 1.5264
Best acc: 62.790
--------------------------------------------------------------------------------
Test time: 141.95889520645142


      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=100, bias=False)
  (bn21): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 29.450 Prec@5 59.380 Loss 3.9395
Pre-trained Prec@1 with 19 layers frozen: 29.44999885559082 	 Loss: 3.939453125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.5762 (1.6326)	Prec@1 58.203 (56.410)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.3213 (1.5518)	Prec@1 63.672 (58.288)	
Epoch: [0][116/196]	LR: 0.1	Loss 1.4385 (1.5242)	Prec@1 57.812 (58.797)	
Epoch: [0][155/196]	LR: 0.1	Loss 1.4033 (1.4932)	Prec@1 58.203 (59.465)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.4707 (1.4794)	Prec@1 56.250 (59.730)	
Total train loss: 1.4794

Train time: 627.1669447422028
 * Prec@1 58.550 Prec@5 84.450 Loss 1.5879
Best acc: 58.550
--------------------------------------------------------------------------------
Test time: 676.723260641098

Epoch: [1][38/196]	LR: 0.1	Loss 1.2725 (1.3670)	Prec@1 61.328 (62.220)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.3574 (1.3589)	Prec@1 62.109 (62.675)	
Epoch: [1][116/196]	LR: 0.1	Loss 1.3223 (1.3681)	Prec@1 61.328 (62.390)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.2549 (1.3686)	Prec@1 63.281 (62.282)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.4512 (1.3683)	Prec@1 59.375 (62.181)	
Total train loss: 1.3683

Train time: 313.0727872848511
 * Prec@1 59.100 Prec@5 85.030 Loss 1.5547
Best acc: 59.100
--------------------------------------------------------------------------------
Test time: 353.26511311531067

Epoch: [2][38/196]	LR: 0.1	Loss 1.3887 (1.3176)	Prec@1 60.547 (63.682)	
Epoch: [2][77/196]	LR: 0.1	Loss 1.1963 (1.3212)	Prec@1 66.016 (63.627)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.3242 (1.3352)	Prec@1 64.844 (63.054)	
Epoch: [2][155/196]	LR: 0.1	Loss 1.2812 (1.3380)	Prec@1 60.938 (62.878)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.3438 (1.3402)	Prec@1 63.672 (62.704)	
Total train loss: 1.3399

Train time: 223.45831489562988
 * Prec@1 58.960 Prec@5 84.740 Loss 1.5596
Best acc: 59.100
--------------------------------------------------------------------------------
Test time: 255.78224229812622

Epoch: [3][38/196]	LR: 0.1	Loss 1.3027 (1.3081)	Prec@1 65.625 (64.012)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.2266 (1.3057)	Prec@1 66.406 (63.902)	
Epoch: [3][116/196]	LR: 0.1	Loss 1.2070 (1.3076)	Prec@1 65.625 (63.628)	
Epoch: [3][155/196]	LR: 0.1	Loss 1.2695 (1.3144)	Prec@1 67.578 (63.326)	
Epoch: [3][194/196]	LR: 0.1	Loss 1.2480 (1.3239)	Prec@1 62.891 (63.073)	
Total train loss: 1.3238

Train time: 122.56156492233276
 * Prec@1 59.110 Prec@5 85.360 Loss 1.5469
Best acc: 59.110
--------------------------------------------------------------------------------
Test time: 142.48514699935913

Epoch: [4][38/196]	LR: 0.1	Loss 1.2070 (1.2711)	Prec@1 66.797 (64.433)	
Epoch: [4][77/196]	LR: 0.1	Loss 1.2197 (1.2780)	Prec@1 64.844 (64.288)	
Epoch: [4][116/196]	LR: 0.1	Loss 1.3223 (1.2925)	Prec@1 60.938 (63.916)	
Epoch: [4][155/196]	LR: 0.1	Loss 1.4141 (1.3002)	Prec@1 58.203 (63.627)	
Epoch: [4][194/196]	LR: 0.1	Loss 1.3613 (1.3069)	Prec@1 62.500 (63.450)	
Total train loss: 1.3077

Train time: 110.67522954940796
 * Prec@1 59.490 Prec@5 85.150 Loss 1.5547
Best acc: 59.490
--------------------------------------------------------------------------------
Test time: 134.820481300354

Epoch: [5][38/196]	LR: 0.1	Loss 1.2969 (1.2913)	Prec@1 65.625 (63.852)	
Epoch: [5][77/196]	LR: 0.1	Loss 1.3232 (1.2967)	Prec@1 64.453 (63.707)	
Epoch: [5][116/196]	LR: 0.1	Loss 1.3857 (1.3031)	Prec@1 63.281 (63.408)	
Epoch: [5][155/196]	LR: 0.1	Loss 1.2471 (1.2981)	Prec@1 66.797 (63.447)	
Epoch: [5][194/196]	LR: 0.1	Loss 1.2305 (1.3015)	Prec@1 64.844 (63.383)	
Total train loss: 1.3011

Train time: 121.80566358566284
 * Prec@1 59.950 Prec@5 85.390 Loss 1.5430
Best acc: 59.950
--------------------------------------------------------------------------------
Test time: 145.26064038276672

Epoch: [6][38/196]	LR: 0.1	Loss 1.3252 (1.2561)	Prec@1 63.281 (64.563)	
Epoch: [6][77/196]	LR: 0.1	Loss 1.3711 (1.2809)	Prec@1 61.719 (63.872)	
Epoch: [6][116/196]	LR: 0.1	Loss 1.2744 (1.2876)	Prec@1 63.672 (63.705)	
Epoch: [6][155/196]	LR: 0.1	Loss 1.3613 (1.2898)	Prec@1 61.328 (63.664)	
Epoch: [6][194/196]	LR: 0.1	Loss 1.0947 (1.2953)	Prec@1 69.141 (63.452)	
Total train loss: 1.2952

Train time: 121.32576727867126
 * Prec@1 59.530 Prec@5 85.240 Loss 1.5420
Best acc: 59.950
--------------------------------------------------------------------------------
Test time: 144.73124408721924

Epoch: [7][38/196]	LR: 0.1	Loss 1.2666 (1.2715)	Prec@1 64.844 (64.283)	
Epoch: [7][77/196]	LR: 0.1	Loss 1.2549 (1.2727)	Prec@1 66.406 (64.418)	
Epoch: [7][116/196]	LR: 0.1	Loss 1.3682 (1.2717)	Prec@1 60.547 (64.246)	
Epoch: [7][155/196]	LR: 0.1	Loss 1.3896 (1.2787)	Prec@1 63.281 (64.143)	
Epoch: [7][194/196]	LR: 0.1	Loss 1.3223 (1.2880)	Prec@1 60.156 (63.956)	
Total train loss: 1.2880

Train time: 122.45709276199341
 * Prec@1 59.390 Prec@5 85.600 Loss 1.5527
Best acc: 59.950
--------------------------------------------------------------------------------
Test time: 147.9260070323944

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 1.0742 (1.2199)	Prec@1 69.922 (65.585)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 1.3008 (1.2372)	Prec@1 65.234 (65.049)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 1.1895 (1.2439)	Prec@1 66.406 (65.014)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 1.2881 (1.2389)	Prec@1 64.453 (65.194)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 1.2314 (1.2349)	Prec@1 60.547 (65.172)	
Total train loss: 1.2351

Train time: 122.8513011932373
 * Prec@1 60.230 Prec@5 85.780 Loss 1.5205
Best acc: 60.230
--------------------------------------------------------------------------------
Test time: 145.14266228675842

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 1.3721 (1.2274)	Prec@1 62.500 (65.655)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 1.1543 (1.2282)	Prec@1 66.016 (65.345)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 1.0430 (1.2220)	Prec@1 72.656 (65.435)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 1.2354 (1.2230)	Prec@1 70.312 (65.520)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 1.2559 (1.2265)	Prec@1 62.891 (65.317)	
Total train loss: 1.2272

Train time: 64.01198172569275
 * Prec@1 60.200 Prec@5 85.720 Loss 1.5225
Best acc: 60.230
--------------------------------------------------------------------------------
Test time: 67.75796127319336

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 1.2441 (1.2368)	Prec@1 64.844 (65.695)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 1.3584 (1.2119)	Prec@1 63.672 (66.326)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 1.1621 (1.2210)	Prec@1 67.969 (65.845)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 1.2471 (1.2262)	Prec@1 65.625 (65.617)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 1.1387 (1.2234)	Prec@1 65.625 (65.699)	
Total train loss: 1.2236

Train time: 45.42336702346802
 * Prec@1 60.210 Prec@5 85.800 Loss 1.5205
Best acc: 60.230
--------------------------------------------------------------------------------
Test time: 52.584840297698975

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 1.1465 (1.2281)	Prec@1 65.234 (65.515)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 1.3447 (1.2247)	Prec@1 60.547 (65.320)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 1.1514 (1.2222)	Prec@1 67.578 (65.682)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 1.3555 (1.2234)	Prec@1 60.938 (65.745)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 1.1836 (1.2219)	Prec@1 62.891 (65.713)	
Total train loss: 1.2221

Train time: 45.15935015678406
 * Prec@1 60.160 Prec@5 85.840 Loss 1.5186
Best acc: 60.230
--------------------------------------------------------------------------------
Test time: 56.23629140853882

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 1.3711 (1.2359)	Prec@1 64.453 (65.335)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 1.1953 (1.2327)	Prec@1 66.406 (65.209)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 1.2705 (1.2220)	Prec@1 67.578 (65.678)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 1.1475 (1.2269)	Prec@1 71.484 (65.577)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 1.2471 (1.2218)	Prec@1 67.578 (65.705)	
Total train loss: 1.2218

Train time: 37.35798501968384
 * Prec@1 60.600 Prec@5 85.740 Loss 1.5215
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 44.17905282974243

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 1.3613 (1.2163)	Prec@1 62.500 (65.685)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 1.2529 (1.2221)	Prec@1 62.891 (65.805)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 1.1650 (1.2171)	Prec@1 69.922 (65.962)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 1.2139 (1.2209)	Prec@1 69.531 (65.675)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 1.1602 (1.2233)	Prec@1 66.406 (65.583)	
Total train loss: 1.2235

Train time: 49.34116721153259
 * Prec@1 60.210 Prec@5 85.800 Loss 1.5205
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 62.40561389923096

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 1.2275 (1.2207)	Prec@1 66.406 (65.865)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 1.2256 (1.2192)	Prec@1 62.500 (65.715)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 1.2881 (1.2194)	Prec@1 64.844 (65.635)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 1.2646 (1.2205)	Prec@1 64.844 (65.765)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 1.1670 (1.2222)	Prec@1 66.016 (65.589)	
Total train loss: 1.2224

Train time: 52.84140419960022
 * Prec@1 60.360 Prec@5 85.730 Loss 1.5195
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 57.220762968063354

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 1.1650 (1.2223)	Prec@1 66.016 (65.455)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 1.2295 (1.2279)	Prec@1 64.453 (65.325)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 1.2432 (1.2188)	Prec@1 67.969 (65.662)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 1.1182 (1.2203)	Prec@1 66.016 (65.703)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 1.1455 (1.2209)	Prec@1 68.359 (65.751)	
Total train loss: 1.2213

Train time: 22.14497947692871
 * Prec@1 60.300 Prec@5 85.920 Loss 1.5205
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 24.73511815071106

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 1.2139 (1.1893)	Prec@1 64.453 (66.526)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 1.1475 (1.2112)	Prec@1 69.531 (65.835)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 1.2939 (1.2166)	Prec@1 62.109 (65.622)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 1.2959 (1.2184)	Prec@1 63.281 (65.708)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 1.3438 (1.2147)	Prec@1 62.891 (65.819)	
Total train loss: 1.2146

Train time: 73.24251747131348
 * Prec@1 60.190 Prec@5 85.740 Loss 1.5205
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 91.0505485534668

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 1.1924 (1.2091)	Prec@1 65.234 (66.306)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 1.1699 (1.2050)	Prec@1 67.969 (66.136)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 1.3184 (1.2162)	Prec@1 60.938 (65.779)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 1.2051 (1.2156)	Prec@1 64.844 (65.813)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 1.2178 (1.2135)	Prec@1 65.234 (65.867)	
Total train loss: 1.2133

Train time: 92.3556706905365
 * Prec@1 60.180 Prec@5 85.880 Loss 1.5205
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 113.13688564300537

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 1.1582 (1.1962)	Prec@1 67.969 (66.506)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 1.2334 (1.2096)	Prec@1 64.453 (65.986)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 1.1270 (1.2133)	Prec@1 67.188 (65.885)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 1.2637 (1.2107)	Prec@1 62.891 (65.845)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 1.1836 (1.2154)	Prec@1 67.188 (65.749)	
Total train loss: 1.2151

Train time: 89.05468559265137
 * Prec@1 60.300 Prec@5 85.830 Loss 1.5215
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 108.48002576828003

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 1.2285 (1.2275)	Prec@1 62.500 (64.964)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 1.1777 (1.2091)	Prec@1 67.969 (65.910)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 1.2812 (1.2121)	Prec@1 64.062 (65.879)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 1.1729 (1.2136)	Prec@1 67.578 (65.810)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 1.2227 (1.2106)	Prec@1 62.500 (65.867)	
Total train loss: 1.2104

Train time: 92.50647783279419
 * Prec@1 60.060 Prec@5 85.800 Loss 1.5195
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 178.04392290115356

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 1.1982 (1.2048)	Prec@1 64.453 (65.315)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 1.2939 (1.2113)	Prec@1 64.062 (65.550)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 1.1758 (1.2153)	Prec@1 65.625 (65.725)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 1.3203 (1.2154)	Prec@1 60.156 (65.783)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 1.1533 (1.2128)	Prec@1 67.578 (65.889)	
Total train loss: 1.2129

Train time: 199.5691692829132
 * Prec@1 60.370 Prec@5 85.810 Loss 1.5195
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 221.62273931503296

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 1.1953 (1.2123)	Prec@1 68.359 (65.956)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 1.2070 (1.2278)	Prec@1 63.672 (65.304)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 1.2178 (1.2154)	Prec@1 63.281 (65.742)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 1.1963 (1.2122)	Prec@1 66.016 (65.840)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 1.2881 (1.2140)	Prec@1 65.234 (65.875)	
Total train loss: 1.2141

Train time: 88.79112434387207
 * Prec@1 60.290 Prec@5 85.860 Loss 1.5195
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 107.49660801887512

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 1.2705 (1.2152)	Prec@1 63.672 (65.805)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 1.1650 (1.2301)	Prec@1 66.406 (65.304)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 1.2217 (1.2285)	Prec@1 68.750 (65.204)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 1.1514 (1.2210)	Prec@1 65.625 (65.392)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 1.2168 (1.2155)	Prec@1 64.453 (65.537)	
Total train loss: 1.2156

Train time: 94.41909050941467
 * Prec@1 60.330 Prec@5 85.750 Loss 1.5166
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 114.48143863677979

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 1.1758 (1.2120)	Prec@1 68.359 (65.855)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 1.1973 (1.2222)	Prec@1 65.625 (65.380)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 1.1914 (1.2243)	Prec@1 63.281 (65.231)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 1.1885 (1.2160)	Prec@1 68.750 (65.615)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 1.1426 (1.2124)	Prec@1 66.797 (65.807)	
Total train loss: 1.2122

Train time: 95.61314368247986
 * Prec@1 60.250 Prec@5 85.910 Loss 1.5186
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 114.95133686065674

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 1.3223 (1.2280)	Prec@1 64.062 (65.094)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 1.1846 (1.2139)	Prec@1 65.234 (65.810)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 1.2334 (1.2037)	Prec@1 66.016 (65.972)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 1.1797 (1.2094)	Prec@1 66.016 (65.810)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 1.0742 (1.2117)	Prec@1 69.531 (65.773)	
Total train loss: 1.2117

Train time: 95.56781816482544
 * Prec@1 60.290 Prec@5 85.920 Loss 1.5166
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 115.60260677337646

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 1.3604 (1.2170)	Prec@1 65.234 (65.805)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 1.1201 (1.2067)	Prec@1 68.750 (66.181)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 1.2861 (1.2091)	Prec@1 64.062 (66.012)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 1.3145 (1.2130)	Prec@1 64.453 (65.783)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 1.1475 (1.2131)	Prec@1 67.188 (65.705)	
Total train loss: 1.2129

Train time: 97.27375411987305
 * Prec@1 60.140 Prec@5 85.740 Loss 1.5215
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 119.00031304359436

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 1.2480 (1.2223)	Prec@1 65.625 (65.555)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 1.1426 (1.2209)	Prec@1 65.625 (65.800)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 1.1455 (1.2225)	Prec@1 67.578 (65.625)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 1.2432 (1.2181)	Prec@1 60.938 (65.810)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 1.1855 (1.2145)	Prec@1 65.234 (65.827)	
Total train loss: 1.2148

Train time: 92.69093537330627
 * Prec@1 60.270 Prec@5 85.910 Loss 1.5186
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 112.54757881164551

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 1.1924 (1.2163)	Prec@1 64.453 (65.565)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 1.2988 (1.2090)	Prec@1 61.328 (65.700)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 1.2666 (1.2088)	Prec@1 64.062 (65.879)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 1.4580 (1.2104)	Prec@1 58.984 (65.870)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 1.1416 (1.2115)	Prec@1 65.234 (65.827)	
Total train loss: 1.2116

Train time: 93.41601276397705
 * Prec@1 60.190 Prec@5 85.780 Loss 1.5215
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 112.73343777656555

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 1.2812 (1.2218)	Prec@1 65.234 (65.375)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 1.3408 (1.2190)	Prec@1 64.062 (65.445)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 1.1436 (1.2124)	Prec@1 66.797 (65.782)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 1.1689 (1.2098)	Prec@1 68.359 (66.048)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 1.2539 (1.2147)	Prec@1 64.844 (65.893)	
Total train loss: 1.2152

Train time: 97.3768539428711
 * Prec@1 60.350 Prec@5 85.870 Loss 1.5166
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 116.38382601737976

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 1.1631 (1.1981)	Prec@1 69.531 (66.647)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 1.2246 (1.2110)	Prec@1 66.797 (66.036)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 1.2148 (1.2190)	Prec@1 66.406 (65.885)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 1.3848 (1.2116)	Prec@1 58.203 (65.918)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 1.2285 (1.2131)	Prec@1 66.797 (65.737)	
Total train loss: 1.2132

Train time: 29.558343648910522
 * Prec@1 60.170 Prec@5 85.890 Loss 1.5215
Best acc: 60.600
--------------------------------------------------------------------------------
Test time: 36.07656121253967

