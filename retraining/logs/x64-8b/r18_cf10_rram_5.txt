
      ==> Arguments:
          dataset: cifar10
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode: rram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 5
Savedir:  ../pretrained_models/frozen/x64-8b/rram/cifar10/resnet18
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar10/resnet18/train/relu5
Test path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar10/resnet18/test/relu5
ResNet18(
  (conv6): QConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): QConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=512, out_features=10, bias=False)
  (bn19): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 8.500 Prec@5 42.850 Loss 2.3379
Avg Loading time: 9.5202 seconds
Avg Batch time: 9.5654 seconds

Pre-trained Prec@1 with 5 layers frozen: 8.5 	 Loss: 2.337890625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	DT: 0.278 (9.027)	BT: 0.345 (9.098)	Loss 0.7070 (1.1017)	Prec@1 85.938 (69.091)	
Epoch: [0][155/391]	LR: 0.01	DT: 0.000 (8.119)	BT: 0.080 (8.190)	Loss 0.5269 (0.8793)	Prec@1 82.812 (75.666)	
Epoch: [0][233/391]	LR: 0.01	DT: 3.389 (8.215)	BT: 3.464 (8.286)	Loss 0.4419 (0.7580)	Prec@1 89.062 (78.863)	
Epoch: [0][311/391]	LR: 0.01	DT: 0.000 (8.450)	BT: 0.057 (8.520)	Loss 0.4961 (0.6838)	Prec@1 85.938 (80.842)	
Epoch: [0][389/391]	LR: 0.01	DT: 0.000 (8.731)	BT: 0.058 (8.801)	Loss 0.3665 (0.6323)	Prec@1 89.062 (82.073)	
Total train loss: 0.6318
Avg Loading time: 8.7089 seconds
Avg Batch time: 8.7787 seconds

Train time: 3432.571135997772
 * Prec@1 87.270 Prec@5 99.570 Loss 0.4128
Avg Loading time: 7.5030 seconds
Avg Batch time: 7.5368 seconds

Best acc: 87.270
--------------------------------------------------------------------------------
Test time: 597.0423858165741

Epoch: [1][77/391]	LR: 0.01	DT: 0.000 (6.732)	BT: 0.061 (6.801)	Loss 0.4211 (0.3783)	Prec@1 86.719 (88.421)	
Epoch: [1][155/391]	LR: 0.01	DT: 0.000 (7.782)	BT: 0.063 (7.851)	Loss 0.2898 (0.3716)	Prec@1 90.625 (88.582)	
Epoch: [1][233/391]	LR: 0.01	DT: 0.330 (8.292)	BT: 0.403 (8.360)	Loss 0.2856 (0.3689)	Prec@1 92.188 (88.605)	
Epoch: [1][311/391]	LR: 0.01	DT: 0.000 (8.288)	BT: 0.056 (8.356)	Loss 0.3318 (0.3633)	Prec@1 89.062 (88.732)	
Epoch: [1][389/391]	LR: 0.01	DT: 0.000 (8.340)	BT: 0.057 (8.408)	Loss 0.4517 (0.3620)	Prec@1 87.500 (88.644)	
Total train loss: 0.3620
Avg Loading time: 8.3186 seconds
Avg Batch time: 8.3863 seconds

Train time: 3279.1160922050476
 * Prec@1 88.250 Prec@5 99.630 Loss 0.3591
Avg Loading time: 8.2843 seconds
Avg Batch time: 8.3194 seconds

Best acc: 88.250
--------------------------------------------------------------------------------
Test time: 658.9001688957214

Epoch: [2][77/391]	LR: 0.01	DT: 0.000 (8.862)	BT: 0.066 (8.929)	Loss 0.2449 (0.3226)	Prec@1 92.969 (89.784)	
Epoch: [2][155/391]	LR: 0.01	DT: 0.000 (8.075)	BT: 0.067 (8.142)	Loss 0.3586 (0.3315)	Prec@1 87.500 (89.328)	
Epoch: [2][233/391]	LR: 0.01	DT: 0.000 (7.730)	BT: 0.068 (7.796)	Loss 0.3625 (0.3297)	Prec@1 85.938 (89.416)	
Epoch: [2][311/391]	LR: 0.01	DT: 0.000 (8.052)	BT: 0.056 (8.118)	Loss 0.2302 (0.3328)	Prec@1 92.188 (89.333)	
Epoch: [2][389/391]	LR: 0.01	DT: 0.000 (8.851)	BT: 0.057 (8.917)	Loss 0.2869 (0.3360)	Prec@1 90.625 (89.147)	
Total train loss: 0.3360
Avg Loading time: 8.8281 seconds
Avg Batch time: 8.8939 seconds

Train time: 3477.58900141716
 * Prec@1 89.050 Prec@5 99.680 Loss 0.3372
Avg Loading time: 10.6047 seconds
Avg Batch time: 10.6380 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 841.9873807430267

Epoch: [3][77/391]	LR: 0.01	DT: 0.000 (8.806)	BT: 0.069 (8.876)	Loss 0.3032 (0.3273)	Prec@1 91.406 (89.453)	
Epoch: [3][155/391]	LR: 0.01	DT: 0.718 (8.219)	BT: 0.794 (8.288)	Loss 0.2847 (0.3286)	Prec@1 91.406 (89.268)	
Epoch: [3][233/391]	LR: 0.01	DT: 1.368 (8.793)	BT: 1.446 (8.862)	Loss 0.3777 (0.3311)	Prec@1 85.156 (89.119)	
Epoch: [3][311/391]	LR: 0.01	DT: 0.000 (9.077)	BT: 0.057 (9.146)	Loss 0.2974 (0.3307)	Prec@1 89.844 (89.140)	
Epoch: [3][389/391]	LR: 0.01	DT: 0.000 (9.420)	BT: 0.056 (9.488)	Loss 0.3533 (0.3257)	Prec@1 89.844 (89.299)	
Total train loss: 0.3257
Avg Loading time: 9.3958 seconds
Avg Batch time: 9.4639 seconds

Train time: 3700.475510597229
 * Prec@1 88.750 Prec@5 99.680 Loss 0.3367
Avg Loading time: 10.6188 seconds
Avg Batch time: 10.6524 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 842.3710594177246

Epoch: [4][77/391]	LR: 0.01	DT: 0.000 (10.482)	BT: 0.058 (10.547)	Loss 0.3701 (0.2844)	Prec@1 87.500 (90.545)	
Epoch: [4][155/391]	LR: 0.01	DT: 0.000 (10.304)	BT: 0.071 (10.371)	Loss 0.3120 (0.2928)	Prec@1 90.625 (90.480)	
Epoch: [4][233/391]	LR: 0.01	DT: 0.441 (10.021)	BT: 0.515 (10.089)	Loss 0.2542 (0.2947)	Prec@1 92.188 (90.361)	
Epoch: [4][311/391]	LR: 0.01	DT: 0.000 (9.284)	BT: 0.067 (9.352)	Loss 0.4351 (0.2957)	Prec@1 87.500 (90.257)	
Epoch: [4][389/391]	LR: 0.01	DT: 0.000 (9.561)	BT: 0.058 (9.629)	Loss 0.2257 (0.2989)	Prec@1 92.188 (90.138)	
Total train loss: 0.2990
Avg Loading time: 9.5362 seconds
Avg Batch time: 9.6044 seconds

Train time: 3755.396707057953
 * Prec@1 87.480 Prec@5 99.490 Loss 0.3735
Avg Loading time: 11.1615 seconds
Avg Batch time: 11.1916 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 885.0249426364899

Epoch: [5][77/391]	LR: 0.01	DT: 0.478 (11.177)	BT: 0.541 (11.244)	Loss 0.3655 (0.2920)	Prec@1 87.500 (90.234)	
Epoch: [5][155/391]	LR: 0.01	DT: 0.000 (10.141)	BT: 0.068 (10.211)	Loss 0.2593 (0.2921)	Prec@1 90.625 (90.194)	
Epoch: [5][233/391]	LR: 0.01	DT: 0.989 (9.187)	BT: 1.061 (9.256)	Loss 0.3250 (0.2949)	Prec@1 86.719 (90.194)	
Epoch: [5][311/391]	LR: 0.01	DT: 0.000 (9.245)	BT: 0.055 (9.314)	Loss 0.2778 (0.2999)	Prec@1 90.625 (90.094)	
Epoch: [5][389/391]	LR: 0.01	DT: 0.000 (9.119)	BT: 0.058 (9.188)	Loss 0.2434 (0.3035)	Prec@1 92.188 (89.898)	
Total train loss: 0.3036
Avg Loading time: 9.0956 seconds
Avg Batch time: 9.1646 seconds

Train time: 3583.4152324199677
 * Prec@1 68.760 Prec@5 95.170 Loss 0.9370
Avg Loading time: 0.0978 seconds
Avg Batch time: 0.1379 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.589898586273193

Epoch: [6][77/391]	LR: 0.01	DT: 0.155 (0.064)	BT: 0.223 (0.154)	Loss 0.3384 (0.3133)	Prec@1 87.500 (89.533)	
Epoch: [6][155/391]	LR: 0.01	DT: 0.106 (0.057)	BT: 0.174 (0.147)	Loss 0.2852 (0.3127)	Prec@1 89.844 (89.628)	
Epoch: [6][233/391]	LR: 0.01	DT: 0.037 (0.058)	BT: 0.103 (0.144)	Loss 0.3860 (0.3126)	Prec@1 86.719 (89.550)	
Epoch: [6][311/391]	LR: 0.01	DT: 0.000 (0.060)	BT: 0.084 (0.145)	Loss 0.4026 (0.3193)	Prec@1 85.156 (89.290)	
Epoch: [6][389/391]	LR: 0.01	DT: 0.000 (0.061)	BT: 0.075 (0.145)	Loss 0.2966 (0.3233)	Prec@1 90.625 (89.121)	
Total train loss: 0.3234
Avg Loading time: 0.0608 seconds
Avg Batch time: 0.1450 seconds

Train time: 56.82485055923462
 * Prec@1 80.050 Prec@5 98.740 Loss 0.5967
Avg Loading time: 0.1034 seconds
Avg Batch time: 0.1461 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.1977858543396

Epoch: [7][77/391]	LR: 0.01	DT: 0.000 (0.070)	BT: 0.184 (0.159)	Loss 0.2351 (0.3226)	Prec@1 92.188 (89.203)	
Epoch: [7][155/391]	LR: 0.01	DT: 0.000 (0.069)	BT: 0.099 (0.155)	Loss 0.2703 (0.3292)	Prec@1 89.844 (88.772)	
Epoch: [7][233/391]	LR: 0.01	DT: 0.000 (0.065)	BT: 0.103 (0.149)	Loss 0.3035 (0.3313)	Prec@1 87.500 (88.722)	
Epoch: [7][311/391]	LR: 0.01	DT: 0.049 (0.064)	BT: 0.125 (0.147)	Loss 0.2786 (0.3311)	Prec@1 90.625 (88.729)	
Epoch: [7][389/391]	LR: 0.01	DT: 0.068 (0.064)	BT: 0.133 (0.148)	Loss 0.4290 (0.3324)	Prec@1 86.719 (88.648)	
Total train loss: 0.3323
Avg Loading time: 0.0636 seconds
Avg Batch time: 0.1474 seconds

Train time: 57.76244521141052
 * Prec@1 87.530 Prec@5 99.500 Loss 0.3708
Avg Loading time: 0.0961 seconds
Avg Batch time: 0.1413 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.837347030639648

Epoch: [8][77/391]	LR: 0.01	DT: 0.084 (0.070)	BT: 0.150 (0.152)	Loss 0.2979 (0.3119)	Prec@1 90.625 (89.373)	
Epoch: [8][155/391]	LR: 0.01	DT: 0.139 (0.073)	BT: 0.233 (0.154)	Loss 0.4336 (0.3191)	Prec@1 85.938 (89.188)	
Epoch: [8][233/391]	LR: 0.01	DT: 0.060 (0.071)	BT: 0.134 (0.152)	Loss 0.3098 (0.3294)	Prec@1 91.406 (88.896)	
Epoch: [8][311/391]	LR: 0.01	DT: 0.087 (0.070)	BT: 0.173 (0.150)	Loss 0.4136 (0.3328)	Prec@1 86.719 (88.787)	
Epoch: [8][389/391]	LR: 0.01	DT: 0.131 (0.071)	BT: 0.192 (0.151)	Loss 0.3623 (0.3331)	Prec@1 87.500 (88.748)	
Total train loss: 0.3333
Avg Loading time: 0.0708 seconds
Avg Batch time: 0.1509 seconds

Train time: 59.0894558429718
 * Prec@1 87.740 Prec@5 99.580 Loss 0.3660
Avg Loading time: 0.1072 seconds
Avg Batch time: 0.1482 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.337667226791382

Epoch: [9][77/391]	LR: 0.01	DT: 0.000 (0.064)	BT: 0.099 (0.151)	Loss 0.3159 (0.3038)	Prec@1 90.625 (89.653)	
Epoch: [9][155/391]	LR: 0.01	DT: 0.056 (0.066)	BT: 0.170 (0.152)	Loss 0.4231 (0.3241)	Prec@1 82.812 (88.872)	
Epoch: [9][233/391]	LR: 0.01	DT: 0.048 (0.067)	BT: 0.116 (0.152)	Loss 0.4099 (0.3285)	Prec@1 84.375 (88.689)	
Epoch: [9][311/391]	LR: 0.01	DT: 0.096 (0.066)	BT: 0.186 (0.149)	Loss 0.4019 (0.3272)	Prec@1 88.281 (88.785)	
Epoch: [9][389/391]	LR: 0.01	DT: 0.000 (0.066)	BT: 0.066 (0.150)	Loss 0.3945 (0.3286)	Prec@1 88.281 (88.700)	
Total train loss: 0.3286
Avg Loading time: 0.0663 seconds
Avg Batch time: 0.1493 seconds

Train time: 58.49104595184326
 * Prec@1 87.610 Prec@5 99.460 Loss 0.3701
Avg Loading time: 0.1090 seconds
Avg Batch time: 0.1515 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.626868486404419

Epoch: [10][77/391]	LR: 0.002	DT: 0.139 (0.060)	BT: 0.224 (0.142)	Loss 0.2947 (0.2936)	Prec@1 89.844 (89.683)	
Epoch: [10][155/391]	LR: 0.002	DT: 0.000 (0.064)	BT: 0.071 (0.145)	Loss 0.2839 (0.2914)	Prec@1 91.406 (89.889)	
Epoch: [10][233/391]	LR: 0.002	DT: 0.000 (0.066)	BT: 0.061 (0.149)	Loss 0.2815 (0.2974)	Prec@1 89.844 (89.724)	
Epoch: [10][311/391]	LR: 0.002	DT: 0.046 (0.066)	BT: 0.107 (0.147)	Loss 0.2903 (0.2971)	Prec@1 90.625 (89.744)	
Epoch: [10][389/391]	LR: 0.002	DT: 0.098 (0.066)	BT: 0.157 (0.147)	Loss 0.2313 (0.2974)	Prec@1 90.625 (89.768)	
Total train loss: 0.2975
Avg Loading time: 0.0663 seconds
Avg Batch time: 0.1465 seconds

Train time: 57.40892672538757
 * Prec@1 88.590 Prec@5 99.510 Loss 0.3467
Avg Loading time: 0.1010 seconds
Avg Batch time: 0.1480 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.357522249221802

Epoch: [11][77/391]	LR: 0.002	DT: 0.042 (0.060)	BT: 0.101 (0.145)	Loss 0.2305 (0.2762)	Prec@1 92.188 (90.405)	
Epoch: [11][155/391]	LR: 0.002	DT: 0.000 (0.062)	BT: 0.108 (0.146)	Loss 0.3489 (0.2837)	Prec@1 89.844 (90.264)	
Epoch: [11][233/391]	LR: 0.002	DT: 0.089 (0.062)	BT: 0.167 (0.146)	Loss 0.2671 (0.2821)	Prec@1 90.625 (90.408)	
Epoch: [11][311/391]	LR: 0.002	DT: 0.134 (0.061)	BT: 0.201 (0.145)	Loss 0.2925 (0.2872)	Prec@1 89.844 (90.227)	
Epoch: [11][389/391]	LR: 0.002	DT: 0.046 (0.061)	BT: 0.127 (0.144)	Loss 0.3452 (0.2903)	Prec@1 85.938 (90.116)	
Total train loss: 0.2904
Avg Loading time: 0.0613 seconds
Avg Batch time: 0.1438 seconds

Train time: 56.36028695106506
 * Prec@1 88.380 Prec@5 99.580 Loss 0.3450
Avg Loading time: 0.0985 seconds
Avg Batch time: 0.1405 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.781961441040039

Epoch: [12][77/391]	LR: 0.002	DT: 0.165 (0.067)	BT: 0.234 (0.153)	Loss 0.2532 (0.2879)	Prec@1 90.625 (90.204)	
Epoch: [12][155/391]	LR: 0.002	DT: 0.000 (0.063)	BT: 0.077 (0.145)	Loss 0.4258 (0.2919)	Prec@1 86.719 (90.209)	
Epoch: [12][233/391]	LR: 0.002	DT: 0.000 (0.062)	BT: 0.070 (0.146)	Loss 0.4099 (0.2911)	Prec@1 87.500 (90.181)	
Epoch: [12][311/391]	LR: 0.002	DT: 0.104 (0.063)	BT: 0.166 (0.146)	Loss 0.2998 (0.2893)	Prec@1 89.844 (90.262)	
Epoch: [12][389/391]	LR: 0.002	DT: 0.082 (0.060)	BT: 0.150 (0.143)	Loss 0.2146 (0.2897)	Prec@1 93.750 (90.250)	
Total train loss: 0.2897
Avg Loading time: 0.0599 seconds
Avg Batch time: 0.1423 seconds

Train time: 55.77035450935364
 * Prec@1 88.400 Prec@5 99.490 Loss 0.3491
Avg Loading time: 0.1034 seconds
Avg Batch time: 0.1476 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.328240633010864

Epoch: [13][77/391]	LR: 0.002	DT: 0.133 (0.062)	BT: 0.207 (0.153)	Loss 0.3145 (0.2837)	Prec@1 89.062 (90.615)	
Epoch: [13][155/391]	LR: 0.002	DT: 0.000 (0.059)	BT: 0.106 (0.144)	Loss 0.2834 (0.2900)	Prec@1 92.188 (90.320)	
Epoch: [13][233/391]	LR: 0.002	DT: 0.000 (0.058)	BT: 0.132 (0.144)	Loss 0.2186 (0.2874)	Prec@1 94.531 (90.465)	
Epoch: [13][311/391]	LR: 0.002	DT: 0.039 (0.058)	BT: 0.143 (0.144)	Loss 0.2903 (0.2887)	Prec@1 90.625 (90.390)	
Epoch: [13][389/391]	LR: 0.002	DT: 0.127 (0.056)	BT: 0.190 (0.143)	Loss 0.2871 (0.2898)	Prec@1 89.844 (90.331)	
Total train loss: 0.2898
Avg Loading time: 0.0562 seconds
Avg Batch time: 0.1428 seconds

Train time: 55.98330855369568
 * Prec@1 88.280 Prec@5 99.530 Loss 0.3445
Avg Loading time: 0.1007 seconds
Avg Batch time: 0.1402 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.7429518699646

Epoch: [14][77/391]	LR: 0.002	DT: 0.000 (0.071)	BT: 0.081 (0.156)	Loss 0.4041 (0.2863)	Prec@1 85.938 (90.284)	
Epoch: [14][155/391]	LR: 0.002	DT: 0.000 (0.069)	BT: 0.108 (0.150)	Loss 0.1942 (0.2843)	Prec@1 94.531 (90.294)	
Epoch: [14][233/391]	LR: 0.002	DT: 0.072 (0.067)	BT: 0.159 (0.149)	Loss 0.3220 (0.2858)	Prec@1 89.844 (90.291)	
Epoch: [14][311/391]	LR: 0.002	DT: 0.070 (0.066)	BT: 0.138 (0.148)	Loss 0.2213 (0.2834)	Prec@1 95.312 (90.497)	
Epoch: [14][389/391]	LR: 0.002	DT: 0.071 (0.065)	BT: 0.136 (0.148)	Loss 0.2272 (0.2848)	Prec@1 93.750 (90.375)	
Total train loss: 0.2847
Avg Loading time: 0.0652 seconds
Avg Batch time: 0.1474 seconds

Train time: 57.73955678939819
 * Prec@1 88.510 Prec@5 99.540 Loss 0.3435
Avg Loading time: 0.1033 seconds
Avg Batch time: 0.1459 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.199667692184448

Epoch: [15][77/391]	LR: 0.002	DT: 0.150 (0.073)	BT: 0.231 (0.161)	Loss 0.2167 (0.2790)	Prec@1 92.188 (90.685)	
Epoch: [15][155/391]	LR: 0.002	DT: 0.079 (0.071)	BT: 0.150 (0.152)	Loss 0.3203 (0.2776)	Prec@1 88.281 (90.675)	
Epoch: [15][233/391]	LR: 0.002	DT: 0.000 (0.067)	BT: 0.091 (0.148)	Loss 0.2898 (0.2802)	Prec@1 85.938 (90.585)	
Epoch: [15][311/391]	LR: 0.002	DT: 0.000 (0.066)	BT: 0.075 (0.149)	Loss 0.2891 (0.2816)	Prec@1 91.406 (90.495)	
Epoch: [15][389/391]	LR: 0.002	DT: 0.060 (0.065)	BT: 0.120 (0.148)	Loss 0.2257 (0.2825)	Prec@1 92.969 (90.503)	
Total train loss: 0.2825
Avg Loading time: 0.0646 seconds
Avg Batch time: 0.1474 seconds

Train time: 57.72498798370361
 * Prec@1 88.680 Prec@5 99.530 Loss 0.3433
Avg Loading time: 0.0853 seconds
Avg Batch time: 0.1274 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 10.7600576877594

Epoch: [16][77/391]	LR: 0.002	DT: 0.163 (0.072)	BT: 0.276 (0.160)	Loss 0.2861 (0.2732)	Prec@1 90.625 (91.046)	
Epoch: [16][155/391]	LR: 0.002	DT: 0.115 (0.072)	BT: 0.183 (0.156)	Loss 0.2693 (0.2829)	Prec@1 91.406 (90.500)	
Epoch: [16][233/391]	LR: 0.002	DT: 0.076 (0.069)	BT: 0.151 (0.151)	Loss 0.2520 (0.2823)	Prec@1 91.406 (90.368)	
Epoch: [16][311/391]	LR: 0.002	DT: 0.000 (0.069)	BT: 0.064 (0.151)	Loss 0.2339 (0.2828)	Prec@1 91.406 (90.390)	
Epoch: [16][389/391]	LR: 0.002	DT: 0.107 (0.065)	BT: 0.172 (0.149)	Loss 0.3782 (0.2846)	Prec@1 89.062 (90.312)	
Total train loss: 0.2847
Avg Loading time: 0.0651 seconds
Avg Batch time: 0.1485 seconds

Train time: 58.15912985801697
 * Prec@1 88.610 Prec@5 99.530 Loss 0.3435
Avg Loading time: 0.1013 seconds
Avg Batch time: 0.1422 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.165291786193848

Epoch: [17][77/391]	LR: 0.002	DT: 0.000 (0.063)	BT: 0.099 (0.158)	Loss 0.2583 (0.2828)	Prec@1 90.625 (90.495)	
Epoch: [17][155/391]	LR: 0.002	DT: 0.072 (0.066)	BT: 0.137 (0.152)	Loss 0.2810 (0.2825)	Prec@1 89.844 (90.510)	
Epoch: [17][233/391]	LR: 0.002	DT: 0.044 (0.067)	BT: 0.112 (0.150)	Loss 0.3035 (0.2845)	Prec@1 92.969 (90.411)	
Epoch: [17][311/391]	LR: 0.002	DT: 0.000 (0.067)	BT: 0.148 (0.150)	Loss 0.2585 (0.2844)	Prec@1 91.406 (90.410)	
Epoch: [17][389/391]	LR: 0.002	DT: 0.072 (0.065)	BT: 0.135 (0.150)	Loss 0.2368 (0.2848)	Prec@1 92.969 (90.441)	
Total train loss: 0.2847
Avg Loading time: 0.0652 seconds
Avg Batch time: 0.1498 seconds

Train time: 58.70408344268799
 * Prec@1 88.500 Prec@5 99.500 Loss 0.3435
Avg Loading time: 0.0983 seconds
Avg Batch time: 0.1422 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.919772863388062

Epoch: [18][77/391]	LR: 0.002	DT: 0.081 (0.059)	BT: 0.163 (0.143)	Loss 0.2764 (0.2722)	Prec@1 90.625 (90.835)	
Epoch: [18][155/391]	LR: 0.002	DT: 0.000 (0.063)	BT: 0.085 (0.147)	Loss 0.3105 (0.2766)	Prec@1 88.281 (90.800)	
Epoch: [18][233/391]	LR: 0.002	DT: 0.078 (0.060)	BT: 0.141 (0.147)	Loss 0.2544 (0.2832)	Prec@1 91.406 (90.495)	
Epoch: [18][311/391]	LR: 0.002	DT: 0.062 (0.060)	BT: 0.156 (0.145)	Loss 0.3579 (0.2836)	Prec@1 85.156 (90.547)	
Epoch: [18][389/391]	LR: 0.002	DT: 0.053 (0.062)	BT: 0.114 (0.146)	Loss 0.3030 (0.2847)	Prec@1 88.281 (90.519)	
Total train loss: 0.2846
Avg Loading time: 0.0620 seconds
Avg Batch time: 0.1462 seconds

Train time: 57.277183294296265
 * Prec@1 88.580 Prec@5 99.530 Loss 0.3423
Avg Loading time: 0.0992 seconds
Avg Batch time: 0.1452 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.140159368515015

Epoch: [19][77/391]	LR: 0.002	DT: 0.067 (0.056)	BT: 0.149 (0.139)	Loss 0.2524 (0.2737)	Prec@1 92.969 (90.925)	
Epoch: [19][155/391]	LR: 0.002	DT: 0.059 (0.064)	BT: 0.164 (0.147)	Loss 0.2131 (0.2809)	Prec@1 94.531 (90.515)	
Epoch: [19][233/391]	LR: 0.002	DT: 0.120 (0.064)	BT: 0.201 (0.147)	Loss 0.2004 (0.2815)	Prec@1 94.531 (90.665)	
Epoch: [19][311/391]	LR: 0.002	DT: 0.052 (0.062)	BT: 0.113 (0.146)	Loss 0.2678 (0.2806)	Prec@1 92.188 (90.650)	
Epoch: [19][389/391]	LR: 0.002	DT: 0.135 (0.065)	BT: 0.202 (0.147)	Loss 0.2986 (0.2809)	Prec@1 86.719 (90.645)	
Total train loss: 0.2811
Avg Loading time: 0.0646 seconds
Avg Batch time: 0.1471 seconds

Train time: 57.658119678497314
 * Prec@1 88.710 Prec@5 99.570 Loss 0.3411
Avg Loading time: 0.0932 seconds
Avg Batch time: 0.1375 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.502253770828247

Epoch: [20][77/391]	LR: 0.0004	DT: 0.047 (0.060)	BT: 0.107 (0.146)	Loss 0.2920 (0.2748)	Prec@1 91.406 (90.885)	
Epoch: [20][155/391]	LR: 0.0004	DT: 0.101 (0.058)	BT: 0.179 (0.144)	Loss 0.3113 (0.2726)	Prec@1 86.719 (90.971)	
Epoch: [20][233/391]	LR: 0.0004	DT: 0.000 (0.060)	BT: 0.083 (0.145)	Loss 0.2756 (0.2709)	Prec@1 91.406 (91.022)	
Epoch: [20][311/391]	LR: 0.0004	DT: 0.115 (0.062)	BT: 0.193 (0.144)	Loss 0.1766 (0.2713)	Prec@1 94.531 (90.976)	
Epoch: [20][389/391]	LR: 0.0004	DT: 0.158 (0.062)	BT: 0.226 (0.144)	Loss 0.1865 (0.2742)	Prec@1 92.969 (90.869)	
Total train loss: 0.2741
Avg Loading time: 0.0619 seconds
Avg Batch time: 0.1437 seconds

Train time: 56.315436124801636
 * Prec@1 88.690 Prec@5 99.540 Loss 0.3391
Avg Loading time: 0.0979 seconds
Avg Batch time: 0.1456 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.14338731765747

Epoch: [21][77/391]	LR: 0.0004	DT: 0.063 (0.062)	BT: 0.147 (0.153)	Loss 0.2186 (0.2810)	Prec@1 92.969 (90.805)	
Epoch: [21][155/391]	LR: 0.0004	DT: 0.086 (0.062)	BT: 0.173 (0.149)	Loss 0.3567 (0.2807)	Prec@1 89.062 (90.795)	
Epoch: [21][233/391]	LR: 0.0004	DT: 0.230 (0.064)	BT: 0.295 (0.151)	Loss 0.3494 (0.2776)	Prec@1 88.281 (90.905)	
Epoch: [21][311/391]	LR: 0.0004	DT: 0.065 (0.064)	BT: 0.139 (0.149)	Loss 0.2113 (0.2788)	Prec@1 92.969 (90.763)	
Epoch: [21][389/391]	LR: 0.0004	DT: 0.000 (0.063)	BT: 0.060 (0.147)	Loss 0.2273 (0.2767)	Prec@1 92.969 (90.853)	
Total train loss: 0.2767
Avg Loading time: 0.0628 seconds
Avg Batch time: 0.1468 seconds

Train time: 57.54588770866394
 * Prec@1 88.600 Prec@5 99.520 Loss 0.3411
Avg Loading time: 0.1002 seconds
Avg Batch time: 0.1450 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.113167524337769

Epoch: [22][77/391]	LR: 0.0004	DT: 0.000 (0.064)	BT: 0.091 (0.155)	Loss 0.3176 (0.2743)	Prec@1 91.406 (90.785)	
Epoch: [22][155/391]	LR: 0.0004	DT: 0.068 (0.059)	BT: 0.148 (0.144)	Loss 0.3025 (0.2776)	Prec@1 91.406 (90.575)	
Epoch: [22][233/391]	LR: 0.0004	DT: 0.087 (0.057)	BT: 0.150 (0.143)	Loss 0.2517 (0.2777)	Prec@1 93.750 (90.662)	
Epoch: [22][311/391]	LR: 0.0004	DT: 0.050 (0.059)	BT: 0.119 (0.144)	Loss 0.2198 (0.2765)	Prec@1 92.188 (90.790)	
Epoch: [22][389/391]	LR: 0.0004	DT: 0.045 (0.058)	BT: 0.117 (0.142)	Loss 0.3359 (0.2753)	Prec@1 89.844 (90.863)	
Total train loss: 0.2753
Avg Loading time: 0.0580 seconds
Avg Batch time: 0.1422 seconds

Train time: 55.72365927696228
 * Prec@1 88.570 Prec@5 99.480 Loss 0.3423
Avg Loading time: 0.1024 seconds
Avg Batch time: 0.1462 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.239487648010254

Epoch: [23][77/391]	LR: 0.0004	DT: 0.211 (0.070)	BT: 0.289 (0.161)	Loss 0.2664 (0.2788)	Prec@1 92.188 (90.625)	
Epoch: [23][155/391]	LR: 0.0004	DT: 0.035 (0.065)	BT: 0.100 (0.149)	Loss 0.2727 (0.2773)	Prec@1 90.625 (90.785)	
Epoch: [23][233/391]	LR: 0.0004	DT: 0.000 (0.063)	BT: 0.106 (0.149)	Loss 0.1794 (0.2803)	Prec@1 96.875 (90.612)	
Epoch: [23][311/391]	LR: 0.0004	DT: 0.000 (0.064)	BT: 0.120 (0.149)	Loss 0.2544 (0.2783)	Prec@1 92.188 (90.668)	
Epoch: [23][389/391]	LR: 0.0004	DT: 0.126 (0.062)	BT: 0.187 (0.148)	Loss 0.3218 (0.2773)	Prec@1 89.062 (90.769)	
Total train loss: 0.2773
Avg Loading time: 0.0614 seconds
Avg Batch time: 0.1473 seconds

Train time: 57.70648813247681
 * Prec@1 88.750 Prec@5 99.530 Loss 0.3423
Avg Loading time: 0.1037 seconds
Avg Batch time: 0.1422 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.90451192855835

Epoch: [24][77/391]	LR: 0.0004	DT: 0.130 (0.063)	BT: 0.205 (0.156)	Loss 0.2593 (0.2781)	Prec@1 92.188 (90.835)	
Epoch: [24][155/391]	LR: 0.0004	DT: 0.048 (0.066)	BT: 0.140 (0.150)	Loss 0.1925 (0.2765)	Prec@1 96.094 (90.815)	
Epoch: [24][233/391]	LR: 0.0004	DT: 0.081 (0.065)	BT: 0.153 (0.147)	Loss 0.2788 (0.2748)	Prec@1 91.406 (90.812)	
Epoch: [24][311/391]	LR: 0.0004	DT: 0.087 (0.066)	BT: 0.165 (0.148)	Loss 0.1525 (0.2757)	Prec@1 96.875 (90.813)	
Epoch: [24][389/391]	LR: 0.0004	DT: 0.000 (0.065)	BT: 0.059 (0.147)	Loss 0.2710 (0.2747)	Prec@1 90.625 (90.795)	
Total train loss: 0.2748
Avg Loading time: 0.0646 seconds
Avg Batch time: 0.1466 seconds

Train time: 57.38648843765259
 * Prec@1 88.810 Prec@5 99.530 Loss 0.3398
Avg Loading time: 0.1056 seconds
Avg Batch time: 0.1481 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.407861232757568

Epoch: [25][77/391]	LR: 0.0004	DT: 0.071 (0.072)	BT: 0.136 (0.153)	Loss 0.2908 (0.2770)	Prec@1 89.844 (90.966)	
Epoch: [25][155/391]	LR: 0.0004	DT: 0.000 (0.066)	BT: 0.124 (0.148)	Loss 0.2795 (0.2709)	Prec@1 89.062 (91.106)	
Epoch: [25][233/391]	LR: 0.0004	DT: 0.110 (0.066)	BT: 0.195 (0.146)	Loss 0.2876 (0.2730)	Prec@1 89.844 (90.999)	
Epoch: [25][311/391]	LR: 0.0004	DT: 0.002 (0.065)	BT: 0.084 (0.148)	Loss 0.2900 (0.2749)	Prec@1 93.750 (90.893)	
Epoch: [25][389/391]	LR: 0.0004	DT: 0.186 (0.065)	BT: 0.251 (0.148)	Loss 0.2148 (0.2752)	Prec@1 93.750 (90.944)	
Total train loss: 0.2754
Avg Loading time: 0.0649 seconds
Avg Batch time: 0.1479 seconds

Train time: 57.96475124359131
 * Prec@1 88.570 Prec@5 99.490 Loss 0.3418
Avg Loading time: 0.0925 seconds
Avg Batch time: 0.1356 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.40505051612854

Epoch: [26][77/391]	LR: 0.0004	DT: 0.150 (0.064)	BT: 0.213 (0.157)	Loss 0.2803 (0.2699)	Prec@1 89.844 (91.046)	
Epoch: [26][155/391]	LR: 0.0004	DT: 0.142 (0.057)	BT: 0.220 (0.149)	Loss 0.1974 (0.2708)	Prec@1 93.750 (90.915)	
Epoch: [26][233/391]	LR: 0.0004	DT: 0.033 (0.057)	BT: 0.112 (0.144)	Loss 0.2175 (0.2714)	Prec@1 92.969 (90.942)	
Epoch: [26][311/391]	LR: 0.0004	DT: 0.052 (0.057)	BT: 0.138 (0.144)	Loss 0.3438 (0.2744)	Prec@1 87.500 (90.843)	
Epoch: [26][389/391]	LR: 0.0004	DT: 0.048 (0.060)	BT: 0.157 (0.146)	Loss 0.2576 (0.2750)	Prec@1 94.531 (90.811)	
Total train loss: 0.2750
Avg Loading time: 0.0594 seconds
Avg Batch time: 0.1461 seconds

Train time: 57.215932846069336
 * Prec@1 88.620 Prec@5 99.490 Loss 0.3401
Avg Loading time: 0.0983 seconds
Avg Batch time: 0.1380 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 11.561889410018921

Epoch: [27][77/391]	LR: 0.0004	DT: 0.000 (0.057)	BT: 0.095 (0.153)	Loss 0.2551 (0.2685)	Prec@1 90.625 (90.986)	
Epoch: [27][155/391]	LR: 0.0004	DT: 0.000 (0.058)	BT: 0.095 (0.149)	Loss 0.3076 (0.2709)	Prec@1 88.281 (90.790)	
Epoch: [27][233/391]	LR: 0.0004	DT: 0.044 (0.062)	BT: 0.105 (0.147)	Loss 0.2386 (0.2748)	Prec@1 91.406 (90.765)	
Epoch: [27][311/391]	LR: 0.0004	DT: 0.102 (0.061)	BT: 0.204 (0.146)	Loss 0.2622 (0.2751)	Prec@1 91.406 (90.800)	
Epoch: [27][389/391]	LR: 0.0004	DT: 0.074 (0.062)	BT: 0.134 (0.146)	Loss 0.2603 (0.2744)	Prec@1 91.406 (90.783)	
Total train loss: 0.2744
Avg Loading time: 0.0615 seconds
Avg Batch time: 0.1457 seconds

Train time: 57.05660080909729
 * Prec@1 88.730 Prec@5 99.520 Loss 0.3423
Avg Loading time: 0.1007 seconds
Avg Batch time: 0.1438 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.041556358337402

Epoch: [28][77/391]	LR: 0.0004	DT: 0.203 (0.060)	BT: 0.289 (0.153)	Loss 0.3184 (0.2697)	Prec@1 90.625 (91.126)	
Epoch: [28][155/391]	LR: 0.0004	DT: 0.119 (0.061)	BT: 0.197 (0.152)	Loss 0.2993 (0.2711)	Prec@1 88.281 (91.071)	
Epoch: [28][233/391]	LR: 0.0004	DT: 0.055 (0.062)	BT: 0.123 (0.149)	Loss 0.4573 (0.2787)	Prec@1 85.156 (90.839)	
Epoch: [28][311/391]	LR: 0.0004	DT: 0.000 (0.062)	BT: 0.107 (0.147)	Loss 0.2399 (0.2758)	Prec@1 93.750 (90.860)	
Epoch: [28][389/391]	LR: 0.0004	DT: 0.107 (0.060)	BT: 0.166 (0.146)	Loss 0.2881 (0.2766)	Prec@1 91.406 (90.845)	
Total train loss: 0.2765
Avg Loading time: 0.0596 seconds
Avg Batch time: 0.1456 seconds

Train time: 57.027122020721436
 * Prec@1 88.550 Prec@5 99.510 Loss 0.3416
Avg Loading time: 0.1020 seconds
Avg Batch time: 0.1483 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 12.336060762405396

Epoch: [29][77/391]	LR: 0.0004	DT: 0.056 (0.060)	BT: 0.126 (0.133)	Loss 0.2957 (0.2845)	Prec@1 89.062 (90.375)	
Epoch: [29][155/391]	LR: 0.0004	DT: 0.051 (0.053)	BT: 0.118 (0.120)	Loss 0.2930 (0.2770)	Prec@1 86.719 (90.635)	
Epoch: [29][233/391]	LR: 0.0004	DT: 0.000 (0.050)	BT: 0.097 (0.116)	Loss 0.2939 (0.2742)	Prec@1 89.844 (90.715)	
Epoch: [29][311/391]	LR: 0.0004	DT: 0.036 (0.048)	BT: 0.096 (0.113)	Loss 0.3223 (0.2746)	Prec@1 89.844 (90.750)	
Epoch: [29][389/391]	LR: 0.0004	DT: 0.095 (0.047)	BT: 0.154 (0.111)	Loss 0.3005 (0.2749)	Prec@1 88.281 (90.739)	
Total train loss: 0.2747
Avg Loading time: 0.0466 seconds
Avg Batch time: 0.1111 seconds

Train time: 43.524848222732544
 * Prec@1 88.640 Prec@5 99.520 Loss 0.3440
Avg Loading time: 0.0795 seconds
Avg Batch time: 0.1063 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.005967855453491

Epoch: [30][77/391]	LR: 8e-05	DT: 0.036 (0.045)	BT: 0.095 (0.112)	Loss 0.2368 (0.2713)	Prec@1 92.188 (91.126)	
Epoch: [30][155/391]	LR: 8e-05	DT: 0.040 (0.041)	BT: 0.102 (0.107)	Loss 0.2401 (0.2741)	Prec@1 92.969 (90.855)	
Epoch: [30][233/391]	LR: 8e-05	DT: 0.034 (0.041)	BT: 0.094 (0.106)	Loss 0.2864 (0.2745)	Prec@1 89.844 (90.835)	
Epoch: [30][311/391]	LR: 8e-05	DT: 0.053 (0.040)	BT: 0.150 (0.105)	Loss 0.2773 (0.2738)	Prec@1 90.625 (90.840)	
Epoch: [30][389/391]	LR: 8e-05	DT: 0.035 (0.040)	BT: 0.095 (0.105)	Loss 0.2844 (0.2764)	Prec@1 89.062 (90.735)	
Total train loss: 0.2765
Avg Loading time: 0.0398 seconds
Avg Batch time: 0.1045 seconds

Train time: 40.95268201828003
 * Prec@1 88.670 Prec@5 99.500 Loss 0.3418
Avg Loading time: 0.0790 seconds
Avg Batch time: 0.1083 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.19595742225647

Epoch: [31][77/391]	LR: 8e-05	DT: 0.056 (0.046)	BT: 0.117 (0.111)	Loss 0.2333 (0.2708)	Prec@1 92.188 (91.196)	
Epoch: [31][155/391]	LR: 8e-05	DT: 0.039 (0.040)	BT: 0.099 (0.105)	Loss 0.4426 (0.2735)	Prec@1 83.594 (90.996)	
Epoch: [31][233/391]	LR: 8e-05	DT: 0.022 (0.040)	BT: 0.078 (0.104)	Loss 0.3132 (0.2726)	Prec@1 90.625 (90.869)	
Epoch: [31][311/391]	LR: 8e-05	DT: 0.026 (0.040)	BT: 0.096 (0.103)	Loss 0.2352 (0.2750)	Prec@1 93.750 (90.770)	
Epoch: [31][389/391]	LR: 8e-05	DT: 0.035 (0.040)	BT: 0.095 (0.103)	Loss 0.1995 (0.2755)	Prec@1 95.312 (90.777)	
Total train loss: 0.2753
Avg Loading time: 0.0394 seconds
Avg Batch time: 0.1028 seconds

Train time: 40.27644419670105
 * Prec@1 88.610 Prec@5 99.520 Loss 0.3411
Avg Loading time: 0.0763 seconds
Avg Batch time: 0.1058 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.975559711456299

Epoch: [32][77/391]	LR: 8e-05	DT: 0.047 (0.044)	BT: 0.107 (0.111)	Loss 0.2566 (0.2683)	Prec@1 93.750 (91.246)	
Epoch: [32][155/391]	LR: 8e-05	DT: 0.044 (0.040)	BT: 0.105 (0.105)	Loss 0.2260 (0.2693)	Prec@1 91.406 (91.101)	
Epoch: [32][233/391]	LR: 8e-05	DT: 0.000 (0.039)	BT: 0.083 (0.104)	Loss 0.2671 (0.2734)	Prec@1 92.188 (90.895)	
Epoch: [32][311/391]	LR: 8e-05	DT: 0.024 (0.039)	BT: 0.086 (0.104)	Loss 0.3196 (0.2741)	Prec@1 88.281 (90.898)	
Epoch: [32][389/391]	LR: 8e-05	DT: 0.093 (0.039)	BT: 0.152 (0.103)	Loss 0.2893 (0.2741)	Prec@1 91.406 (90.905)	
Total train loss: 0.2740
Avg Loading time: 0.0389 seconds
Avg Batch time: 0.1028 seconds

Train time: 40.291775941848755
 * Prec@1 88.690 Prec@5 99.530 Loss 0.3408
Avg Loading time: 0.0779 seconds
Avg Batch time: 0.1064 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.03087306022644

Epoch: [33][77/391]	LR: 8e-05	DT: 0.038 (0.038)	BT: 0.105 (0.109)	Loss 0.3572 (0.2762)	Prec@1 90.625 (90.655)	
Epoch: [33][155/391]	LR: 8e-05	DT: 0.043 (0.036)	BT: 0.108 (0.105)	Loss 0.2568 (0.2712)	Prec@1 91.406 (90.835)	
Epoch: [33][233/391]	LR: 8e-05	DT: 0.038 (0.037)	BT: 0.098 (0.104)	Loss 0.3027 (0.2759)	Prec@1 90.625 (90.715)	
Epoch: [33][311/391]	LR: 8e-05	DT: 0.042 (0.038)	BT: 0.102 (0.104)	Loss 0.3550 (0.2766)	Prec@1 88.281 (90.640)	
Epoch: [33][389/391]	LR: 8e-05	DT: 0.033 (0.039)	BT: 0.092 (0.104)	Loss 0.2612 (0.2747)	Prec@1 92.969 (90.735)	
Total train loss: 0.2747
Avg Loading time: 0.0385 seconds
Avg Batch time: 0.1036 seconds

Train time: 40.598772048950195
 * Prec@1 88.800 Prec@5 99.520 Loss 0.3403
Avg Loading time: 0.0758 seconds
Avg Batch time: 0.1057 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.973025560379028

Epoch: [34][77/391]	LR: 8e-05	DT: 0.028 (0.040)	BT: 0.088 (0.109)	Loss 0.3508 (0.2791)	Prec@1 85.938 (90.715)	
Epoch: [34][155/391]	LR: 8e-05	DT: 0.043 (0.040)	BT: 0.098 (0.105)	Loss 0.2357 (0.2751)	Prec@1 92.969 (90.935)	
Epoch: [34][233/391]	LR: 8e-05	DT: 0.038 (0.040)	BT: 0.094 (0.104)	Loss 0.2700 (0.2718)	Prec@1 92.188 (91.072)	
Epoch: [34][311/391]	LR: 8e-05	DT: 0.038 (0.041)	BT: 0.101 (0.104)	Loss 0.1705 (0.2728)	Prec@1 95.312 (91.046)	
Epoch: [34][389/391]	LR: 8e-05	DT: 0.083 (0.041)	BT: 0.149 (0.104)	Loss 0.2487 (0.2726)	Prec@1 91.406 (90.972)	
Total train loss: 0.2729
Avg Loading time: 0.0406 seconds
Avg Batch time: 0.1035 seconds

Train time: 40.54776883125305
 * Prec@1 88.740 Prec@5 99.520 Loss 0.3391
Avg Loading time: 0.0751 seconds
Avg Batch time: 0.1038 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.842252731323242

Epoch: [35][77/391]	LR: 8e-05	DT: 0.032 (0.037)	BT: 0.091 (0.105)	Loss 0.2629 (0.2683)	Prec@1 89.062 (91.086)	
Epoch: [35][155/391]	LR: 8e-05	DT: 0.040 (0.038)	BT: 0.102 (0.103)	Loss 0.2179 (0.2694)	Prec@1 94.531 (90.976)	
Epoch: [35][233/391]	LR: 8e-05	DT: 0.039 (0.038)	BT: 0.098 (0.102)	Loss 0.3677 (0.2701)	Prec@1 89.062 (90.972)	
Epoch: [35][311/391]	LR: 8e-05	DT: 0.051 (0.038)	BT: 0.110 (0.101)	Loss 0.3015 (0.2724)	Prec@1 92.188 (90.880)	
Epoch: [35][389/391]	LR: 8e-05	DT: 0.090 (0.038)	BT: 0.146 (0.101)	Loss 0.2086 (0.2744)	Prec@1 93.750 (90.843)	
Total train loss: 0.2746
Avg Loading time: 0.0379 seconds
Avg Batch time: 0.1005 seconds

Train time: 39.38518810272217
 * Prec@1 88.680 Prec@5 99.530 Loss 0.3413
Avg Loading time: 0.0781 seconds
Avg Batch time: 0.1083 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.19496774673462

Epoch: [36][77/391]	LR: 8e-05	DT: 0.028 (0.043)	BT: 0.088 (0.111)	Loss 0.3855 (0.2741)	Prec@1 88.281 (90.925)	
Epoch: [36][155/391]	LR: 8e-05	DT: 0.048 (0.042)	BT: 0.113 (0.107)	Loss 0.3228 (0.2773)	Prec@1 90.625 (90.775)	
Epoch: [36][233/391]	LR: 8e-05	DT: 0.029 (0.041)	BT: 0.098 (0.106)	Loss 0.2800 (0.2738)	Prec@1 89.062 (90.919)	
Epoch: [36][311/391]	LR: 8e-05	DT: 0.044 (0.039)	BT: 0.105 (0.104)	Loss 0.2421 (0.2725)	Prec@1 92.969 (90.971)	
Epoch: [36][389/391]	LR: 8e-05	DT: 0.082 (0.039)	BT: 0.145 (0.103)	Loss 0.2321 (0.2724)	Prec@1 94.531 (90.976)	
Total train loss: 0.2724
Avg Loading time: 0.0385 seconds
Avg Batch time: 0.1033 seconds

Train time: 40.45785307884216
 * Prec@1 88.570 Prec@5 99.500 Loss 0.3403
Avg Loading time: 0.0719 seconds
Avg Batch time: 0.1013 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.629265785217285

Epoch: [37][77/391]	LR: 8e-05	DT: 0.050 (0.047)	BT: 0.112 (0.113)	Loss 0.2244 (0.2759)	Prec@1 91.406 (90.635)	
Epoch: [37][155/391]	LR: 8e-05	DT: 0.054 (0.045)	BT: 0.114 (0.109)	Loss 0.2805 (0.2808)	Prec@1 89.844 (90.550)	
Epoch: [37][233/391]	LR: 8e-05	DT: 0.038 (0.044)	BT: 0.097 (0.107)	Loss 0.3066 (0.2779)	Prec@1 91.406 (90.682)	
Epoch: [37][311/391]	LR: 8e-05	DT: 0.041 (0.044)	BT: 0.102 (0.106)	Loss 0.2080 (0.2732)	Prec@1 93.750 (90.933)	
Epoch: [37][389/391]	LR: 8e-05	DT: 0.092 (0.043)	BT: 0.151 (0.105)	Loss 0.2421 (0.2745)	Prec@1 92.188 (90.825)	
Total train loss: 0.2745
Avg Loading time: 0.0425 seconds
Avg Batch time: 0.1048 seconds

Train time: 41.04315733909607
 * Prec@1 88.790 Prec@5 99.500 Loss 0.3403
Avg Loading time: 0.0764 seconds
Avg Batch time: 0.1045 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.885591506958008

Epoch: [38][77/391]	LR: 8e-05	DT: 0.053 (0.045)	BT: 0.112 (0.112)	Loss 0.2668 (0.2752)	Prec@1 90.625 (90.785)	
Epoch: [38][155/391]	LR: 8e-05	DT: 0.042 (0.043)	BT: 0.101 (0.107)	Loss 0.1947 (0.2770)	Prec@1 92.969 (90.655)	
Epoch: [38][233/391]	LR: 8e-05	DT: 0.042 (0.042)	BT: 0.103 (0.106)	Loss 0.2910 (0.2766)	Prec@1 92.188 (90.698)	
Epoch: [38][311/391]	LR: 8e-05	DT: 0.043 (0.042)	BT: 0.102 (0.105)	Loss 0.1987 (0.2743)	Prec@1 93.750 (90.738)	
Epoch: [38][389/391]	LR: 8e-05	DT: 0.084 (0.041)	BT: 0.143 (0.105)	Loss 0.2285 (0.2745)	Prec@1 92.188 (90.773)	
Total train loss: 0.2747
Avg Loading time: 0.0413 seconds
Avg Batch time: 0.1044 seconds

Train time: 40.89210295677185
 * Prec@1 88.590 Prec@5 99.530 Loss 0.3416
Avg Loading time: 0.0760 seconds
Avg Batch time: 0.1045 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.896057367324829

Epoch: [39][77/391]	LR: 8e-05	DT: 0.139 (0.044)	BT: 0.208 (0.111)	Loss 0.2668 (0.2832)	Prec@1 89.844 (90.435)	
Epoch: [39][155/391]	LR: 8e-05	DT: 0.041 (0.043)	BT: 0.100 (0.108)	Loss 0.4055 (0.2807)	Prec@1 89.844 (90.780)	
Epoch: [39][233/391]	LR: 8e-05	DT: 0.043 (0.041)	BT: 0.106 (0.105)	Loss 0.3052 (0.2767)	Prec@1 91.406 (90.819)	
Epoch: [39][311/391]	LR: 8e-05	DT: 0.128 (0.041)	BT: 0.188 (0.105)	Loss 0.2615 (0.2745)	Prec@1 90.625 (90.810)	
Epoch: [39][389/391]	LR: 8e-05	DT: 0.085 (0.041)	BT: 0.145 (0.104)	Loss 0.3665 (0.2742)	Prec@1 86.719 (90.815)	
Total train loss: 0.2742
Avg Loading time: 0.0406 seconds
Avg Batch time: 0.1041 seconds

Train time: 40.78446006774902
 * Prec@1 88.760 Prec@5 99.530 Loss 0.3401
Avg Loading time: 0.0742 seconds
Avg Batch time: 0.1023 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.707731008529663

Epoch: [40][77/391]	LR: 1.6000000000000003e-05	DT: 0.033 (0.044)	BT: 0.093 (0.112)	Loss 0.2766 (0.2763)	Prec@1 91.406 (90.605)	
Epoch: [40][155/391]	LR: 1.6000000000000003e-05	DT: 0.047 (0.042)	BT: 0.107 (0.107)	Loss 0.1407 (0.2775)	Prec@1 96.875 (90.635)	
Epoch: [40][233/391]	LR: 1.6000000000000003e-05	DT: 0.025 (0.041)	BT: 0.085 (0.106)	Loss 0.3020 (0.2781)	Prec@1 89.844 (90.588)	
Epoch: [40][311/391]	LR: 1.6000000000000003e-05	DT: 0.029 (0.041)	BT: 0.089 (0.105)	Loss 0.2156 (0.2764)	Prec@1 93.750 (90.678)	
Epoch: [40][389/391]	LR: 1.6000000000000003e-05	DT: 0.039 (0.040)	BT: 0.101 (0.104)	Loss 0.2854 (0.2749)	Prec@1 92.188 (90.719)	
Total train loss: 0.2749
Avg Loading time: 0.0399 seconds
Avg Batch time: 0.1038 seconds

Train time: 40.68170380592346
 * Prec@1 88.750 Prec@5 99.510 Loss 0.3403
Avg Loading time: 0.0758 seconds
Avg Batch time: 0.1058 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.994527578353882

Epoch: [41][77/391]	LR: 1.6000000000000003e-05	DT: 0.051 (0.041)	BT: 0.112 (0.107)	Loss 0.2690 (0.2731)	Prec@1 92.188 (90.905)	
Epoch: [41][155/391]	LR: 1.6000000000000003e-05	DT: 0.041 (0.040)	BT: 0.101 (0.104)	Loss 0.1796 (0.2744)	Prec@1 92.188 (91.031)	
Epoch: [41][233/391]	LR: 1.6000000000000003e-05	DT: 0.000 (0.040)	BT: 0.064 (0.104)	Loss 0.2419 (0.2745)	Prec@1 92.188 (90.929)	
Epoch: [41][311/391]	LR: 1.6000000000000003e-05	DT: 0.043 (0.040)	BT: 0.103 (0.103)	Loss 0.3472 (0.2753)	Prec@1 86.719 (90.860)	
Epoch: [41][389/391]	LR: 1.6000000000000003e-05	DT: 0.084 (0.040)	BT: 0.143 (0.103)	Loss 0.2681 (0.2744)	Prec@1 90.625 (90.887)	
Total train loss: 0.2744
Avg Loading time: 0.0401 seconds
Avg Batch time: 0.1027 seconds

Train time: 40.236730337142944
 * Prec@1 88.670 Prec@5 99.480 Loss 0.3423
Avg Loading time: 0.0766 seconds
Avg Batch time: 0.1041 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.868321895599365

Epoch: [42][77/391]	LR: 1.6000000000000003e-05	DT: 0.046 (0.044)	BT: 0.106 (0.110)	Loss 0.2659 (0.2814)	Prec@1 91.406 (90.645)	
Epoch: [42][155/391]	LR: 1.6000000000000003e-05	DT: 0.054 (0.041)	BT: 0.114 (0.105)	Loss 0.2490 (0.2749)	Prec@1 90.625 (90.665)	
Epoch: [42][233/391]	LR: 1.6000000000000003e-05	DT: 0.031 (0.041)	BT: 0.091 (0.103)	Loss 0.3374 (0.2760)	Prec@1 91.406 (90.695)	
Epoch: [42][311/391]	LR: 1.6000000000000003e-05	DT: 0.030 (0.040)	BT: 0.093 (0.102)	Loss 0.2625 (0.2765)	Prec@1 91.406 (90.813)	
Epoch: [42][389/391]	LR: 1.6000000000000003e-05	DT: 0.086 (0.040)	BT: 0.141 (0.102)	Loss 0.3059 (0.2743)	Prec@1 86.719 (90.877)	
Total train loss: 0.2744
Avg Loading time: 0.0398 seconds
Avg Batch time: 0.1018 seconds

Train time: 39.88385319709778
 * Prec@1 88.810 Prec@5 99.500 Loss 0.3418
Avg Loading time: 0.0791 seconds
Avg Batch time: 0.1071 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.084485530853271

Epoch: [43][77/391]	LR: 1.6000000000000003e-05	DT: 0.032 (0.043)	BT: 0.092 (0.112)	Loss 0.3713 (0.2838)	Prec@1 86.719 (90.705)	
Epoch: [43][155/391]	LR: 1.6000000000000003e-05	DT: 0.043 (0.041)	BT: 0.103 (0.107)	Loss 0.2275 (0.2778)	Prec@1 93.750 (91.051)	
Epoch: [43][233/391]	LR: 1.6000000000000003e-05	DT: 0.051 (0.042)	BT: 0.111 (0.107)	Loss 0.2312 (0.2728)	Prec@1 93.750 (91.126)	
Epoch: [43][311/391]	LR: 1.6000000000000003e-05	DT: 0.053 (0.042)	BT: 0.118 (0.106)	Loss 0.1757 (0.2737)	Prec@1 95.312 (91.016)	
Epoch: [43][389/391]	LR: 1.6000000000000003e-05	DT: 0.086 (0.042)	BT: 0.146 (0.105)	Loss 0.2969 (0.2740)	Prec@1 91.406 (90.911)	
Total train loss: 0.2742
Avg Loading time: 0.0414 seconds
Avg Batch time: 0.1050 seconds

Train time: 41.14446306228638
 * Prec@1 88.690 Prec@5 99.510 Loss 0.3406
Avg Loading time: 0.0758 seconds
Avg Batch time: 0.1047 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.888423681259155

Epoch: [44][77/391]	LR: 1.6000000000000003e-05	DT: 0.026 (0.048)	BT: 0.088 (0.113)	Loss 0.2266 (0.2668)	Prec@1 94.531 (90.885)	
Epoch: [44][155/391]	LR: 1.6000000000000003e-05	DT: 0.037 (0.044)	BT: 0.097 (0.108)	Loss 0.2374 (0.2709)	Prec@1 92.188 (90.900)	
Epoch: [44][233/391]	LR: 1.6000000000000003e-05	DT: 0.040 (0.043)	BT: 0.109 (0.107)	Loss 0.2817 (0.2744)	Prec@1 93.750 (90.809)	
Epoch: [44][311/391]	LR: 1.6000000000000003e-05	DT: 0.032 (0.043)	BT: 0.093 (0.105)	Loss 0.2659 (0.2759)	Prec@1 89.844 (90.828)	
Epoch: [44][389/391]	LR: 1.6000000000000003e-05	DT: 0.090 (0.042)	BT: 0.149 (0.105)	Loss 0.2224 (0.2750)	Prec@1 96.094 (90.873)	
Total train loss: 0.2749
Avg Loading time: 0.0418 seconds
Avg Batch time: 0.1044 seconds

Train time: 40.882463693618774
 * Prec@1 88.580 Prec@5 99.490 Loss 0.3413
Avg Loading time: 0.0792 seconds
Avg Batch time: 0.1075 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.105794668197632

Epoch: [45][77/391]	LR: 1.6000000000000003e-05	DT: 0.062 (0.044)	BT: 0.127 (0.110)	Loss 0.2537 (0.2764)	Prec@1 89.844 (90.725)	
Epoch: [45][155/391]	LR: 1.6000000000000003e-05	DT: 0.000 (0.040)	BT: 0.084 (0.104)	Loss 0.2137 (0.2789)	Prec@1 95.312 (90.720)	
Epoch: [45][233/391]	LR: 1.6000000000000003e-05	DT: 0.000 (0.039)	BT: 0.060 (0.102)	Loss 0.1860 (0.2765)	Prec@1 95.312 (90.859)	
Epoch: [45][311/391]	LR: 1.6000000000000003e-05	DT: 0.048 (0.039)	BT: 0.108 (0.101)	Loss 0.2654 (0.2769)	Prec@1 90.625 (90.823)	
Epoch: [45][389/391]	LR: 1.6000000000000003e-05	DT: 0.084 (0.039)	BT: 0.142 (0.101)	Loss 0.2783 (0.2746)	Prec@1 89.062 (90.867)	
Total train loss: 0.2746
Avg Loading time: 0.0390 seconds
Avg Batch time: 0.1011 seconds

Train time: 39.61783790588379
 * Prec@1 88.670 Prec@5 99.520 Loss 0.3401
Avg Loading time: 0.0773 seconds
Avg Batch time: 0.1063 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.00179386138916

Epoch: [46][77/391]	LR: 1.6000000000000003e-05	DT: 0.050 (0.045)	BT: 0.116 (0.111)	Loss 0.2644 (0.2695)	Prec@1 89.844 (91.136)	
Epoch: [46][155/391]	LR: 1.6000000000000003e-05	DT: 0.042 (0.038)	BT: 0.112 (0.106)	Loss 0.2686 (0.2765)	Prec@1 92.188 (90.825)	
Epoch: [46][233/391]	LR: 1.6000000000000003e-05	DT: 0.053 (0.039)	BT: 0.109 (0.105)	Loss 0.2350 (0.2770)	Prec@1 92.188 (90.739)	
Epoch: [46][311/391]	LR: 1.6000000000000003e-05	DT: 0.033 (0.040)	BT: 0.093 (0.104)	Loss 0.2451 (0.2739)	Prec@1 91.406 (90.780)	
Epoch: [46][389/391]	LR: 1.6000000000000003e-05	DT: 0.025 (0.040)	BT: 0.084 (0.103)	Loss 0.1871 (0.2733)	Prec@1 93.750 (90.807)	
Total train loss: 0.2733
Avg Loading time: 0.0395 seconds
Avg Batch time: 0.1032 seconds

Train time: 40.4104483127594
 * Prec@1 88.540 Prec@5 99.500 Loss 0.3423
Avg Loading time: 0.0781 seconds
Avg Batch time: 0.1075 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.105651617050171

Epoch: [47][77/391]	LR: 1.6000000000000003e-05	DT: 0.053 (0.044)	BT: 0.119 (0.111)	Loss 0.2559 (0.2777)	Prec@1 90.625 (90.525)	
Epoch: [47][155/391]	LR: 1.6000000000000003e-05	DT: 0.050 (0.043)	BT: 0.116 (0.108)	Loss 0.2450 (0.2764)	Prec@1 89.062 (90.715)	
Epoch: [47][233/391]	LR: 1.6000000000000003e-05	DT: 0.036 (0.041)	BT: 0.099 (0.106)	Loss 0.3767 (0.2737)	Prec@1 91.406 (90.895)	
Epoch: [47][311/391]	LR: 1.6000000000000003e-05	DT: 0.044 (0.041)	BT: 0.103 (0.105)	Loss 0.2418 (0.2754)	Prec@1 92.969 (90.775)	
Epoch: [47][389/391]	LR: 1.6000000000000003e-05	DT: 0.072 (0.041)	BT: 0.132 (0.105)	Loss 0.2432 (0.2746)	Prec@1 90.625 (90.783)	
Total train loss: 0.2747
Avg Loading time: 0.0408 seconds
Avg Batch time: 0.1046 seconds

Train time: 40.96176505088806
 * Prec@1 88.570 Prec@5 99.490 Loss 0.3423
Avg Loading time: 0.0746 seconds
Avg Batch time: 0.1052 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 8.922563552856445

Epoch: [48][77/391]	LR: 1.6000000000000003e-05	DT: 0.036 (0.045)	BT: 0.124 (0.113)	Loss 0.2482 (0.2744)	Prec@1 89.844 (90.665)	
Epoch: [48][155/391]	LR: 1.6000000000000003e-05	DT: 0.035 (0.041)	BT: 0.095 (0.107)	Loss 0.3252 (0.2750)	Prec@1 89.844 (90.655)	
Epoch: [48][233/391]	LR: 1.6000000000000003e-05	DT: 0.028 (0.041)	BT: 0.088 (0.106)	Loss 0.3098 (0.2744)	Prec@1 90.625 (90.682)	
Epoch: [48][311/391]	LR: 1.6000000000000003e-05	DT: 0.040 (0.040)	BT: 0.101 (0.105)	Loss 0.3079 (0.2757)	Prec@1 88.281 (90.673)	
Epoch: [48][389/391]	LR: 1.6000000000000003e-05	DT: 0.084 (0.040)	BT: 0.142 (0.104)	Loss 0.3308 (0.2753)	Prec@1 90.625 (90.743)	
Total train loss: 0.2753
Avg Loading time: 0.0403 seconds
Avg Batch time: 0.1042 seconds

Train time: 40.8340220451355
 * Prec@1 88.750 Prec@5 99.500 Loss 0.3398
Avg Loading time: 0.0785 seconds
Avg Batch time: 0.1096 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.278726577758789

Epoch: [49][77/391]	LR: 1.6000000000000003e-05	DT: 0.040 (0.044)	BT: 0.101 (0.113)	Loss 0.2542 (0.2813)	Prec@1 91.406 (90.755)	
Epoch: [49][155/391]	LR: 1.6000000000000003e-05	DT: 0.048 (0.044)	BT: 0.108 (0.109)	Loss 0.2979 (0.2746)	Prec@1 91.406 (90.910)	
Epoch: [49][233/391]	LR: 1.6000000000000003e-05	DT: 0.037 (0.042)	BT: 0.103 (0.105)	Loss 0.2141 (0.2751)	Prec@1 93.750 (90.882)	
Epoch: [49][311/391]	LR: 1.6000000000000003e-05	DT: 0.026 (0.042)	BT: 0.088 (0.105)	Loss 0.3293 (0.2746)	Prec@1 89.062 (90.890)	
Epoch: [49][389/391]	LR: 1.6000000000000003e-05	DT: 0.094 (0.041)	BT: 0.153 (0.104)	Loss 0.2949 (0.2749)	Prec@1 89.062 (90.859)	
Total train loss: 0.2750
Avg Loading time: 0.0411 seconds
Avg Batch time: 0.1041 seconds

Train time: 40.787466049194336
 * Prec@1 88.710 Prec@5 99.530 Loss 0.3406
Avg Loading time: 0.0789 seconds
Avg Batch time: 0.1077 seconds

Best acc: 89.050
--------------------------------------------------------------------------------
Test time: 9.130694389343262

