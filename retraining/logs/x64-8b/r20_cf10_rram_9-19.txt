
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 39.780 Prec@5 87.950 Loss 3.0176
Pre-trained Prec@1 with 9 layers frozen: 39.779998779296875 	 Loss: 3.017578125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.2639 (0.3592)	Prec@1 89.062 (88.231)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.3079 (0.3400)	Prec@1 89.453 (88.612)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2384 (0.3143)	Prec@1 91.406 (89.323)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.2196 (0.2993)	Prec@1 92.578 (89.711)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.2039 (0.2911)	Prec@1 93.750 (89.942)	
Total train loss: 0.2909

Train time: 946.0193400382996
 * Prec@1 86.450 Prec@5 99.420 Loss 0.4229
Best acc: 86.450
--------------------------------------------------------------------------------
Test time: 1005.9231038093567

Epoch: [1][38/196]	LR: 0.1	Loss 0.1094 (0.1620)	Prec@1 96.484 (94.391)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1171 (0.1629)	Prec@1 96.875 (94.311)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.2089 (0.1648)	Prec@1 93.359 (94.234)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1893 (0.1716)	Prec@1 93.750 (93.993)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.2295 (0.1749)	Prec@1 91.406 (93.860)	
Total train loss: 0.1749

Train time: 104.38330841064453
 * Prec@1 87.180 Prec@5 99.450 Loss 0.4165
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 123.39391541481018

Epoch: [2][38/196]	LR: 0.1	Loss 0.1166 (0.1147)	Prec@1 96.484 (96.204)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1143 (0.1161)	Prec@1 96.484 (96.009)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1257 (0.1207)	Prec@1 95.312 (95.853)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1268 (0.1262)	Prec@1 96.484 (95.636)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1753 (0.1311)	Prec@1 93.359 (95.457)	
Total train loss: 0.1312

Train time: 101.49386882781982
 * Prec@1 86.140 Prec@5 99.330 Loss 0.4944
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 120.16187834739685

Epoch: [3][38/196]	LR: 0.1	Loss 0.0795 (0.0982)	Prec@1 96.875 (96.785)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.0947 (0.0963)	Prec@1 96.484 (96.840)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1098 (0.0988)	Prec@1 95.312 (96.705)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1085 (0.1047)	Prec@1 95.703 (96.467)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.0850 (0.1071)	Prec@1 96.484 (96.374)	
Total train loss: 0.1072

Train time: 96.61393475532532
 * Prec@1 85.710 Prec@5 99.270 Loss 0.5220
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 117.21806597709656

Epoch: [4][38/196]	LR: 0.1	Loss 0.0636 (0.0850)	Prec@1 98.438 (97.075)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0675 (0.0847)	Prec@1 97.266 (97.045)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.0415 (0.0858)	Prec@1 98.828 (96.989)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1007 (0.0874)	Prec@1 97.266 (96.908)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.0922 (0.0912)	Prec@1 96.875 (96.765)	
Total train loss: 0.0914

Train time: 106.01221036911011
 * Prec@1 85.220 Prec@5 99.270 Loss 0.5718
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 122.54643654823303

Epoch: [5][38/196]	LR: 0.1	Loss 0.1514 (0.0923)	Prec@1 94.922 (96.905)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0716 (0.0811)	Prec@1 98.438 (97.361)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.0701 (0.0821)	Prec@1 98.047 (97.342)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.0823 (0.0814)	Prec@1 97.656 (97.336)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.0799 (0.0833)	Prec@1 98.047 (97.268)	
Total train loss: 0.0834

Train time: 135.3712019920349
 * Prec@1 85.300 Prec@5 99.240 Loss 0.5659
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 161.83833575248718

Epoch: [6][38/196]	LR: 0.1	Loss 0.0561 (0.0673)	Prec@1 97.656 (97.837)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0565 (0.0642)	Prec@1 97.656 (97.942)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.0753 (0.0641)	Prec@1 97.656 (97.900)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.1033 (0.0670)	Prec@1 97.656 (97.756)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.0692 (0.0701)	Prec@1 97.656 (97.636)	
Total train loss: 0.0701

Train time: 117.3288950920105
 * Prec@1 86.240 Prec@5 99.230 Loss 0.5508
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 141.59108567237854

Epoch: [7][38/196]	LR: 0.1	Loss 0.0585 (0.0586)	Prec@1 97.656 (98.027)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0576 (0.0546)	Prec@1 98.047 (98.212)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0629 (0.0564)	Prec@1 97.656 (98.180)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0641 (0.0589)	Prec@1 98.047 (98.069)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.0486 (0.0618)	Prec@1 98.438 (97.971)	
Total train loss: 0.0618

Train time: 105.54847621917725
 * Prec@1 85.700 Prec@5 99.240 Loss 0.5659
Best acc: 87.180
--------------------------------------------------------------------------------
Test time: 128.082927942276

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0403 (0.0441)	Prec@1 98.438 (98.628)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0268 (0.0365)	Prec@1 99.219 (98.943)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0166 (0.0335)	Prec@1 100.000 (99.082)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0301 (0.0311)	Prec@1 99.609 (99.201)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0144 (0.0287)	Prec@1 100.000 (99.285)	
Total train loss: 0.0287

Train time: 100.89110112190247
 * Prec@1 88.510 Prec@5 99.380 Loss 0.4480
Best acc: 88.510
--------------------------------------------------------------------------------
Test time: 120.84214329719543

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0072 (0.0150)	Prec@1 100.000 (99.880)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0238 (0.0160)	Prec@1 99.609 (99.845)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0081 (0.0160)	Prec@1 100.000 (99.820)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0111 (0.0159)	Prec@1 99.609 (99.827)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0113 (0.0158)	Prec@1 100.000 (99.826)	
Total train loss: 0.0158

Train time: 105.39381098747253
 * Prec@1 88.700 Prec@5 99.370 Loss 0.4492
Best acc: 88.700
--------------------------------------------------------------------------------
Test time: 125.41996383666992

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0142 (0.0129)	Prec@1 100.000 (99.940)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0108 (0.0128)	Prec@1 100.000 (99.920)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0185 (0.0130)	Prec@1 99.609 (99.907)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0100 (0.0130)	Prec@1 100.000 (99.902)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0112 (0.0133)	Prec@1 100.000 (99.892)	
Total train loss: 0.0134

Train time: 96.63210606575012
 * Prec@1 88.530 Prec@5 99.340 Loss 0.4507
Best acc: 88.700
--------------------------------------------------------------------------------
Test time: 115.67752385139465

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0106)	Prec@1 100.000 (99.970)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0110 (0.0113)	Prec@1 100.000 (99.955)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0116)	Prec@1 100.000 (99.933)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0076 (0.0116)	Prec@1 100.000 (99.937)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0113 (0.0114)	Prec@1 100.000 (99.944)	
Total train loss: 0.0114

Train time: 108.03646659851074
 * Prec@1 88.630 Prec@5 99.390 Loss 0.4529
Best acc: 88.700
--------------------------------------------------------------------------------
Test time: 125.64422917366028

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0131 (0.0116)	Prec@1 100.000 (99.900)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0096 (0.0110)	Prec@1 100.000 (99.935)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0126 (0.0109)	Prec@1 100.000 (99.940)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0077 (0.0108)	Prec@1 100.000 (99.942)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0121 (0.0107)	Prec@1 99.609 (99.944)	
Total train loss: 0.0107

Train time: 39.30601096153259
 * Prec@1 88.640 Prec@5 99.360 Loss 0.4543
Best acc: 88.700
--------------------------------------------------------------------------------
Test time: 46.22916007041931

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0204 (0.0097)	Prec@1 99.609 (99.970)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0112 (0.0096)	Prec@1 100.000 (99.970)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0130 (0.0097)	Prec@1 100.000 (99.980)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0109 (0.0100)	Prec@1 100.000 (99.975)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0060 (0.0100)	Prec@1 100.000 (99.968)	
Total train loss: 0.0101

Train time: 48.1097514629364
 * Prec@1 88.630 Prec@5 99.390 Loss 0.4563
Best acc: 88.700
--------------------------------------------------------------------------------
Test time: 57.91632962226868

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0053 (0.0098)	Prec@1 100.000 (99.940)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0074 (0.0099)	Prec@1 100.000 (99.945)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0088 (0.0098)	Prec@1 100.000 (99.943)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0153 (0.0097)	Prec@1 100.000 (99.950)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0096 (0.0097)	Prec@1 100.000 (99.950)	
Total train loss: 0.0097

Train time: 48.13088631629944
 * Prec@1 88.730 Prec@5 99.390 Loss 0.4541
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 54.39896011352539

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0106 (0.0087)	Prec@1 100.000 (99.980)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0084 (0.0087)	Prec@1 100.000 (99.975)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0060 (0.0088)	Prec@1 100.000 (99.967)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0136 (0.0088)	Prec@1 100.000 (99.972)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0105 (0.0088)	Prec@1 100.000 (99.974)	
Total train loss: 0.0088

Train time: 42.33954620361328
 * Prec@1 88.650 Prec@5 99.350 Loss 0.4563
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 54.11789846420288

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0102 (0.0089)	Prec@1 100.000 (100.000)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0113 (0.0088)	Prec@1 100.000 (99.990)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0083 (0.0087)	Prec@1 100.000 (99.977)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0093 (0.0087)	Prec@1 100.000 (99.972)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0107 (0.0087)	Prec@1 100.000 (99.974)	
Total train loss: 0.0087

Train time: 51.91164255142212
 * Prec@1 88.600 Prec@5 99.320 Loss 0.4556
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 63.02727937698364

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0082)	Prec@1 100.000 (100.000)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0082)	Prec@1 100.000 (99.990)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0059 (0.0083)	Prec@1 100.000 (99.980)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0082 (0.0083)	Prec@1 100.000 (99.980)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0083)	Prec@1 100.000 (99.976)	
Total train loss: 0.0083

Train time: 47.09060883522034
 * Prec@1 88.690 Prec@5 99.370 Loss 0.4543
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 65.06228423118591

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0054 (0.0083)	Prec@1 100.000 (99.990)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0096 (0.0082)	Prec@1 100.000 (99.990)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0082 (0.0082)	Prec@1 100.000 (99.987)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0085)	Prec@1 100.000 (99.987)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0091 (0.0086)	Prec@1 100.000 (99.986)	
Total train loss: 0.0086

Train time: 77.5513961315155
 * Prec@1 88.710 Prec@5 99.330 Loss 0.4543
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 98.4587333202362

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0065 (0.0083)	Prec@1 100.000 (99.980)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0117 (0.0085)	Prec@1 100.000 (99.980)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0133 (0.0086)	Prec@1 99.609 (99.980)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0054 (0.0086)	Prec@1 100.000 (99.977)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0096 (0.0085)	Prec@1 100.000 (99.976)	
Total train loss: 0.0085

Train time: 96.07815313339233
 * Prec@1 88.710 Prec@5 99.350 Loss 0.4524
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 113.89149975776672

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0083)	Prec@1 100.000 (99.990)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0096 (0.0083)	Prec@1 100.000 (99.995)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0084)	Prec@1 100.000 (99.987)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0082 (0.0085)	Prec@1 100.000 (99.980)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0089 (0.0085)	Prec@1 100.000 (99.980)	
Total train loss: 0.0085

Train time: 153.24598693847656
 * Prec@1 88.690 Prec@5 99.320 Loss 0.4573
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 178.85183000564575

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0085)	Prec@1 100.000 (99.970)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0126 (0.0088)	Prec@1 100.000 (99.965)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0102 (0.0085)	Prec@1 100.000 (99.973)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0075 (0.0085)	Prec@1 100.000 (99.977)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0073 (0.0085)	Prec@1 100.000 (99.972)	
Total train loss: 0.0085

Train time: 130.6566619873047
 * Prec@1 88.620 Prec@5 99.400 Loss 0.4556
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 151.87606501579285

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0066 (0.0086)	Prec@1 100.000 (99.990)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0074 (0.0085)	Prec@1 100.000 (99.985)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0118 (0.0084)	Prec@1 100.000 (99.990)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0059 (0.0085)	Prec@1 100.000 (99.982)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0078 (0.0088)	Prec@1 100.000 (99.976)	
Total train loss: 0.0088

Train time: 97.04733872413635
 * Prec@1 88.670 Prec@5 99.360 Loss 0.4568
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 116.12677717208862

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0084)	Prec@1 100.000 (99.990)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0071 (0.0083)	Prec@1 100.000 (99.985)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0054 (0.0083)	Prec@1 100.000 (99.983)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0053 (0.0086)	Prec@1 100.000 (99.985)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0086)	Prec@1 100.000 (99.982)	
Total train loss: 0.0086

Train time: 104.45230007171631
 * Prec@1 88.590 Prec@5 99.330 Loss 0.4563
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 124.02525043487549

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0080 (0.0082)	Prec@1 100.000 (99.990)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0074 (0.0083)	Prec@1 100.000 (99.980)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0106 (0.0084)	Prec@1 100.000 (99.977)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0144 (0.0085)	Prec@1 100.000 (99.970)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0084 (0.0086)	Prec@1 100.000 (99.966)	
Total train loss: 0.0086

Train time: 97.25041675567627
 * Prec@1 88.720 Prec@5 99.380 Loss 0.4556
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 117.3958911895752

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0064 (0.0087)	Prec@1 100.000 (99.940)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0065 (0.0087)	Prec@1 100.000 (99.970)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0102 (0.0087)	Prec@1 100.000 (99.963)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0061 (0.0085)	Prec@1 100.000 (99.967)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0084)	Prec@1 100.000 (99.970)	
Total train loss: 0.0084

Train time: 100.19616842269897
 * Prec@1 88.580 Prec@5 99.360 Loss 0.4592
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 120.47634744644165

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0093)	Prec@1 100.000 (99.990)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0088 (0.0086)	Prec@1 100.000 (99.995)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0138 (0.0085)	Prec@1 99.609 (99.987)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0059 (0.0087)	Prec@1 100.000 (99.982)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0071 (0.0087)	Prec@1 100.000 (99.984)	
Total train loss: 0.0087

Train time: 125.97806692123413
 * Prec@1 88.670 Prec@5 99.370 Loss 0.4541
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 154.56264066696167

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0076 (0.0084)	Prec@1 100.000 (99.990)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0081 (0.0082)	Prec@1 100.000 (99.990)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0089 (0.0082)	Prec@1 100.000 (99.983)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0065 (0.0082)	Prec@1 100.000 (99.987)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0067 (0.0081)	Prec@1 100.000 (99.986)	
Total train loss: 0.0082

Train time: 148.39031624794006
 * Prec@1 88.600 Prec@5 99.350 Loss 0.4551
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 171.1276981830597

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0100 (0.0081)	Prec@1 100.000 (99.990)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0066 (0.0085)	Prec@1 100.000 (99.975)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0060 (0.0085)	Prec@1 100.000 (99.973)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0076 (0.0086)	Prec@1 100.000 (99.977)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0085 (0.0086)	Prec@1 100.000 (99.978)	
Total train loss: 0.0086

Train time: 121.07774782180786
 * Prec@1 88.660 Prec@5 99.340 Loss 0.4561
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 143.96209406852722

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0091 (0.0079)	Prec@1 100.000 (99.990)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0092 (0.0087)	Prec@1 100.000 (99.985)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0101 (0.0087)	Prec@1 100.000 (99.987)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0051 (0.0087)	Prec@1 100.000 (99.982)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0060 (0.0086)	Prec@1 100.000 (99.984)	
Total train loss: 0.0086

Train time: 151.60709476470947
 * Prec@1 88.700 Prec@5 99.390 Loss 0.4563
Best acc: 88.730
--------------------------------------------------------------------------------
Test time: 156.038907289505


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 34.980 Prec@5 85.260 Loss 3.3828
Pre-trained Prec@1 with 11 layers frozen: 34.97999954223633 	 Loss: 3.3828125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.2457 (0.3920)	Prec@1 93.750 (87.190)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.2542 (0.3442)	Prec@1 90.625 (88.386)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.3318 (0.3220)	Prec@1 89.453 (88.999)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.2063 (0.3094)	Prec@1 92.969 (89.360)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.2496 (0.3005)	Prec@1 91.406 (89.599)	
Total train loss: 0.3004

Train time: 467.308087348938
 * Prec@1 85.210 Prec@5 99.330 Loss 0.4688
Best acc: 85.210
--------------------------------------------------------------------------------
Test time: 521.9102301597595

Epoch: [1][38/196]	LR: 0.1	Loss 0.1382 (0.1697)	Prec@1 94.531 (94.081)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1814 (0.1696)	Prec@1 94.531 (94.141)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1593 (0.1749)	Prec@1 92.578 (93.900)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.2532 (0.1813)	Prec@1 91.016 (93.615)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.2703 (0.1858)	Prec@1 91.406 (93.458)	
Total train loss: 0.1857

Train time: 118.64558482170105
 * Prec@1 86.620 Prec@5 99.440 Loss 0.4373
Best acc: 86.620
--------------------------------------------------------------------------------
Test time: 137.93926525115967

Epoch: [2][38/196]	LR: 0.1	Loss 0.1110 (0.1294)	Prec@1 95.703 (95.613)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1582 (0.1260)	Prec@1 95.703 (95.718)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1740 (0.1273)	Prec@1 93.359 (95.546)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1257 (0.1311)	Prec@1 96.875 (95.425)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1400 (0.1372)	Prec@1 95.703 (95.214)	
Total train loss: 0.1372

Train time: 105.90860629081726
 * Prec@1 85.700 Prec@5 99.230 Loss 0.4983
Best acc: 86.620
--------------------------------------------------------------------------------
Test time: 123.51247763633728

Epoch: [3][38/196]	LR: 0.1	Loss 0.0801 (0.1002)	Prec@1 96.875 (96.544)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.1097 (0.1012)	Prec@1 97.266 (96.509)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1395 (0.1053)	Prec@1 94.141 (96.324)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1013 (0.1082)	Prec@1 97.266 (96.254)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1063 (0.1124)	Prec@1 96.875 (96.070)	
Total train loss: 0.1124

Train time: 148.3517837524414
 * Prec@1 86.710 Prec@5 99.300 Loss 0.5132
Best acc: 86.710
--------------------------------------------------------------------------------
Test time: 169.01804780960083

Epoch: [4][38/196]	LR: 0.1	Loss 0.0723 (0.0861)	Prec@1 97.266 (97.256)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0725 (0.0822)	Prec@1 96.484 (97.371)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.0776 (0.0843)	Prec@1 97.266 (97.262)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1595 (0.0874)	Prec@1 93.750 (97.140)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.1447 (0.0910)	Prec@1 93.750 (96.941)	
Total train loss: 0.0910

Train time: 107.00140118598938
 * Prec@1 85.430 Prec@5 99.240 Loss 0.5327
Best acc: 86.710
--------------------------------------------------------------------------------
Test time: 125.12724757194519

Epoch: [5][38/196]	LR: 0.1	Loss 0.0854 (0.0710)	Prec@1 96.875 (97.736)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0823 (0.0694)	Prec@1 96.484 (97.801)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.1234 (0.0702)	Prec@1 96.875 (97.760)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.0901 (0.0713)	Prec@1 96.484 (97.719)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.0773 (0.0745)	Prec@1 97.266 (97.570)	
Total train loss: 0.0746

Train time: 89.79434108734131
 * Prec@1 85.220 Prec@5 99.000 Loss 0.6123
Best acc: 86.710
--------------------------------------------------------------------------------
Test time: 109.36290264129639

Epoch: [6][38/196]	LR: 0.1	Loss 0.0490 (0.0664)	Prec@1 98.828 (97.907)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0412 (0.0655)	Prec@1 99.609 (97.907)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.1417 (0.0667)	Prec@1 94.922 (97.833)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.0505 (0.0673)	Prec@1 97.656 (97.801)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.0988 (0.0692)	Prec@1 96.875 (97.730)	
Total train loss: 0.0692

Train time: 89.07181358337402
 * Prec@1 85.380 Prec@5 99.180 Loss 0.5801
Best acc: 86.710
--------------------------------------------------------------------------------
Test time: 108.77823853492737

Epoch: [7][38/196]	LR: 0.1	Loss 0.0732 (0.0524)	Prec@1 97.266 (98.417)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0489 (0.0509)	Prec@1 99.609 (98.503)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0578 (0.0520)	Prec@1 98.828 (98.421)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0630 (0.0552)	Prec@1 97.656 (98.282)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.0683 (0.0571)	Prec@1 97.266 (98.189)	
Total train loss: 0.0571

Train time: 104.40195155143738
 * Prec@1 85.110 Prec@5 99.050 Loss 0.6172
Best acc: 86.710
--------------------------------------------------------------------------------
Test time: 123.07473015785217

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0172 (0.0369)	Prec@1 100.000 (98.938)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0319 (0.0320)	Prec@1 99.219 (99.149)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0172 (0.0290)	Prec@1 99.609 (99.289)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0207 (0.0267)	Prec@1 99.609 (99.377)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0269 (0.0256)	Prec@1 99.219 (99.411)	
Total train loss: 0.0256

Train time: 93.2243800163269
 * Prec@1 87.960 Prec@5 99.410 Loss 0.4829
Best acc: 87.960
--------------------------------------------------------------------------------
Test time: 112.15237998962402

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0117 (0.0153)	Prec@1 100.000 (99.830)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0147 (0.0149)	Prec@1 100.000 (99.860)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0163 (0.0144)	Prec@1 99.609 (99.876)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0220 (0.0146)	Prec@1 99.609 (99.872)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0203 (0.0147)	Prec@1 99.609 (99.852)	
Total train loss: 0.0148

Train time: 107.8389675617218
 * Prec@1 87.920 Prec@5 99.400 Loss 0.4868
Best acc: 87.960
--------------------------------------------------------------------------------
Test time: 128.2044005393982

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0123 (0.0130)	Prec@1 100.000 (99.910)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0087 (0.0124)	Prec@1 100.000 (99.915)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0158 (0.0121)	Prec@1 100.000 (99.913)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0093 (0.0120)	Prec@1 100.000 (99.920)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0111 (0.0119)	Prec@1 100.000 (99.924)	
Total train loss: 0.0119

Train time: 98.90397953987122
 * Prec@1 88.210 Prec@5 99.420 Loss 0.4805
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 118.60092258453369

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0106 (0.0106)	Prec@1 100.000 (99.960)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0180 (0.0107)	Prec@1 99.219 (99.950)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0099 (0.0110)	Prec@1 100.000 (99.940)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0088 (0.0109)	Prec@1 100.000 (99.947)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0109)	Prec@1 100.000 (99.942)	
Total train loss: 0.0109

Train time: 109.62953281402588
 * Prec@1 88.190 Prec@5 99.440 Loss 0.4817
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 126.02678298950195

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0080 (0.0102)	Prec@1 100.000 (99.960)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0076 (0.0101)	Prec@1 100.000 (99.960)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0097 (0.0104)	Prec@1 100.000 (99.953)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0094 (0.0101)	Prec@1 100.000 (99.965)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0093 (0.0102)	Prec@1 100.000 (99.958)	
Total train loss: 0.0103

Train time: 42.654640674591064
 * Prec@1 88.200 Prec@5 99.410 Loss 0.4807
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 48.939972162246704

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0062 (0.0107)	Prec@1 100.000 (99.930)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0086 (0.0102)	Prec@1 100.000 (99.950)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0076 (0.0099)	Prec@1 100.000 (99.957)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0094 (0.0099)	Prec@1 100.000 (99.960)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0069 (0.0098)	Prec@1 100.000 (99.960)	
Total train loss: 0.0099

Train time: 46.738935708999634
 * Prec@1 88.180 Prec@5 99.450 Loss 0.4849
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 59.65253925323486

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0070 (0.0090)	Prec@1 100.000 (99.970)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0109 (0.0087)	Prec@1 100.000 (99.975)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0106 (0.0088)	Prec@1 100.000 (99.973)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0059 (0.0089)	Prec@1 100.000 (99.970)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0102 (0.0089)	Prec@1 100.000 (99.972)	
Total train loss: 0.0089

Train time: 51.09763813018799
 * Prec@1 88.120 Prec@5 99.430 Loss 0.4785
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 61.13863563537598

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0075)	Prec@1 100.000 (99.990)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0061 (0.0079)	Prec@1 100.000 (99.995)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0083 (0.0082)	Prec@1 100.000 (99.983)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0086)	Prec@1 100.000 (99.975)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0086)	Prec@1 100.000 (99.974)	
Total train loss: 0.0087

Train time: 40.637908697128296
 * Prec@1 88.030 Prec@5 99.460 Loss 0.4829
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 48.00576615333557

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0057 (0.0081)	Prec@1 100.000 (99.970)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0096 (0.0083)	Prec@1 100.000 (99.975)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0058 (0.0084)	Prec@1 100.000 (99.977)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0084)	Prec@1 100.000 (99.977)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0068 (0.0084)	Prec@1 100.000 (99.978)	
Total train loss: 0.0084

Train time: 50.13361167907715
 * Prec@1 88.150 Prec@5 99.450 Loss 0.4832
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 59.14621090888977

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0082)	Prec@1 100.000 (99.990)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0113 (0.0083)	Prec@1 100.000 (99.985)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0107 (0.0082)	Prec@1 100.000 (99.987)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0170 (0.0079)	Prec@1 99.609 (99.987)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0112 (0.0080)	Prec@1 100.000 (99.988)	
Total train loss: 0.0080

Train time: 47.97903490066528
 * Prec@1 88.080 Prec@5 99.410 Loss 0.4849
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 52.927786350250244

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0085)	Prec@1 100.000 (99.950)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0093 (0.0083)	Prec@1 100.000 (99.960)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0082)	Prec@1 100.000 (99.967)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0061 (0.0081)	Prec@1 100.000 (99.975)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0071 (0.0081)	Prec@1 100.000 (99.974)	
Total train loss: 0.0081

Train time: 40.040072441101074
 * Prec@1 88.100 Prec@5 99.440 Loss 0.4832
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 55.107422828674316

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0079)	Prec@1 100.000 (99.980)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0081 (0.0081)	Prec@1 100.000 (99.980)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0058 (0.0079)	Prec@1 100.000 (99.983)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0078)	Prec@1 100.000 (99.985)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0080 (0.0079)	Prec@1 100.000 (99.982)	
Total train loss: 0.0080

Train time: 78.52623152732849
 * Prec@1 88.100 Prec@5 99.450 Loss 0.4832
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 97.84565210342407

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0075)	Prec@1 100.000 (99.990)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0079)	Prec@1 100.000 (99.990)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0052 (0.0080)	Prec@1 100.000 (99.993)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0085 (0.0082)	Prec@1 100.000 (99.985)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0086 (0.0083)	Prec@1 100.000 (99.978)	
Total train loss: 0.0083

Train time: 90.240469455719
 * Prec@1 88.080 Prec@5 99.430 Loss 0.4851
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 112.54465222358704

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0079)	Prec@1 100.000 (100.000)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0081)	Prec@1 100.000 (99.975)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0114 (0.0082)	Prec@1 100.000 (99.977)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0095 (0.0082)	Prec@1 100.000 (99.970)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0034 (0.0080)	Prec@1 100.000 (99.972)	
Total train loss: 0.0080

Train time: 157.83769249916077
 * Prec@1 88.140 Prec@5 99.430 Loss 0.4836
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 179.32195043563843

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0086)	Prec@1 100.000 (99.970)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0057 (0.0083)	Prec@1 100.000 (99.975)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0148 (0.0082)	Prec@1 100.000 (99.977)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0079)	Prec@1 100.000 (99.980)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0126 (0.0080)	Prec@1 99.609 (99.982)	
Total train loss: 0.0080

Train time: 115.9531762599945
 * Prec@1 88.180 Prec@5 99.430 Loss 0.4844
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 189.99249148368835

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0070 (0.0077)	Prec@1 100.000 (99.990)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0077)	Prec@1 100.000 (99.985)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0064 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0083)	Prec@1 100.000 (99.985)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0134 (0.0084)	Prec@1 99.609 (99.980)	
Total train loss: 0.0084

Train time: 166.97454571723938
 * Prec@1 88.050 Prec@5 99.450 Loss 0.4856
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 191.5328323841095

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0058 (0.0074)	Prec@1 100.000 (99.980)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0091 (0.0074)	Prec@1 100.000 (99.990)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0051 (0.0077)	Prec@1 100.000 (99.980)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0062 (0.0078)	Prec@1 100.000 (99.975)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0113 (0.0080)	Prec@1 100.000 (99.976)	
Total train loss: 0.0080

Train time: 98.05589127540588
 * Prec@1 88.160 Prec@5 99.390 Loss 0.4863
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 119.40222096443176

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0127 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0074 (0.0081)	Prec@1 100.000 (99.980)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0082 (0.0084)	Prec@1 100.000 (99.977)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0120 (0.0082)	Prec@1 100.000 (99.982)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0081)	Prec@1 100.000 (99.984)	
Total train loss: 0.0081

Train time: 103.35159873962402
 * Prec@1 88.190 Prec@5 99.420 Loss 0.4863
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 125.0463638305664

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0103 (0.0082)	Prec@1 100.000 (99.980)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0126 (0.0082)	Prec@1 100.000 (99.975)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0070 (0.0081)	Prec@1 100.000 (99.977)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0089 (0.0083)	Prec@1 100.000 (99.970)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0121 (0.0083)	Prec@1 100.000 (99.974)	
Total train loss: 0.0084

Train time: 118.12517642974854
 * Prec@1 88.040 Prec@5 99.450 Loss 0.4836
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 142.0224072933197

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0091 (0.0086)	Prec@1 100.000 (99.990)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0045 (0.0084)	Prec@1 100.000 (99.975)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0117 (0.0082)	Prec@1 99.609 (99.977)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0092 (0.0081)	Prec@1 100.000 (99.980)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0082 (0.0080)	Prec@1 100.000 (99.980)	
Total train loss: 0.0081

Train time: 111.652259349823
 * Prec@1 88.130 Prec@5 99.460 Loss 0.4863
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 136.5462942123413

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0081 (0.0080)	Prec@1 100.000 (100.000)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0066 (0.0079)	Prec@1 100.000 (99.995)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0110 (0.0080)	Prec@1 100.000 (99.997)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0052 (0.0080)	Prec@1 100.000 (99.992)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0083 (0.0080)	Prec@1 100.000 (99.994)	
Total train loss: 0.0080

Train time: 142.41418552398682
 * Prec@1 88.050 Prec@5 99.440 Loss 0.4844
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 187.74625062942505

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0074 (0.0077)	Prec@1 100.000 (100.000)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0080)	Prec@1 100.000 (99.985)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0074 (0.0080)	Prec@1 100.000 (99.987)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0086 (0.0079)	Prec@1 100.000 (99.990)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0072 (0.0080)	Prec@1 100.000 (99.988)	
Total train loss: 0.0080

Train time: 154.778085231781
 * Prec@1 88.010 Prec@5 99.430 Loss 0.4844
Best acc: 88.210
--------------------------------------------------------------------------------
Test time: 176.40307116508484


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 30.030 Prec@5 81.860 Loss 3.7031
Pre-trained Prec@1 with 13 layers frozen: 30.029998779296875 	 Loss: 3.703125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.3730 (0.4723)	Prec@1 87.109 (85.036)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.3149 (0.4083)	Prec@1 89.062 (86.594)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.3606 (0.3782)	Prec@1 87.109 (87.477)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.2847 (0.3602)	Prec@1 89.844 (87.906)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.2346 (0.3470)	Prec@1 92.578 (88.233)	
Total train loss: 0.3471

Train time: 557.6330442428589
 * Prec@1 84.970 Prec@5 99.300 Loss 0.4744
Best acc: 84.970
--------------------------------------------------------------------------------
Test time: 654.2836329936981

Epoch: [1][38/196]	LR: 0.1	Loss 0.2087 (0.2118)	Prec@1 91.406 (92.488)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.2671 (0.2069)	Prec@1 90.625 (92.819)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.2035 (0.2101)	Prec@1 92.578 (92.618)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.2625 (0.2101)	Prec@1 91.797 (92.653)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.2573 (0.2144)	Prec@1 89.062 (92.446)	
Total train loss: 0.2144

Train time: 96.79426622390747
 * Prec@1 85.670 Prec@5 99.340 Loss 0.4712
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 116.29612231254578

Epoch: [2][38/196]	LR: 0.1	Loss 0.1732 (0.1473)	Prec@1 93.359 (94.912)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1769 (0.1484)	Prec@1 92.578 (94.952)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.2461 (0.1514)	Prec@1 91.016 (94.828)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1973 (0.1578)	Prec@1 92.578 (94.521)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1833 (0.1651)	Prec@1 92.188 (94.207)	
Total train loss: 0.1651

Train time: 95.76653981208801
 * Prec@1 85.190 Prec@5 99.320 Loss 0.5039
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 115.27202486991882

Epoch: [3][38/196]	LR: 0.1	Loss 0.1072 (0.1178)	Prec@1 96.484 (96.194)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.1110 (0.1197)	Prec@1 95.312 (95.873)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.1156 (0.1201)	Prec@1 96.484 (95.807)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.2238 (0.1262)	Prec@1 92.188 (95.550)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1487 (0.1306)	Prec@1 94.531 (95.375)	
Total train loss: 0.1306

Train time: 93.8447413444519
 * Prec@1 84.860 Prec@5 99.180 Loss 0.5625
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 112.59184050559998

Epoch: [4][38/196]	LR: 0.1	Loss 0.0971 (0.1011)	Prec@1 97.266 (96.645)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0507 (0.0952)	Prec@1 99.609 (96.885)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.1632 (0.0973)	Prec@1 93.750 (96.751)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.1054 (0.1017)	Prec@1 96.094 (96.534)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.2234 (0.1077)	Prec@1 91.797 (96.282)	
Total train loss: 0.1078

Train time: 94.20287823677063
 * Prec@1 84.800 Prec@5 99.190 Loss 0.5718
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 114.12623691558838

Epoch: [5][38/196]	LR: 0.1	Loss 0.0854 (0.0852)	Prec@1 97.656 (97.366)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0767 (0.0856)	Prec@1 97.266 (97.165)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.0952 (0.0870)	Prec@1 96.875 (97.105)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.1207 (0.0901)	Prec@1 95.312 (96.968)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.1161 (0.0929)	Prec@1 95.703 (96.873)	
Total train loss: 0.0929

Train time: 101.81984543800354
 * Prec@1 85.200 Prec@5 99.070 Loss 0.5874
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 119.5637903213501

Epoch: [6][38/196]	LR: 0.1	Loss 0.0561 (0.0661)	Prec@1 98.438 (97.947)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0761 (0.0657)	Prec@1 98.047 (97.902)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.0899 (0.0674)	Prec@1 95.703 (97.827)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.0711 (0.0691)	Prec@1 98.047 (97.759)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.1200 (0.0738)	Prec@1 96.875 (97.550)	
Total train loss: 0.0738

Train time: 88.15566825866699
 * Prec@1 84.570 Prec@5 98.850 Loss 0.6333
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 106.23168015480042

Epoch: [7][38/196]	LR: 0.1	Loss 0.0450 (0.0635)	Prec@1 98.438 (97.947)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0534 (0.0629)	Prec@1 98.047 (97.987)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0583 (0.0630)	Prec@1 97.656 (98.010)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0772 (0.0649)	Prec@1 97.656 (97.934)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.1252 (0.0680)	Prec@1 94.531 (97.796)	
Total train loss: 0.0680

Train time: 93.36396265029907
 * Prec@1 84.200 Prec@5 98.910 Loss 0.6768
Best acc: 85.670
--------------------------------------------------------------------------------
Test time: 113.82943630218506

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0407 (0.0481)	Prec@1 99.609 (98.618)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0357 (0.0404)	Prec@1 98.438 (98.913)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0549 (0.0369)	Prec@1 98.047 (99.042)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0235 (0.0339)	Prec@1 99.219 (99.166)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0164 (0.0320)	Prec@1 100.000 (99.249)	
Total train loss: 0.0320

Train time: 90.50793552398682
 * Prec@1 87.140 Prec@5 99.340 Loss 0.5239
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 123.13170719146729

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0214 (0.0193)	Prec@1 99.609 (99.750)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0133 (0.0194)	Prec@1 100.000 (99.755)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0221 (0.0191)	Prec@1 99.609 (99.793)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0195 (0.0188)	Prec@1 99.609 (99.782)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0163 (0.0185)	Prec@1 99.609 (99.776)	
Total train loss: 0.0185

Train time: 199.63301301002502
 * Prec@1 86.980 Prec@5 99.330 Loss 0.5225
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 222.74081110954285

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0153 (0.0158)	Prec@1 100.000 (99.850)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0184 (0.0152)	Prec@1 99.609 (99.845)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0079 (0.0148)	Prec@1 100.000 (99.886)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0152 (0.0147)	Prec@1 100.000 (99.887)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0184 (0.0147)	Prec@1 99.609 (99.880)	
Total train loss: 0.0147

Train time: 93.68651270866394
 * Prec@1 87.040 Prec@5 99.330 Loss 0.5229
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 112.31563758850098

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0079 (0.0126)	Prec@1 100.000 (99.970)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0079 (0.0133)	Prec@1 100.000 (99.935)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0100 (0.0130)	Prec@1 100.000 (99.937)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0158 (0.0133)	Prec@1 100.000 (99.927)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0074 (0.0133)	Prec@1 100.000 (99.924)	
Total train loss: 0.0133

Train time: 78.28598237037659
 * Prec@1 87.080 Prec@5 99.320 Loss 0.5254
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 87.26584720611572

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0127 (0.0127)	Prec@1 100.000 (99.930)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0103 (0.0127)	Prec@1 100.000 (99.940)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0181 (0.0124)	Prec@1 100.000 (99.947)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0220 (0.0126)	Prec@1 100.000 (99.950)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0110 (0.0125)	Prec@1 100.000 (99.954)	
Total train loss: 0.0125

Train time: 33.47626543045044
 * Prec@1 87.100 Prec@5 99.320 Loss 0.5273
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 40.93086528778076

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0079 (0.0119)	Prec@1 100.000 (99.920)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0124 (0.0116)	Prec@1 99.609 (99.930)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0114)	Prec@1 100.000 (99.940)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0067 (0.0115)	Prec@1 100.000 (99.940)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0092 (0.0115)	Prec@1 100.000 (99.940)	
Total train loss: 0.0116

Train time: 50.20832085609436
 * Prec@1 87.240 Prec@5 99.270 Loss 0.5303
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 61.28396415710449

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0086 (0.0102)	Prec@1 100.000 (99.980)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0138 (0.0105)	Prec@1 100.000 (99.970)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0162 (0.0106)	Prec@1 100.000 (99.973)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0075 (0.0107)	Prec@1 100.000 (99.960)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0089 (0.0107)	Prec@1 100.000 (99.958)	
Total train loss: 0.0108

Train time: 48.955435276031494
 * Prec@1 87.120 Prec@5 99.300 Loss 0.5288
Best acc: 87.240
--------------------------------------------------------------------------------
Test time: 55.50548982620239

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0189 (0.0104)	Prec@1 100.000 (99.960)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0085 (0.0101)	Prec@1 100.000 (99.965)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0107 (0.0102)	Prec@1 100.000 (99.963)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0136 (0.0103)	Prec@1 100.000 (99.967)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0101 (0.0103)	Prec@1 100.000 (99.964)	
Total train loss: 0.0103

Train time: 37.040183544158936
 * Prec@1 87.260 Prec@5 99.280 Loss 0.5322
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 47.530433893203735

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0089 (0.0096)	Prec@1 100.000 (100.000)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0099 (0.0096)	Prec@1 100.000 (100.000)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0116 (0.0097)	Prec@1 100.000 (99.993)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0071 (0.0095)	Prec@1 100.000 (99.995)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0078 (0.0096)	Prec@1 100.000 (99.996)	
Total train loss: 0.0096

Train time: 45.29062223434448
 * Prec@1 87.260 Prec@5 99.320 Loss 0.5327
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 52.760013818740845

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0058 (0.0097)	Prec@1 100.000 (99.990)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0071 (0.0097)	Prec@1 100.000 (99.980)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0100 (0.0098)	Prec@1 99.609 (99.977)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0066 (0.0100)	Prec@1 100.000 (99.965)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0099)	Prec@1 100.000 (99.968)	
Total train loss: 0.0099

Train time: 412.6206569671631
 * Prec@1 87.160 Prec@5 99.290 Loss 0.5308
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 525.9862670898438

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0097 (0.0093)	Prec@1 100.000 (99.990)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0125 (0.0096)	Prec@1 100.000 (99.980)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0096)	Prec@1 100.000 (99.977)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0075 (0.0099)	Prec@1 100.000 (99.962)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0068 (0.0098)	Prec@1 100.000 (99.960)	
Total train loss: 0.0099

Train time: 137.45361995697021
 * Prec@1 87.110 Prec@5 99.230 Loss 0.5327
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 156.72747707366943

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0109 (0.0100)	Prec@1 100.000 (99.970)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0090 (0.0099)	Prec@1 100.000 (99.970)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0107 (0.0098)	Prec@1 100.000 (99.980)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0091 (0.0096)	Prec@1 100.000 (99.982)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0094 (0.0098)	Prec@1 100.000 (99.980)	
Total train loss: 0.0099

Train time: 90.50243210792542
 * Prec@1 87.140 Prec@5 99.320 Loss 0.5342
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 111.48399710655212

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0098 (0.0092)	Prec@1 100.000 (100.000)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0081 (0.0096)	Prec@1 100.000 (99.985)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0111 (0.0096)	Prec@1 100.000 (99.977)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0066 (0.0097)	Prec@1 100.000 (99.975)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0155 (0.0097)	Prec@1 99.609 (99.974)	
Total train loss: 0.0097

Train time: 99.44633555412292
 * Prec@1 86.970 Prec@5 99.240 Loss 0.5352
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 116.48814177513123

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0080 (0.0105)	Prec@1 100.000 (99.950)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0186 (0.0100)	Prec@1 99.609 (99.960)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0081 (0.0100)	Prec@1 100.000 (99.963)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0120 (0.0099)	Prec@1 99.609 (99.962)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0091 (0.0100)	Prec@1 100.000 (99.964)	
Total train loss: 0.0100

Train time: 96.95334386825562
 * Prec@1 87.040 Prec@5 99.250 Loss 0.5366
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 117.0993857383728

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0108 (0.0093)	Prec@1 100.000 (99.980)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0171 (0.0097)	Prec@1 100.000 (99.980)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0057 (0.0095)	Prec@1 100.000 (99.980)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0097)	Prec@1 100.000 (99.970)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0116 (0.0100)	Prec@1 100.000 (99.970)	
Total train loss: 0.0101

Train time: 96.03678131103516
 * Prec@1 87.090 Prec@5 99.290 Loss 0.5327
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 115.03742551803589

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0130 (0.0100)	Prec@1 100.000 (99.980)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0112 (0.0101)	Prec@1 100.000 (99.975)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0098 (0.0097)	Prec@1 100.000 (99.980)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0099)	Prec@1 100.000 (99.975)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0199 (0.0101)	Prec@1 99.219 (99.970)	
Total train loss: 0.0101

Train time: 92.94209814071655
 * Prec@1 87.140 Prec@5 99.280 Loss 0.5322
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 112.95731854438782

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0115 (0.0097)	Prec@1 100.000 (99.990)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0101 (0.0099)	Prec@1 100.000 (99.980)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0091 (0.0100)	Prec@1 100.000 (99.973)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0103 (0.0099)	Prec@1 100.000 (99.972)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0080 (0.0099)	Prec@1 100.000 (99.970)	
Total train loss: 0.0099

Train time: 102.22922968864441
 * Prec@1 87.110 Prec@5 99.240 Loss 0.5322
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 121.4532322883606

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0098 (0.0107)	Prec@1 100.000 (99.960)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0069 (0.0100)	Prec@1 100.000 (99.980)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0067 (0.0101)	Prec@1 100.000 (99.977)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0082 (0.0099)	Prec@1 100.000 (99.982)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0086 (0.0098)	Prec@1 100.000 (99.984)	
Total train loss: 0.0098

Train time: 88.14464807510376
 * Prec@1 87.200 Prec@5 99.310 Loss 0.5322
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 108.27360105514526

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0107)	Prec@1 100.000 (99.960)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0089 (0.0103)	Prec@1 100.000 (99.975)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0170 (0.0101)	Prec@1 100.000 (99.977)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0105 (0.0100)	Prec@1 100.000 (99.977)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0072 (0.0099)	Prec@1 100.000 (99.982)	
Total train loss: 0.0099

Train time: 107.50304985046387
 * Prec@1 87.080 Prec@5 99.350 Loss 0.5327
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 128.35347485542297

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0097 (0.0100)	Prec@1 100.000 (99.960)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0078 (0.0100)	Prec@1 100.000 (99.965)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0124 (0.0100)	Prec@1 100.000 (99.967)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0065 (0.0097)	Prec@1 100.000 (99.970)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0113 (0.0098)	Prec@1 100.000 (99.966)	
Total train loss: 0.0098

Train time: 93.91861009597778
 * Prec@1 87.150 Prec@5 99.300 Loss 0.5386
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 114.70242810249329

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0103 (0.0103)	Prec@1 100.000 (99.960)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0120 (0.0104)	Prec@1 100.000 (99.960)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0088 (0.0102)	Prec@1 100.000 (99.970)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0101 (0.0100)	Prec@1 100.000 (99.975)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0098 (0.0100)	Prec@1 100.000 (99.976)	
Total train loss: 0.0100

Train time: 97.27077984809875
 * Prec@1 87.130 Prec@5 99.240 Loss 0.5312
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 117.42933011054993

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0134 (0.0100)	Prec@1 100.000 (99.970)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0137 (0.0102)	Prec@1 99.609 (99.970)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0117 (0.0103)	Prec@1 100.000 (99.970)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0072 (0.0100)	Prec@1 100.000 (99.977)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0084 (0.0099)	Prec@1 100.000 (99.980)	
Total train loss: 0.0099

Train time: 47.21975040435791
 * Prec@1 87.190 Prec@5 99.280 Loss 0.5322
Best acc: 87.260
--------------------------------------------------------------------------------
Test time: 56.49164009094238


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 20.590 Prec@5 77.870 Loss 3.8809
Pre-trained Prec@1 with 15 layers frozen: 20.59000015258789 	 Loss: 3.880859375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.7153 (1.0328)	Prec@1 76.562 (69.591)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.4998 (0.8546)	Prec@1 83.203 (73.127)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.4670 (0.7649)	Prec@1 83.203 (75.314)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.4412 (0.7125)	Prec@1 84.766 (76.768)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.5640 (0.6807)	Prec@1 80.078 (77.682)	
Total train loss: 0.6805

Train time: 603.1950702667236
 * Prec@1 80.700 Prec@5 98.870 Loss 0.5625
Best acc: 80.700
--------------------------------------------------------------------------------
Test time: 642.5964090824127

Epoch: [1][38/196]	LR: 0.1	Loss 0.4973 (0.4578)	Prec@1 83.984 (84.485)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.4902 (0.4627)	Prec@1 82.422 (84.400)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.4802 (0.4605)	Prec@1 83.594 (84.338)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.4695 (0.4615)	Prec@1 83.594 (84.205)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.4470 (0.4643)	Prec@1 83.984 (84.020)	
Total train loss: 0.4643

Train time: 91.66130805015564
 * Prec@1 81.660 Prec@5 99.060 Loss 0.5405
Best acc: 81.660
--------------------------------------------------------------------------------
Test time: 108.92670583724976

Epoch: [2][38/196]	LR: 0.1	Loss 0.3982 (0.3828)	Prec@1 87.109 (86.719)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.3323 (0.3840)	Prec@1 89.453 (86.664)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.3545 (0.3893)	Prec@1 87.500 (86.485)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.3560 (0.3951)	Prec@1 87.109 (86.230)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.4324 (0.4003)	Prec@1 85.547 (86.022)	
Total train loss: 0.4005

Train time: 84.27134871482849
 * Prec@1 80.690 Prec@5 98.840 Loss 0.5908
Best acc: 81.660
--------------------------------------------------------------------------------
Test time: 102.34717297554016

Epoch: [3][38/196]	LR: 0.1	Loss 0.2861 (0.3217)	Prec@1 89.062 (88.582)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.3469 (0.3341)	Prec@1 89.453 (88.301)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.3792 (0.3417)	Prec@1 85.156 (88.074)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.4285 (0.3454)	Prec@1 83.203 (87.883)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.4553 (0.3526)	Prec@1 84.375 (87.624)	
Total train loss: 0.3528

Train time: 94.83755230903625
 * Prec@1 82.060 Prec@5 99.120 Loss 0.5537
Best acc: 82.060
--------------------------------------------------------------------------------
Test time: 117.26335453987122

Epoch: [4][38/196]	LR: 0.1	Loss 0.3381 (0.2899)	Prec@1 88.672 (89.844)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.2603 (0.2967)	Prec@1 91.016 (89.638)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.3621 (0.3037)	Prec@1 88.672 (89.406)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.3638 (0.3087)	Prec@1 89.453 (89.295)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.2006 (0.3105)	Prec@1 93.359 (89.181)	
Total train loss: 0.3106

Train time: 112.5555202960968
 * Prec@1 81.710 Prec@5 99.030 Loss 0.6138
Best acc: 82.060
--------------------------------------------------------------------------------
Test time: 134.72097992897034

Epoch: [5][38/196]	LR: 0.1	Loss 0.2050 (0.2555)	Prec@1 91.797 (91.446)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.3315 (0.2608)	Prec@1 88.672 (91.116)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.2534 (0.2682)	Prec@1 91.016 (90.722)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.2751 (0.2745)	Prec@1 89.453 (90.515)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.3335 (0.2818)	Prec@1 88.672 (90.214)	
Total train loss: 0.2820

Train time: 106.87147927284241
 * Prec@1 79.470 Prec@5 98.600 Loss 0.7002
Best acc: 82.060
--------------------------------------------------------------------------------
Test time: 127.91689157485962

Epoch: [6][38/196]	LR: 0.1	Loss 0.2812 (0.2410)	Prec@1 88.672 (91.707)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.1846 (0.2318)	Prec@1 94.922 (92.002)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.1826 (0.2400)	Prec@1 94.141 (91.720)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.2898 (0.2437)	Prec@1 88.672 (91.589)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.2947 (0.2523)	Prec@1 89.453 (91.290)	
Total train loss: 0.2523

Train time: 102.62582230567932
 * Prec@1 82.210 Prec@5 98.850 Loss 0.6162
Best acc: 82.210
--------------------------------------------------------------------------------
Test time: 123.60017347335815

Epoch: [7][38/196]	LR: 0.1	Loss 0.2040 (0.2132)	Prec@1 95.312 (92.668)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.1494 (0.2056)	Prec@1 95.312 (92.874)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.2274 (0.2133)	Prec@1 89.453 (92.581)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.3633 (0.2204)	Prec@1 87.500 (92.283)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.2676 (0.2272)	Prec@1 89.844 (92.001)	
Total train loss: 0.2277

Train time: 106.82764935493469
 * Prec@1 78.770 Prec@5 98.430 Loss 0.8047
Best acc: 82.210
--------------------------------------------------------------------------------
Test time: 127.3519515991211

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.1682 (0.1707)	Prec@1 94.141 (94.401)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.1003 (0.1508)	Prec@1 96.875 (95.182)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.1108 (0.1427)	Prec@1 96.875 (95.503)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0997 (0.1355)	Prec@1 96.875 (95.831)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.1565 (0.1320)	Prec@1 96.094 (95.966)	
Total train loss: 0.1320

Train time: 105.07185482978821
 * Prec@1 83.880 Prec@5 99.000 Loss 0.5718
Best acc: 83.880
--------------------------------------------------------------------------------
Test time: 124.9166476726532

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.1000 (0.0960)	Prec@1 97.266 (97.436)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0798 (0.0946)	Prec@1 97.656 (97.441)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0844 (0.0954)	Prec@1 97.656 (97.449)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0876 (0.0955)	Prec@1 98.828 (97.408)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0982 (0.0943)	Prec@1 97.266 (97.468)	
Total train loss: 0.0945

Train time: 102.40214657783508
 * Prec@1 83.780 Prec@5 99.030 Loss 0.5864
Best acc: 83.880
--------------------------------------------------------------------------------
Test time: 121.7042248249054

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0863 (0.0831)	Prec@1 98.047 (97.927)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0629 (0.0824)	Prec@1 98.828 (98.012)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0878 (0.0814)	Prec@1 97.656 (98.100)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0699 (0.0804)	Prec@1 97.656 (98.124)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0786 (0.0814)	Prec@1 98.828 (98.121)	
Total train loss: 0.0814

Train time: 110.2110526561737
 * Prec@1 84.000 Prec@5 98.950 Loss 0.5894
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 131.6229054927826

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0814 (0.0714)	Prec@1 98.047 (98.548)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0488 (0.0708)	Prec@1 100.000 (98.633)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0604 (0.0715)	Prec@1 98.438 (98.574)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0652 (0.0708)	Prec@1 98.828 (98.570)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0713 (0.0715)	Prec@1 98.828 (98.502)	
Total train loss: 0.0715

Train time: 100.32624840736389
 * Prec@1 83.670 Prec@5 98.950 Loss 0.6099
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 106.61986351013184

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0454 (0.0628)	Prec@1 100.000 (98.958)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0644 (0.0632)	Prec@1 98.438 (98.868)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0599 (0.0633)	Prec@1 99.609 (98.865)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0863 (0.0638)	Prec@1 98.047 (98.826)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0455 (0.0650)	Prec@1 99.609 (98.768)	
Total train loss: 0.0651

Train time: 36.249810457229614
 * Prec@1 83.420 Prec@5 98.790 Loss 0.6235
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 43.16417717933655

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0557 (0.0585)	Prec@1 98.438 (99.028)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0698 (0.0606)	Prec@1 98.828 (98.968)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0532 (0.0605)	Prec@1 98.828 (98.945)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0580 (0.0608)	Prec@1 98.047 (98.926)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0695 (0.0610)	Prec@1 99.609 (98.916)	
Total train loss: 0.0610

Train time: 53.781938552856445
 * Prec@1 83.430 Prec@5 98.790 Loss 0.6260
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 63.89932107925415

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0506 (0.0533)	Prec@1 99.609 (99.179)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0428 (0.0539)	Prec@1 99.609 (99.144)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0699 (0.0538)	Prec@1 98.828 (99.182)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0765 (0.0547)	Prec@1 97.656 (99.156)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0535 (0.0556)	Prec@1 99.609 (99.093)	
Total train loss: 0.0556

Train time: 232.09270858764648
 * Prec@1 83.380 Prec@5 98.710 Loss 0.6338
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 239.81919932365417

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0580 (0.0520)	Prec@1 98.828 (99.169)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0597 (0.0507)	Prec@1 98.828 (99.294)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0600 (0.0510)	Prec@1 98.438 (99.272)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0533 (0.0515)	Prec@1 99.219 (99.251)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0464 (0.0522)	Prec@1 99.609 (99.227)	
Total train loss: 0.0522

Train time: 53.79021716117859
 * Prec@1 83.140 Prec@5 98.620 Loss 0.6514
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 70.10490036010742

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0351 (0.0445)	Prec@1 100.000 (99.649)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0514 (0.0457)	Prec@1 99.609 (99.564)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0433 (0.0460)	Prec@1 99.219 (99.499)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0388 (0.0458)	Prec@1 100.000 (99.522)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0314 (0.0462)	Prec@1 100.000 (99.531)	
Total train loss: 0.0462

Train time: 93.23756074905396
 * Prec@1 83.280 Prec@5 98.680 Loss 0.6470
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 111.79244136810303

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0598 (0.0469)	Prec@1 98.828 (99.479)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0420 (0.0475)	Prec@1 99.609 (99.414)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0662 (0.0474)	Prec@1 98.438 (99.446)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0576 (0.0466)	Prec@1 99.219 (99.484)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0418 (0.0468)	Prec@1 99.609 (99.475)	
Total train loss: 0.0469

Train time: 94.37591695785522
 * Prec@1 83.350 Prec@5 98.690 Loss 0.6450
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 112.18088293075562

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0508 (0.0459)	Prec@1 99.219 (99.409)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0488 (0.0472)	Prec@1 99.609 (99.454)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0499 (0.0474)	Prec@1 100.000 (99.486)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0330 (0.0462)	Prec@1 99.609 (99.522)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0386 (0.0462)	Prec@1 100.000 (99.515)	
Total train loss: 0.0463

Train time: 106.40907049179077
 * Prec@1 83.100 Prec@5 98.700 Loss 0.6538
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 127.11107540130615

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0538 (0.0437)	Prec@1 98.828 (99.629)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0351 (0.0457)	Prec@1 100.000 (99.524)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0446 (0.0459)	Prec@1 99.609 (99.523)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0446 (0.0466)	Prec@1 99.219 (99.487)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0280 (0.0459)	Prec@1 100.000 (99.515)	
Total train loss: 0.0459

Train time: 102.16513705253601
 * Prec@1 83.190 Prec@5 98.670 Loss 0.6489
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 119.18382358551025

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0500 (0.0481)	Prec@1 99.219 (99.399)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0456 (0.0460)	Prec@1 99.219 (99.524)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0441 (0.0460)	Prec@1 99.609 (99.546)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0359 (0.0463)	Prec@1 99.609 (99.534)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0381 (0.0464)	Prec@1 99.609 (99.515)	
Total train loss: 0.0465

Train time: 90.17594790458679
 * Prec@1 83.290 Prec@5 98.650 Loss 0.6494
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 107.83812189102173

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0505 (0.0475)	Prec@1 99.609 (99.449)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0581 (0.0461)	Prec@1 98.438 (99.514)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0445 (0.0465)	Prec@1 100.000 (99.509)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0285 (0.0460)	Prec@1 100.000 (99.522)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0463 (0.0460)	Prec@1 99.609 (99.533)	
Total train loss: 0.0460

Train time: 98.57600903511047
 * Prec@1 83.320 Prec@5 98.660 Loss 0.6519
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 119.02964401245117

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0436 (0.0446)	Prec@1 99.609 (99.559)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0460 (0.0456)	Prec@1 99.609 (99.484)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0438 (0.0456)	Prec@1 99.219 (99.496)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0397 (0.0450)	Prec@1 99.609 (99.529)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0397 (0.0454)	Prec@1 100.000 (99.515)	
Total train loss: 0.0455

Train time: 101.54079675674438
 * Prec@1 83.290 Prec@5 98.650 Loss 0.6519
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 122.36630272865295

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0409 (0.0469)	Prec@1 100.000 (99.489)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0490 (0.0471)	Prec@1 99.609 (99.509)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0492 (0.0454)	Prec@1 99.609 (99.579)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0382 (0.0456)	Prec@1 100.000 (99.562)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0561 (0.0459)	Prec@1 99.219 (99.519)	
Total train loss: 0.0459

Train time: 101.49598002433777
 * Prec@1 83.290 Prec@5 98.640 Loss 0.6562
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 124.04180145263672

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0627 (0.0477)	Prec@1 98.438 (99.469)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0637 (0.0464)	Prec@1 99.219 (99.509)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0353 (0.0462)	Prec@1 100.000 (99.513)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0564 (0.0456)	Prec@1 98.438 (99.527)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0372 (0.0454)	Prec@1 100.000 (99.535)	
Total train loss: 0.0455

Train time: 272.82913851737976
 * Prec@1 83.300 Prec@5 98.640 Loss 0.6440
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 296.5009717941284

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0475 (0.0454)	Prec@1 99.609 (99.509)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0485 (0.0443)	Prec@1 99.609 (99.539)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0474 (0.0449)	Prec@1 100.000 (99.526)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0396 (0.0446)	Prec@1 99.609 (99.552)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0565 (0.0453)	Prec@1 98.828 (99.539)	
Total train loss: 0.0453

Train time: 96.05728316307068
 * Prec@1 83.290 Prec@5 98.650 Loss 0.6494
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 113.58260154724121

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0518 (0.0438)	Prec@1 98.828 (99.529)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0303 (0.0439)	Prec@1 100.000 (99.569)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0573 (0.0445)	Prec@1 99.609 (99.563)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0623 (0.0450)	Prec@1 99.219 (99.547)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0613 (0.0453)	Prec@1 99.219 (99.551)	
Total train loss: 0.0454

Train time: 103.40608596801758
 * Prec@1 83.340 Prec@5 98.670 Loss 0.6499
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 124.18286204338074

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0297 (0.0456)	Prec@1 100.000 (99.529)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0319 (0.0449)	Prec@1 100.000 (99.559)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0561 (0.0454)	Prec@1 99.219 (99.583)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0417 (0.0448)	Prec@1 99.219 (99.592)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0388 (0.0447)	Prec@1 100.000 (99.579)	
Total train loss: 0.0447

Train time: 105.13584423065186
 * Prec@1 83.290 Prec@5 98.640 Loss 0.6460
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 125.67017149925232

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0653 (0.0468)	Prec@1 98.828 (99.529)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0630 (0.0458)	Prec@1 99.609 (99.529)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0687 (0.0450)	Prec@1 98.438 (99.553)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0480 (0.0450)	Prec@1 99.219 (99.569)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0451 (0.0454)	Prec@1 100.000 (99.565)	
Total train loss: 0.0454

Train time: 55.145397424697876
 * Prec@1 83.410 Prec@5 98.620 Loss 0.6470
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 59.01593232154846

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0565 (0.0481)	Prec@1 99.219 (99.439)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0378 (0.0468)	Prec@1 99.219 (99.499)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0456 (0.0455)	Prec@1 99.219 (99.553)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0524 (0.0453)	Prec@1 99.609 (99.552)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0654 (0.0455)	Prec@1 99.609 (99.545)	
Total train loss: 0.0455

Train time: 35.210031270980835
 * Prec@1 83.290 Prec@5 98.650 Loss 0.6455
Best acc: 84.000
--------------------------------------------------------------------------------
Test time: 41.95764660835266


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 20.140 Prec@5 77.020 Loss 3.8711
Pre-trained Prec@1 with 17 layers frozen: 20.139999389648438 	 Loss: 3.87109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.8633 (1.2205)	Prec@1 69.531 (63.431)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.6426 (1.0161)	Prec@1 76.953 (67.763)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.6758 (0.9304)	Prec@1 75.781 (69.822)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.7275 (0.8764)	Prec@1 74.219 (71.231)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.6787 (0.8382)	Prec@1 78.125 (72.288)	
Total train loss: 0.8379

Train time: 605.9679307937622
 * Prec@1 75.630 Prec@5 98.200 Loss 0.7393
Best acc: 75.630
--------------------------------------------------------------------------------
Test time: 633.7659673690796

Epoch: [1][38/196]	LR: 0.1	Loss 0.5200 (0.6366)	Prec@1 80.469 (77.764)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.5811 (0.6365)	Prec@1 78.516 (78.000)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.6318 (0.6290)	Prec@1 79.688 (78.095)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.6152 (0.6310)	Prec@1 76.562 (78.050)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.8076 (0.6275)	Prec@1 72.656 (78.215)	
Total train loss: 0.6274

Train time: 103.85667538642883
 * Prec@1 75.030 Prec@5 97.490 Loss 0.8018
Best acc: 75.630
--------------------------------------------------------------------------------
Test time: 126.75255918502808

Epoch: [2][38/196]	LR: 0.1	Loss 0.5562 (0.5838)	Prec@1 82.031 (80.078)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.4941 (0.5764)	Prec@1 81.250 (80.073)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.6255 (0.5725)	Prec@1 77.734 (80.265)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.5669 (0.5718)	Prec@1 77.344 (80.276)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.6230 (0.5721)	Prec@1 77.734 (80.256)	
Total train loss: 0.5721

Train time: 98.77727556228638
 * Prec@1 78.350 Prec@5 98.420 Loss 0.6519
Best acc: 78.350
--------------------------------------------------------------------------------
Test time: 120.0360734462738

Epoch: [3][38/196]	LR: 0.1	Loss 0.5225 (0.5197)	Prec@1 80.469 (82.442)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.5850 (0.5198)	Prec@1 79.688 (82.207)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.6392 (0.5237)	Prec@1 74.609 (81.858)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.5288 (0.5241)	Prec@1 80.469 (81.778)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.5176 (0.5285)	Prec@1 81.641 (81.725)	
Total train loss: 0.5283

Train time: 104.18822240829468
 * Prec@1 78.040 Prec@5 98.470 Loss 0.6846
Best acc: 78.350
--------------------------------------------------------------------------------
Test time: 128.47349500656128

Epoch: [4][38/196]	LR: 0.1	Loss 0.5020 (0.4761)	Prec@1 83.594 (83.744)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.4458 (0.4865)	Prec@1 85.156 (83.278)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.5781 (0.4887)	Prec@1 80.469 (83.103)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.5161 (0.4944)	Prec@1 82.812 (82.993)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.5132 (0.4954)	Prec@1 80.859 (82.893)	
Total train loss: 0.4953

Train time: 106.54032492637634
 * Prec@1 79.440 Prec@5 98.630 Loss 0.6230
Best acc: 79.440
--------------------------------------------------------------------------------
Test time: 127.2172691822052

Epoch: [5][38/196]	LR: 0.1	Loss 0.4788 (0.4504)	Prec@1 83.203 (84.866)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.4758 (0.4528)	Prec@1 81.641 (84.290)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.4946 (0.4632)	Prec@1 80.859 (83.931)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.4915 (0.4674)	Prec@1 85.547 (83.812)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.5234 (0.4676)	Prec@1 83.203 (83.766)	
Total train loss: 0.4678

Train time: 101.58439326286316
 * Prec@1 77.130 Prec@5 98.370 Loss 0.7222
Best acc: 79.440
--------------------------------------------------------------------------------
Test time: 126.06229829788208

Epoch: [6][38/196]	LR: 0.1	Loss 0.4626 (0.4257)	Prec@1 83.203 (84.916)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.5249 (0.4320)	Prec@1 83.594 (84.746)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.4841 (0.4371)	Prec@1 81.250 (84.549)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.4644 (0.4365)	Prec@1 85.156 (84.673)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.5005 (0.4427)	Prec@1 80.469 (84.499)	
Total train loss: 0.4427

Train time: 105.22817039489746
 * Prec@1 77.400 Prec@5 98.520 Loss 0.7305
Best acc: 79.440
--------------------------------------------------------------------------------
Test time: 127.72949552536011

Epoch: [7][38/196]	LR: 0.1	Loss 0.3401 (0.3908)	Prec@1 87.109 (86.739)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.4314 (0.4063)	Prec@1 83.594 (86.023)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.4548 (0.4130)	Prec@1 84.766 (85.824)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.4165 (0.4173)	Prec@1 84.375 (85.665)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.4863 (0.4197)	Prec@1 82.812 (85.557)	
Total train loss: 0.4196

Train time: 98.11377429962158
 * Prec@1 77.290 Prec@5 98.540 Loss 0.7407
Best acc: 79.440
--------------------------------------------------------------------------------
Test time: 120.28842425346375

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.3511 (0.3598)	Prec@1 89.453 (87.770)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.3123 (0.3445)	Prec@1 91.016 (88.381)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.3040 (0.3325)	Prec@1 91.406 (88.849)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.3401 (0.3254)	Prec@1 89.844 (89.047)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.3064 (0.3218)	Prec@1 91.406 (89.141)	
Total train loss: 0.3218

Train time: 105.4871175289154
 * Prec@1 80.840 Prec@5 98.730 Loss 0.6118
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 125.89265012741089

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.3054 (0.2864)	Prec@1 88.281 (90.655)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.3381 (0.2907)	Prec@1 87.891 (90.525)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.2637 (0.2891)	Prec@1 91.406 (90.522)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.2739 (0.2891)	Prec@1 90.234 (90.580)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.4119 (0.2917)	Prec@1 87.500 (90.495)	
Total train loss: 0.2918

Train time: 105.08848667144775
 * Prec@1 80.590 Prec@5 98.600 Loss 0.6230
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 125.31189060211182

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.3120 (0.2755)	Prec@1 88.281 (90.956)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.3008 (0.2813)	Prec@1 89.844 (90.815)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.2559 (0.2818)	Prec@1 92.578 (90.879)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.3025 (0.2798)	Prec@1 89.062 (90.818)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.3062 (0.2800)	Prec@1 90.625 (90.775)	
Total train loss: 0.2800

Train time: 103.2418053150177
 * Prec@1 80.350 Prec@5 98.610 Loss 0.6279
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 123.79458951950073

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.2717 (0.2547)	Prec@1 92.188 (91.927)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.2112 (0.2621)	Prec@1 92.578 (91.592)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.2537 (0.2634)	Prec@1 91.406 (91.627)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.3022 (0.2675)	Prec@1 90.234 (91.376)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.2423 (0.2691)	Prec@1 91.797 (91.280)	
Total train loss: 0.2692

Train time: 45.07096791267395
 * Prec@1 80.540 Prec@5 98.700 Loss 0.6372
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 51.32073092460632

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.2094 (0.2500)	Prec@1 93.359 (91.737)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.3008 (0.2598)	Prec@1 90.234 (91.456)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.2993 (0.2613)	Prec@1 90.234 (91.406)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.2803 (0.2640)	Prec@1 90.234 (91.331)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.3135 (0.2622)	Prec@1 89.844 (91.426)	
Total train loss: 0.2624

Train time: 43.995386600494385
 * Prec@1 80.300 Prec@5 98.490 Loss 0.6499
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 54.74044132232666

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.2927 (0.2484)	Prec@1 91.016 (91.977)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.2512 (0.2556)	Prec@1 92.578 (91.682)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.2333 (0.2540)	Prec@1 93.359 (91.743)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.2649 (0.2522)	Prec@1 91.797 (91.759)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.2019 (0.2537)	Prec@1 92.969 (91.675)	
Total train loss: 0.2536

Train time: 52.243934869766235
 * Prec@1 80.480 Prec@5 98.510 Loss 0.6504
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 62.66348147392273

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.1849 (0.2470)	Prec@1 96.484 (92.268)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.2156 (0.2478)	Prec@1 92.188 (92.157)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.2278 (0.2464)	Prec@1 91.797 (92.084)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.3254 (0.2495)	Prec@1 90.234 (91.987)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.2627 (0.2474)	Prec@1 91.016 (92.071)	
Total train loss: 0.2475

Train time: 47.292003870010376
 * Prec@1 80.280 Prec@5 98.460 Loss 0.6567
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 54.05404758453369

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.2690 (0.2369)	Prec@1 91.797 (92.588)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.2214 (0.2329)	Prec@1 95.312 (92.733)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.2600 (0.2366)	Prec@1 93.359 (92.605)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.2490 (0.2384)	Prec@1 91.406 (92.518)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.2288 (0.2395)	Prec@1 92.578 (92.434)	
Total train loss: 0.2396

Train time: 50.942432165145874
 * Prec@1 80.290 Prec@5 98.520 Loss 0.6655
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 61.583351373672485

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.2415 (0.2238)	Prec@1 93.359 (93.129)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.2108 (0.2199)	Prec@1 94.141 (93.239)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.2693 (0.2228)	Prec@1 92.969 (93.179)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.1785 (0.2233)	Prec@1 93.750 (93.127)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.2269 (0.2238)	Prec@1 94.922 (93.157)	
Total train loss: 0.2238

Train time: 46.44279408454895
 * Prec@1 80.060 Prec@5 98.490 Loss 0.6665
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 51.04011368751526

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.2465 (0.2142)	Prec@1 92.188 (93.600)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.2216 (0.2162)	Prec@1 92.188 (93.334)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.1932 (0.2162)	Prec@1 95.312 (93.416)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.1825 (0.2187)	Prec@1 94.141 (93.324)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.2109 (0.2206)	Prec@1 92.188 (93.247)	
Total train loss: 0.2207

Train time: 33.07145142555237
 * Prec@1 80.240 Prec@5 98.450 Loss 0.6655
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 47.117433071136475

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.2045 (0.2228)	Prec@1 94.531 (93.249)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.1930 (0.2217)	Prec@1 95.703 (93.284)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.2164 (0.2180)	Prec@1 92.969 (93.450)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.2554 (0.2201)	Prec@1 91.797 (93.384)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.1554 (0.2204)	Prec@1 96.094 (93.337)	
Total train loss: 0.2205

Train time: 69.71944165229797
 * Prec@1 80.290 Prec@5 98.470 Loss 0.6626
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 91.25006604194641

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.2324 (0.2171)	Prec@1 92.969 (93.359)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.2006 (0.2181)	Prec@1 92.578 (93.284)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.2927 (0.2166)	Prec@1 89.062 (93.406)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.2544 (0.2197)	Prec@1 92.969 (93.249)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.1910 (0.2194)	Prec@1 94.141 (93.305)	
Total train loss: 0.2195

Train time: 115.52545189857483
 * Prec@1 80.140 Prec@5 98.460 Loss 0.6646
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 137.74955797195435

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.2036 (0.2148)	Prec@1 93.359 (93.650)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.1943 (0.2138)	Prec@1 96.094 (93.670)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.2590 (0.2179)	Prec@1 91.797 (93.419)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.2225 (0.2174)	Prec@1 92.578 (93.402)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.1340 (0.2168)	Prec@1 95.703 (93.435)	
Total train loss: 0.2171

Train time: 128.29103589057922
 * Prec@1 80.140 Prec@5 98.510 Loss 0.6680
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 155.2309274673462

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.1997 (0.2260)	Prec@1 92.578 (93.019)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.2372 (0.2209)	Prec@1 92.188 (93.219)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.1844 (0.2199)	Prec@1 94.141 (93.266)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.2238 (0.2210)	Prec@1 93.750 (93.247)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.1866 (0.2196)	Prec@1 96.484 (93.313)	
Total train loss: 0.2196

Train time: 144.78676843643188
 * Prec@1 80.200 Prec@5 98.460 Loss 0.6655
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 191.16990685462952

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.2856 (0.2228)	Prec@1 90.625 (93.009)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.2961 (0.2230)	Prec@1 89.453 (93.169)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.2915 (0.2229)	Prec@1 89.453 (93.179)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.2161 (0.2212)	Prec@1 92.578 (93.242)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.2148 (0.2196)	Prec@1 92.969 (93.281)	
Total train loss: 0.2196

Train time: 104.22952222824097
 * Prec@1 80.170 Prec@5 98.460 Loss 0.6655
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 130.00211143493652

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.1880 (0.2098)	Prec@1 93.750 (93.680)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.1935 (0.2159)	Prec@1 95.312 (93.575)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.1973 (0.2169)	Prec@1 94.922 (93.506)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.2637 (0.2176)	Prec@1 91.797 (93.527)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.2037 (0.2178)	Prec@1 93.750 (93.496)	
Total train loss: 0.2183

Train time: 94.9758415222168
 * Prec@1 80.260 Prec@5 98.520 Loss 0.6694
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 120.75072646141052

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.2520 (0.2161)	Prec@1 91.016 (93.670)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.2686 (0.2174)	Prec@1 91.016 (93.470)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.2206 (0.2177)	Prec@1 92.969 (93.389)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.2175 (0.2190)	Prec@1 95.312 (93.287)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.1914 (0.2190)	Prec@1 93.750 (93.301)	
Total train loss: 0.2189

Train time: 104.68924236297607
 * Prec@1 80.020 Prec@5 98.470 Loss 0.6704
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 123.08162498474121

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.2617 (0.2158)	Prec@1 93.750 (93.680)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.2563 (0.2163)	Prec@1 91.016 (93.560)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.2292 (0.2176)	Prec@1 93.359 (93.446)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.2385 (0.2171)	Prec@1 93.359 (93.455)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.2754 (0.2174)	Prec@1 91.016 (93.466)	
Total train loss: 0.2175

Train time: 88.84069633483887
 * Prec@1 80.150 Prec@5 98.480 Loss 0.6714
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 107.92928791046143

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.1412 (0.2218)	Prec@1 95.703 (93.089)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.1699 (0.2171)	Prec@1 94.531 (93.434)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.2168 (0.2157)	Prec@1 93.359 (93.526)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.2386 (0.2177)	Prec@1 93.359 (93.417)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.2407 (0.2172)	Prec@1 91.797 (93.421)	
Total train loss: 0.2172

Train time: 87.46776390075684
 * Prec@1 80.160 Prec@5 98.490 Loss 0.6680
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 112.90420436859131

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.1926 (0.2254)	Prec@1 94.531 (93.209)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.1862 (0.2216)	Prec@1 94.531 (93.219)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.2089 (0.2151)	Prec@1 92.969 (93.523)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.2195 (0.2169)	Prec@1 93.359 (93.507)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.1992 (0.2160)	Prec@1 92.969 (93.554)	
Total train loss: 0.2160

Train time: 98.31990909576416
 * Prec@1 80.090 Prec@5 98.460 Loss 0.6709
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 119.38287615776062

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.2620 (0.2230)	Prec@1 91.016 (93.239)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.2324 (0.2198)	Prec@1 92.969 (93.384)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.2374 (0.2181)	Prec@1 91.406 (93.336)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.2278 (0.2201)	Prec@1 91.797 (93.267)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.2019 (0.2176)	Prec@1 93.750 (93.387)	
Total train loss: 0.2176

Train time: 106.89558029174805
 * Prec@1 80.330 Prec@5 98.490 Loss 0.6650
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 126.75805425643921

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.2300 (0.2138)	Prec@1 91.797 (93.189)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.2344 (0.2168)	Prec@1 94.531 (93.174)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.2471 (0.2186)	Prec@1 92.188 (93.192)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.2089 (0.2203)	Prec@1 95.312 (93.202)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.1884 (0.2207)	Prec@1 94.922 (93.193)	
Total train loss: 0.2207

Train time: 119.90129709243774
 * Prec@1 80.020 Prec@5 98.490 Loss 0.6709
Best acc: 80.840
--------------------------------------------------------------------------------
Test time: 150.73710799217224


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 17.440 Prec@5 76.930 Loss 3.9102
Pre-trained Prec@1 with 19 layers frozen: 17.439998626708984 	 Loss: 3.91015625

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 1.0957 (1.5215)	Prec@1 63.281 (57.772)	
Epoch: [0][77/196]	LR: 0.1	Loss 1.0615 (1.3216)	Prec@1 61.719 (59.761)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.9858 (1.2323)	Prec@1 61.719 (61.268)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.9536 (1.1871)	Prec@1 67.188 (62.129)	
Epoch: [0][194/196]	LR: 0.1	Loss 1.1338 (1.1584)	Prec@1 62.500 (62.510)	
Total train loss: 1.1584

Train time: 647.3512213230133
 * Prec@1 67.500 Prec@5 96.390 Loss 0.9727
Best acc: 67.500
--------------------------------------------------------------------------------
Test time: 680.2373616695404

Epoch: [1][38/196]	LR: 0.1	Loss 0.9546 (0.9796)	Prec@1 67.969 (66.867)	
Epoch: [1][77/196]	LR: 0.1	Loss 1.0186 (1.0034)	Prec@1 64.844 (66.006)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.9932 (1.0077)	Prec@1 65.625 (65.735)	
Epoch: [1][155/196]	LR: 0.1	Loss 1.1270 (1.0134)	Prec@1 61.328 (65.530)	
Epoch: [1][194/196]	LR: 0.1	Loss 1.0537 (1.0146)	Prec@1 66.406 (65.471)	
Total train loss: 1.0152

Train time: 318.128142118454
 * Prec@1 68.190 Prec@5 96.540 Loss 0.9502
Best acc: 68.190
--------------------------------------------------------------------------------
Test time: 361.03201055526733

Epoch: [2][38/196]	LR: 0.1	Loss 0.9966 (1.0130)	Prec@1 66.016 (65.355)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.9331 (0.9998)	Prec@1 66.016 (65.780)	
Epoch: [2][116/196]	LR: 0.1	Loss 1.1221 (1.0016)	Prec@1 59.766 (65.956)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.8135 (1.0006)	Prec@1 71.875 (65.883)	
Epoch: [2][194/196]	LR: 0.1	Loss 1.0752 (1.0028)	Prec@1 64.453 (65.851)	
Total train loss: 1.0032

Train time: 211.81727123260498
 * Prec@1 67.850 Prec@5 96.620 Loss 0.9575
Best acc: 68.190
--------------------------------------------------------------------------------
Test time: 240.86403131484985

Epoch: [3][38/196]	LR: 0.1	Loss 1.0771 (1.0110)	Prec@1 62.109 (65.815)	
Epoch: [3][77/196]	LR: 0.1	Loss 1.0205 (1.0007)	Prec@1 64.453 (66.221)	
Epoch: [3][116/196]	LR: 0.1	Loss 1.1367 (1.0002)	Prec@1 60.547 (65.949)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.9839 (1.0004)	Prec@1 64.453 (65.830)	
Epoch: [3][194/196]	LR: 0.1	Loss 1.1836 (1.0011)	Prec@1 59.375 (65.839)	
Total train loss: 1.0008

Train time: 119.22891783714294
 * Prec@1 68.140 Prec@5 96.620 Loss 0.9438
Best acc: 68.190
--------------------------------------------------------------------------------
Test time: 142.9608097076416

Epoch: [4][38/196]	LR: 0.1	Loss 1.0205 (1.0072)	Prec@1 64.453 (65.755)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.9473 (1.0054)	Prec@1 68.359 (65.705)	
Epoch: [4][116/196]	LR: 0.1	Loss 1.0557 (0.9991)	Prec@1 64.453 (66.072)	
Epoch: [4][155/196]	LR: 0.1	Loss 1.0137 (1.0002)	Prec@1 64.453 (66.001)	
Epoch: [4][194/196]	LR: 0.1	Loss 1.0439 (0.9963)	Prec@1 66.016 (66.050)	
Total train loss: 0.9965

Train time: 111.4362518787384
 * Prec@1 68.620 Prec@5 96.650 Loss 0.9346
Best acc: 68.620
--------------------------------------------------------------------------------
Test time: 136.33494758605957

Epoch: [5][38/196]	LR: 0.1	Loss 1.0967 (0.9964)	Prec@1 61.719 (66.056)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.9814 (1.0021)	Prec@1 68.750 (65.890)	
Epoch: [5][116/196]	LR: 0.1	Loss 1.0273 (0.9983)	Prec@1 65.625 (65.899)	
Epoch: [5][155/196]	LR: 0.1	Loss 1.0664 (0.9946)	Prec@1 61.328 (65.963)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.9634 (0.9947)	Prec@1 69.141 (66.006)	
Total train loss: 0.9949

Train time: 120.79872679710388
 * Prec@1 68.520 Prec@5 96.900 Loss 0.9336
Best acc: 68.620
--------------------------------------------------------------------------------
Test time: 146.33289289474487

Epoch: [6][38/196]	LR: 0.1	Loss 1.0352 (0.9778)	Prec@1 62.109 (66.927)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.9375 (0.9923)	Prec@1 67.188 (66.211)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.9795 (0.9935)	Prec@1 66.406 (66.166)	
Epoch: [6][155/196]	LR: 0.1	Loss 1.1094 (0.9917)	Prec@1 64.062 (66.208)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.9780 (0.9967)	Prec@1 69.141 (66.130)	
Total train loss: 0.9967

Train time: 117.9884090423584
 * Prec@1 68.520 Prec@5 96.730 Loss 0.9370
Best acc: 68.620
--------------------------------------------------------------------------------
Test time: 144.7561547756195

Epoch: [7][38/196]	LR: 0.1	Loss 0.9277 (0.9885)	Prec@1 67.969 (66.126)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.9595 (0.9831)	Prec@1 64.453 (66.296)	
Epoch: [7][116/196]	LR: 0.1	Loss 1.1006 (0.9915)	Prec@1 61.328 (66.169)	
Epoch: [7][155/196]	LR: 0.1	Loss 1.0547 (0.9938)	Prec@1 64.453 (66.118)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.9819 (0.9931)	Prec@1 66.016 (66.150)	
Total train loss: 0.9929

Train time: 123.98302245140076
 * Prec@1 68.390 Prec@5 96.640 Loss 0.9414
Best acc: 68.620
--------------------------------------------------------------------------------
Test time: 148.53075337409973

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.9165 (0.9727)	Prec@1 71.875 (66.697)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.8818 (0.9722)	Prec@1 70.703 (66.707)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.9761 (0.9732)	Prec@1 66.406 (66.653)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.9116 (0.9791)	Prec@1 71.484 (66.554)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 1.0508 (0.9844)	Prec@1 60.938 (66.402)	
Total train loss: 0.9844

Train time: 121.84777188301086
 * Prec@1 68.670 Prec@5 96.780 Loss 0.9321
Best acc: 68.670
--------------------------------------------------------------------------------
Test time: 143.38479590415955

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.9272 (0.9609)	Prec@1 67.188 (67.188)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 1.1465 (0.9721)	Prec@1 63.281 (66.957)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 1.0713 (0.9767)	Prec@1 62.109 (66.697)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 1.0234 (0.9817)	Prec@1 67.578 (66.544)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 1.0449 (0.9811)	Prec@1 58.984 (66.556)	
Total train loss: 0.9811

Train time: 47.58989405632019
 * Prec@1 68.480 Prec@5 96.740 Loss 0.9297
Best acc: 68.670
--------------------------------------------------------------------------------
Test time: 58.73140573501587

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.9385 (0.9838)	Prec@1 67.578 (66.516)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.9863 (0.9792)	Prec@1 62.891 (66.632)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 1.2031 (0.9787)	Prec@1 60.938 (66.740)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 1.0146 (0.9785)	Prec@1 66.406 (66.747)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 1.0693 (0.9799)	Prec@1 64.062 (66.701)	
Total train loss: 0.9799

Train time: 42.84146595001221
 * Prec@1 68.630 Prec@5 96.780 Loss 0.9321
Best acc: 68.670
--------------------------------------------------------------------------------
Test time: 51.96611404418945

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 1.0586 (0.9796)	Prec@1 66.797 (66.767)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.9478 (0.9753)	Prec@1 65.625 (66.692)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.9951 (0.9732)	Prec@1 67.578 (66.927)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 1.0957 (0.9786)	Prec@1 65.625 (66.747)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 1.1729 (0.9794)	Prec@1 61.328 (66.745)	
Total train loss: 0.9795

Train time: 44.54340863227844
 * Prec@1 68.710 Prec@5 96.790 Loss 0.9297
Best acc: 68.710
--------------------------------------------------------------------------------
Test time: 54.147611141204834

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.9932 (0.9658)	Prec@1 67.969 (67.027)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.8984 (0.9724)	Prec@1 68.359 (66.767)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 1.1475 (0.9762)	Prec@1 59.375 (66.580)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.9531 (0.9790)	Prec@1 69.922 (66.664)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.9189 (0.9783)	Prec@1 68.750 (66.661)	
Total train loss: 0.9783

Train time: 37.213783740997314
 * Prec@1 68.740 Prec@5 96.860 Loss 0.9272
Best acc: 68.740
--------------------------------------------------------------------------------
Test time: 44.41302466392517

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.8525 (0.9777)	Prec@1 69.531 (66.456)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 1.0557 (0.9784)	Prec@1 65.625 (66.712)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.9482 (0.9787)	Prec@1 68.750 (66.643)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.9136 (0.9814)	Prec@1 69.922 (66.642)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.8618 (0.9781)	Prec@1 67.578 (66.743)	
Total train loss: 0.9783

Train time: 53.91412973403931
 * Prec@1 68.740 Prec@5 96.780 Loss 0.9248
Best acc: 68.740
--------------------------------------------------------------------------------
Test time: 64.73627400398254

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 1.0488 (0.9758)	Prec@1 66.797 (67.027)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 1.0244 (0.9713)	Prec@1 61.328 (67.067)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.9346 (0.9737)	Prec@1 67.188 (66.817)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 1.0557 (0.9768)	Prec@1 60.938 (66.649)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 1.0469 (0.9780)	Prec@1 64.453 (66.625)	
Total train loss: 0.9782

Train time: 47.44363713264465
 * Prec@1 68.770 Prec@5 96.890 Loss 0.9282
Best acc: 68.770
--------------------------------------------------------------------------------
Test time: 51.15809178352356

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.8945 (0.9688)	Prec@1 69.531 (67.248)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 1.0469 (0.9770)	Prec@1 65.234 (66.777)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.9795 (0.9812)	Prec@1 64.453 (66.400)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.9429 (0.9782)	Prec@1 69.141 (66.639)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 1.0068 (0.9796)	Prec@1 65.234 (66.629)	
Total train loss: 0.9795

Train time: 20.254823923110962
 * Prec@1 68.830 Prec@5 96.830 Loss 0.9287
Best acc: 68.830
--------------------------------------------------------------------------------
Test time: 32.8380126953125

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.9341 (0.9787)	Prec@1 69.922 (66.526)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 1.0098 (0.9721)	Prec@1 69.141 (66.962)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.9756 (0.9772)	Prec@1 69.531 (66.780)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.9873 (0.9797)	Prec@1 69.531 (66.599)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.9346 (0.9781)	Prec@1 67.578 (66.619)	
Total train loss: 0.9781

Train time: 79.20236277580261
 * Prec@1 68.840 Prec@5 96.830 Loss 0.9263
Best acc: 68.840
--------------------------------------------------------------------------------
Test time: 98.22920823097229

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.9375 (0.9939)	Prec@1 67.969 (65.996)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.9658 (0.9837)	Prec@1 68.750 (66.376)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.8569 (0.9813)	Prec@1 72.656 (66.423)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 1.1426 (0.9799)	Prec@1 62.891 (66.549)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 1.0010 (0.9779)	Prec@1 66.406 (66.647)	
Total train loss: 0.9780

Train time: 94.90622186660767
 * Prec@1 68.920 Prec@5 96.810 Loss 0.9258
Best acc: 68.920
--------------------------------------------------------------------------------
Test time: 113.92228651046753

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.9443 (0.9764)	Prec@1 69.531 (66.837)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.8394 (0.9672)	Prec@1 72.266 (67.052)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.9736 (0.9728)	Prec@1 69.141 (66.784)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.9575 (0.9806)	Prec@1 69.141 (66.481)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 1.0283 (0.9772)	Prec@1 63.672 (66.591)	
Total train loss: 0.9772

Train time: 89.70128536224365
 * Prec@1 68.940 Prec@5 96.850 Loss 0.9287
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 110.88302302360535

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 1.1016 (0.9668)	Prec@1 60.938 (67.278)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 1.0039 (0.9770)	Prec@1 64.453 (66.747)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 1.1406 (0.9815)	Prec@1 57.812 (66.420)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.9307 (0.9767)	Prec@1 64.062 (66.541)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 1.0430 (0.9781)	Prec@1 64.453 (66.615)	
Total train loss: 0.9781

Train time: 167.54432487487793
 * Prec@1 68.860 Prec@5 96.810 Loss 0.9248
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 257.8166272640228

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.9287 (0.9764)	Prec@1 67.578 (66.997)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.9346 (0.9700)	Prec@1 67.188 (66.967)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.9932 (0.9746)	Prec@1 69.531 (66.810)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 1.0488 (0.9773)	Prec@1 63.672 (66.642)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 1.0771 (0.9772)	Prec@1 65.625 (66.727)	
Total train loss: 0.9773

Train time: 120.01017928123474
 * Prec@1 68.780 Prec@5 96.800 Loss 0.9248
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 136.93535709381104

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 1.0439 (0.9810)	Prec@1 65.625 (66.617)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 1.0342 (0.9832)	Prec@1 65.234 (66.476)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.8818 (0.9752)	Prec@1 70.312 (66.787)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.9404 (0.9770)	Prec@1 67.969 (66.719)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 1.0625 (0.9765)	Prec@1 62.109 (66.749)	
Total train loss: 0.9763

Train time: 90.57525825500488
 * Prec@1 68.780 Prec@5 96.810 Loss 0.9263
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 111.43455862998962

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.9604 (0.9797)	Prec@1 66.797 (66.516)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 1.0068 (0.9790)	Prec@1 66.797 (66.712)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.9736 (0.9772)	Prec@1 65.625 (66.774)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.9619 (0.9769)	Prec@1 69.922 (66.677)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 1.0352 (0.9792)	Prec@1 65.234 (66.573)	
Total train loss: 0.9796

Train time: 93.67995476722717
 * Prec@1 68.840 Prec@5 96.810 Loss 0.9282
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 114.77725791931152

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.8779 (0.9590)	Prec@1 68.359 (67.288)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.9756 (0.9639)	Prec@1 71.484 (67.348)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.8784 (0.9708)	Prec@1 69.922 (66.867)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 1.0420 (0.9725)	Prec@1 65.625 (66.812)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 1.1221 (0.9788)	Prec@1 62.891 (66.627)	
Total train loss: 0.9784

Train time: 94.09814763069153
 * Prec@1 68.770 Prec@5 96.830 Loss 0.9272
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 112.73020505905151

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.8701 (0.9870)	Prec@1 69.141 (66.456)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.8657 (0.9773)	Prec@1 70.312 (66.832)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 1.0059 (0.9810)	Prec@1 66.406 (66.600)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 1.0186 (0.9825)	Prec@1 64.844 (66.471)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.9204 (0.9784)	Prec@1 67.969 (66.615)	
Total train loss: 0.9784

Train time: 97.17121481895447
 * Prec@1 68.870 Prec@5 96.790 Loss 0.9258
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 116.94819045066833

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.9351 (0.9669)	Prec@1 66.797 (67.198)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 1.0303 (0.9707)	Prec@1 66.016 (67.047)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.7949 (0.9762)	Prec@1 73.438 (66.750)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.9141 (0.9752)	Prec@1 64.844 (66.724)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.9004 (0.9775)	Prec@1 69.922 (66.627)	
Total train loss: 0.9775

Train time: 100.04661130905151
 * Prec@1 68.800 Prec@5 96.830 Loss 0.9282
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 119.07503128051758

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 1.0693 (0.9757)	Prec@1 67.188 (66.366)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 1.0205 (0.9821)	Prec@1 64.453 (66.191)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 1.0088 (0.9776)	Prec@1 62.109 (66.433)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.9614 (0.9737)	Prec@1 66.797 (66.624)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.9590 (0.9781)	Prec@1 67.969 (66.625)	
Total train loss: 0.9777

Train time: 93.21315240859985
 * Prec@1 68.890 Prec@5 96.840 Loss 0.9272
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 112.2212278842926

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 1.0410 (0.9657)	Prec@1 64.453 (67.588)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 1.0469 (0.9752)	Prec@1 66.016 (66.877)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 1.1104 (0.9796)	Prec@1 57.812 (66.733)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.8135 (0.9774)	Prec@1 72.656 (66.779)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.9492 (0.9778)	Prec@1 66.797 (66.653)	
Total train loss: 0.9779

Train time: 93.95498752593994
 * Prec@1 68.830 Prec@5 96.850 Loss 0.9287
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 114.12370347976685

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.9531 (0.9785)	Prec@1 68.750 (66.416)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.9238 (0.9809)	Prec@1 65.625 (66.266)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 1.0234 (0.9817)	Prec@1 64.844 (66.286)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.8452 (0.9775)	Prec@1 68.750 (66.471)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.8374 (0.9772)	Prec@1 70.703 (66.587)	
Total train loss: 0.9773

Train time: 95.79517436027527
 * Prec@1 68.860 Prec@5 96.860 Loss 0.9248
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 109.7475049495697

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.9053 (0.9743)	Prec@1 71.875 (67.137)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.8408 (0.9708)	Prec@1 72.656 (66.837)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 1.0059 (0.9749)	Prec@1 65.625 (66.730)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 1.0068 (0.9774)	Prec@1 66.406 (66.654)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 1.1523 (0.9768)	Prec@1 64.062 (66.663)	
Total train loss: 0.9766

Train time: 21.337167024612427
 * Prec@1 68.790 Prec@5 96.820 Loss 0.9248
Best acc: 68.940
--------------------------------------------------------------------------------
Test time: 25.426944732666016

