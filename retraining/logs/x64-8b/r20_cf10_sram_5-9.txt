
      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 89.230 Prec@5 99.590 Loss 0.4253
Pre-trained Prec@1 with 5 layers frozen: 89.22999572753906 	 Loss: 0.42529296875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0965 (0.0966)	Prec@1 95.312 (96.635)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.1175 (0.1178)	Prec@1 96.094 (95.813)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.1381 (0.1307)	Prec@1 94.141 (95.353)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1562 (0.1344)	Prec@1 94.531 (95.197)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.1390 (0.1387)	Prec@1 93.750 (95.028)	
Total train loss: 0.1388

Train time: 201.75587439537048
 * Prec@1 88.280 Prec@5 99.410 Loss 0.3882
Best acc: 88.280
--------------------------------------------------------------------------------
Test time: 206.65240168571472

Epoch: [1][38/196]	LR: 0.1	Loss 0.0891 (0.0962)	Prec@1 96.875 (96.765)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.0887 (0.0923)	Prec@1 96.484 (96.980)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1088 (0.0933)	Prec@1 95.703 (96.875)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1301 (0.0956)	Prec@1 94.922 (96.745)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1487 (0.1006)	Prec@1 93.750 (96.550)	
Total train loss: 0.1007

Train time: 21.857588529586792
 * Prec@1 88.040 Prec@5 99.340 Loss 0.4297
Best acc: 88.280
--------------------------------------------------------------------------------
Test time: 26.3848774433136

Epoch: [2][38/196]	LR: 0.1	Loss 0.0359 (0.0811)	Prec@1 100.000 (97.366)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.0496 (0.0745)	Prec@1 98.828 (97.586)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.0828 (0.0707)	Prec@1 96.484 (97.706)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.0825 (0.0737)	Prec@1 96.875 (97.579)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.0651 (0.0762)	Prec@1 98.047 (97.490)	
Total train loss: 0.0763

Train time: 21.871520519256592
 * Prec@1 88.010 Prec@5 99.450 Loss 0.4536
Best acc: 88.280
--------------------------------------------------------------------------------
Test time: 26.455121755599976

Epoch: [3][38/196]	LR: 0.1	Loss 0.0403 (0.0537)	Prec@1 98.828 (98.427)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.0795 (0.0493)	Prec@1 96.875 (98.543)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.0973 (0.0511)	Prec@1 96.094 (98.461)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.0848 (0.0558)	Prec@1 97.266 (98.270)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.0735 (0.0591)	Prec@1 96.875 (98.107)	
Total train loss: 0.0593

Train time: 21.803439378738403
 * Prec@1 88.540 Prec@5 99.480 Loss 0.4324
Best acc: 88.540
--------------------------------------------------------------------------------
Test time: 26.89668369293213

Epoch: [4][38/196]	LR: 0.1	Loss 0.0659 (0.0563)	Prec@1 96.875 (98.187)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0432 (0.0545)	Prec@1 98.438 (98.232)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.0581 (0.0551)	Prec@1 97.266 (98.231)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.0818 (0.0567)	Prec@1 96.094 (98.165)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.0699 (0.0579)	Prec@1 97.656 (98.111)	
Total train loss: 0.0579

Train time: 22.01864790916443
 * Prec@1 88.060 Prec@5 99.430 Loss 0.4719
Best acc: 88.540
--------------------------------------------------------------------------------
Test time: 26.889832973480225

Epoch: [5][38/196]	LR: 0.1	Loss 0.0523 (0.0503)	Prec@1 98.438 (98.347)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0554 (0.0448)	Prec@1 98.047 (98.628)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.0392 (0.0416)	Prec@1 99.219 (98.745)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.0352 (0.0426)	Prec@1 99.219 (98.693)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.0571 (0.0445)	Prec@1 96.875 (98.630)	
Total train loss: 0.0446

Train time: 20.971706867218018
 * Prec@1 87.630 Prec@5 99.090 Loss 0.5249
Best acc: 88.540
--------------------------------------------------------------------------------
Test time: 25.3330717086792

Epoch: [6][38/196]	LR: 0.1	Loss 0.0324 (0.0417)	Prec@1 98.828 (98.748)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0287 (0.0368)	Prec@1 98.828 (98.923)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.0351 (0.0357)	Prec@1 99.219 (98.978)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.0429 (0.0351)	Prec@1 98.438 (99.001)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.0263 (0.0351)	Prec@1 99.609 (99.022)	
Total train loss: 0.0351

Train time: 21.23183584213257
 * Prec@1 87.940 Prec@5 99.170 Loss 0.4932
Best acc: 88.540
--------------------------------------------------------------------------------
Test time: 26.56155514717102

Epoch: [7][38/196]	LR: 0.1	Loss 0.0446 (0.0270)	Prec@1 98.438 (99.349)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0118 (0.0263)	Prec@1 100.000 (99.294)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0310 (0.0282)	Prec@1 98.438 (99.225)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0557 (0.0308)	Prec@1 98.828 (99.126)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.0323 (0.0322)	Prec@1 98.828 (99.067)	
Total train loss: 0.0323

Train time: 20.667524099349976
 * Prec@1 88.690 Prec@5 99.300 Loss 0.4795
Best acc: 88.690
--------------------------------------------------------------------------------
Test time: 24.543536901474

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0120 (0.0243)	Prec@1 100.000 (99.339)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0115 (0.0199)	Prec@1 100.000 (99.509)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0147 (0.0186)	Prec@1 99.609 (99.569)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0079 (0.0172)	Prec@1 100.000 (99.617)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0120 (0.0163)	Prec@1 100.000 (99.651)	
Total train loss: 0.0163

Train time: 20.965089559555054
 * Prec@1 90.160 Prec@5 99.470 Loss 0.4021
Best acc: 90.160
--------------------------------------------------------------------------------
Test time: 25.8670654296875

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0104 (0.0107)	Prec@1 100.000 (99.830)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0054 (0.0098)	Prec@1 100.000 (99.905)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0062 (0.0095)	Prec@1 100.000 (99.923)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0095 (0.0092)	Prec@1 100.000 (99.925)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0066 (0.0093)	Prec@1 100.000 (99.914)	
Total train loss: 0.0094

Train time: 21.16772484779358
 * Prec@1 90.140 Prec@5 99.460 Loss 0.3977
Best acc: 90.160
--------------------------------------------------------------------------------
Test time: 25.57894802093506

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0084)	Prec@1 100.000 (99.940)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0096 (0.0080)	Prec@1 100.000 (99.965)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0038 (0.0079)	Prec@1 100.000 (99.960)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0081)	Prec@1 100.000 (99.945)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0085 (0.0080)	Prec@1 100.000 (99.944)	
Total train loss: 0.0080

Train time: 21.758256673812866
 * Prec@1 90.310 Prec@5 99.460 Loss 0.3940
Best acc: 90.310
--------------------------------------------------------------------------------
Test time: 26.648256301879883

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0059 (0.0072)	Prec@1 100.000 (99.950)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0070)	Prec@1 100.000 (99.965)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0054 (0.0068)	Prec@1 100.000 (99.967)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0069 (0.0070)	Prec@1 100.000 (99.970)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0071)	Prec@1 100.000 (99.968)	
Total train loss: 0.0071

Train time: 21.412875413894653
 * Prec@1 90.440 Prec@5 99.470 Loss 0.3953
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 25.470947742462158

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0055 (0.0067)	Prec@1 100.000 (99.960)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0044 (0.0065)	Prec@1 100.000 (99.975)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0075 (0.0064)	Prec@1 100.000 (99.963)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0064)	Prec@1 100.000 (99.967)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0090 (0.0065)	Prec@1 99.609 (99.966)	
Total train loss: 0.0065

Train time: 21.06847834587097
 * Prec@1 90.380 Prec@5 99.490 Loss 0.3938
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.207502841949463

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0037 (0.0061)	Prec@1 100.000 (99.980)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0045 (0.0060)	Prec@1 100.000 (99.985)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0060)	Prec@1 100.000 (99.990)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0043 (0.0059)	Prec@1 100.000 (99.987)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0069 (0.0060)	Prec@1 100.000 (99.988)	
Total train loss: 0.0060

Train time: 20.837788343429565
 * Prec@1 90.420 Prec@5 99.470 Loss 0.3933
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 25.170332431793213

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0039 (0.0058)	Prec@1 100.000 (99.970)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0103 (0.0057)	Prec@1 99.609 (99.965)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0057 (0.0056)	Prec@1 100.000 (99.977)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0124 (0.0056)	Prec@1 100.000 (99.980)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0038 (0.0056)	Prec@1 100.000 (99.980)	
Total train loss: 0.0057

Train time: 21.709386587142944
 * Prec@1 90.420 Prec@5 99.490 Loss 0.3926
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.366140127182007

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0066 (0.0066)	Prec@1 100.000 (99.960)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0060 (0.0058)	Prec@1 100.000 (99.975)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0043 (0.0058)	Prec@1 100.000 (99.977)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0071 (0.0058)	Prec@1 100.000 (99.977)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0074 (0.0057)	Prec@1 100.000 (99.978)	
Total train loss: 0.0057

Train time: 20.83260488510132
 * Prec@1 90.530 Prec@5 99.460 Loss 0.3950
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 25.844683408737183

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0039 (0.0057)	Prec@1 100.000 (99.970)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0041 (0.0052)	Prec@1 100.000 (99.985)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0053 (0.0053)	Prec@1 100.000 (99.987)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0048 (0.0054)	Prec@1 100.000 (99.985)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0034 (0.0054)	Prec@1 100.000 (99.986)	
Total train loss: 0.0054

Train time: 21.15458583831787
 * Prec@1 90.410 Prec@5 99.450 Loss 0.3958
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 25.907660007476807

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0044 (0.0053)	Prec@1 100.000 (99.990)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0040 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0053)	Prec@1 100.000 (99.993)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0083 (0.0054)	Prec@1 100.000 (99.995)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0045 (0.0053)	Prec@1 100.000 (99.994)	
Total train loss: 0.0053

Train time: 22.339264154434204
 * Prec@1 90.340 Prec@5 99.460 Loss 0.3948
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 26.638018131256104

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0040 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0023 (0.0053)	Prec@1 100.000 (99.985)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0060 (0.0053)	Prec@1 100.000 (99.987)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0038 (0.0053)	Prec@1 100.000 (99.990)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0067 (0.0054)	Prec@1 100.000 (99.988)	
Total train loss: 0.0054

Train time: 21.904603242874146
 * Prec@1 90.420 Prec@5 99.440 Loss 0.3965
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 27.58309006690979

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0032 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0043 (0.0054)	Prec@1 100.000 (99.985)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0030 (0.0053)	Prec@1 100.000 (99.990)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0054 (0.0054)	Prec@1 100.000 (99.987)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0049 (0.0054)	Prec@1 100.000 (99.986)	
Total train loss: 0.0054

Train time: 21.946216821670532
 * Prec@1 90.410 Prec@5 99.480 Loss 0.3953
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 26.166977882385254

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0033 (0.0051)	Prec@1 100.000 (99.980)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0042 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0071 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0058 (0.0053)	Prec@1 100.000 (99.992)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0048 (0.0053)	Prec@1 100.000 (99.994)	
Total train loss: 0.0053

Train time: 21.687530755996704
 * Prec@1 90.350 Prec@5 99.430 Loss 0.3933
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 26.695881128311157

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0076 (0.0054)	Prec@1 100.000 (100.000)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0041 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0047 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0053)	Prec@1 100.000 (99.992)	
Total train loss: 0.0053

Train time: 20.990518808364868
 * Prec@1 90.510 Prec@5 99.500 Loss 0.3916
Best acc: 90.530
--------------------------------------------------------------------------------
Test time: 25.70733070373535

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0028 (0.0051)	Prec@1 100.000 (100.000)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0035 (0.0054)	Prec@1 100.000 (100.000)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0076 (0.0055)	Prec@1 100.000 (100.000)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0054)	Prec@1 100.000 (100.000)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0093 (0.0054)	Prec@1 100.000 (100.000)	
Total train loss: 0.0054

Train time: 21.390446424484253
 * Prec@1 90.580 Prec@5 99.470 Loss 0.3928
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 26.116390466690063

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0081 (0.0050)	Prec@1 100.000 (100.000)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0089 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0028 (0.0052)	Prec@1 100.000 (99.993)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0051 (0.0051)	Prec@1 100.000 (99.995)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0052)	Prec@1 100.000 (99.992)	
Total train loss: 0.0053

Train time: 21.373292684555054
 * Prec@1 90.360 Prec@5 99.470 Loss 0.3945
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 25.840322494506836

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0048 (0.0051)	Prec@1 100.000 (99.990)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0046 (0.0051)	Prec@1 100.000 (99.990)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0052)	Prec@1 100.000 (99.990)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0041 (0.0052)	Prec@1 100.000 (99.985)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0054 (0.0053)	Prec@1 100.000 (99.988)	
Total train loss: 0.0054

Train time: 22.451035022735596
 * Prec@1 90.400 Prec@5 99.470 Loss 0.3948
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 28.037935495376587

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0055)	Prec@1 100.000 (100.000)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0082 (0.0053)	Prec@1 99.609 (99.990)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0046 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0039 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0075 (0.0054)	Prec@1 100.000 (99.986)	
Total train loss: 0.0054

Train time: 21.096195459365845
 * Prec@1 90.330 Prec@5 99.480 Loss 0.3970
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 25.269511938095093

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0054 (0.0050)	Prec@1 100.000 (100.000)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0043 (0.0052)	Prec@1 100.000 (99.995)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0040 (0.0053)	Prec@1 100.000 (99.993)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0033 (0.0052)	Prec@1 100.000 (99.995)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0033 (0.0052)	Prec@1 100.000 (99.994)	
Total train loss: 0.0052

Train time: 22.055999040603638
 * Prec@1 90.270 Prec@5 99.500 Loss 0.3928
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 26.68379497528076

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0029 (0.0058)	Prec@1 100.000 (99.980)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0061 (0.0058)	Prec@1 100.000 (99.990)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0063 (0.0056)	Prec@1 100.000 (99.987)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0027 (0.0055)	Prec@1 100.000 (99.987)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0048 (0.0055)	Prec@1 100.000 (99.988)	
Total train loss: 0.0055

Train time: 22.40089249610901
 * Prec@1 90.410 Prec@5 99.460 Loss 0.3955
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 27.444267749786377

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0055 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0064 (0.0055)	Prec@1 100.000 (99.985)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0060 (0.0054)	Prec@1 100.000 (99.987)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0040 (0.0053)	Prec@1 100.000 (99.990)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0061 (0.0052)	Prec@1 100.000 (99.990)	
Total train loss: 0.0053

Train time: 21.576865434646606
 * Prec@1 90.440 Prec@5 99.490 Loss 0.3960
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 26.21698832511902

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0046 (0.0055)	Prec@1 100.000 (99.980)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0054)	Prec@1 100.000 (99.990)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0067 (0.0054)	Prec@1 100.000 (99.987)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0079 (0.0053)	Prec@1 100.000 (99.990)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0052)	Prec@1 100.000 (99.992)	
Total train loss: 0.0052

Train time: 21.072391748428345
 * Prec@1 90.260 Prec@5 99.480 Loss 0.3965
Best acc: 90.580
--------------------------------------------------------------------------------
Test time: 25.131024599075317


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv8): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 81.330 Prec@5 98.920 Loss 0.8574
Pre-trained Prec@1 with 7 layers frozen: 81.32999420166016 	 Loss: 0.857421875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.0981 (0.1351)	Prec@1 95.703 (95.473)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.1779 (0.1615)	Prec@1 94.531 (94.491)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2179 (0.1683)	Prec@1 91.406 (94.161)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1368 (0.1669)	Prec@1 94.141 (94.098)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.1423 (0.1685)	Prec@1 94.531 (94.040)	
Total train loss: 0.1684

Train time: 185.30184149742126
 * Prec@1 87.140 Prec@5 99.370 Loss 0.4573
Best acc: 87.140
--------------------------------------------------------------------------------
Test time: 192.22547221183777

Epoch: [1][38/196]	LR: 0.1	Loss 0.1037 (0.1102)	Prec@1 96.094 (96.074)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1365 (0.1023)	Prec@1 95.312 (96.404)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1497 (0.1029)	Prec@1 95.703 (96.411)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1296 (0.1056)	Prec@1 95.312 (96.274)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1913 (0.1087)	Prec@1 93.750 (96.124)	
Total train loss: 0.1087

Train time: 19.665530920028687
 * Prec@1 87.310 Prec@5 99.510 Loss 0.4312
Best acc: 87.310
--------------------------------------------------------------------------------
Test time: 23.932209491729736

Epoch: [2][38/196]	LR: 0.1	Loss 0.0905 (0.0708)	Prec@1 96.484 (97.666)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.0764 (0.0706)	Prec@1 97.266 (97.646)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.0815 (0.0735)	Prec@1 96.875 (97.503)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.0658 (0.0758)	Prec@1 97.266 (97.431)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.0861 (0.0796)	Prec@1 96.094 (97.310)	
Total train loss: 0.0796

Train time: 21.251665830612183
 * Prec@1 87.330 Prec@5 99.330 Loss 0.5225
Best acc: 87.330
--------------------------------------------------------------------------------
Test time: 25.862187385559082

Epoch: [3][38/196]	LR: 0.1	Loss 0.0687 (0.0689)	Prec@1 97.266 (97.556)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.0790 (0.0644)	Prec@1 97.266 (97.806)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.0682 (0.0642)	Prec@1 96.875 (97.830)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.1259 (0.0661)	Prec@1 95.312 (97.754)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.1106 (0.0687)	Prec@1 95.703 (97.626)	
Total train loss: 0.0687

Train time: 20.59428310394287
 * Prec@1 88.420 Prec@5 99.160 Loss 0.4519
Best acc: 88.420
--------------------------------------------------------------------------------
Test time: 25.38909339904785

Epoch: [4][38/196]	LR: 0.1	Loss 0.0471 (0.0496)	Prec@1 98.828 (98.448)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0346 (0.0481)	Prec@1 98.828 (98.488)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.0757 (0.0484)	Prec@1 96.484 (98.501)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.0537 (0.0518)	Prec@1 98.828 (98.335)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.0700 (0.0558)	Prec@1 98.438 (98.181)	
Total train loss: 0.0558

Train time: 20.577255249023438
 * Prec@1 87.690 Prec@5 99.260 Loss 0.4763
Best acc: 88.420
--------------------------------------------------------------------------------
Test time: 25.40997838973999

Epoch: [5][38/196]	LR: 0.1	Loss 0.0454 (0.0426)	Prec@1 97.656 (98.678)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0384 (0.0438)	Prec@1 98.828 (98.603)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.0831 (0.0457)	Prec@1 95.312 (98.528)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.0647 (0.0471)	Prec@1 98.438 (98.478)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.0778 (0.0481)	Prec@1 97.656 (98.448)	
Total train loss: 0.0481

Train time: 20.611242055892944
 * Prec@1 88.680 Prec@5 99.240 Loss 0.4460
Best acc: 88.680
--------------------------------------------------------------------------------
Test time: 25.00733709335327

Epoch: [6][38/196]	LR: 0.1	Loss 0.0284 (0.0418)	Prec@1 99.219 (98.638)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0663 (0.0402)	Prec@1 98.047 (98.773)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.0651 (0.0429)	Prec@1 97.266 (98.711)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.0428 (0.0450)	Prec@1 99.219 (98.595)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.0464 (0.0478)	Prec@1 98.438 (98.468)	
Total train loss: 0.0478

Train time: 21.3413348197937
 * Prec@1 88.100 Prec@5 99.320 Loss 0.4807
Best acc: 88.680
--------------------------------------------------------------------------------
Test time: 26.712963581085205

Epoch: [7][38/196]	LR: 0.1	Loss 0.0492 (0.0400)	Prec@1 98.438 (98.858)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0424 (0.0383)	Prec@1 99.219 (98.853)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0503 (0.0389)	Prec@1 97.656 (98.828)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0210 (0.0395)	Prec@1 99.219 (98.788)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.0594 (0.0410)	Prec@1 96.875 (98.710)	
Total train loss: 0.0412

Train time: 20.779486656188965
 * Prec@1 87.730 Prec@5 99.210 Loss 0.5010
Best acc: 88.680
--------------------------------------------------------------------------------
Test time: 24.598310232162476

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0264 (0.0330)	Prec@1 99.219 (99.058)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0271 (0.0274)	Prec@1 99.219 (99.284)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0187 (0.0245)	Prec@1 99.219 (99.392)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0120 (0.0221)	Prec@1 100.000 (99.474)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0133 (0.0207)	Prec@1 100.000 (99.529)	
Total train loss: 0.0207

Train time: 18.999513626098633
 * Prec@1 90.030 Prec@5 99.470 Loss 0.3994
Best acc: 90.030
--------------------------------------------------------------------------------
Test time: 23.39161729812622

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0119 (0.0110)	Prec@1 100.000 (99.900)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0107)	Prec@1 100.000 (99.925)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0106)	Prec@1 100.000 (99.930)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0096 (0.0104)	Prec@1 100.000 (99.927)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0106)	Prec@1 100.000 (99.928)	
Total train loss: 0.0106

Train time: 20.297751665115356
 * Prec@1 90.220 Prec@5 99.480 Loss 0.3936
Best acc: 90.220
--------------------------------------------------------------------------------
Test time: 24.646102905273438

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0046 (0.0092)	Prec@1 100.000 (99.940)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0126 (0.0096)	Prec@1 100.000 (99.925)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0064 (0.0095)	Prec@1 100.000 (99.933)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0059 (0.0093)	Prec@1 100.000 (99.945)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0077 (0.0092)	Prec@1 100.000 (99.944)	
Total train loss: 0.0093

Train time: 20.809338569641113
 * Prec@1 90.350 Prec@5 99.470 Loss 0.3955
Best acc: 90.350
--------------------------------------------------------------------------------
Test time: 25.396604299545288

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0073 (0.0085)	Prec@1 100.000 (99.940)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0100 (0.0085)	Prec@1 100.000 (99.945)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0086)	Prec@1 100.000 (99.950)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0045 (0.0083)	Prec@1 100.000 (99.955)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0046 (0.0084)	Prec@1 100.000 (99.952)	
Total train loss: 0.0084

Train time: 20.81539559364319
 * Prec@1 90.280 Prec@5 99.480 Loss 0.3950
Best acc: 90.350
--------------------------------------------------------------------------------
Test time: 24.82610297203064

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0075)	Prec@1 100.000 (99.980)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0081 (0.0074)	Prec@1 100.000 (99.985)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0075)	Prec@1 100.000 (99.987)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0084 (0.0074)	Prec@1 100.000 (99.982)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0098 (0.0075)	Prec@1 100.000 (99.982)	
Total train loss: 0.0075

Train time: 20.898715019226074
 * Prec@1 90.410 Prec@5 99.480 Loss 0.3972
Best acc: 90.410
--------------------------------------------------------------------------------
Test time: 26.70459532737732

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0055 (0.0067)	Prec@1 100.000 (99.980)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0064 (0.0072)	Prec@1 100.000 (99.970)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0055 (0.0071)	Prec@1 100.000 (99.973)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0072 (0.0070)	Prec@1 100.000 (99.975)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0045 (0.0071)	Prec@1 100.000 (99.974)	
Total train loss: 0.0071

Train time: 20.540040016174316
 * Prec@1 90.380 Prec@5 99.470 Loss 0.3955
Best acc: 90.410
--------------------------------------------------------------------------------
Test time: 24.254881858825684

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0069 (0.0070)	Prec@1 100.000 (99.980)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0068 (0.0070)	Prec@1 100.000 (99.980)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0067 (0.0071)	Prec@1 100.000 (99.967)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0094 (0.0069)	Prec@1 100.000 (99.967)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0047 (0.0070)	Prec@1 100.000 (99.970)	
Total train loss: 0.0070

Train time: 20.209737062454224
 * Prec@1 90.320 Prec@5 99.480 Loss 0.3975
Best acc: 90.410
--------------------------------------------------------------------------------
Test time: 24.82144522666931

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0068 (0.0063)	Prec@1 100.000 (100.000)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0065)	Prec@1 100.000 (99.980)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0047 (0.0065)	Prec@1 100.000 (99.987)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0043 (0.0066)	Prec@1 100.000 (99.980)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0062 (0.0065)	Prec@1 100.000 (99.982)	
Total train loss: 0.0065

Train time: 21.701614379882812
 * Prec@1 90.330 Prec@5 99.440 Loss 0.3953
Best acc: 90.410
--------------------------------------------------------------------------------
Test time: 26.36909794807434

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0039 (0.0058)	Prec@1 100.000 (100.000)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0062 (0.0061)	Prec@1 100.000 (99.985)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0070 (0.0061)	Prec@1 100.000 (99.990)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0063)	Prec@1 100.000 (99.985)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0042 (0.0063)	Prec@1 100.000 (99.986)	
Total train loss: 0.0064

Train time: 21.98247766494751
 * Prec@1 90.440 Prec@5 99.480 Loss 0.3948
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 27.720439672470093

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0073 (0.0060)	Prec@1 100.000 (100.000)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0065 (0.0060)	Prec@1 100.000 (99.995)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0044 (0.0060)	Prec@1 100.000 (99.997)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0061)	Prec@1 100.000 (99.995)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0044 (0.0062)	Prec@1 100.000 (99.994)	
Total train loss: 0.0062

Train time: 24.235243797302246
 * Prec@1 90.440 Prec@5 99.450 Loss 0.3933
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 28.406275987625122

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0070)	Prec@1 100.000 (99.980)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0088 (0.0069)	Prec@1 100.000 (99.980)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0066)	Prec@1 100.000 (99.983)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0066)	Prec@1 100.000 (99.985)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0076 (0.0064)	Prec@1 100.000 (99.988)	
Total train loss: 0.0064

Train time: 25.228094339370728
 * Prec@1 90.390 Prec@5 99.460 Loss 0.3977
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 30.885469675064087

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0064)	Prec@1 100.000 (99.980)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0062 (0.0068)	Prec@1 100.000 (99.970)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0069)	Prec@1 100.000 (99.973)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0099 (0.0067)	Prec@1 100.000 (99.980)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0062 (0.0065)	Prec@1 100.000 (99.982)	
Total train loss: 0.0065

Train time: 21.012966632843018
 * Prec@1 90.410 Prec@5 99.420 Loss 0.3989
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 25.279985666275024

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0059 (0.0059)	Prec@1 100.000 (99.980)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0060)	Prec@1 100.000 (99.990)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0061)	Prec@1 100.000 (99.993)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0103 (0.0063)	Prec@1 99.609 (99.987)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0060 (0.0063)	Prec@1 100.000 (99.986)	
Total train loss: 0.0063

Train time: 21.613747358322144
 * Prec@1 90.430 Prec@5 99.460 Loss 0.3936
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.652700185775757

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0066)	Prec@1 100.000 (99.980)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0034 (0.0068)	Prec@1 100.000 (99.965)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0048 (0.0067)	Prec@1 100.000 (99.977)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0049 (0.0065)	Prec@1 100.000 (99.980)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0043 (0.0063)	Prec@1 100.000 (99.984)	
Total train loss: 0.0064

Train time: 21.212656497955322
 * Prec@1 90.290 Prec@5 99.460 Loss 0.3972
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.083164930343628

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0020 (0.0060)	Prec@1 100.000 (99.980)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0060)	Prec@1 100.000 (99.980)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0058)	Prec@1 100.000 (99.983)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0052 (0.0058)	Prec@1 100.000 (99.987)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0051 (0.0060)	Prec@1 100.000 (99.984)	
Total train loss: 0.0060

Train time: 21.499008417129517
 * Prec@1 90.400 Prec@5 99.470 Loss 0.3950
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.051628589630127

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0060 (0.0064)	Prec@1 100.000 (100.000)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0046 (0.0065)	Prec@1 100.000 (99.980)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0040 (0.0063)	Prec@1 100.000 (99.980)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0054 (0.0062)	Prec@1 100.000 (99.982)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0048 (0.0063)	Prec@1 100.000 (99.984)	
Total train loss: 0.0063

Train time: 20.318670988082886
 * Prec@1 90.320 Prec@5 99.460 Loss 0.3943
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 24.457945108413696

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0068 (0.0061)	Prec@1 100.000 (99.980)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0043 (0.0061)	Prec@1 100.000 (99.990)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0037 (0.0062)	Prec@1 100.000 (99.990)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0043 (0.0064)	Prec@1 100.000 (99.987)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0138 (0.0064)	Prec@1 99.609 (99.986)	
Total train loss: 0.0064

Train time: 21.176553964614868
 * Prec@1 90.390 Prec@5 99.450 Loss 0.3948
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.464259386062622

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0036 (0.0062)	Prec@1 100.000 (99.990)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0053 (0.0060)	Prec@1 100.000 (99.985)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0069 (0.0061)	Prec@1 100.000 (99.987)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0051 (0.0060)	Prec@1 100.000 (99.987)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0078 (0.0060)	Prec@1 100.000 (99.990)	
Total train loss: 0.0060

Train time: 20.329293251037598
 * Prec@1 90.420 Prec@5 99.450 Loss 0.3933
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 24.24348282814026

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0042 (0.0063)	Prec@1 100.000 (99.980)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0066 (0.0062)	Prec@1 100.000 (99.980)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0077 (0.0061)	Prec@1 100.000 (99.983)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0053 (0.0062)	Prec@1 100.000 (99.985)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0055 (0.0063)	Prec@1 100.000 (99.984)	
Total train loss: 0.0063

Train time: 21.44982933998108
 * Prec@1 90.290 Prec@5 99.420 Loss 0.3972
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.171602964401245

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0087 (0.0061)	Prec@1 100.000 (99.990)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0071 (0.0065)	Prec@1 100.000 (99.980)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0106 (0.0064)	Prec@1 99.609 (99.973)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0168 (0.0065)	Prec@1 99.219 (99.970)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0045 (0.0065)	Prec@1 100.000 (99.970)	
Total train loss: 0.0065

Train time: 21.466378211975098
 * Prec@1 90.350 Prec@5 99.480 Loss 0.3926
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 26.449934005737305

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0039 (0.0057)	Prec@1 100.000 (100.000)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0070 (0.0059)	Prec@1 100.000 (99.985)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0055 (0.0062)	Prec@1 100.000 (99.980)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0041 (0.0063)	Prec@1 100.000 (99.980)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0092 (0.0064)	Prec@1 100.000 (99.978)	
Total train loss: 0.0064

Train time: 20.97766947746277
 * Prec@1 90.330 Prec@5 99.440 Loss 0.3977
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 25.528703689575195

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0050 (0.0065)	Prec@1 100.000 (99.970)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0051 (0.0064)	Prec@1 100.000 (99.980)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0060 (0.0063)	Prec@1 100.000 (99.980)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0068 (0.0062)	Prec@1 100.000 (99.980)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0106 (0.0061)	Prec@1 100.000 (99.984)	
Total train loss: 0.0061

Train time: 20.93875741958618
 * Prec@1 90.400 Prec@5 99.450 Loss 0.3953
Best acc: 90.440
--------------------------------------------------------------------------------
Test time: 24.960333824157715


      ==> Arguments:
          dataset: cifar10
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar
          mode_train: sram
          mode_test: sram
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 256
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [8, 16, 24]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar10.pth.tar ...
Original model accuracy: 91.93
ResNet_cifar(
  (conv10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu18): ReLU(inplace=True)
  (conv19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu19): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (bn20): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=64, out_features=10, bias=False)
  (bn21): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 54.740 Prec@5 92.470 Loss 2.1113
Pre-trained Prec@1 with 9 layers frozen: 54.73999786376953 	 Loss: 2.111328125

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.1	Loss 0.2498 (0.3257)	Prec@1 89.453 (89.123)	
Epoch: [0][77/196]	LR: 0.1	Loss 0.2849 (0.3151)	Prec@1 88.672 (89.158)	
Epoch: [0][116/196]	LR: 0.1	Loss 0.2043 (0.2919)	Prec@1 91.797 (89.867)	
Epoch: [0][155/196]	LR: 0.1	Loss 0.1886 (0.2799)	Prec@1 92.969 (90.194)	
Epoch: [0][194/196]	LR: 0.1	Loss 0.2051 (0.2728)	Prec@1 92.969 (90.417)	
Total train loss: 0.2726

Train time: 162.4430809020996
 * Prec@1 86.690 Prec@5 99.490 Loss 0.4216
Best acc: 86.690
--------------------------------------------------------------------------------
Test time: 166.35536003112793

Epoch: [1][38/196]	LR: 0.1	Loss 0.0895 (0.1462)	Prec@1 97.266 (95.052)	
Epoch: [1][77/196]	LR: 0.1	Loss 0.1274 (0.1483)	Prec@1 95.703 (94.982)	
Epoch: [1][116/196]	LR: 0.1	Loss 0.1825 (0.1532)	Prec@1 92.969 (94.745)	
Epoch: [1][155/196]	LR: 0.1	Loss 0.1493 (0.1600)	Prec@1 94.531 (94.416)	
Epoch: [1][194/196]	LR: 0.1	Loss 0.1871 (0.1623)	Prec@1 94.531 (94.375)	
Total train loss: 0.1624

Train time: 20.788691759109497
 * Prec@1 87.200 Prec@5 99.340 Loss 0.4304
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 24.319772005081177

Epoch: [2][38/196]	LR: 0.1	Loss 0.1083 (0.1103)	Prec@1 95.703 (96.184)	
Epoch: [2][77/196]	LR: 0.1	Loss 0.1499 (0.1115)	Prec@1 95.312 (96.184)	
Epoch: [2][116/196]	LR: 0.1	Loss 0.1063 (0.1153)	Prec@1 95.312 (95.984)	
Epoch: [2][155/196]	LR: 0.1	Loss 0.1110 (0.1196)	Prec@1 95.312 (95.838)	
Epoch: [2][194/196]	LR: 0.1	Loss 0.1769 (0.1253)	Prec@1 92.969 (95.605)	
Total train loss: 0.1254

Train time: 19.20996880531311
 * Prec@1 84.580 Prec@5 99.150 Loss 0.5703
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 22.566331386566162

Epoch: [3][38/196]	LR: 0.1	Loss 0.0856 (0.0955)	Prec@1 96.094 (96.905)	
Epoch: [3][77/196]	LR: 0.1	Loss 0.1129 (0.0902)	Prec@1 94.922 (96.995)	
Epoch: [3][116/196]	LR: 0.1	Loss 0.0911 (0.0916)	Prec@1 96.484 (96.902)	
Epoch: [3][155/196]	LR: 0.1	Loss 0.0704 (0.0958)	Prec@1 97.656 (96.737)	
Epoch: [3][194/196]	LR: 0.1	Loss 0.0947 (0.0989)	Prec@1 97.266 (96.613)	
Total train loss: 0.0989

Train time: 20.23638653755188
 * Prec@1 85.510 Prec@5 99.100 Loss 0.5566
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 24.23486590385437

Epoch: [4][38/196]	LR: 0.1	Loss 0.0503 (0.0754)	Prec@1 98.438 (97.526)	
Epoch: [4][77/196]	LR: 0.1	Loss 0.0622 (0.0751)	Prec@1 98.438 (97.551)	
Epoch: [4][116/196]	LR: 0.1	Loss 0.0629 (0.0744)	Prec@1 98.047 (97.579)	
Epoch: [4][155/196]	LR: 0.1	Loss 0.0914 (0.0743)	Prec@1 96.484 (97.589)	
Epoch: [4][194/196]	LR: 0.1	Loss 0.0864 (0.0775)	Prec@1 96.484 (97.432)	
Total train loss: 0.0776

Train time: 20.46734118461609
 * Prec@1 86.540 Prec@5 99.260 Loss 0.5098
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 24.21017074584961

Epoch: [5][38/196]	LR: 0.1	Loss 0.0823 (0.0683)	Prec@1 97.656 (97.917)	
Epoch: [5][77/196]	LR: 0.1	Loss 0.0681 (0.0654)	Prec@1 97.266 (97.962)	
Epoch: [5][116/196]	LR: 0.1	Loss 0.0853 (0.0670)	Prec@1 98.438 (97.900)	
Epoch: [5][155/196]	LR: 0.1	Loss 0.0389 (0.0679)	Prec@1 98.438 (97.824)	
Epoch: [5][194/196]	LR: 0.1	Loss 0.1227 (0.0712)	Prec@1 95.312 (97.662)	
Total train loss: 0.0712

Train time: 20.037631034851074
 * Prec@1 84.350 Prec@5 99.160 Loss 0.6553
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 23.686383962631226

Epoch: [6][38/196]	LR: 0.1	Loss 0.0645 (0.0670)	Prec@1 97.266 (97.736)	
Epoch: [6][77/196]	LR: 0.1	Loss 0.0362 (0.0590)	Prec@1 99.219 (98.152)	
Epoch: [6][116/196]	LR: 0.1	Loss 0.0638 (0.0575)	Prec@1 98.047 (98.180)	
Epoch: [6][155/196]	LR: 0.1	Loss 0.0609 (0.0599)	Prec@1 97.266 (98.089)	
Epoch: [6][194/196]	LR: 0.1	Loss 0.0906 (0.0620)	Prec@1 96.484 (97.985)	
Total train loss: 0.0621

Train time: 20.35644268989563
 * Prec@1 85.430 Prec@5 99.110 Loss 0.5459
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 24.68775963783264

Epoch: [7][38/196]	LR: 0.1	Loss 0.0585 (0.0577)	Prec@1 98.438 (98.207)	
Epoch: [7][77/196]	LR: 0.1	Loss 0.0635 (0.0527)	Prec@1 96.875 (98.352)	
Epoch: [7][116/196]	LR: 0.1	Loss 0.0250 (0.0506)	Prec@1 98.828 (98.407)	
Epoch: [7][155/196]	LR: 0.1	Loss 0.0798 (0.0519)	Prec@1 97.656 (98.335)	
Epoch: [7][194/196]	LR: 0.1	Loss 0.0468 (0.0546)	Prec@1 97.656 (98.265)	
Total train loss: 0.0546

Train time: 21.27394723892212
 * Prec@1 84.290 Prec@5 98.870 Loss 0.6631
Best acc: 87.200
--------------------------------------------------------------------------------
Test time: 25.028459310531616

Epoch: [8][38/196]	LR: 0.010000000000000002	Loss 0.0476 (0.0399)	Prec@1 98.828 (98.918)	
Epoch: [8][77/196]	LR: 0.010000000000000002	Loss 0.0163 (0.0328)	Prec@1 100.000 (99.174)	
Epoch: [8][116/196]	LR: 0.010000000000000002	Loss 0.0143 (0.0298)	Prec@1 99.609 (99.269)	
Epoch: [8][155/196]	LR: 0.010000000000000002	Loss 0.0301 (0.0280)	Prec@1 98.438 (99.296)	
Epoch: [8][194/196]	LR: 0.010000000000000002	Loss 0.0218 (0.0263)	Prec@1 99.609 (99.365)	
Total train loss: 0.0263

Train time: 18.72312021255493
 * Prec@1 88.830 Prec@5 99.470 Loss 0.4417
Best acc: 88.830
--------------------------------------------------------------------------------
Test time: 22.253628969192505

Epoch: [9][38/196]	LR: 0.010000000000000002	Loss 0.0139 (0.0143)	Prec@1 99.609 (99.870)	
Epoch: [9][77/196]	LR: 0.010000000000000002	Loss 0.0137 (0.0150)	Prec@1 100.000 (99.870)	
Epoch: [9][116/196]	LR: 0.010000000000000002	Loss 0.0137 (0.0148)	Prec@1 99.609 (99.866)	
Epoch: [9][155/196]	LR: 0.010000000000000002	Loss 0.0086 (0.0146)	Prec@1 100.000 (99.852)	
Epoch: [9][194/196]	LR: 0.010000000000000002	Loss 0.0125 (0.0144)	Prec@1 100.000 (99.840)	
Total train loss: 0.0144

Train time: 19.6534903049469
 * Prec@1 89.010 Prec@5 99.460 Loss 0.4436
Best acc: 89.010
--------------------------------------------------------------------------------
Test time: 23.776195764541626

Epoch: [10][38/196]	LR: 0.010000000000000002	Loss 0.0120 (0.0113)	Prec@1 100.000 (99.930)	
Epoch: [10][77/196]	LR: 0.010000000000000002	Loss 0.0105 (0.0114)	Prec@1 100.000 (99.935)	
Epoch: [10][116/196]	LR: 0.010000000000000002	Loss 0.0252 (0.0116)	Prec@1 99.219 (99.917)	
Epoch: [10][155/196]	LR: 0.010000000000000002	Loss 0.0097 (0.0115)	Prec@1 100.000 (99.930)	
Epoch: [10][194/196]	LR: 0.010000000000000002	Loss 0.0080 (0.0119)	Prec@1 100.000 (99.922)	
Total train loss: 0.0119

Train time: 18.970027685165405
 * Prec@1 89.030 Prec@5 99.440 Loss 0.4424
Best acc: 89.030
--------------------------------------------------------------------------------
Test time: 22.61824941635132

Epoch: [11][38/196]	LR: 0.010000000000000002	Loss 0.0076 (0.0097)	Prec@1 100.000 (99.970)	
Epoch: [11][77/196]	LR: 0.010000000000000002	Loss 0.0080 (0.0096)	Prec@1 100.000 (99.975)	
Epoch: [11][116/196]	LR: 0.010000000000000002	Loss 0.0077 (0.0104)	Prec@1 100.000 (99.960)	
Epoch: [11][155/196]	LR: 0.010000000000000002	Loss 0.0088 (0.0104)	Prec@1 100.000 (99.957)	
Epoch: [11][194/196]	LR: 0.010000000000000002	Loss 0.0090 (0.0104)	Prec@1 100.000 (99.952)	
Total train loss: 0.0104

Train time: 26.62060832977295
 * Prec@1 89.100 Prec@5 99.460 Loss 0.4412
Best acc: 89.100
--------------------------------------------------------------------------------
Test time: 30.209376573562622

Epoch: [12][38/196]	LR: 0.010000000000000002	Loss 0.0086 (0.0108)	Prec@1 100.000 (99.950)	
Epoch: [12][77/196]	LR: 0.010000000000000002	Loss 0.0064 (0.0098)	Prec@1 100.000 (99.970)	
Epoch: [12][116/196]	LR: 0.010000000000000002	Loss 0.0096 (0.0096)	Prec@1 100.000 (99.970)	
Epoch: [12][155/196]	LR: 0.010000000000000002	Loss 0.0077 (0.0096)	Prec@1 100.000 (99.970)	
Epoch: [12][194/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0096)	Prec@1 100.000 (99.966)	
Total train loss: 0.0096

Train time: 20.138440370559692
 * Prec@1 89.140 Prec@5 99.450 Loss 0.4443
Best acc: 89.140
--------------------------------------------------------------------------------
Test time: 24.032030820846558

Epoch: [13][38/196]	LR: 0.010000000000000002	Loss 0.0166 (0.0090)	Prec@1 99.609 (99.990)	
Epoch: [13][77/196]	LR: 0.010000000000000002	Loss 0.0084 (0.0090)	Prec@1 100.000 (99.975)	
Epoch: [13][116/196]	LR: 0.010000000000000002	Loss 0.0161 (0.0090)	Prec@1 100.000 (99.973)	
Epoch: [13][155/196]	LR: 0.010000000000000002	Loss 0.0070 (0.0093)	Prec@1 100.000 (99.960)	
Epoch: [13][194/196]	LR: 0.010000000000000002	Loss 0.0044 (0.0092)	Prec@1 100.000 (99.968)	
Total train loss: 0.0093

Train time: 18.40491771697998
 * Prec@1 89.090 Prec@5 99.440 Loss 0.4492
Best acc: 89.140
--------------------------------------------------------------------------------
Test time: 21.757296800613403

Epoch: [14][38/196]	LR: 0.010000000000000002	Loss 0.0062 (0.0087)	Prec@1 100.000 (99.950)	
Epoch: [14][77/196]	LR: 0.010000000000000002	Loss 0.0071 (0.0089)	Prec@1 100.000 (99.970)	
Epoch: [14][116/196]	LR: 0.010000000000000002	Loss 0.0078 (0.0090)	Prec@1 100.000 (99.967)	
Epoch: [14][155/196]	LR: 0.010000000000000002	Loss 0.0089 (0.0089)	Prec@1 100.000 (99.972)	
Epoch: [14][194/196]	LR: 0.010000000000000002	Loss 0.0082 (0.0088)	Prec@1 100.000 (99.974)	
Total train loss: 0.0088

Train time: 20.62187910079956
 * Prec@1 89.180 Prec@5 99.430 Loss 0.4436
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 24.286449193954468

Epoch: [15][38/196]	LR: 0.010000000000000002	Loss 0.0061 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [15][77/196]	LR: 0.010000000000000002	Loss 0.0072 (0.0081)	Prec@1 100.000 (99.975)	
Epoch: [15][116/196]	LR: 0.010000000000000002	Loss 0.0063 (0.0080)	Prec@1 100.000 (99.973)	
Epoch: [15][155/196]	LR: 0.010000000000000002	Loss 0.0136 (0.0081)	Prec@1 99.609 (99.972)	
Epoch: [15][194/196]	LR: 0.010000000000000002	Loss 0.0088 (0.0080)	Prec@1 100.000 (99.974)	
Total train loss: 0.0080

Train time: 18.52594256401062
 * Prec@1 89.150 Prec@5 99.410 Loss 0.4460
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 22.477124452590942

Epoch: [16][38/196]	LR: 0.0010000000000000002	Loss 0.0097 (0.0078)	Prec@1 100.000 (99.990)	
Epoch: [16][77/196]	LR: 0.0010000000000000002	Loss 0.0083 (0.0080)	Prec@1 100.000 (99.985)	
Epoch: [16][116/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0082)	Prec@1 100.000 (99.977)	
Epoch: [16][155/196]	LR: 0.0010000000000000002	Loss 0.0063 (0.0082)	Prec@1 100.000 (99.975)	
Epoch: [16][194/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0081)	Prec@1 100.000 (99.976)	
Total train loss: 0.0082

Train time: 20.246161460876465
 * Prec@1 89.100 Prec@5 99.410 Loss 0.4436
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.62242364883423

Epoch: [17][38/196]	LR: 0.0010000000000000002	Loss 0.0068 (0.0073)	Prec@1 100.000 (100.000)	
Epoch: [17][77/196]	LR: 0.0010000000000000002	Loss 0.0125 (0.0078)	Prec@1 100.000 (99.985)	
Epoch: [17][116/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0077)	Prec@1 100.000 (99.987)	
Epoch: [17][155/196]	LR: 0.0010000000000000002	Loss 0.0076 (0.0077)	Prec@1 100.000 (99.990)	
Epoch: [17][194/196]	LR: 0.0010000000000000002	Loss 0.0072 (0.0077)	Prec@1 100.000 (99.988)	
Total train loss: 0.0077

Train time: 19.589709997177124
 * Prec@1 89.130 Prec@5 99.450 Loss 0.4443
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.25210475921631

Epoch: [18][38/196]	LR: 0.0010000000000000002	Loss 0.0078 (0.0080)	Prec@1 100.000 (99.960)	
Epoch: [18][77/196]	LR: 0.0010000000000000002	Loss 0.0078 (0.0076)	Prec@1 100.000 (99.980)	
Epoch: [18][116/196]	LR: 0.0010000000000000002	Loss 0.0079 (0.0078)	Prec@1 100.000 (99.977)	
Epoch: [18][155/196]	LR: 0.0010000000000000002	Loss 0.0070 (0.0081)	Prec@1 100.000 (99.972)	
Epoch: [18][194/196]	LR: 0.0010000000000000002	Loss 0.0056 (0.0081)	Prec@1 100.000 (99.968)	
Total train loss: 0.0081

Train time: 19.315403938293457
 * Prec@1 89.050 Prec@5 99.440 Loss 0.4443
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.346431732177734

Epoch: [19][38/196]	LR: 0.0010000000000000002	Loss 0.0055 (0.0071)	Prec@1 100.000 (99.970)	
Epoch: [19][77/196]	LR: 0.0010000000000000002	Loss 0.0134 (0.0077)	Prec@1 99.609 (99.970)	
Epoch: [19][116/196]	LR: 0.0010000000000000002	Loss 0.0109 (0.0081)	Prec@1 100.000 (99.967)	
Epoch: [19][155/196]	LR: 0.0010000000000000002	Loss 0.0057 (0.0081)	Prec@1 100.000 (99.975)	
Epoch: [19][194/196]	LR: 0.0010000000000000002	Loss 0.0050 (0.0080)	Prec@1 100.000 (99.976)	
Total train loss: 0.0080

Train time: 20.6368350982666
 * Prec@1 89.020 Prec@5 99.420 Loss 0.4429
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.9818115234375

Epoch: [20][38/196]	LR: 0.0010000000000000002	Loss 0.0059 (0.0079)	Prec@1 100.000 (99.960)	
Epoch: [20][77/196]	LR: 0.0010000000000000002	Loss 0.0080 (0.0082)	Prec@1 100.000 (99.960)	
Epoch: [20][116/196]	LR: 0.0010000000000000002	Loss 0.0086 (0.0083)	Prec@1 100.000 (99.957)	
Epoch: [20][155/196]	LR: 0.0010000000000000002	Loss 0.0059 (0.0082)	Prec@1 100.000 (99.960)	
Epoch: [20][194/196]	LR: 0.0010000000000000002	Loss 0.0057 (0.0082)	Prec@1 100.000 (99.964)	
Total train loss: 0.0082

Train time: 20.01109552383423
 * Prec@1 89.030 Prec@5 99.420 Loss 0.4473
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.674898624420166

Epoch: [21][38/196]	LR: 0.0010000000000000002	Loss 0.0078 (0.0075)	Prec@1 100.000 (99.990)	
Epoch: [21][77/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0077)	Prec@1 100.000 (99.995)	
Epoch: [21][116/196]	LR: 0.0010000000000000002	Loss 0.0085 (0.0076)	Prec@1 100.000 (99.990)	
Epoch: [21][155/196]	LR: 0.0010000000000000002	Loss 0.0064 (0.0076)	Prec@1 100.000 (99.992)	
Epoch: [21][194/196]	LR: 0.0010000000000000002	Loss 0.0060 (0.0077)	Prec@1 100.000 (99.990)	
Total train loss: 0.0077

Train time: 20.391736030578613
 * Prec@1 89.010 Prec@5 99.430 Loss 0.4448
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 24.554624319076538

Epoch: [22][38/196]	LR: 0.0010000000000000002	Loss 0.0069 (0.0084)	Prec@1 100.000 (99.970)	
Epoch: [22][77/196]	LR: 0.0010000000000000002	Loss 0.0061 (0.0082)	Prec@1 100.000 (99.975)	
Epoch: [22][116/196]	LR: 0.0010000000000000002	Loss 0.0100 (0.0080)	Prec@1 100.000 (99.983)	
Epoch: [22][155/196]	LR: 0.0010000000000000002	Loss 0.0091 (0.0081)	Prec@1 100.000 (99.982)	
Epoch: [22][194/196]	LR: 0.0010000000000000002	Loss 0.0084 (0.0083)	Prec@1 100.000 (99.978)	
Total train loss: 0.0083

Train time: 20.881773471832275
 * Prec@1 89.060 Prec@5 99.470 Loss 0.4456
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 24.611977577209473

Epoch: [23][38/196]	LR: 0.0010000000000000002	Loss 0.0082 (0.0075)	Prec@1 100.000 (99.990)	
Epoch: [23][77/196]	LR: 0.0010000000000000002	Loss 0.0080 (0.0077)	Prec@1 100.000 (99.980)	
Epoch: [23][116/196]	LR: 0.0010000000000000002	Loss 0.0070 (0.0077)	Prec@1 100.000 (99.987)	
Epoch: [23][155/196]	LR: 0.0010000000000000002	Loss 0.0065 (0.0079)	Prec@1 100.000 (99.987)	
Epoch: [23][194/196]	LR: 0.0010000000000000002	Loss 0.0103 (0.0079)	Prec@1 100.000 (99.986)	
Total train loss: 0.0079

Train time: 20.020740747451782
 * Prec@1 89.080 Prec@5 99.430 Loss 0.4468
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.76022744178772

Epoch: [24][38/196]	LR: 0.00010000000000000003	Loss 0.0047 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [24][77/196]	LR: 0.00010000000000000003	Loss 0.0138 (0.0079)	Prec@1 100.000 (99.975)	
Epoch: [24][116/196]	LR: 0.00010000000000000003	Loss 0.0115 (0.0079)	Prec@1 100.000 (99.973)	
Epoch: [24][155/196]	LR: 0.00010000000000000003	Loss 0.0110 (0.0080)	Prec@1 100.000 (99.975)	
Epoch: [24][194/196]	LR: 0.00010000000000000003	Loss 0.0057 (0.0079)	Prec@1 100.000 (99.976)	
Total train loss: 0.0080

Train time: 19.342424631118774
 * Prec@1 89.010 Prec@5 99.430 Loss 0.4468
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.0361487865448

Epoch: [25][38/196]	LR: 0.00010000000000000003	Loss 0.0081 (0.0081)	Prec@1 100.000 (99.980)	
Epoch: [25][77/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0078)	Prec@1 100.000 (99.980)	
Epoch: [25][116/196]	LR: 0.00010000000000000003	Loss 0.0125 (0.0079)	Prec@1 100.000 (99.973)	
Epoch: [25][155/196]	LR: 0.00010000000000000003	Loss 0.0072 (0.0079)	Prec@1 100.000 (99.975)	
Epoch: [25][194/196]	LR: 0.00010000000000000003	Loss 0.0048 (0.0078)	Prec@1 100.000 (99.970)	
Total train loss: 0.0079

Train time: 20.427468299865723
 * Prec@1 89.020 Prec@5 99.420 Loss 0.4485
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.998560428619385

Epoch: [26][38/196]	LR: 0.00010000000000000003	Loss 0.0059 (0.0081)	Prec@1 100.000 (99.980)	
Epoch: [26][77/196]	LR: 0.00010000000000000003	Loss 0.0097 (0.0078)	Prec@1 100.000 (99.990)	
Epoch: [26][116/196]	LR: 0.00010000000000000003	Loss 0.0074 (0.0077)	Prec@1 100.000 (99.990)	
Epoch: [26][155/196]	LR: 0.00010000000000000003	Loss 0.0088 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [26][194/196]	LR: 0.00010000000000000003	Loss 0.0056 (0.0079)	Prec@1 100.000 (99.978)	
Total train loss: 0.0080

Train time: 19.690650939941406
 * Prec@1 89.090 Prec@5 99.450 Loss 0.4431
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.502582788467407

Epoch: [27][38/196]	LR: 0.00010000000000000003	Loss 0.0073 (0.0077)	Prec@1 100.000 (99.970)	
Epoch: [27][77/196]	LR: 0.00010000000000000003	Loss 0.0073 (0.0078)	Prec@1 100.000 (99.970)	
Epoch: [27][116/196]	LR: 0.00010000000000000003	Loss 0.0083 (0.0076)	Prec@1 100.000 (99.980)	
Epoch: [27][155/196]	LR: 0.00010000000000000003	Loss 0.0055 (0.0075)	Prec@1 100.000 (99.985)	
Epoch: [27][194/196]	LR: 0.00010000000000000003	Loss 0.0073 (0.0075)	Prec@1 100.000 (99.978)	
Total train loss: 0.0076

Train time: 20.222810983657837
 * Prec@1 89.090 Prec@5 99.450 Loss 0.4436
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.93375587463379

Epoch: [28][38/196]	LR: 0.00010000000000000003	Loss 0.0060 (0.0076)	Prec@1 100.000 (99.990)	
Epoch: [28][77/196]	LR: 0.00010000000000000003	Loss 0.0087 (0.0079)	Prec@1 100.000 (99.980)	
Epoch: [28][116/196]	LR: 0.00010000000000000003	Loss 0.0064 (0.0078)	Prec@1 100.000 (99.980)	
Epoch: [28][155/196]	LR: 0.00010000000000000003	Loss 0.0073 (0.0079)	Prec@1 100.000 (99.982)	
Epoch: [28][194/196]	LR: 0.00010000000000000003	Loss 0.0068 (0.0080)	Prec@1 100.000 (99.982)	
Total train loss: 0.0080

Train time: 19.193429946899414
 * Prec@1 89.010 Prec@5 99.420 Loss 0.4456
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 23.137744426727295

Epoch: [29][38/196]	LR: 0.00010000000000000003	Loss 0.0085 (0.0078)	Prec@1 100.000 (99.990)	
Epoch: [29][77/196]	LR: 0.00010000000000000003	Loss 0.0139 (0.0082)	Prec@1 100.000 (99.965)	
Epoch: [29][116/196]	LR: 0.00010000000000000003	Loss 0.0110 (0.0080)	Prec@1 100.000 (99.977)	
Epoch: [29][155/196]	LR: 0.00010000000000000003	Loss 0.0081 (0.0080)	Prec@1 100.000 (99.980)	
Epoch: [29][194/196]	LR: 0.00010000000000000003	Loss 0.0061 (0.0079)	Prec@1 100.000 (99.982)	
Total train loss: 0.0079

Train time: 20.571054697036743
 * Prec@1 89.060 Prec@5 99.450 Loss 0.4453
Best acc: 89.180
--------------------------------------------------------------------------------
Test time: 24.251907348632812

