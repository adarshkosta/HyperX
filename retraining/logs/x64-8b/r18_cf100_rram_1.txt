
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode: rram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
Savedir:  ../pretrained_models/frozen/x64-8b/rram/cifar100/resnet18
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar100/resnet18/train/relu1
Test path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar100/resnet18/test/relu1
ResNet18(
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): QConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): QConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): QConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (conv7): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (conv10): QConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): QConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (conv11): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): QConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): QConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): QLinear(in_features=512, out_features=100, bias=False)
  (bn19): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 1.020 Prec@5 5.560 Loss 4.5898
Avg Loading time: 5.4824 seconds
Avg Batch time: 5.5824 seconds

Pre-trained Prec@1 with 1 layers frozen: 1.0199999809265137 	 Loss: 4.58984375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	DT: 0.000 (16.683)	BT: 0.236 (16.934)	Loss 4.1484 (4.6003)	Prec@1 6.250 (3.986)	
Epoch: [0][155/391]	LR: 0.001	DT: 0.000 (16.262)	BT: 0.254 (16.519)	Loss 4.0547 (4.3419)	Prec@1 10.156 (6.871)	
Epoch: [0][233/391]	LR: 0.001	DT: 0.000 (15.528)	BT: 0.283 (15.788)	Loss 4.2148 (4.2649)	Prec@1 8.594 (8.160)	
Epoch: [0][311/391]	LR: 0.001	DT: 0.000 (13.974)	BT: 0.259 (14.237)	Loss 4.2109 (4.2280)	Prec@1 5.469 (8.954)	
Epoch: [0][389/391]	LR: 0.001	DT: 0.000 (13.273)	BT: 0.242 (13.537)	Loss 4.0859 (4.2010)	Prec@1 11.719 (9.619)	
Total train loss: 4.2007
Avg Loading time: 13.2390 seconds
Avg Batch time: 13.5030 seconds

Train time: 5279.7253584861755
 * Prec@1 13.370 Prec@5 33.620 Loss 4.0586
Avg Loading time: 5.3314 seconds
Avg Batch time: 5.4216 seconds

Best acc: 13.370
--------------------------------------------------------------------------------
Test time: 429.3155779838562

Epoch: [1][77/391]	LR: 0.001	DT: 0.000 (8.419)	BT: 0.243 (8.680)	Loss 4.1523 (4.0498)	Prec@1 10.156 (13.191)	
Epoch: [1][155/391]	LR: 0.001	DT: 0.000 (8.963)	BT: 0.260 (9.227)	Loss 3.9297 (4.0396)	Prec@1 19.531 (13.632)	
Epoch: [1][233/391]	LR: 0.001	DT: 3.264 (9.242)	BT: 3.524 (9.508)	Loss 4.0234 (4.0312)	Prec@1 13.281 (13.732)	
Epoch: [1][311/391]	LR: 0.001	DT: 0.000 (9.361)	BT: 0.241 (9.626)	Loss 3.9023 (4.0213)	Prec@1 18.750 (13.982)	
Epoch: [1][389/391]	LR: 0.001	DT: 0.000 (9.357)	BT: 0.230 (9.621)	Loss 4.0039 (4.0125)	Prec@1 14.062 (14.327)	
Total train loss: 4.0125
Avg Loading time: 9.3330 seconds
Avg Batch time: 9.5965 seconds

Train time: 3752.304344892502
 * Prec@1 15.980 Prec@5 37.720 Loss 3.9707
Avg Loading time: 6.1917 seconds
Avg Batch time: 6.2770 seconds

Best acc: 15.980
--------------------------------------------------------------------------------
Test time: 496.964555978775

Epoch: [2][77/391]	LR: 0.001	DT: 0.000 (8.215)	BT: 0.257 (8.479)	Loss 4.0547 (3.9604)	Prec@1 18.750 (16.106)	
Epoch: [2][155/391]	LR: 0.001	DT: 0.000 (9.142)	BT: 0.248 (9.410)	Loss 3.9004 (3.9496)	Prec@1 17.969 (16.036)	
Epoch: [2][233/391]	LR: 0.001	DT: 0.000 (9.516)	BT: 0.314 (9.783)	Loss 3.8809 (3.9475)	Prec@1 12.500 (16.082)	
Epoch: [2][311/391]	LR: 0.001	DT: 0.000 (9.235)	BT: 0.249 (9.503)	Loss 3.9746 (3.9453)	Prec@1 20.312 (16.276)	
Epoch: [2][389/391]	LR: 0.001	DT: 0.000 (9.234)	BT: 0.242 (9.502)	Loss 4.0820 (3.9496)	Prec@1 12.500 (16.246)	
Total train loss: 3.9498
Avg Loading time: 9.2101 seconds
Avg Batch time: 9.4778 seconds

Train time: 3705.8953063488007
 * Prec@1 17.480 Prec@5 40.110 Loss 3.9590
Avg Loading time: 6.6464 seconds
Avg Batch time: 6.7343 seconds

Best acc: 17.480
--------------------------------------------------------------------------------
Test time: 534.7500071525574

Epoch: [3][77/391]	LR: 0.001	DT: 0.000 (8.229)	BT: 0.235 (8.476)	Loss 3.9902 (3.9477)	Prec@1 19.531 (17.638)	
Epoch: [3][155/391]	LR: 0.001	DT: 0.000 (8.843)	BT: 0.234 (9.096)	Loss 3.8574 (3.9523)	Prec@1 25.000 (17.358)	
Epoch: [3][233/391]	LR: 0.001	DT: 3.678 (9.022)	BT: 3.954 (9.280)	Loss 3.9922 (3.9523)	Prec@1 18.750 (17.551)	
Epoch: [3][311/391]	LR: 0.001	DT: 0.000 (8.948)	BT: 0.269 (9.210)	Loss 4.0586 (3.9521)	Prec@1 10.938 (17.513)	
Epoch: [3][389/391]	LR: 0.001	DT: 0.000 (9.015)	BT: 0.230 (9.279)	Loss 3.9648 (3.9536)	Prec@1 17.188 (17.382)	
Total train loss: 3.9536
Avg Loading time: 8.9915 seconds
Avg Batch time: 9.2552 seconds

Train time: 3618.8492698669434
 * Prec@1 18.550 Prec@5 41.980 Loss 3.9395
Avg Loading time: 6.8147 seconds
Avg Batch time: 6.9062 seconds

Best acc: 18.550
--------------------------------------------------------------------------------
Test time: 547.3532304763794

Epoch: [4][77/391]	LR: 0.001	DT: 0.000 (7.309)	BT: 0.254 (7.572)	Loss 3.9023 (3.9514)	Prec@1 25.781 (17.728)	
Epoch: [4][155/391]	LR: 0.001	DT: 0.000 (8.400)	BT: 0.254 (8.667)	Loss 3.9883 (3.9474)	Prec@1 17.188 (17.879)	
Epoch: [4][233/391]	LR: 0.001	DT: 0.000 (8.753)	BT: 0.291 (9.023)	Loss 4.0352 (3.9451)	Prec@1 13.281 (17.982)	
Epoch: [4][311/391]	LR: 0.001	DT: 0.000 (8.787)	BT: 0.274 (9.057)	Loss 3.9414 (3.9430)	Prec@1 21.094 (18.279)	
Epoch: [4][389/391]	LR: 0.001	DT: 0.000 (8.964)	BT: 0.232 (9.234)	Loss 4.0391 (3.9420)	Prec@1 12.500 (18.331)	
Total train loss: 3.9417
Avg Loading time: 8.9411 seconds
Avg Batch time: 9.2103 seconds

Train time: 3601.279515028
 * Prec@1 19.220 Prec@5 42.710 Loss 3.9492
Avg Loading time: 6.3351 seconds
Avg Batch time: 6.4341 seconds

Best acc: 19.220
--------------------------------------------------------------------------------
Test time: 509.39483165740967

Epoch: [5][77/391]	LR: 0.001	DT: 0.000 (7.003)	BT: 0.253 (7.275)	Loss 3.8926 (3.9412)	Prec@1 25.000 (19.241)	
Epoch: [5][155/391]	LR: 0.001	DT: 0.000 (8.159)	BT: 0.296 (8.435)	Loss 3.8867 (3.9411)	Prec@1 18.750 (19.005)	
Epoch: [5][233/391]	LR: 0.001	DT: 14.918 (8.880)	BT: 15.211 (9.158)	Loss 3.9355 (3.9396)	Prec@1 19.531 (19.087)	
Epoch: [5][311/391]	LR: 0.001	DT: 0.000 (9.154)	BT: 0.265 (9.433)	Loss 3.8633 (3.9420)	Prec@1 14.844 (18.983)	
Epoch: [5][389/391]	LR: 0.001	DT: 0.000 (9.321)	BT: 0.247 (9.600)	Loss 3.8359 (3.9414)	Prec@1 26.562 (18.918)	
Total train loss: 3.9414
Avg Loading time: 9.2976 seconds
Avg Batch time: 9.5754 seconds

Train time: 3744.0611159801483
