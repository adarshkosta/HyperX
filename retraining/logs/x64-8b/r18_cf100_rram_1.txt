
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/x64-8b/
          savedir: ../pretrained_models/frozen/x64-8b/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          mode_train: rram
          mode_test: rram
          workers: 8
          epochs: 50
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30, 40]
          loss: crossentropy
          optim: sgd
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
Savedir:  ../pretrained_models/frozen/x64-8b/rram/cifar100/resnet18
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
Train path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar100/resnet18/train/relu1
Test path:  /home/nano01/a/esoufler/activations/x64-8b/rram/one_batch/cifar100/resnet18/test/relu1
ResNet18(
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (conv6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (conv7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (conv8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (conv9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (conv10): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (conv12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (conv13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (conv14): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (resconv3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (conv17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc): Linear(in_features=512, out_features=100, bias=False)
  (bn19): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
 * Prec@1 1.110 Prec@5 5.720 Loss 4.5898
Avg Loading time: 4.4752 seconds
Avg Batch time: 4.5485 seconds

Pre-trained Prec@1 with 1 layers frozen: 1.1100000143051147 	 Loss: 4.58984375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	DT: 0.000 (7.477)	BT: 0.142 (7.620)	Loss 1.8701 (2.9771)	Prec@1 53.906 (33.574)	
Epoch: [0][155/391]	LR: 0.01	DT: 0.000 (8.477)	BT: 0.140 (8.622)	Loss 1.4775 (2.3530)	Prec@1 66.406 (46.469)	
Epoch: [0][233/391]	LR: 0.01	DT: 4.594 (8.693)	BT: 4.742 (8.839)	Loss 1.3281 (2.0399)	Prec@1 73.438 (52.634)	
Epoch: [0][311/391]	LR: 0.01	DT: 0.000 (8.376)	BT: 0.118 (8.520)	Loss 1.3418 (1.8496)	Prec@1 65.625 (56.283)	
Epoch: [0][389/391]	LR: 0.01	DT: 0.000 (8.365)	BT: 0.145 (8.510)	Loss 1.0518 (1.7205)	Prec@1 68.750 (58.752)	
Total train loss: 1.7197
Avg Loading time: 8.3434 seconds
Avg Batch time: 8.4885 seconds

Train time: 3319.0947728157043
 * Prec@1 73.250 Prec@5 94.070 Loss 1.0283
Avg Loading time: 5.4571 seconds
Avg Batch time: 5.5515 seconds

Best acc: 73.250
--------------------------------------------------------------------------------
Test time: 439.6921362876892

Epoch: [1][77/391]	LR: 0.01	DT: 0.001 (6.830)	BT: 0.128 (6.975)	Loss 0.8506 (0.8991)	Prec@1 78.906 (76.803)	
Epoch: [1][155/391]	LR: 0.01	DT: 3.089 (7.219)	BT: 3.273 (7.367)	Loss 0.9023 (0.8804)	Prec@1 73.438 (77.028)	
Epoch: [1][233/391]	LR: 0.01	DT: 0.000 (7.081)	BT: 0.242 (7.235)	Loss 0.7222 (0.8724)	Prec@1 84.375 (77.160)	
Epoch: [1][311/391]	LR: 0.01	DT: 0.000 (7.103)	BT: 0.130 (7.258)	Loss 0.8926 (0.8695)	Prec@1 78.125 (76.966)	
Epoch: [1][389/391]	LR: 0.01	DT: 0.000 (7.440)	BT: 0.148 (7.594)	Loss 0.8682 (0.8658)	Prec@1 76.562 (76.899)	
Total train loss: 0.8661
Avg Loading time: 7.4207 seconds
Avg Batch time: 7.5746 seconds

Train time: 2961.812054157257
 * Prec@1 75.420 Prec@5 94.640 Loss 0.9033
Avg Loading time: 6.1671 seconds
Avg Batch time: 6.2435 seconds

Best acc: 75.420
--------------------------------------------------------------------------------
Test time: 494.54172372817993

Epoch: [2][77/391]	LR: 0.01	DT: 0.689 (6.104)	BT: 0.847 (6.259)	Loss 0.5239 (0.5557)	Prec@1 87.500 (85.998)	
Epoch: [2][155/391]	LR: 0.01	DT: 0.000 (5.963)	BT: 0.140 (6.118)	Loss 0.5098 (0.5467)	Prec@1 83.594 (86.018)	
Epoch: [2][233/391]	LR: 0.01	DT: 0.000 (6.066)	BT: 0.148 (6.222)	Loss 0.5469 (0.5544)	Prec@1 85.156 (85.567)	
Epoch: [2][311/391]	LR: 0.01	DT: 0.000 (6.486)	BT: 0.142 (6.642)	Loss 0.5576 (0.5627)	Prec@1 85.938 (85.131)	
Epoch: [2][389/391]	LR: 0.01	DT: 0.000 (7.021)	BT: 0.139 (7.178)	Loss 0.4329 (0.5689)	Prec@1 90.625 (84.796)	
Total train loss: 0.5698
Avg Loading time: 7.0030 seconds
Avg Batch time: 7.1600 seconds

Train time: 2799.662024974823
 * Prec@1 75.300 Prec@5 94.280 Loss 0.9224
Avg Loading time: 5.8854 seconds
Avg Batch time: 5.9706 seconds

Best acc: 75.420
--------------------------------------------------------------------------------
Test time: 472.2937412261963

Epoch: [3][77/391]	LR: 0.01	DT: 0.000 (4.419)	BT: 0.148 (4.581)	Loss 0.2080 (0.3552)	Prec@1 97.656 (91.306)	
Epoch: [3][155/391]	LR: 0.01	DT: 0.000 (4.856)	BT: 0.156 (5.019)	Loss 0.4561 (0.3483)	Prec@1 85.938 (91.341)	
Epoch: [3][233/391]	LR: 0.01	DT: 0.202 (5.513)	BT: 0.381 (5.675)	Loss 0.4475 (0.3565)	Prec@1 89.844 (91.109)	
Epoch: [3][311/391]	LR: 0.01	DT: 0.000 (5.949)	BT: 0.144 (6.112)	Loss 0.4558 (0.3609)	Prec@1 88.281 (90.838)	
Epoch: [3][389/391]	LR: 0.01	DT: 0.000 (6.410)	BT: 0.174 (6.575)	Loss 0.4004 (0.3693)	Prec@1 89.844 (90.551)	
Total train loss: 0.3694
Avg Loading time: 6.3938 seconds
Avg Batch time: 6.5584 seconds

Train time: 2564.4245975017548
 * Prec@1 76.690 Prec@5 94.230 Loss 0.8950
Avg Loading time: 5.9586 seconds
Avg Batch time: 6.0487 seconds

Best acc: 76.690
--------------------------------------------------------------------------------
Test time: 478.993061542511

Epoch: [4][77/391]	LR: 0.01	DT: 0.781 (5.198)	BT: 0.932 (5.356)	Loss 0.1666 (0.2370)	Prec@1 96.094 (94.732)	
Epoch: [4][155/391]	LR: 0.01	DT: 0.000 (5.404)	BT: 0.138 (5.559)	Loss 0.3040 (0.2310)	Prec@1 94.531 (95.047)	
Epoch: [4][233/391]	LR: 0.01	DT: 18.617 (5.824)	BT: 18.805 (5.979)	Loss 0.2522 (0.2352)	Prec@1 94.531 (94.848)	
Epoch: [4][311/391]	LR: 0.01	DT: 0.000 (6.055)	BT: 0.148 (6.210)	Loss 0.1702 (0.2341)	Prec@1 97.656 (94.794)	
Epoch: [4][389/391]	LR: 0.01	DT: 0.000 (6.455)	BT: 0.145 (6.611)	Loss 0.2168 (0.2357)	Prec@1 94.531 (94.683)	
Total train loss: 0.2359
Avg Loading time: 6.4382 seconds
Avg Batch time: 6.5946 seconds

Train time: 2578.57999420166
 * Prec@1 76.250 Prec@5 93.930 Loss 0.9321
Avg Loading time: 4.6366 seconds
Avg Batch time: 4.7267 seconds

Best acc: 76.690
--------------------------------------------------------------------------------
Test time: 374.13877964019775

Epoch: [5][77/391]	LR: 0.01	DT: 0.254 (4.240)	BT: 0.413 (4.394)	Loss 0.1459 (0.1554)	Prec@1 97.656 (97.105)	
Epoch: [5][155/391]	LR: 0.01	DT: 0.000 (4.787)	BT: 0.148 (4.941)	Loss 0.1785 (0.1541)	Prec@1 94.531 (97.125)	
Epoch: [5][233/391]	LR: 0.01	DT: 0.000 (5.489)	BT: 0.163 (5.645)	Loss 0.1733 (0.1550)	Prec@1 96.094 (97.002)	
Epoch: [5][311/391]	LR: 0.01	DT: 0.000 (5.802)	BT: 0.143 (5.957)	Loss 0.1411 (0.1581)	Prec@1 96.875 (96.860)	
Epoch: [5][389/391]	LR: 0.01	DT: 0.000 (6.289)	BT: 0.136 (6.443)	Loss 0.1536 (0.1602)	Prec@1 96.875 (96.767)	
Total train loss: 0.1603
Avg Loading time: 6.2730 seconds
Avg Batch time: 6.4270 seconds

Train time: 2513.0632462501526
 * Prec@1 76.960 Prec@5 93.440 Loss 0.9409
Avg Loading time: 5.6785 seconds
Avg Batch time: 5.7611 seconds

Best acc: 76.960
--------------------------------------------------------------------------------
Test time: 456.27038860321045

Epoch: [6][77/391]	LR: 0.01	DT: 0.000 (6.043)	BT: 0.136 (6.186)	Loss 0.1150 (0.1144)	Prec@1 97.656 (98.147)	
Epoch: [6][155/391]	LR: 0.01	DT: 0.873 (5.627)	BT: 1.032 (5.773)	Loss 0.1136 (0.1155)	Prec@1 97.656 (98.012)	
Epoch: [6][233/391]	LR: 0.01	DT: 0.000 (5.721)	BT: 0.160 (5.870)	Loss 0.1082 (0.1168)	Prec@1 99.219 (98.020)	
Epoch: [6][311/391]	LR: 0.01	DT: 0.000 (5.976)	BT: 0.153 (6.127)	Loss 0.0765 (0.1171)	Prec@1 99.219 (98.009)	
Epoch: [6][389/391]	LR: 0.01	DT: 7.035 (6.438)	BT: 7.200 (6.593)	Loss 0.1301 (0.1174)	Prec@1 98.438 (97.969)	
Total train loss: 0.1175
Avg Loading time: 6.4218 seconds
Avg Batch time: 6.5767 seconds

Train time: 2571.5846724510193
 * Prec@1 77.560 Prec@5 93.570 Loss 0.9272
Avg Loading time: 5.0233 seconds
Avg Batch time: 5.1083 seconds

Best acc: 77.560
--------------------------------------------------------------------------------
Test time: 404.73140358924866

Epoch: [7][77/391]	LR: 0.01	DT: 0.000 (4.711)	BT: 0.133 (4.871)	Loss 0.0858 (0.0902)	Prec@1 99.219 (98.778)	
Epoch: [7][155/391]	LR: 0.01	DT: 0.000 (4.769)	BT: 0.133 (4.924)	Loss 0.0727 (0.0915)	Prec@1 99.219 (98.643)	
Epoch: [7][233/391]	LR: 0.01	DT: 0.894 (5.518)	BT: 1.050 (5.674)	Loss 0.1016 (0.0935)	Prec@1 99.219 (98.548)	
Epoch: [7][311/391]	LR: 0.01	DT: 0.000 (6.035)	BT: 0.146 (6.192)	Loss 0.0779 (0.0953)	Prec@1 100.000 (98.490)	
Epoch: [7][389/391]	LR: 0.01	DT: 0.000 (6.606)	BT: 0.138 (6.763)	Loss 0.1133 (0.0953)	Prec@1 96.875 (98.498)	
Total train loss: 0.0955
Avg Loading time: 6.5889 seconds
Avg Batch time: 6.7463 seconds

Train time: 2637.932662963867
 * Prec@1 77.430 Prec@5 93.320 Loss 0.9521
Avg Loading time: 5.8418 seconds
Avg Batch time: 5.9264 seconds

Best acc: 77.560
--------------------------------------------------------------------------------
Test time: 468.900315284729

Epoch: [8][77/391]	LR: 0.01	DT: 0.000 (5.303)	BT: 0.136 (5.465)	Loss 0.0571 (0.0716)	Prec@1 100.000 (99.058)	
Epoch: [8][155/391]	LR: 0.01	DT: 7.044 (5.273)	BT: 7.212 (5.433)	Loss 0.0586 (0.0741)	Prec@1 99.219 (98.918)	
Epoch: [8][233/391]	LR: 0.01	DT: 0.000 (5.483)	BT: 0.151 (5.640)	Loss 0.0387 (0.0728)	Prec@1 100.000 (98.932)	
Epoch: [8][311/391]	LR: 0.01	DT: 0.000 (5.812)	BT: 0.130 (5.967)	Loss 0.0524 (0.0735)	Prec@1 99.219 (98.888)	
Epoch: [8][389/391]	LR: 0.01	DT: 0.000 (6.348)	BT: 0.124 (6.501)	Loss 0.0526 (0.0755)	Prec@1 100.000 (98.880)	
Total train loss: 0.0756
Avg Loading time: 6.3313 seconds
Avg Batch time: 6.4850 seconds

Train time: 2535.773093700409
 * Prec@1 77.850 Prec@5 93.340 Loss 0.9312
Avg Loading time: 5.2086 seconds
Avg Batch time: 5.2856 seconds

Best acc: 77.850
--------------------------------------------------------------------------------
Test time: 418.71953892707825

Epoch: [9][77/391]	LR: 0.01	DT: 0.000 (4.631)	BT: 0.141 (4.785)	Loss 0.0663 (0.0646)	Prec@1 99.219 (99.058)	
Epoch: [9][155/391]	LR: 0.01	DT: 0.000 (5.151)	BT: 0.135 (5.303)	Loss 0.0697 (0.0620)	Prec@1 98.438 (99.199)	
Epoch: [9][233/391]	LR: 0.01	DT: 0.000 (5.649)	BT: 0.152 (5.801)	Loss 0.1036 (0.0637)	Prec@1 97.656 (99.175)	
Epoch: [9][311/391]	LR: 0.01	DT: 0.000 (5.954)	BT: 0.131 (6.107)	Loss 0.0539 (0.0641)	Prec@1 99.219 (99.181)	
Epoch: [9][389/391]	LR: 0.01	DT: 0.000 (6.416)	BT: 0.146 (6.570)	Loss 0.0688 (0.0643)	Prec@1 99.219 (99.169)	
Total train loss: 0.0645
Avg Loading time: 6.3994 seconds
Avg Batch time: 6.5531 seconds

Train time: 2562.33709859848
 * Prec@1 77.950 Prec@5 93.350 Loss 0.9399
Avg Loading time: 5.7659 seconds
Avg Batch time: 5.8455 seconds

Best acc: 77.950
--------------------------------------------------------------------------------
Test time: 462.92171931266785

Epoch: [10][77/391]	LR: 0.002	DT: 0.592 (5.742)	BT: 0.831 (5.916)	Loss 0.0531 (0.0485)	Prec@1 99.219 (99.599)	
Epoch: [10][155/391]	LR: 0.002	DT: 3.302 (5.556)	BT: 3.483 (5.729)	Loss 0.0635 (0.0482)	Prec@1 98.438 (99.594)	
Epoch: [10][233/391]	LR: 0.002	DT: 0.000 (5.681)	BT: 0.156 (5.851)	Loss 0.0330 (0.0465)	Prec@1 100.000 (99.616)	
Epoch: [10][311/391]	LR: 0.002	DT: 0.000 (6.031)	BT: 0.148 (6.198)	Loss 0.0237 (0.0448)	Prec@1 100.000 (99.634)	
Epoch: [10][389/391]	LR: 0.002	DT: 0.000 (6.747)	BT: 0.154 (6.914)	Loss 0.0695 (0.0443)	Prec@1 98.438 (99.631)	
Total train loss: 0.0443
Avg Loading time: 6.7301 seconds
Avg Batch time: 6.8964 seconds

Train time: 2696.6431410312653
 * Prec@1 78.890 Prec@5 93.970 Loss 0.8906
Avg Loading time: 5.4223 seconds
Avg Batch time: 5.5145 seconds

Best acc: 78.890
--------------------------------------------------------------------------------
Test time: 436.7425112724304

Epoch: [11][77/391]	LR: 0.002	DT: 0.000 (4.314)	BT: 0.169 (4.479)	Loss 0.0370 (0.0381)	Prec@1 99.219 (99.700)	
Epoch: [11][155/391]	LR: 0.002	DT: 0.000 (4.957)	BT: 0.157 (5.125)	Loss 0.0567 (0.0376)	Prec@1 99.219 (99.730)	
Epoch: [11][233/391]	LR: 0.002	DT: 0.000 (5.717)	BT: 0.169 (5.885)	Loss 0.0961 (0.0384)	Prec@1 98.438 (99.713)	
Epoch: [11][311/391]	LR: 0.002	DT: 0.000 (6.340)	BT: 0.157 (6.507)	Loss 0.0500 (0.0382)	Prec@1 100.000 (99.712)	
Epoch: [11][389/391]	LR: 0.002	DT: 0.000 (6.910)	BT: 0.147 (7.078)	Loss 0.0254 (0.0375)	Prec@1 100.000 (99.722)	
Total train loss: 0.0375
Avg Loading time: 6.8926 seconds
Avg Batch time: 7.0603 seconds

Train time: 2760.6788177490234
 * Prec@1 78.950 Prec@5 94.100 Loss 0.8799
Avg Loading time: 5.7196 seconds
Avg Batch time: 5.8162 seconds

Best acc: 78.950
--------------------------------------------------------------------------------
Test time: 460.637393951416

Epoch: [12][77/391]	LR: 0.002	DT: 0.000 (5.583)	BT: 0.161 (5.760)	Loss 0.0246 (0.0372)	Prec@1 100.000 (99.760)	
Epoch: [12][155/391]	LR: 0.002	DT: 6.615 (5.912)	BT: 6.796 (6.087)	Loss 0.0405 (0.0354)	Prec@1 100.000 (99.815)	
Epoch: [12][233/391]	LR: 0.002	DT: 0.000 (6.294)	BT: 0.155 (6.467)	Loss 0.0443 (0.0354)	Prec@1 100.000 (99.783)	
Epoch: [12][311/391]	LR: 0.002	DT: 1.906 (6.681)	BT: 2.084 (6.854)	Loss 0.0222 (0.0348)	Prec@1 100.000 (99.800)	
Epoch: [12][389/391]	LR: 0.002	DT: 0.000 (7.064)	BT: 0.147 (7.236)	Loss 0.0396 (0.0350)	Prec@1 100.000 (99.794)	
Total train loss: 0.0351
Avg Loading time: 7.0455 seconds
Avg Batch time: 7.2176 seconds

Train time: 2822.1676712036133
 * Prec@1 78.880 Prec@5 94.010 Loss 0.8857
Avg Loading time: 4.7736 seconds
Avg Batch time: 4.8722 seconds

Best acc: 78.950
--------------------------------------------------------------------------------
Test time: 385.52909660339355

Epoch: [13][77/391]	LR: 0.002	DT: 0.000 (4.794)	BT: 0.163 (4.965)	Loss 0.0186 (0.0312)	Prec@1 100.000 (99.810)	
Epoch: [13][155/391]	LR: 0.002	DT: 0.000 (6.014)	BT: 0.154 (6.183)	Loss 0.0653 (0.0326)	Prec@1 99.219 (99.800)	
Epoch: [13][233/391]	LR: 0.002	DT: 0.000 (6.520)	BT: 0.154 (6.689)	Loss 0.0313 (0.0325)	Prec@1 100.000 (99.820)	
Epoch: [13][311/391]	LR: 0.002	DT: 0.000 (7.053)	BT: 0.154 (7.222)	Loss 0.0512 (0.0329)	Prec@1 99.219 (99.807)	
Epoch: [13][389/391]	LR: 0.002	DT: 0.000 (7.569)	BT: 0.181 (7.739)	Loss 0.0405 (0.0332)	Prec@1 100.000 (99.810)	
Total train loss: 0.0332
Avg Loading time: 7.5498 seconds
Avg Batch time: 7.7198 seconds

Train time: 3018.5698680877686
 * Prec@1 79.200 Prec@5 94.000 Loss 0.8799
Avg Loading time: 6.4564 seconds
Avg Batch time: 6.5721 seconds

Best acc: 79.200
--------------------------------------------------------------------------------
Test time: 520.3090648651123

Epoch: [14][77/391]	LR: 0.002	DT: 0.000 (5.447)	BT: 0.238 (5.621)	Loss 0.0146 (0.0326)	Prec@1 100.000 (99.800)	
Epoch: [14][155/391]	LR: 0.002	DT: 0.259 (5.219)	BT: 0.426 (5.395)	Loss 0.0286 (0.0320)	Prec@1 100.000 (99.835)	
Epoch: [14][233/391]	LR: 0.002	DT: 8.685 (5.066)	BT: 8.870 (5.242)	Loss 0.0508 (0.0324)	Prec@1 98.438 (99.823)	
Epoch: [14][311/391]	LR: 0.002	DT: 0.000 (5.408)	BT: 0.159 (5.584)	Loss 0.0192 (0.0322)	Prec@1 100.000 (99.835)	
Epoch: [14][389/391]	LR: 0.002	DT: 0.000 (5.838)	BT: 0.197 (6.014)	Loss 0.0280 (0.0319)	Prec@1 100.000 (99.846)	
Total train loss: 0.0319
Avg Loading time: 5.8227 seconds
Avg Batch time: 5.9986 seconds

Train time: 2345.568353176117
