
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 1.090 Prec@5 5.550 Loss 4.6211
Pre-trained Prec@1 with 11 layers frozen: 1.0899999141693115 	 Loss: 4.62109375

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 4.0859 (4.6427)	Prec@1 8.594 (4.347)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.5137 (4.2035)	Prec@1 20.703 (10.342)	
Epoch: [0][116/196]	LR: 0.002	Loss 3.1777 (3.9181)	Prec@1 32.031 (15.228)	
Epoch: [0][155/196]	LR: 0.002	Loss 3.0039 (3.7051)	Prec@1 30.078 (19.401)	
Epoch: [0][194/196]	LR: 0.002	Loss 2.7598 (3.5400)	Prec@1 41.797 (22.664)	
Total train loss: 3.5389

 * Prec@1 38.590 Prec@5 69.450 Loss 2.7383
Best acc: 38.590
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.6191 (2.6287)	Prec@1 42.578 (42.808)	
Epoch: [1][77/196]	LR: 0.002	Loss 2.4336 (2.5812)	Prec@1 51.172 (43.510)	
Epoch: [1][116/196]	LR: 0.002	Loss 2.4902 (2.5377)	Prec@1 44.531 (44.615)	
Epoch: [1][155/196]	LR: 0.002	Loss 2.3574 (2.5011)	Prec@1 45.703 (45.335)	
Epoch: [1][194/196]	LR: 0.002	Loss 2.3965 (2.4627)	Prec@1 47.266 (46.164)	
Total train loss: 2.4628

 * Prec@1 48.330 Prec@5 78.640 Loss 2.3008
Best acc: 48.330
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 2.1738 (2.1757)	Prec@1 50.391 (53.385)	
Epoch: [2][77/196]	LR: 0.002	Loss 2.0195 (2.1570)	Prec@1 58.203 (53.596)	
Epoch: [2][116/196]	LR: 0.002	Loss 2.1582 (2.1356)	Prec@1 54.688 (53.996)	
Epoch: [2][155/196]	LR: 0.002	Loss 2.0859 (2.1198)	Prec@1 54.297 (54.262)	
Epoch: [2][194/196]	LR: 0.002	Loss 2.0586 (2.0996)	Prec@1 54.688 (54.593)	
Total train loss: 2.0995

 * Prec@1 52.830 Prec@5 82.230 Loss 2.0840
Best acc: 52.830
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 1.8135 (1.9302)	Prec@1 61.719 (59.245)	
Epoch: [3][77/196]	LR: 0.002	Loss 1.9004 (1.9257)	Prec@1 55.859 (58.784)	
Epoch: [3][116/196]	LR: 0.002	Loss 1.7617 (1.9111)	Prec@1 58.594 (59.008)	
Epoch: [3][155/196]	LR: 0.002	Loss 1.7129 (1.8959)	Prec@1 61.328 (59.192)	
Epoch: [3][194/196]	LR: 0.002	Loss 1.7178 (1.8812)	Prec@1 64.062 (59.493)	
Total train loss: 1.8817

 * Prec@1 55.960 Prec@5 84.430 Loss 1.9375
Best acc: 55.960
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 1.7852 (1.7469)	Prec@1 60.938 (63.061)	
Epoch: [4][77/196]	LR: 0.002	Loss 1.7480 (1.7397)	Prec@1 63.672 (63.416)	
Epoch: [4][116/196]	LR: 0.002	Loss 1.7773 (1.7322)	Prec@1 61.328 (63.462)	
Epoch: [4][155/196]	LR: 0.002	Loss 1.6934 (1.7208)	Prec@1 65.234 (63.527)	
Epoch: [4][194/196]	LR: 0.002	Loss 1.7051 (1.7180)	Prec@1 63.672 (63.508)	
Total train loss: 1.7179

 * Prec@1 57.580 Prec@5 85.790 Loss 1.8486
Best acc: 57.580
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.5967 (1.5889)	Prec@1 63.672 (67.077)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.5508 (1.5955)	Prec@1 66.016 (66.632)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.5498 (1.5975)	Prec@1 68.750 (66.456)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.5820 (1.5920)	Prec@1 65.625 (66.409)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.5303 (1.5916)	Prec@1 64.453 (66.358)	
Total train loss: 1.5916

 * Prec@1 59.160 Prec@5 86.450 Loss 1.7734
Best acc: 59.160
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.4834 (1.4939)	Prec@1 66.406 (69.040)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.4785 (1.4839)	Prec@1 67.578 (69.251)	
Epoch: [6][116/196]	LR: 0.002	Loss 1.5547 (1.4856)	Prec@1 66.797 (69.057)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.5254 (1.4836)	Prec@1 66.016 (68.980)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.4424 (1.4818)	Prec@1 71.094 (68.974)	
Total train loss: 1.4822

 * Prec@1 60.330 Prec@5 86.880 Loss 1.7139
Best acc: 60.330
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 1.4150 (1.3894)	Prec@1 69.922 (71.314)	
Epoch: [7][77/196]	LR: 0.002	Loss 1.4062 (1.3911)	Prec@1 69.531 (71.544)	
Epoch: [7][116/196]	LR: 0.002	Loss 1.4307 (1.3876)	Prec@1 71.484 (71.474)	
Epoch: [7][155/196]	LR: 0.002	Loss 1.4385 (1.3873)	Prec@1 67.188 (71.307)	
Epoch: [7][194/196]	LR: 0.002	Loss 1.5293 (1.3868)	Prec@1 62.891 (71.328)	
Total train loss: 1.3870

 * Prec@1 61.210 Prec@5 87.260 Loss 1.6758
Best acc: 61.210
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 1.4053 (1.3075)	Prec@1 71.484 (73.798)	
Epoch: [8][77/196]	LR: 0.002	Loss 1.3252 (1.3078)	Prec@1 78.125 (73.728)	
Epoch: [8][116/196]	LR: 0.002	Loss 1.2383 (1.2989)	Prec@1 79.297 (73.952)	
Epoch: [8][155/196]	LR: 0.002	Loss 1.2695 (1.3018)	Prec@1 76.172 (73.771)	
Epoch: [8][194/196]	LR: 0.002	Loss 1.3770 (1.3006)	Prec@1 68.750 (73.942)	
Total train loss: 1.3009

 * Prec@1 61.990 Prec@5 87.550 Loss 1.6455
Best acc: 61.990
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 1.1484 (1.2097)	Prec@1 78.125 (76.633)	
Epoch: [9][77/196]	LR: 0.002	Loss 1.1406 (1.2263)	Prec@1 78.125 (75.846)	
Epoch: [9][116/196]	LR: 0.002	Loss 1.2539 (1.2228)	Prec@1 75.781 (76.062)	
Epoch: [9][155/196]	LR: 0.002	Loss 1.2812 (1.2219)	Prec@1 72.656 (76.024)	
Epoch: [9][194/196]	LR: 0.002	Loss 1.1914 (1.2262)	Prec@1 78.125 (75.733)	
Total train loss: 1.2267

 * Prec@1 62.180 Prec@5 87.620 Loss 1.6230
Best acc: 62.180
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 1.2773 (1.1373)	Prec@1 73.438 (78.556)	
Epoch: [10][77/196]	LR: 0.0004	Loss 1.1123 (1.1321)	Prec@1 78.906 (78.936)	
Epoch: [10][116/196]	LR: 0.0004	Loss 1.2041 (1.1386)	Prec@1 77.344 (78.699)	
Epoch: [10][155/196]	LR: 0.0004	Loss 1.1836 (1.1423)	Prec@1 76.172 (78.686)	
Epoch: [10][194/196]	LR: 0.0004	Loss 1.2373 (1.1428)	Prec@1 75.391 (78.632)	
Total train loss: 1.1430

 * Prec@1 62.540 Prec@5 87.970 Loss 1.6201
Best acc: 62.540
--------------------------------------------------------------------------------
Epoch: [11][38/196]	LR: 0.0004	Loss 1.1318 (1.1257)	Prec@1 78.516 (79.657)	
Epoch: [11][77/196]	LR: 0.0004	Loss 1.1494 (1.1283)	Prec@1 79.688 (79.282)	
Epoch: [11][116/196]	LR: 0.0004	Loss 1.1025 (1.1299)	Prec@1 78.516 (79.233)	
Epoch: [11][155/196]	LR: 0.0004	Loss 1.1299 (1.1338)	Prec@1 78.516 (79.142)	
Epoch: [11][194/196]	LR: 0.0004	Loss 1.1768 (1.1347)	Prec@1 77.344 (78.982)	
Total train loss: 1.1348

 * Prec@1 62.400 Prec@5 87.850 Loss 1.6211
Best acc: 62.540
--------------------------------------------------------------------------------
Epoch: [12][38/196]	LR: 0.0004	Loss 1.1230 (1.1215)	Prec@1 78.906 (79.477)	
Epoch: [12][77/196]	LR: 0.0004	Loss 1.1113 (1.1251)	Prec@1 80.078 (79.462)	
Epoch: [12][116/196]	LR: 0.0004	Loss 1.1572 (1.1257)	Prec@1 78.125 (79.434)	
Epoch: [12][155/196]	LR: 0.0004	Loss 1.0811 (1.1290)	Prec@1 78.516 (79.157)	
Epoch: [12][194/196]	LR: 0.0004	Loss 1.1318 (1.1313)	Prec@1 79.297 (79.024)	
Total train loss: 1.1316

 * Prec@1 62.390 Prec@5 87.730 Loss 1.6191
Best acc: 62.540
--------------------------------------------------------------------------------
Epoch: [13][38/196]	LR: 0.0004	Loss 1.0977 (1.1194)	Prec@1 82.422 (79.547)	
Epoch: [13][77/196]	LR: 0.0004	Loss 1.1074 (1.1234)	Prec@1 80.078 (79.402)	
Epoch: [13][116/196]	LR: 0.0004	Loss 1.1387 (1.1217)	Prec@1 80.078 (79.347)	
Epoch: [13][155/196]	LR: 0.0004	Loss 1.1113 (1.1225)	Prec@1 80.469 (79.214)	
Epoch: [13][194/196]	LR: 0.0004	Loss 1.2666 (1.1239)	Prec@1 76.172 (79.169)	
Total train loss: 1.1243

 * Prec@1 62.610 Prec@5 87.880 Loss 1.6191
Best acc: 62.610
--------------------------------------------------------------------------------
