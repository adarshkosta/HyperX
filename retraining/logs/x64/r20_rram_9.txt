
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.220 Prec@5 90.310 Loss 1.1758
Pre-trained Prec@1 with 9 layers frozen: 69.22000122070312 	 Loss: 1.17578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.5171 (0.5756)	Prec@1 85.156 (83.914)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.5884 (0.5566)	Prec@1 87.500 (84.600)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.5034 (0.5588)	Prec@1 88.281 (84.455)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.4939 (0.5608)	Prec@1 86.719 (84.375)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5298 (0.5635)	Prec@1 85.156 (84.251)	
Total train loss: 0.5637

 * Prec@1 69.430 Prec@5 90.390 Loss 1.1680
Best acc: 69.430
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.6631 (0.5681)	Prec@1 80.469 (83.564)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.4807 (0.5670)	Prec@1 85.938 (83.809)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5469 (0.5672)	Prec@1 84.375 (83.761)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.4641 (0.5614)	Prec@1 87.500 (84.095)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.7114 (0.5650)	Prec@1 82.812 (84.095)	
Total train loss: 0.5655

 * Prec@1 69.400 Prec@5 90.680 Loss 1.1670
Best acc: 69.430
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.5088 (0.5528)	Prec@1 81.250 (84.125)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.6514 (0.5570)	Prec@1 80.469 (84.135)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.4832 (0.5592)	Prec@1 83.594 (84.085)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.6113 (0.5607)	Prec@1 82.812 (83.999)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.6846 (0.5597)	Prec@1 81.250 (84.087)	
Total train loss: 0.5598

 * Prec@1 69.410 Prec@5 90.590 Loss 1.1621
Best acc: 69.430
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.5283 (0.5576)	Prec@1 85.156 (84.125)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.4036 (0.5595)	Prec@1 89.062 (84.059)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.6885 (0.5606)	Prec@1 80.469 (84.008)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5581 (0.5610)	Prec@1 83.594 (84.080)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.5723 (0.5620)	Prec@1 82.812 (84.113)	
Total train loss: 0.5623

 * Prec@1 69.530 Prec@5 90.760 Loss 1.1631
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.5752 (0.5535)	Prec@1 82.031 (84.405)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.5542 (0.5609)	Prec@1 83.594 (84.225)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.5713 (0.5607)	Prec@1 80.469 (84.125)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.4072 (0.5594)	Prec@1 89.844 (84.217)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.5728 (0.5595)	Prec@1 84.375 (84.265)	
Total train loss: 0.5598

 * Prec@1 69.490 Prec@5 90.630 Loss 1.1641
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.4822 (0.5424)	Prec@1 89.062 (85.296)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.5181 (0.5519)	Prec@1 84.375 (84.741)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.7109 (0.5537)	Prec@1 80.469 (84.552)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.5669 (0.5583)	Prec@1 87.500 (84.393)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.5762 (0.5606)	Prec@1 79.688 (84.301)	
Total train loss: 0.5608

 * Prec@1 69.200 Prec@5 90.520 Loss 1.1641
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.5396 (0.5546)	Prec@1 85.938 (84.575)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.4192 (0.5535)	Prec@1 90.625 (84.560)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.6567 (0.5610)	Prec@1 79.688 (84.312)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.5684 (0.5572)	Prec@1 82.031 (84.357)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.6631 (0.5567)	Prec@1 79.688 (84.369)	
Total train loss: 0.5566

 * Prec@1 69.450 Prec@5 90.490 Loss 1.1689
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.6606 (0.5729)	Prec@1 81.250 (83.844)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.4915 (0.5657)	Prec@1 86.719 (83.909)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.5361 (0.5588)	Prec@1 89.062 (84.138)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.6421 (0.5577)	Prec@1 82.812 (84.270)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.5527 (0.5578)	Prec@1 80.469 (84.249)	
Total train loss: 0.5579

 * Prec@1 69.410 Prec@5 90.480 Loss 1.1670
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.7075 (0.5612)	Prec@1 80.469 (84.165)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.6118 (0.5680)	Prec@1 82.031 (83.974)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.7256 (0.5638)	Prec@1 78.125 (84.131)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.4419 (0.5620)	Prec@1 89.844 (84.257)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.4814 (0.5603)	Prec@1 88.281 (84.277)	
Total train loss: 0.5604

 * Prec@1 69.310 Prec@5 90.660 Loss 1.1641
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.4961 (0.5573)	Prec@1 85.156 (84.095)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.5195 (0.5563)	Prec@1 87.500 (84.195)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.6099 (0.5572)	Prec@1 84.375 (84.215)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.5928 (0.5589)	Prec@1 80.469 (84.217)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.4590 (0.5578)	Prec@1 87.500 (84.233)	
Total train loss: 0.5577

 * Prec@1 69.210 Prec@5 90.570 Loss 1.1611
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.5156 (0.5536)	Prec@1 82.812 (84.545)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.5977 (0.5555)	Prec@1 85.156 (84.375)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.5845 (0.5602)	Prec@1 83.594 (84.228)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.6406 (0.5623)	Prec@1 84.375 (84.077)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.6001 (0.5612)	Prec@1 81.250 (84.137)	
Total train loss: 0.5611

 * Prec@1 69.440 Prec@5 90.500 Loss 1.1631
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.6396 (0.5730)	Prec@1 78.906 (83.524)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.7085 (0.5679)	Prec@1 76.562 (83.714)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.4236 (0.5604)	Prec@1 87.500 (84.168)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.4941 (0.5580)	Prec@1 82.031 (84.177)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5576 (0.5590)	Prec@1 84.375 (84.215)	
Total train loss: 0.5592

 * Prec@1 69.400 Prec@5 90.610 Loss 1.1572
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.6372 (0.5691)	Prec@1 82.812 (83.524)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.6240 (0.5635)	Prec@1 82.812 (83.959)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.5361 (0.5613)	Prec@1 88.281 (84.115)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.5835 (0.5628)	Prec@1 82.812 (84.057)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.4941 (0.5598)	Prec@1 85.156 (84.167)	
Total train loss: 0.5597

 * Prec@1 69.430 Prec@5 90.600 Loss 1.1650
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.5708 (0.5514)	Prec@1 86.719 (84.445)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.5303 (0.5511)	Prec@1 85.156 (84.395)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.6172 (0.5550)	Prec@1 82.812 (84.185)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.4377 (0.5585)	Prec@1 84.375 (84.125)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.6147 (0.5570)	Prec@1 82.031 (84.165)	
Total train loss: 0.5573

 * Prec@1 69.570 Prec@5 90.680 Loss 1.1621
Best acc: 69.570
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.4917 (0.5477)	Prec@1 87.500 (84.966)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.4939 (0.5520)	Prec@1 87.500 (84.670)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.5584)	Prec@1 83.594 (84.378)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.6294 (0.5588)	Prec@1 78.125 (84.337)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.5259 (0.5607)	Prec@1 83.594 (84.281)	
Total train loss: 0.5609

 * Prec@1 69.370 Prec@5 90.480 Loss 1.1660
Best acc: 69.570
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.5273 (0.5641)	Prec@1 86.719 (83.964)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.6074 (0.5625)	Prec@1 81.250 (84.085)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.4497 (0.5602)	Prec@1 89.062 (84.171)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.4844 (0.5590)	Prec@1 88.281 (84.230)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.5786 (0.5594)	Prec@1 85.156 (84.259)	
Total train loss: 0.5596

 * Prec@1 69.260 Prec@5 90.490 Loss 1.1660
Best acc: 69.570
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.5698 (0.5626)	Prec@1 83.594 (84.265)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.6372 (0.5598)	Prec@1 82.031 (84.330)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.5054 (0.5576)	Prec@1 85.156 (84.368)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.5835 (0.5582)	Prec@1 83.594 (84.337)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.6006 (0.5577)	Prec@1 83.594 (84.433)	
Total train loss: 0.5579

 * Prec@1 69.210 Prec@5 90.570 Loss 1.1582
Best acc: 69.570
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.4456 (0.5520)	Prec@1 89.062 (84.415)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.4268 (0.5489)	Prec@1 89.844 (84.520)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.6538 (0.5536)	Prec@1 80.469 (84.318)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.5190 (0.5556)	Prec@1 89.062 (84.393)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.5352 (0.5569)	Prec@1 89.844 (84.343)	
Total train loss: 0.5571

 * Prec@1 69.600 Prec@5 90.450 Loss 1.1641
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.4980 (0.5647)	Prec@1 85.938 (84.075)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.5493 (0.5587)	Prec@1 82.812 (84.150)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.5645 (0.5575)	Prec@1 84.375 (84.292)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.5088 (0.5601)	Prec@1 88.281 (84.190)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.5234 (0.5572)	Prec@1 85.938 (84.367)	
Total train loss: 0.5575

 * Prec@1 69.510 Prec@5 90.530 Loss 1.1611
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.6216 (0.5627)	Prec@1 81.250 (83.894)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.6992 (0.5512)	Prec@1 78.906 (84.365)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.5610 (0.5576)	Prec@1 82.812 (84.208)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.6079 (0.5570)	Prec@1 79.688 (84.245)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.5381 (0.5582)	Prec@1 83.594 (84.263)	
Total train loss: 0.5585

 * Prec@1 69.550 Prec@5 90.610 Loss 1.1650
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.5215 (0.5562)	Prec@1 84.375 (84.385)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.5967 (0.5580)	Prec@1 83.594 (84.320)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.6523 (0.5541)	Prec@1 81.250 (84.535)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.6055 (0.5573)	Prec@1 85.938 (84.383)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.4932 (0.5574)	Prec@1 85.938 (84.435)	
Total train loss: 0.5576

 * Prec@1 69.430 Prec@5 90.680 Loss 1.1670
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.6641 (0.5555)	Prec@1 79.688 (84.395)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.4634 (0.5575)	Prec@1 89.062 (84.440)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.4536 (0.5604)	Prec@1 92.188 (84.268)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.6328 (0.5596)	Prec@1 79.688 (84.252)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.4875 (0.5589)	Prec@1 87.500 (84.315)	
Total train loss: 0.5587

 * Prec@1 69.140 Prec@5 90.570 Loss 1.1680
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.5200 (0.5475)	Prec@1 85.938 (84.706)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.5674 (0.5572)	Prec@1 88.281 (84.310)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.5366 (0.5614)	Prec@1 83.594 (84.198)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.5303 (0.5597)	Prec@1 85.156 (84.185)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.5635 (0.5590)	Prec@1 83.594 (84.231)	
Total train loss: 0.5590

 * Prec@1 69.460 Prec@5 90.640 Loss 1.1689
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.5308 (0.5525)	Prec@1 87.500 (84.756)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.5776 (0.5503)	Prec@1 86.719 (84.716)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.4331 (0.5492)	Prec@1 89.062 (84.679)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.7153 (0.5527)	Prec@1 78.906 (84.473)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.5933 (0.5548)	Prec@1 82.031 (84.483)	
Total train loss: 0.5551

 * Prec@1 69.540 Prec@5 90.570 Loss 1.1631
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.6465 (0.5533)	Prec@1 83.594 (84.335)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5544)	Prec@1 82.812 (84.480)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5112 (0.5532)	Prec@1 83.594 (84.505)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.5054 (0.5567)	Prec@1 84.375 (84.352)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5923 (0.5601)	Prec@1 84.375 (84.179)	
Total train loss: 0.5603

 * Prec@1 69.380 Prec@5 90.520 Loss 1.1641
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.4500 (0.5578)	Prec@1 89.062 (84.495)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.4922 (0.5548)	Prec@1 87.500 (84.270)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.5908 (0.5607)	Prec@1 83.594 (84.168)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.5068 (0.5587)	Prec@1 86.719 (84.265)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.4619 (0.5599)	Prec@1 86.719 (84.239)	
Total train loss: 0.5600

 * Prec@1 69.490 Prec@5 90.640 Loss 1.1611
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.6050 (0.5479)	Prec@1 82.031 (84.726)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.4648 (0.5561)	Prec@1 85.938 (84.550)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.5479 (0.5576)	Prec@1 85.156 (84.505)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.4331 (0.5525)	Prec@1 88.281 (84.688)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.6357 (0.5550)	Prec@1 84.375 (84.503)	
Total train loss: 0.5553

 * Prec@1 69.490 Prec@5 90.700 Loss 1.1592
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.6025 (0.5684)	Prec@1 85.938 (84.165)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.4495 (0.5517)	Prec@1 85.938 (84.721)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.8018 (0.5584)	Prec@1 75.000 (84.355)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.4158 (0.5575)	Prec@1 90.625 (84.305)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.4971 (0.5590)	Prec@1 84.375 (84.307)	
Total train loss: 0.5590

 * Prec@1 69.530 Prec@5 90.700 Loss 1.1611
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.4524 (0.5553)	Prec@1 87.500 (84.325)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.5981 (0.5539)	Prec@1 79.688 (84.235)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.6729 (0.5573)	Prec@1 82.031 (84.168)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.4800 (0.5612)	Prec@1 89.062 (84.130)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.5840 (0.5593)	Prec@1 85.156 (84.227)	
Total train loss: 0.5596

 * Prec@1 69.420 Prec@5 90.590 Loss 1.1660
Best acc: 69.600
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5508)	Prec@1 85.156 (84.415)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5550)	Prec@1 83.594 (84.415)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.5205 (0.5579)	Prec@1 84.375 (84.365)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.4749 (0.5594)	Prec@1 85.938 (84.282)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5190 (0.5570)	Prec@1 85.156 (84.379)	
Total train loss: 0.5572

 * Prec@1 69.560 Prec@5 90.550 Loss 1.1582
Best acc: 69.600
--------------------------------------------------------------------------------
