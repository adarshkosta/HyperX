
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [10, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.310 Prec@5 90.430 Loss 1.1748
Pre-trained Prec@1 with 5 layers frozen: 69.30999755859375 	 Loss: 1.1748046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	Loss 0.7295 (0.6732)	Prec@1 81.250 (80.789)	
Epoch: [0][155/391]	LR: 0.01	Loss 0.8301 (0.7333)	Prec@1 70.312 (78.686)	
Epoch: [0][233/391]	LR: 0.01	Loss 0.7773 (0.7610)	Prec@1 76.562 (77.698)	
Epoch: [0][311/391]	LR: 0.01	Loss 0.8433 (0.7761)	Prec@1 73.438 (77.171)	
Epoch: [0][389/391]	LR: 0.01	Loss 0.7393 (0.7860)	Prec@1 75.781 (76.967)	
Total train loss: 0.7859

 * Prec@1 65.000 Prec@5 88.590 Loss 1.3330
Best acc: 65.000
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.01	Loss 0.6514 (0.6317)	Prec@1 81.250 (81.951)	
Epoch: [1][155/391]	LR: 0.01	Loss 0.6309 (0.6480)	Prec@1 82.812 (81.510)	
Epoch: [1][233/391]	LR: 0.01	Loss 0.6177 (0.6643)	Prec@1 82.812 (81.147)	
Epoch: [1][311/391]	LR: 0.01	Loss 0.6401 (0.6815)	Prec@1 82.031 (80.501)	
Epoch: [1][389/391]	LR: 0.01	Loss 0.7603 (0.6966)	Prec@1 74.219 (79.980)	
Total train loss: 0.6971

 * Prec@1 65.130 Prec@5 88.200 Loss 1.3330
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.01	Loss 0.5898 (0.5627)	Prec@1 84.375 (84.485)	
Epoch: [2][155/391]	LR: 0.01	Loss 0.6616 (0.5950)	Prec@1 79.688 (83.283)	
Epoch: [2][233/391]	LR: 0.01	Loss 0.6323 (0.6063)	Prec@1 84.375 (82.816)	
Epoch: [2][311/391]	LR: 0.01	Loss 0.7778 (0.6209)	Prec@1 78.125 (82.357)	
Epoch: [2][389/391]	LR: 0.01	Loss 0.7031 (0.6332)	Prec@1 81.250 (81.965)	
Total train loss: 0.6332

 * Prec@1 64.200 Prec@5 87.520 Loss 1.3965
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.01	Loss 0.5234 (0.5276)	Prec@1 86.719 (85.337)	
Epoch: [3][155/391]	LR: 0.01	Loss 0.5005 (0.5433)	Prec@1 89.062 (84.861)	
Epoch: [3][233/391]	LR: 0.01	Loss 0.6074 (0.5516)	Prec@1 83.594 (84.559)	
Epoch: [3][311/391]	LR: 0.01	Loss 0.5791 (0.5673)	Prec@1 82.812 (83.982)	
Epoch: [3][389/391]	LR: 0.01	Loss 0.6548 (0.5803)	Prec@1 79.688 (83.550)	
Total train loss: 0.5805

 * Prec@1 64.750 Prec@5 87.950 Loss 1.3594
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.01	Loss 0.4351 (0.4620)	Prec@1 89.844 (88.061)	
Epoch: [4][155/391]	LR: 0.01	Loss 0.3896 (0.4750)	Prec@1 92.188 (87.440)	
Epoch: [4][233/391]	LR: 0.01	Loss 0.4744 (0.4975)	Prec@1 83.594 (86.552)	
Epoch: [4][311/391]	LR: 0.01	Loss 0.4954 (0.5101)	Prec@1 88.281 (86.005)	
Epoch: [4][389/391]	LR: 0.01	Loss 0.6177 (0.5243)	Prec@1 77.344 (85.487)	
Total train loss: 0.5248

 * Prec@1 64.010 Prec@5 87.560 Loss 1.4121
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.01	Loss 0.3552 (0.4372)	Prec@1 92.969 (88.722)	
Epoch: [5][155/391]	LR: 0.01	Loss 0.3472 (0.4421)	Prec@1 89.844 (88.557)	
Epoch: [5][233/391]	LR: 0.01	Loss 0.4409 (0.4627)	Prec@1 89.844 (87.667)	
Epoch: [5][311/391]	LR: 0.01	Loss 0.5845 (0.4717)	Prec@1 83.594 (87.310)	
Epoch: [5][389/391]	LR: 0.01	Loss 0.4868 (0.4844)	Prec@1 88.281 (86.831)	
Total train loss: 0.4845

 * Prec@1 62.540 Prec@5 86.830 Loss 1.4951
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.01	Loss 0.3821 (0.3876)	Prec@1 92.188 (90.635)	
Epoch: [6][155/391]	LR: 0.01	Loss 0.4490 (0.3964)	Prec@1 88.281 (90.330)	
Epoch: [6][233/391]	LR: 0.01	Loss 0.4717 (0.4098)	Prec@1 87.500 (89.767)	
Epoch: [6][311/391]	LR: 0.01	Loss 0.3730 (0.4250)	Prec@1 92.188 (89.163)	
Epoch: [6][389/391]	LR: 0.01	Loss 0.6592 (0.4389)	Prec@1 79.688 (88.692)	
Total train loss: 0.4392

 * Prec@1 62.360 Prec@5 86.680 Loss 1.5381
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.01	Loss 0.3281 (0.3545)	Prec@1 91.406 (91.997)	
Epoch: [7][155/391]	LR: 0.01	Loss 0.3899 (0.3631)	Prec@1 89.062 (91.476)	
Epoch: [7][233/391]	LR: 0.01	Loss 0.3740 (0.3809)	Prec@1 91.406 (90.815)	
Epoch: [7][311/391]	LR: 0.01	Loss 0.4041 (0.3966)	Prec@1 92.969 (90.229)	
Epoch: [7][389/391]	LR: 0.01	Loss 0.4067 (0.4092)	Prec@1 87.500 (89.732)	
Total train loss: 0.4094

 * Prec@1 62.440 Prec@5 86.510 Loss 1.5312
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.01	Loss 0.2979 (0.3366)	Prec@1 92.969 (92.157)	
Epoch: [8][155/391]	LR: 0.01	Loss 0.3669 (0.3425)	Prec@1 89.062 (91.942)	
Epoch: [8][233/391]	LR: 0.01	Loss 0.4231 (0.3523)	Prec@1 87.500 (91.570)	
Epoch: [8][311/391]	LR: 0.01	Loss 0.4248 (0.3649)	Prec@1 87.500 (91.118)	
Epoch: [8][389/391]	LR: 0.01	Loss 0.3823 (0.3752)	Prec@1 89.844 (90.745)	
Total train loss: 0.3755

 * Prec@1 62.860 Prec@5 86.260 Loss 1.5420
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.01	Loss 0.3450 (0.2989)	Prec@1 92.188 (93.960)	
Epoch: [9][155/391]	LR: 0.01	Loss 0.2477 (0.3087)	Prec@1 93.750 (93.349)	
Epoch: [9][233/391]	LR: 0.01	Loss 0.3918 (0.3215)	Prec@1 92.188 (92.812)	
Epoch: [9][311/391]	LR: 0.01	Loss 0.3584 (0.3321)	Prec@1 89.844 (92.373)	
Epoch: [9][389/391]	LR: 0.01	Loss 0.4778 (0.3418)	Prec@1 88.281 (92.047)	
Total train loss: 0.3419

 * Prec@1 61.720 Prec@5 85.590 Loss 1.5898
Best acc: 65.130
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.001	Loss 0.2429 (0.2622)	Prec@1 96.094 (94.852)	
Epoch: [10][155/391]	LR: 0.001	Loss 0.2433 (0.2498)	Prec@1 95.312 (95.302)	
Epoch: [10][233/391]	LR: 0.001	Loss 0.2773 (0.2443)	Prec@1 93.750 (95.523)	
Epoch: [10][311/391]	LR: 0.001	Loss 0.2004 (0.2380)	Prec@1 96.875 (95.768)	
Epoch: [10][389/391]	LR: 0.001	Loss 0.2465 (0.2320)	Prec@1 95.312 (95.998)	
Total train loss: 0.2321

 * Prec@1 65.610 Prec@5 88.050 Loss 1.4102
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.001	Loss 0.1875 (0.2021)	Prec@1 96.094 (97.165)	
Epoch: [11][155/391]	LR: 0.001	Loss 0.1610 (0.2011)	Prec@1 99.219 (97.130)	
Epoch: [11][233/391]	LR: 0.001	Loss 0.1747 (0.2002)	Prec@1 97.656 (97.162)	
Epoch: [11][311/391]	LR: 0.001	Loss 0.1335 (0.1987)	Prec@1 98.438 (97.158)	
Epoch: [11][389/391]	LR: 0.001	Loss 0.1383 (0.1981)	Prec@1 98.438 (97.188)	
Total train loss: 0.1983

 * Prec@1 65.480 Prec@5 87.760 Loss 1.4062
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.001	Loss 0.1699 (0.1824)	Prec@1 100.000 (97.646)	
Epoch: [12][155/391]	LR: 0.001	Loss 0.1826 (0.1826)	Prec@1 98.438 (97.766)	
Epoch: [12][233/391]	LR: 0.001	Loss 0.2051 (0.1816)	Prec@1 94.531 (97.750)	
Epoch: [12][311/391]	LR: 0.001	Loss 0.2344 (0.1816)	Prec@1 96.094 (97.699)	
Epoch: [12][389/391]	LR: 0.001	Loss 0.1692 (0.1825)	Prec@1 98.438 (97.662)	
Total train loss: 0.1827

 * Prec@1 65.450 Prec@5 87.620 Loss 1.4131
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.001	Loss 0.1494 (0.1740)	Prec@1 99.219 (98.017)	
Epoch: [13][155/391]	LR: 0.001	Loss 0.1947 (0.1747)	Prec@1 98.438 (97.917)	
Epoch: [13][233/391]	LR: 0.001	Loss 0.1664 (0.1749)	Prec@1 98.438 (97.890)	
Epoch: [13][311/391]	LR: 0.001	Loss 0.1799 (0.1750)	Prec@1 99.219 (97.942)	
Epoch: [13][389/391]	LR: 0.001	Loss 0.1212 (0.1755)	Prec@1 100.000 (97.923)	
Total train loss: 0.1756

 * Prec@1 65.310 Prec@5 87.520 Loss 1.4219
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.001	Loss 0.1971 (0.1741)	Prec@1 99.219 (97.997)	
Epoch: [14][155/391]	LR: 0.001	Loss 0.1561 (0.1733)	Prec@1 98.438 (98.037)	
Epoch: [14][233/391]	LR: 0.001	Loss 0.1447 (0.1719)	Prec@1 98.438 (98.104)	
Epoch: [14][311/391]	LR: 0.001	Loss 0.1646 (0.1714)	Prec@1 99.219 (98.127)	
Epoch: [14][389/391]	LR: 0.001	Loss 0.1956 (0.1711)	Prec@1 97.656 (98.121)	
Total train loss: 0.1712

 * Prec@1 65.360 Prec@5 87.500 Loss 1.4180
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.001	Loss 0.2100 (0.1702)	Prec@1 96.094 (98.137)	
Epoch: [15][155/391]	LR: 0.001	Loss 0.1505 (0.1698)	Prec@1 99.219 (98.182)	
Epoch: [15][233/391]	LR: 0.001	Loss 0.1589 (0.1683)	Prec@1 97.656 (98.190)	
Epoch: [15][311/391]	LR: 0.001	Loss 0.1752 (0.1669)	Prec@1 99.219 (98.217)	
Epoch: [15][389/391]	LR: 0.001	Loss 0.1582 (0.1665)	Prec@1 98.438 (98.239)	
Total train loss: 0.1666

 * Prec@1 65.550 Prec@5 87.540 Loss 1.4180
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.001	Loss 0.2068 (0.1623)	Prec@1 98.438 (98.448)	
Epoch: [16][155/391]	LR: 0.001	Loss 0.1755 (0.1604)	Prec@1 100.000 (98.432)	
Epoch: [16][233/391]	LR: 0.001	Loss 0.1613 (0.1622)	Prec@1 100.000 (98.381)	
Epoch: [16][311/391]	LR: 0.001	Loss 0.1622 (0.1622)	Prec@1 99.219 (98.387)	
Epoch: [16][389/391]	LR: 0.001	Loss 0.1675 (0.1622)	Prec@1 98.438 (98.379)	
Total train loss: 0.1623

 * Prec@1 65.290 Prec@5 87.260 Loss 1.4316
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.001	Loss 0.1311 (0.1553)	Prec@1 97.656 (98.518)	
Epoch: [17][155/391]	LR: 0.001	Loss 0.1467 (0.1562)	Prec@1 98.438 (98.568)	
Epoch: [17][233/391]	LR: 0.001	Loss 0.1869 (0.1579)	Prec@1 97.656 (98.528)	
Epoch: [17][311/391]	LR: 0.001	Loss 0.1464 (0.1582)	Prec@1 97.656 (98.458)	
Epoch: [17][389/391]	LR: 0.001	Loss 0.1394 (0.1584)	Prec@1 98.438 (98.474)	
Total train loss: 0.1585

 * Prec@1 65.200 Prec@5 87.290 Loss 1.4209
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.001	Loss 0.1331 (0.1495)	Prec@1 97.656 (98.648)	
Epoch: [18][155/391]	LR: 0.001	Loss 0.1692 (0.1514)	Prec@1 97.656 (98.623)	
Epoch: [18][233/391]	LR: 0.001	Loss 0.2151 (0.1537)	Prec@1 96.875 (98.574)	
Epoch: [18][311/391]	LR: 0.001	Loss 0.1591 (0.1550)	Prec@1 97.656 (98.505)	
Epoch: [18][389/391]	LR: 0.001	Loss 0.1638 (0.1559)	Prec@1 97.656 (98.492)	
Total train loss: 0.1560

 * Prec@1 65.240 Prec@5 87.320 Loss 1.4307
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.001	Loss 0.1606 (0.1486)	Prec@1 97.656 (98.648)	
Epoch: [19][155/391]	LR: 0.001	Loss 0.1288 (0.1494)	Prec@1 100.000 (98.583)	
Epoch: [19][233/391]	LR: 0.001	Loss 0.1791 (0.1512)	Prec@1 96.875 (98.618)	
Epoch: [19][311/391]	LR: 0.001	Loss 0.1705 (0.1522)	Prec@1 98.438 (98.555)	
Epoch: [19][389/391]	LR: 0.001	Loss 0.1675 (0.1532)	Prec@1 98.438 (98.546)	
Total train loss: 0.1533

 * Prec@1 65.080 Prec@5 87.240 Loss 1.4316
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0001	Loss 0.1532 (0.1507)	Prec@1 99.219 (98.728)	
Epoch: [20][155/391]	LR: 0.0001	Loss 0.1583 (0.1499)	Prec@1 98.438 (98.648)	
Epoch: [20][233/391]	LR: 0.0001	Loss 0.2056 (0.1501)	Prec@1 96.875 (98.668)	
Epoch: [20][311/391]	LR: 0.0001	Loss 0.1421 (0.1505)	Prec@1 99.219 (98.610)	
Epoch: [20][389/391]	LR: 0.0001	Loss 0.1381 (0.1503)	Prec@1 99.219 (98.638)	
Total train loss: 0.1504

 * Prec@1 65.520 Prec@5 87.500 Loss 1.4307
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0001	Loss 0.1604 (0.1521)	Prec@1 97.656 (98.488)	
Epoch: [21][155/391]	LR: 0.0001	Loss 0.1238 (0.1486)	Prec@1 98.438 (98.653)	
Epoch: [21][233/391]	LR: 0.0001	Loss 0.1152 (0.1488)	Prec@1 99.219 (98.685)	
Epoch: [21][311/391]	LR: 0.0001	Loss 0.1411 (0.1499)	Prec@1 97.656 (98.683)	
Epoch: [21][389/391]	LR: 0.0001	Loss 0.1268 (0.1494)	Prec@1 99.219 (98.676)	
Total train loss: 0.1495

 * Prec@1 65.260 Prec@5 87.220 Loss 1.4355
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0001	Loss 0.1372 (0.1464)	Prec@1 100.000 (98.838)	
Epoch: [22][155/391]	LR: 0.0001	Loss 0.1681 (0.1500)	Prec@1 97.656 (98.728)	
Epoch: [22][233/391]	LR: 0.0001	Loss 0.1421 (0.1483)	Prec@1 98.438 (98.771)	
Epoch: [22][311/391]	LR: 0.0001	Loss 0.1613 (0.1488)	Prec@1 98.438 (98.715)	
Epoch: [22][389/391]	LR: 0.0001	Loss 0.1532 (0.1478)	Prec@1 98.438 (98.736)	
Total train loss: 0.1479

 * Prec@1 65.090 Prec@5 87.310 Loss 1.4404
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0001	Loss 0.1460 (0.1496)	Prec@1 99.219 (98.858)	
Epoch: [23][155/391]	LR: 0.0001	Loss 0.1853 (0.1478)	Prec@1 98.438 (98.883)	
Epoch: [23][233/391]	LR: 0.0001	Loss 0.1726 (0.1474)	Prec@1 97.656 (98.865)	
Epoch: [23][311/391]	LR: 0.0001	Loss 0.1836 (0.1481)	Prec@1 96.875 (98.831)	
Epoch: [23][389/391]	LR: 0.0001	Loss 0.1732 (0.1485)	Prec@1 98.438 (98.800)	
Total train loss: 0.1487

 * Prec@1 65.170 Prec@5 87.230 Loss 1.4326
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0001	Loss 0.1504 (0.1476)	Prec@1 99.219 (98.698)	
Epoch: [24][155/391]	LR: 0.0001	Loss 0.1538 (0.1507)	Prec@1 99.219 (98.668)	
Epoch: [24][233/391]	LR: 0.0001	Loss 0.1671 (0.1499)	Prec@1 98.438 (98.688)	
Epoch: [24][311/391]	LR: 0.0001	Loss 0.1206 (0.1494)	Prec@1 100.000 (98.735)	
Epoch: [24][389/391]	LR: 0.0001	Loss 0.1510 (0.1496)	Prec@1 100.000 (98.724)	
Total train loss: 0.1496

 * Prec@1 65.190 Prec@5 87.290 Loss 1.4404
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0001	Loss 0.1229 (0.1490)	Prec@1 99.219 (98.488)	
Epoch: [25][155/391]	LR: 0.0001	Loss 0.1753 (0.1500)	Prec@1 97.656 (98.568)	
Epoch: [25][233/391]	LR: 0.0001	Loss 0.1884 (0.1503)	Prec@1 96.875 (98.551)	
Epoch: [25][311/391]	LR: 0.0001	Loss 0.1233 (0.1500)	Prec@1 99.219 (98.595)	
Epoch: [25][389/391]	LR: 0.0001	Loss 0.1508 (0.1498)	Prec@1 99.219 (98.598)	
Total train loss: 0.1499

 * Prec@1 65.390 Prec@5 87.190 Loss 1.4355
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0001	Loss 0.1372 (0.1477)	Prec@1 99.219 (98.888)	
Epoch: [26][155/391]	LR: 0.0001	Loss 0.1318 (0.1481)	Prec@1 98.438 (98.703)	
Epoch: [26][233/391]	LR: 0.0001	Loss 0.1848 (0.1475)	Prec@1 97.656 (98.698)	
Epoch: [26][311/391]	LR: 0.0001	Loss 0.1521 (0.1470)	Prec@1 96.875 (98.705)	
Epoch: [26][389/391]	LR: 0.0001	Loss 0.1794 (0.1478)	Prec@1 96.875 (98.678)	
Total train loss: 0.1480

 * Prec@1 65.270 Prec@5 87.250 Loss 1.4336
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0001	Loss 0.1909 (0.1499)	Prec@1 96.094 (98.608)	
Epoch: [27][155/391]	LR: 0.0001	Loss 0.1278 (0.1495)	Prec@1 100.000 (98.653)	
Epoch: [27][233/391]	LR: 0.0001	Loss 0.1355 (0.1495)	Prec@1 100.000 (98.661)	
Epoch: [27][311/391]	LR: 0.0001	Loss 0.1528 (0.1502)	Prec@1 98.438 (98.653)	
Epoch: [27][389/391]	LR: 0.0001	Loss 0.1201 (0.1497)	Prec@1 100.000 (98.654)	
Total train loss: 0.1498

 * Prec@1 65.120 Prec@5 87.250 Loss 1.4307
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0001	Loss 0.1305 (0.1444)	Prec@1 100.000 (98.878)	
Epoch: [28][155/391]	LR: 0.0001	Loss 0.1188 (0.1456)	Prec@1 99.219 (98.853)	
Epoch: [28][233/391]	LR: 0.0001	Loss 0.1534 (0.1484)	Prec@1 99.219 (98.771)	
Epoch: [28][311/391]	LR: 0.0001	Loss 0.1678 (0.1494)	Prec@1 99.219 (98.748)	
Epoch: [28][389/391]	LR: 0.0001	Loss 0.1661 (0.1493)	Prec@1 98.438 (98.756)	
Total train loss: 0.1495

 * Prec@1 65.390 Prec@5 87.260 Loss 1.4346
Best acc: 65.610
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0001	Loss 0.1409 (0.1445)	Prec@1 98.438 (98.858)	
Epoch: [29][155/391]	LR: 0.0001	Loss 0.1421 (0.1495)	Prec@1 98.438 (98.713)	
Epoch: [29][233/391]	LR: 0.0001	Loss 0.1198 (0.1496)	Prec@1 100.000 (98.725)	
Epoch: [29][311/391]	LR: 0.0001	Loss 0.1404 (0.1500)	Prec@1 97.656 (98.693)	
Epoch: [29][389/391]	LR: 0.0001	Loss 0.1438 (0.1496)	Prec@1 96.875 (98.682)	
Total train loss: 0.1498

 * Prec@1 65.360 Prec@5 87.340 Loss 1.4326
Best acc: 65.610
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [10, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.330 Prec@5 90.450 Loss 1.1748
Pre-trained Prec@1 with 7 layers frozen: 69.33000183105469 	 Loss: 1.1748046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	Loss 0.7402 (0.6649)	Prec@1 76.562 (80.980)	
Epoch: [0][155/391]	LR: 0.01	Loss 0.6250 (0.7143)	Prec@1 83.594 (79.227)	
Epoch: [0][233/391]	LR: 0.01	Loss 0.8550 (0.7431)	Prec@1 77.344 (78.292)	
Epoch: [0][311/391]	LR: 0.01	Loss 0.7998 (0.7616)	Prec@1 76.562 (77.712)	
Epoch: [0][389/391]	LR: 0.01	Loss 0.8286 (0.7750)	Prec@1 73.438 (77.294)	
Total train loss: 0.7750

 * Prec@1 65.230 Prec@5 88.430 Loss 1.3203
Best acc: 65.230
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.01	Loss 0.6919 (0.6283)	Prec@1 79.688 (82.452)	
Epoch: [1][155/391]	LR: 0.01	Loss 0.7192 (0.6462)	Prec@1 81.250 (81.591)	
Epoch: [1][233/391]	LR: 0.01	Loss 0.7544 (0.6681)	Prec@1 78.125 (80.963)	
Epoch: [1][311/391]	LR: 0.01	Loss 0.7109 (0.6793)	Prec@1 82.812 (80.514)	
Epoch: [1][389/391]	LR: 0.01	Loss 0.8188 (0.6931)	Prec@1 73.438 (79.992)	
Total train loss: 0.6936

 * Prec@1 64.690 Prec@5 88.670 Loss 1.3457
Best acc: 65.230
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.01	Loss 0.6680 (0.5709)	Prec@1 85.156 (84.325)	
Epoch: [2][155/391]	LR: 0.01	Loss 0.6992 (0.5910)	Prec@1 78.906 (83.589)	
Epoch: [2][233/391]	LR: 0.01	Loss 0.6304 (0.6086)	Prec@1 81.250 (82.796)	
Epoch: [2][311/391]	LR: 0.01	Loss 0.6704 (0.6193)	Prec@1 82.812 (82.469)	
Epoch: [2][389/391]	LR: 0.01	Loss 0.6587 (0.6289)	Prec@1 84.375 (82.117)	
Total train loss: 0.6289

 * Prec@1 65.550 Prec@5 88.390 Loss 1.3350
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.01	Loss 0.5532 (0.5120)	Prec@1 85.156 (86.428)	
Epoch: [3][155/391]	LR: 0.01	Loss 0.6538 (0.5275)	Prec@1 81.250 (85.812)	
Epoch: [3][233/391]	LR: 0.01	Loss 0.7871 (0.5432)	Prec@1 77.344 (85.196)	
Epoch: [3][311/391]	LR: 0.01	Loss 0.6870 (0.5556)	Prec@1 78.906 (84.731)	
Epoch: [3][389/391]	LR: 0.01	Loss 0.6919 (0.5684)	Prec@1 81.250 (84.283)	
Total train loss: 0.5683

 * Prec@1 64.240 Prec@5 88.010 Loss 1.4004
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.01	Loss 0.3701 (0.4651)	Prec@1 91.406 (87.770)	
Epoch: [4][155/391]	LR: 0.01	Loss 0.4756 (0.4811)	Prec@1 84.375 (87.164)	
Epoch: [4][233/391]	LR: 0.01	Loss 0.5757 (0.4940)	Prec@1 82.812 (86.645)	
Epoch: [4][311/391]	LR: 0.01	Loss 0.6948 (0.5111)	Prec@1 78.125 (86.105)	
Epoch: [4][389/391]	LR: 0.01	Loss 0.5439 (0.5251)	Prec@1 81.250 (85.591)	
Total train loss: 0.5253

 * Prec@1 62.920 Prec@5 87.130 Loss 1.4717
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.01	Loss 0.4648 (0.4167)	Prec@1 89.844 (89.623)	
Epoch: [5][155/391]	LR: 0.01	Loss 0.5103 (0.4373)	Prec@1 87.500 (88.812)	
Epoch: [5][233/391]	LR: 0.01	Loss 0.5664 (0.4505)	Prec@1 84.375 (88.325)	
Epoch: [5][311/391]	LR: 0.01	Loss 0.5479 (0.4680)	Prec@1 86.719 (87.663)	
Epoch: [5][389/391]	LR: 0.01	Loss 0.6289 (0.4822)	Prec@1 82.031 (87.101)	
Total train loss: 0.4824

 * Prec@1 63.030 Prec@5 87.230 Loss 1.4492
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.01	Loss 0.4412 (0.3948)	Prec@1 86.719 (90.234)	
Epoch: [6][155/391]	LR: 0.01	Loss 0.4026 (0.4036)	Prec@1 92.188 (89.954)	
Epoch: [6][233/391]	LR: 0.01	Loss 0.4038 (0.4116)	Prec@1 88.281 (89.530)	
Epoch: [6][311/391]	LR: 0.01	Loss 0.4993 (0.4257)	Prec@1 89.062 (88.955)	
Epoch: [6][389/391]	LR: 0.01	Loss 0.4102 (0.4366)	Prec@1 90.625 (88.594)	
Total train loss: 0.4368

 * Prec@1 63.910 Prec@5 87.460 Loss 1.4277
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.01	Loss 0.4148 (0.3616)	Prec@1 92.188 (91.216)	
Epoch: [7][155/391]	LR: 0.01	Loss 0.4985 (0.3679)	Prec@1 89.844 (91.126)	
Epoch: [7][233/391]	LR: 0.01	Loss 0.3450 (0.3809)	Prec@1 89.844 (90.708)	
Epoch: [7][311/391]	LR: 0.01	Loss 0.4832 (0.3939)	Prec@1 87.500 (90.229)	
Epoch: [7][389/391]	LR: 0.01	Loss 0.4983 (0.4052)	Prec@1 87.500 (89.778)	
Total train loss: 0.4054

 * Prec@1 63.520 Prec@5 86.410 Loss 1.4717
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.01	Loss 0.3398 (0.3196)	Prec@1 91.406 (92.879)	
Epoch: [8][155/391]	LR: 0.01	Loss 0.4131 (0.3314)	Prec@1 89.062 (92.273)	
Epoch: [8][233/391]	LR: 0.01	Loss 0.3369 (0.3443)	Prec@1 92.969 (91.884)	
Epoch: [8][311/391]	LR: 0.01	Loss 0.3625 (0.3591)	Prec@1 89.062 (91.354)	
Epoch: [8][389/391]	LR: 0.01	Loss 0.3035 (0.3712)	Prec@1 95.312 (90.986)	
Total train loss: 0.3713

 * Prec@1 63.420 Prec@5 86.910 Loss 1.4795
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.01	Loss 0.2981 (0.3117)	Prec@1 94.531 (93.409)	
Epoch: [9][155/391]	LR: 0.01	Loss 0.2947 (0.3149)	Prec@1 93.750 (93.174)	
Epoch: [9][233/391]	LR: 0.01	Loss 0.3147 (0.3259)	Prec@1 92.188 (92.652)	
Epoch: [9][311/391]	LR: 0.01	Loss 0.3816 (0.3375)	Prec@1 89.062 (92.125)	
Epoch: [9][389/391]	LR: 0.01	Loss 0.4941 (0.3465)	Prec@1 86.719 (91.785)	
Total train loss: 0.3466

 * Prec@1 61.960 Prec@5 85.640 Loss 1.5732
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.001	Loss 0.2065 (0.2599)	Prec@1 96.875 (94.852)	
Epoch: [10][155/391]	LR: 0.001	Loss 0.2593 (0.2503)	Prec@1 95.312 (95.302)	
Epoch: [10][233/391]	LR: 0.001	Loss 0.1912 (0.2428)	Prec@1 96.875 (95.616)	
Epoch: [10][311/391]	LR: 0.001	Loss 0.2661 (0.2393)	Prec@1 96.875 (95.728)	
Epoch: [10][389/391]	LR: 0.001	Loss 0.2242 (0.2323)	Prec@1 97.656 (96.038)	
Total train loss: 0.2324

 * Prec@1 65.430 Prec@5 87.380 Loss 1.4150
Best acc: 65.550
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.001	Loss 0.1786 (0.1996)	Prec@1 96.875 (97.065)	
Epoch: [11][155/391]	LR: 0.001	Loss 0.2134 (0.1985)	Prec@1 97.656 (97.180)	
Epoch: [11][233/391]	LR: 0.001	Loss 0.1348 (0.1990)	Prec@1 99.219 (97.189)	
Epoch: [11][311/391]	LR: 0.001	Loss 0.2302 (0.1984)	Prec@1 96.094 (97.185)	
Epoch: [11][389/391]	LR: 0.001	Loss 0.1709 (0.1972)	Prec@1 98.438 (97.175)	
Total train loss: 0.1973

 * Prec@1 65.760 Prec@5 87.450 Loss 1.4121
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.001	Loss 0.1639 (0.1852)	Prec@1 97.656 (97.636)	
Epoch: [12][155/391]	LR: 0.001	Loss 0.1780 (0.1834)	Prec@1 99.219 (97.686)	
Epoch: [12][233/391]	LR: 0.001	Loss 0.1791 (0.1829)	Prec@1 96.094 (97.683)	
Epoch: [12][311/391]	LR: 0.001	Loss 0.1641 (0.1825)	Prec@1 97.656 (97.719)	
Epoch: [12][389/391]	LR: 0.001	Loss 0.1696 (0.1842)	Prec@1 97.656 (97.614)	
Total train loss: 0.1842

 * Prec@1 65.370 Prec@5 87.460 Loss 1.4229
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.001	Loss 0.1326 (0.1741)	Prec@1 100.000 (98.057)	
Epoch: [13][155/391]	LR: 0.001	Loss 0.1775 (0.1737)	Prec@1 96.875 (98.027)	
Epoch: [13][233/391]	LR: 0.001	Loss 0.1926 (0.1763)	Prec@1 97.656 (97.907)	
Epoch: [13][311/391]	LR: 0.001	Loss 0.1727 (0.1771)	Prec@1 96.094 (97.894)	
Epoch: [13][389/391]	LR: 0.001	Loss 0.1765 (0.1763)	Prec@1 98.438 (97.903)	
Total train loss: 0.1764

 * Prec@1 65.240 Prec@5 87.240 Loss 1.4268
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.001	Loss 0.2228 (0.1726)	Prec@1 96.094 (97.796)	
Epoch: [14][155/391]	LR: 0.001	Loss 0.1267 (0.1752)	Prec@1 99.219 (97.907)	
Epoch: [14][233/391]	LR: 0.001	Loss 0.1584 (0.1751)	Prec@1 98.438 (97.867)	
Epoch: [14][311/391]	LR: 0.001	Loss 0.1637 (0.1745)	Prec@1 99.219 (97.877)	
Epoch: [14][389/391]	LR: 0.001	Loss 0.1608 (0.1733)	Prec@1 96.875 (97.869)	
Total train loss: 0.1733

 * Prec@1 65.080 Prec@5 87.560 Loss 1.4238
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.001	Loss 0.1848 (0.1645)	Prec@1 96.875 (98.047)	
Epoch: [15][155/391]	LR: 0.001	Loss 0.2104 (0.1644)	Prec@1 98.438 (98.132)	
Epoch: [15][233/391]	LR: 0.001	Loss 0.1497 (0.1649)	Prec@1 99.219 (98.170)	
Epoch: [15][311/391]	LR: 0.001	Loss 0.1543 (0.1654)	Prec@1 98.438 (98.157)	
Epoch: [15][389/391]	LR: 0.001	Loss 0.1526 (0.1654)	Prec@1 99.219 (98.139)	
Total train loss: 0.1654

 * Prec@1 65.260 Prec@5 87.360 Loss 1.4209
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.001	Loss 0.1582 (0.1571)	Prec@1 99.219 (98.548)	
Epoch: [16][155/391]	LR: 0.001	Loss 0.1685 (0.1617)	Prec@1 97.656 (98.362)	
Epoch: [16][233/391]	LR: 0.001	Loss 0.1433 (0.1633)	Prec@1 98.438 (98.281)	
Epoch: [16][311/391]	LR: 0.001	Loss 0.1600 (0.1623)	Prec@1 97.656 (98.350)	
Epoch: [16][389/391]	LR: 0.001	Loss 0.1931 (0.1620)	Prec@1 99.219 (98.371)	
Total train loss: 0.1621

 * Prec@1 64.940 Prec@5 87.190 Loss 1.4316
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.001	Loss 0.1401 (0.1595)	Prec@1 98.438 (98.468)	
Epoch: [17][155/391]	LR: 0.001	Loss 0.1302 (0.1612)	Prec@1 99.219 (98.307)	
Epoch: [17][233/391]	LR: 0.001	Loss 0.1650 (0.1599)	Prec@1 97.656 (98.307)	
Epoch: [17][311/391]	LR: 0.001	Loss 0.1471 (0.1597)	Prec@1 100.000 (98.357)	
Epoch: [17][389/391]	LR: 0.001	Loss 0.1400 (0.1597)	Prec@1 98.438 (98.387)	
Total train loss: 0.1600

 * Prec@1 64.970 Prec@5 87.250 Loss 1.4355
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.001	Loss 0.1913 (0.1567)	Prec@1 97.656 (98.548)	
Epoch: [18][155/391]	LR: 0.001	Loss 0.1564 (0.1566)	Prec@1 99.219 (98.518)	
Epoch: [18][233/391]	LR: 0.001	Loss 0.1265 (0.1562)	Prec@1 100.000 (98.511)	
Epoch: [18][311/391]	LR: 0.001	Loss 0.2021 (0.1562)	Prec@1 98.438 (98.513)	
Epoch: [18][389/391]	LR: 0.001	Loss 0.1130 (0.1567)	Prec@1 100.000 (98.488)	
Total train loss: 0.1567

 * Prec@1 65.480 Prec@5 87.190 Loss 1.4326
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.001	Loss 0.1913 (0.1516)	Prec@1 96.094 (98.698)	
Epoch: [19][155/391]	LR: 0.001	Loss 0.1626 (0.1535)	Prec@1 97.656 (98.598)	
Epoch: [19][233/391]	LR: 0.001	Loss 0.1454 (0.1560)	Prec@1 100.000 (98.534)	
Epoch: [19][311/391]	LR: 0.001	Loss 0.1403 (0.1560)	Prec@1 96.875 (98.520)	
Epoch: [19][389/391]	LR: 0.001	Loss 0.1329 (0.1545)	Prec@1 98.438 (98.522)	
Total train loss: 0.1545

 * Prec@1 65.330 Prec@5 87.200 Loss 1.4316
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0001	Loss 0.1819 (0.1512)	Prec@1 98.438 (98.668)	
Epoch: [20][155/391]	LR: 0.0001	Loss 0.1511 (0.1521)	Prec@1 97.656 (98.593)	
Epoch: [20][233/391]	LR: 0.0001	Loss 0.1650 (0.1509)	Prec@1 99.219 (98.631)	
Epoch: [20][311/391]	LR: 0.0001	Loss 0.2181 (0.1516)	Prec@1 97.656 (98.608)	
Epoch: [20][389/391]	LR: 0.0001	Loss 0.1481 (0.1514)	Prec@1 97.656 (98.624)	
Total train loss: 0.1515

 * Prec@1 65.260 Prec@5 86.960 Loss 1.4395
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0001	Loss 0.1831 (0.1524)	Prec@1 98.438 (98.678)	
Epoch: [21][155/391]	LR: 0.0001	Loss 0.1995 (0.1522)	Prec@1 95.312 (98.668)	
Epoch: [21][233/391]	LR: 0.0001	Loss 0.1080 (0.1516)	Prec@1 100.000 (98.631)	
Epoch: [21][311/391]	LR: 0.0001	Loss 0.1372 (0.1502)	Prec@1 100.000 (98.695)	
Epoch: [21][389/391]	LR: 0.0001	Loss 0.1139 (0.1503)	Prec@1 99.219 (98.674)	
Total train loss: 0.1504

 * Prec@1 65.370 Prec@5 87.170 Loss 1.4355
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0001	Loss 0.1663 (0.1489)	Prec@1 98.438 (98.918)	
Epoch: [22][155/391]	LR: 0.0001	Loss 0.1964 (0.1504)	Prec@1 95.312 (98.738)	
Epoch: [22][233/391]	LR: 0.0001	Loss 0.1519 (0.1512)	Prec@1 100.000 (98.641)	
Epoch: [22][311/391]	LR: 0.0001	Loss 0.1708 (0.1513)	Prec@1 99.219 (98.643)	
Epoch: [22][389/391]	LR: 0.0001	Loss 0.1776 (0.1516)	Prec@1 96.875 (98.630)	
Total train loss: 0.1517

 * Prec@1 65.350 Prec@5 87.080 Loss 1.4375
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0001	Loss 0.1786 (0.1517)	Prec@1 98.438 (98.708)	
Epoch: [23][155/391]	LR: 0.0001	Loss 0.2367 (0.1518)	Prec@1 94.531 (98.633)	
Epoch: [23][233/391]	LR: 0.0001	Loss 0.1667 (0.1515)	Prec@1 98.438 (98.621)	
Epoch: [23][311/391]	LR: 0.0001	Loss 0.1519 (0.1514)	Prec@1 97.656 (98.593)	
Epoch: [23][389/391]	LR: 0.0001	Loss 0.1527 (0.1519)	Prec@1 96.875 (98.564)	
Total train loss: 0.1522

 * Prec@1 65.240 Prec@5 87.230 Loss 1.4316
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0001	Loss 0.1969 (0.1492)	Prec@1 96.875 (98.708)	
Epoch: [24][155/391]	LR: 0.0001	Loss 0.1512 (0.1521)	Prec@1 97.656 (98.608)	
Epoch: [24][233/391]	LR: 0.0001	Loss 0.1649 (0.1518)	Prec@1 96.875 (98.634)	
Epoch: [24][311/391]	LR: 0.0001	Loss 0.1772 (0.1512)	Prec@1 96.094 (98.575)	
Epoch: [24][389/391]	LR: 0.0001	Loss 0.1283 (0.1504)	Prec@1 98.438 (98.636)	
Total train loss: 0.1506

 * Prec@1 65.160 Prec@5 87.020 Loss 1.4404
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0001	Loss 0.1066 (0.1512)	Prec@1 100.000 (98.387)	
Epoch: [25][155/391]	LR: 0.0001	Loss 0.1591 (0.1521)	Prec@1 97.656 (98.483)	
Epoch: [25][233/391]	LR: 0.0001	Loss 0.1125 (0.1515)	Prec@1 100.000 (98.578)	
Epoch: [25][311/391]	LR: 0.0001	Loss 0.1141 (0.1506)	Prec@1 98.438 (98.633)	
Epoch: [25][389/391]	LR: 0.0001	Loss 0.1106 (0.1507)	Prec@1 99.219 (98.630)	
Total train loss: 0.1507

 * Prec@1 65.150 Prec@5 87.090 Loss 1.4443
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0001	Loss 0.1421 (0.1491)	Prec@1 98.438 (98.808)	
Epoch: [26][155/391]	LR: 0.0001	Loss 0.1954 (0.1508)	Prec@1 99.219 (98.723)	
Epoch: [26][233/391]	LR: 0.0001	Loss 0.1644 (0.1508)	Prec@1 99.219 (98.745)	
Epoch: [26][311/391]	LR: 0.0001	Loss 0.1686 (0.1509)	Prec@1 97.656 (98.725)	
Epoch: [26][389/391]	LR: 0.0001	Loss 0.1697 (0.1504)	Prec@1 96.875 (98.698)	
Total train loss: 0.1504

 * Prec@1 65.210 Prec@5 87.140 Loss 1.4385
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0001	Loss 0.1307 (0.1500)	Prec@1 100.000 (98.438)	
Epoch: [27][155/391]	LR: 0.0001	Loss 0.1569 (0.1507)	Prec@1 96.875 (98.583)	
Epoch: [27][233/391]	LR: 0.0001	Loss 0.1200 (0.1494)	Prec@1 98.438 (98.648)	
Epoch: [27][311/391]	LR: 0.0001	Loss 0.1501 (0.1485)	Prec@1 98.438 (98.683)	
Epoch: [27][389/391]	LR: 0.0001	Loss 0.1224 (0.1491)	Prec@1 99.219 (98.638)	
Total train loss: 0.1493

 * Prec@1 65.260 Prec@5 87.140 Loss 1.4395
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0001	Loss 0.1893 (0.1499)	Prec@1 96.875 (98.598)	
Epoch: [28][155/391]	LR: 0.0001	Loss 0.1309 (0.1519)	Prec@1 100.000 (98.668)	
Epoch: [28][233/391]	LR: 0.0001	Loss 0.1567 (0.1527)	Prec@1 100.000 (98.638)	
Epoch: [28][311/391]	LR: 0.0001	Loss 0.1639 (0.1531)	Prec@1 97.656 (98.615)	
Epoch: [28][389/391]	LR: 0.0001	Loss 0.1266 (0.1522)	Prec@1 98.438 (98.650)	
Total train loss: 0.1523

 * Prec@1 65.420 Prec@5 87.160 Loss 1.4336
Best acc: 65.760
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0001	Loss 0.1537 (0.1456)	Prec@1 97.656 (98.848)	
Epoch: [29][155/391]	LR: 0.0001	Loss 0.1058 (0.1479)	Prec@1 99.219 (98.823)	
Epoch: [29][233/391]	LR: 0.0001	Loss 0.1504 (0.1472)	Prec@1 99.219 (98.818)	
Epoch: [29][311/391]	LR: 0.0001	Loss 0.1349 (0.1482)	Prec@1 99.219 (98.783)	
Epoch: [29][389/391]	LR: 0.0001	Loss 0.1246 (0.1486)	Prec@1 98.438 (98.746)	
Total train loss: 0.1488

 * Prec@1 65.220 Prec@5 87.210 Loss 1.4346
Best acc: 65.760
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [10, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.220 Prec@5 90.310 Loss 1.1758
Pre-trained Prec@1 with 9 layers frozen: 69.22000122070312 	 Loss: 1.17578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	Loss 0.7446 (0.6815)	Prec@1 83.594 (80.218)	
Epoch: [0][155/391]	LR: 0.01	Loss 0.7720 (0.6995)	Prec@1 78.125 (79.688)	
Epoch: [0][233/391]	LR: 0.01	Loss 0.7222 (0.7242)	Prec@1 80.469 (78.950)	
Epoch: [0][311/391]	LR: 0.01	Loss 0.7437 (0.7402)	Prec@1 78.125 (78.430)	
Epoch: [0][389/391]	LR: 0.01	Loss 0.8174 (0.7538)	Prec@1 78.906 (77.999)	
Total train loss: 0.7540

 * Prec@1 64.170 Prec@5 88.190 Loss 1.3809
Best acc: 64.170
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.01	Loss 0.7690 (0.6228)	Prec@1 76.562 (82.242)	
Epoch: [1][155/391]	LR: 0.01	Loss 0.6045 (0.6350)	Prec@1 84.375 (81.726)	
Epoch: [1][233/391]	LR: 0.01	Loss 0.6523 (0.6506)	Prec@1 79.688 (81.173)	
Epoch: [1][311/391]	LR: 0.01	Loss 0.5679 (0.6584)	Prec@1 85.938 (80.877)	
Epoch: [1][389/391]	LR: 0.01	Loss 0.8818 (0.6733)	Prec@1 71.875 (80.481)	
Total train loss: 0.6739

 * Prec@1 65.200 Prec@5 88.450 Loss 1.3613
Best acc: 65.200
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.01	Loss 0.5669 (0.5482)	Prec@1 84.375 (84.345)	
Epoch: [2][155/391]	LR: 0.01	Loss 0.6226 (0.5614)	Prec@1 84.375 (84.140)	
Epoch: [2][233/391]	LR: 0.01	Loss 0.5913 (0.5764)	Prec@1 82.812 (83.721)	
Epoch: [2][311/391]	LR: 0.01	Loss 0.7051 (0.5929)	Prec@1 78.125 (83.211)	
Epoch: [2][389/391]	LR: 0.01	Loss 0.8569 (0.6042)	Prec@1 75.000 (82.808)	
Total train loss: 0.6046

 * Prec@1 65.690 Prec@5 88.670 Loss 1.3281
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.01	Loss 0.5679 (0.5017)	Prec@1 85.156 (86.649)	
Epoch: [3][155/391]	LR: 0.01	Loss 0.3691 (0.5093)	Prec@1 93.750 (86.278)	
Epoch: [3][233/391]	LR: 0.01	Loss 0.7168 (0.5262)	Prec@1 81.250 (85.647)	
Epoch: [3][311/391]	LR: 0.01	Loss 0.5601 (0.5390)	Prec@1 88.281 (85.171)	
Epoch: [3][389/391]	LR: 0.01	Loss 0.5918 (0.5527)	Prec@1 83.594 (84.613)	
Total train loss: 0.5531

 * Prec@1 65.480 Prec@5 88.350 Loss 1.3779
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.01	Loss 0.4836 (0.4529)	Prec@1 83.594 (88.221)	
Epoch: [4][155/391]	LR: 0.01	Loss 0.5156 (0.4684)	Prec@1 86.719 (87.700)	
Epoch: [4][233/391]	LR: 0.01	Loss 0.5386 (0.4819)	Prec@1 83.594 (87.153)	
Epoch: [4][311/391]	LR: 0.01	Loss 0.4148 (0.4930)	Prec@1 90.625 (86.766)	
Epoch: [4][389/391]	LR: 0.01	Loss 0.6138 (0.5051)	Prec@1 84.375 (86.268)	
Total train loss: 0.5053

 * Prec@1 64.150 Prec@5 87.340 Loss 1.4150
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.01	Loss 0.3345 (0.4003)	Prec@1 92.969 (90.234)	
Epoch: [5][155/391]	LR: 0.01	Loss 0.3313 (0.4127)	Prec@1 91.406 (89.779)	
Epoch: [5][233/391]	LR: 0.01	Loss 0.6274 (0.4257)	Prec@1 82.812 (89.276)	
Epoch: [5][311/391]	LR: 0.01	Loss 0.5312 (0.4432)	Prec@1 84.375 (88.549)	
Epoch: [5][389/391]	LR: 0.01	Loss 0.5479 (0.4591)	Prec@1 84.375 (87.995)	
Total train loss: 0.4595

 * Prec@1 64.590 Prec@5 87.690 Loss 1.4053
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.01	Loss 0.3047 (0.3724)	Prec@1 93.750 (91.256)	
Epoch: [6][155/391]	LR: 0.01	Loss 0.3145 (0.3861)	Prec@1 92.969 (90.685)	
Epoch: [6][233/391]	LR: 0.01	Loss 0.4182 (0.4047)	Prec@1 86.719 (89.967)	
Epoch: [6][311/391]	LR: 0.01	Loss 0.4880 (0.4141)	Prec@1 89.062 (89.573)	
Epoch: [6][389/391]	LR: 0.01	Loss 0.5581 (0.4241)	Prec@1 84.375 (89.169)	
Total train loss: 0.4242

 * Prec@1 64.150 Prec@5 87.580 Loss 1.4463
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.01	Loss 0.3567 (0.3513)	Prec@1 92.969 (92.067)	
Epoch: [7][155/391]	LR: 0.01	Loss 0.4055 (0.3572)	Prec@1 87.500 (91.797)	
Epoch: [7][233/391]	LR: 0.01	Loss 0.3364 (0.3657)	Prec@1 92.969 (91.316)	
Epoch: [7][311/391]	LR: 0.01	Loss 0.4875 (0.3779)	Prec@1 88.281 (90.863)	
Epoch: [7][389/391]	LR: 0.01	Loss 0.4006 (0.3891)	Prec@1 89.062 (90.483)	
Total train loss: 0.3893

 * Prec@1 63.800 Prec@5 86.910 Loss 1.4736
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.01	Loss 0.2988 (0.3209)	Prec@1 91.406 (93.059)	
Epoch: [8][155/391]	LR: 0.01	Loss 0.3445 (0.3320)	Prec@1 89.062 (92.543)	
Epoch: [8][233/391]	LR: 0.01	Loss 0.4263 (0.3425)	Prec@1 89.844 (92.074)	
Epoch: [8][311/391]	LR: 0.01	Loss 0.3315 (0.3514)	Prec@1 90.625 (91.784)	
Epoch: [8][389/391]	LR: 0.01	Loss 0.3652 (0.3618)	Prec@1 91.406 (91.392)	
Total train loss: 0.3619

 * Prec@1 63.160 Prec@5 86.540 Loss 1.5176
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.01	Loss 0.3235 (0.2932)	Prec@1 89.844 (93.700)	
Epoch: [9][155/391]	LR: 0.01	Loss 0.2588 (0.3028)	Prec@1 94.531 (93.369)	
Epoch: [9][233/391]	LR: 0.01	Loss 0.3149 (0.3132)	Prec@1 92.188 (92.952)	
Epoch: [9][311/391]	LR: 0.01	Loss 0.3328 (0.3247)	Prec@1 91.406 (92.551)	
Epoch: [9][389/391]	LR: 0.01	Loss 0.2935 (0.3338)	Prec@1 96.875 (92.276)	
Total train loss: 0.3338

 * Prec@1 63.640 Prec@5 86.360 Loss 1.5195
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.001	Loss 0.2228 (0.2552)	Prec@1 96.875 (95.292)	
Epoch: [10][155/391]	LR: 0.001	Loss 0.2285 (0.2435)	Prec@1 96.094 (95.658)	
Epoch: [10][233/391]	LR: 0.001	Loss 0.1978 (0.2383)	Prec@1 95.312 (95.807)	
Epoch: [10][311/391]	LR: 0.001	Loss 0.2627 (0.2343)	Prec@1 94.531 (95.991)	
Epoch: [10][389/391]	LR: 0.001	Loss 0.2512 (0.2296)	Prec@1 95.312 (96.138)	
Total train loss: 0.2296

 * Prec@1 65.680 Prec@5 87.370 Loss 1.4219
Best acc: 65.690
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.001	Loss 0.2279 (0.2025)	Prec@1 94.531 (97.055)	
Epoch: [11][155/391]	LR: 0.001	Loss 0.2349 (0.2003)	Prec@1 95.312 (97.155)	
Epoch: [11][233/391]	LR: 0.001	Loss 0.1785 (0.1983)	Prec@1 97.656 (97.189)	
Epoch: [11][311/391]	LR: 0.001	Loss 0.1895 (0.1973)	Prec@1 97.656 (97.261)	
Epoch: [11][389/391]	LR: 0.001	Loss 0.2218 (0.1965)	Prec@1 96.875 (97.306)	
Total train loss: 0.1967

 * Prec@1 65.720 Prec@5 87.440 Loss 1.4092
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.001	Loss 0.1899 (0.1829)	Prec@1 98.438 (97.776)	
Epoch: [12][155/391]	LR: 0.001	Loss 0.2258 (0.1820)	Prec@1 97.656 (97.801)	
Epoch: [12][233/391]	LR: 0.001	Loss 0.1582 (0.1819)	Prec@1 98.438 (97.803)	
Epoch: [12][311/391]	LR: 0.001	Loss 0.1831 (0.1829)	Prec@1 97.656 (97.746)	
Epoch: [12][389/391]	LR: 0.001	Loss 0.1473 (0.1832)	Prec@1 97.656 (97.740)	
Total train loss: 0.1832

 * Prec@1 65.590 Prec@5 87.430 Loss 1.4199
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.001	Loss 0.1976 (0.1723)	Prec@1 96.094 (98.047)	
Epoch: [13][155/391]	LR: 0.001	Loss 0.1740 (0.1724)	Prec@1 96.094 (98.077)	
Epoch: [13][233/391]	LR: 0.001	Loss 0.1821 (0.1742)	Prec@1 96.875 (98.057)	
Epoch: [13][311/391]	LR: 0.001	Loss 0.1451 (0.1746)	Prec@1 98.438 (98.037)	
Epoch: [13][389/391]	LR: 0.001	Loss 0.1534 (0.1743)	Prec@1 99.219 (98.089)	
Total train loss: 0.1744

 * Prec@1 65.430 Prec@5 87.350 Loss 1.4209
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.001	Loss 0.1593 (0.1693)	Prec@1 98.438 (97.987)	
Epoch: [14][155/391]	LR: 0.001	Loss 0.1487 (0.1698)	Prec@1 99.219 (98.117)	
Epoch: [14][233/391]	LR: 0.001	Loss 0.1796 (0.1710)	Prec@1 98.438 (98.094)	
Epoch: [14][311/391]	LR: 0.001	Loss 0.1749 (0.1708)	Prec@1 97.656 (98.097)	
Epoch: [14][389/391]	LR: 0.001	Loss 0.1644 (0.1713)	Prec@1 98.438 (98.081)	
Total train loss: 0.1715

 * Prec@1 65.480 Prec@5 87.160 Loss 1.4268
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.001	Loss 0.1583 (0.1625)	Prec@1 97.656 (98.277)	
Epoch: [15][155/391]	LR: 0.001	Loss 0.1729 (0.1641)	Prec@1 98.438 (98.287)	
Epoch: [15][233/391]	LR: 0.001	Loss 0.1655 (0.1648)	Prec@1 98.438 (98.291)	
Epoch: [15][311/391]	LR: 0.001	Loss 0.1646 (0.1652)	Prec@1 98.438 (98.242)	
Epoch: [15][389/391]	LR: 0.001	Loss 0.1801 (0.1661)	Prec@1 96.094 (98.205)	
Total train loss: 0.1662

 * Prec@1 65.330 Prec@5 87.050 Loss 1.4404
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.001	Loss 0.1913 (0.1619)	Prec@1 97.656 (98.538)	
Epoch: [16][155/391]	LR: 0.001	Loss 0.1913 (0.1623)	Prec@1 96.094 (98.427)	
Epoch: [16][233/391]	LR: 0.001	Loss 0.1542 (0.1618)	Prec@1 97.656 (98.381)	
Epoch: [16][311/391]	LR: 0.001	Loss 0.1843 (0.1625)	Prec@1 96.875 (98.405)	
Epoch: [16][389/391]	LR: 0.001	Loss 0.1510 (0.1618)	Prec@1 99.219 (98.417)	
Total train loss: 0.1620

 * Prec@1 65.560 Prec@5 87.230 Loss 1.4229
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.001	Loss 0.1329 (0.1540)	Prec@1 100.000 (98.668)	
Epoch: [17][155/391]	LR: 0.001	Loss 0.1642 (0.1560)	Prec@1 96.875 (98.488)	
Epoch: [17][233/391]	LR: 0.001	Loss 0.1685 (0.1593)	Prec@1 98.438 (98.451)	
Epoch: [17][311/391]	LR: 0.001	Loss 0.1295 (0.1605)	Prec@1 98.438 (98.412)	
Epoch: [17][389/391]	LR: 0.001	Loss 0.1604 (0.1602)	Prec@1 99.219 (98.419)	
Total train loss: 0.1604

 * Prec@1 65.300 Prec@5 86.990 Loss 1.4326
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.001	Loss 0.1238 (0.1566)	Prec@1 99.219 (98.658)	
Epoch: [18][155/391]	LR: 0.001	Loss 0.1399 (0.1583)	Prec@1 98.438 (98.508)	
Epoch: [18][233/391]	LR: 0.001	Loss 0.1620 (0.1584)	Prec@1 96.875 (98.518)	
Epoch: [18][311/391]	LR: 0.001	Loss 0.1250 (0.1584)	Prec@1 100.000 (98.560)	
Epoch: [18][389/391]	LR: 0.001	Loss 0.1225 (0.1585)	Prec@1 98.438 (98.508)	
Total train loss: 0.1586

 * Prec@1 65.230 Prec@5 86.980 Loss 1.4355
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.001	Loss 0.1836 (0.1551)	Prec@1 96.875 (98.658)	
Epoch: [19][155/391]	LR: 0.001	Loss 0.2236 (0.1545)	Prec@1 96.875 (98.688)	
Epoch: [19][233/391]	LR: 0.001	Loss 0.1833 (0.1564)	Prec@1 96.875 (98.608)	
Epoch: [19][311/391]	LR: 0.001	Loss 0.1422 (0.1557)	Prec@1 99.219 (98.623)	
Epoch: [19][389/391]	LR: 0.001	Loss 0.1676 (0.1557)	Prec@1 99.219 (98.580)	
Total train loss: 0.1559

 * Prec@1 65.310 Prec@5 87.100 Loss 1.4336
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0001	Loss 0.1757 (0.1498)	Prec@1 98.438 (98.718)	
Epoch: [20][155/391]	LR: 0.0001	Loss 0.1218 (0.1490)	Prec@1 99.219 (98.823)	
Epoch: [20][233/391]	LR: 0.0001	Loss 0.1704 (0.1492)	Prec@1 98.438 (98.808)	
Epoch: [20][311/391]	LR: 0.0001	Loss 0.1549 (0.1501)	Prec@1 98.438 (98.803)	
Epoch: [20][389/391]	LR: 0.0001	Loss 0.1902 (0.1511)	Prec@1 97.656 (98.754)	
Total train loss: 0.1514

 * Prec@1 65.330 Prec@5 86.940 Loss 1.4521
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0001	Loss 0.1465 (0.1518)	Prec@1 98.438 (98.628)	
Epoch: [21][155/391]	LR: 0.0001	Loss 0.1384 (0.1539)	Prec@1 99.219 (98.558)	
Epoch: [21][233/391]	LR: 0.0001	Loss 0.1281 (0.1541)	Prec@1 100.000 (98.578)	
Epoch: [21][311/391]	LR: 0.0001	Loss 0.1565 (0.1528)	Prec@1 97.656 (98.628)	
Epoch: [21][389/391]	LR: 0.0001	Loss 0.1625 (0.1525)	Prec@1 99.219 (98.630)	
Total train loss: 0.1525

 * Prec@1 65.090 Prec@5 87.060 Loss 1.4434
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0001	Loss 0.1290 (0.1515)	Prec@1 99.219 (98.608)	
Epoch: [22][155/391]	LR: 0.0001	Loss 0.1516 (0.1515)	Prec@1 98.438 (98.613)	
Epoch: [22][233/391]	LR: 0.0001	Loss 0.1298 (0.1522)	Prec@1 96.875 (98.634)	
Epoch: [22][311/391]	LR: 0.0001	Loss 0.1532 (0.1508)	Prec@1 97.656 (98.688)	
Epoch: [22][389/391]	LR: 0.0001	Loss 0.1373 (0.1502)	Prec@1 100.000 (98.684)	
Total train loss: 0.1503

 * Prec@1 65.230 Prec@5 86.920 Loss 1.4443
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0001	Loss 0.1522 (0.1498)	Prec@1 99.219 (98.688)	
Epoch: [23][155/391]	LR: 0.0001	Loss 0.1481 (0.1498)	Prec@1 98.438 (98.678)	
Epoch: [23][233/391]	LR: 0.0001	Loss 0.1359 (0.1504)	Prec@1 98.438 (98.661)	
Epoch: [23][311/391]	LR: 0.0001	Loss 0.1600 (0.1504)	Prec@1 99.219 (98.703)	
Epoch: [23][389/391]	LR: 0.0001	Loss 0.1248 (0.1506)	Prec@1 99.219 (98.736)	
Total train loss: 0.1508

 * Prec@1 65.410 Prec@5 87.110 Loss 1.4404
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0001	Loss 0.2158 (0.1470)	Prec@1 94.531 (98.838)	
Epoch: [24][155/391]	LR: 0.0001	Loss 0.1418 (0.1466)	Prec@1 97.656 (98.878)	
Epoch: [24][233/391]	LR: 0.0001	Loss 0.1272 (0.1472)	Prec@1 100.000 (98.845)	
Epoch: [24][311/391]	LR: 0.0001	Loss 0.1427 (0.1491)	Prec@1 99.219 (98.773)	
Epoch: [24][389/391]	LR: 0.0001	Loss 0.1388 (0.1504)	Prec@1 100.000 (98.710)	
Total train loss: 0.1506

 * Prec@1 65.210 Prec@5 86.980 Loss 1.4482
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0001	Loss 0.1340 (0.1520)	Prec@1 98.438 (98.938)	
Epoch: [25][155/391]	LR: 0.0001	Loss 0.1754 (0.1518)	Prec@1 100.000 (98.793)	
Epoch: [25][233/391]	LR: 0.0001	Loss 0.1278 (0.1530)	Prec@1 100.000 (98.758)	
Epoch: [25][311/391]	LR: 0.0001	Loss 0.1190 (0.1526)	Prec@1 100.000 (98.733)	
Epoch: [25][389/391]	LR: 0.0001	Loss 0.1198 (0.1515)	Prec@1 99.219 (98.740)	
Total train loss: 0.1516

 * Prec@1 65.180 Prec@5 87.020 Loss 1.4375
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0001	Loss 0.1797 (0.1485)	Prec@1 96.094 (98.618)	
Epoch: [26][155/391]	LR: 0.0001	Loss 0.1935 (0.1505)	Prec@1 96.875 (98.673)	
Epoch: [26][233/391]	LR: 0.0001	Loss 0.1178 (0.1516)	Prec@1 100.000 (98.675)	
Epoch: [26][311/391]	LR: 0.0001	Loss 0.1254 (0.1503)	Prec@1 98.438 (98.705)	
Epoch: [26][389/391]	LR: 0.0001	Loss 0.1835 (0.1510)	Prec@1 98.438 (98.710)	
Total train loss: 0.1511

 * Prec@1 65.320 Prec@5 87.030 Loss 1.4385
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0001	Loss 0.1614 (0.1532)	Prec@1 98.438 (98.768)	
Epoch: [27][155/391]	LR: 0.0001	Loss 0.1329 (0.1513)	Prec@1 99.219 (98.743)	
Epoch: [27][233/391]	LR: 0.0001	Loss 0.2465 (0.1526)	Prec@1 94.531 (98.624)	
Epoch: [27][311/391]	LR: 0.0001	Loss 0.1327 (0.1516)	Prec@1 98.438 (98.685)	
Epoch: [27][389/391]	LR: 0.0001	Loss 0.1654 (0.1522)	Prec@1 96.875 (98.694)	
Total train loss: 0.1522

 * Prec@1 65.250 Prec@5 87.190 Loss 1.4355
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0001	Loss 0.1008 (0.1522)	Prec@1 99.219 (98.538)	
Epoch: [28][155/391]	LR: 0.0001	Loss 0.1490 (0.1513)	Prec@1 99.219 (98.653)	
Epoch: [28][233/391]	LR: 0.0001	Loss 0.1526 (0.1513)	Prec@1 99.219 (98.628)	
Epoch: [28][311/391]	LR: 0.0001	Loss 0.1321 (0.1524)	Prec@1 98.438 (98.608)	
Epoch: [28][389/391]	LR: 0.0001	Loss 0.1191 (0.1511)	Prec@1 99.219 (98.666)	
Total train loss: 0.1513

 * Prec@1 65.250 Prec@5 86.810 Loss 1.4453
Best acc: 65.720
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0001	Loss 0.1820 (0.1455)	Prec@1 96.875 (98.738)	
Epoch: [29][155/391]	LR: 0.0001	Loss 0.1648 (0.1486)	Prec@1 98.438 (98.718)	
Epoch: [29][233/391]	LR: 0.0001	Loss 0.1436 (0.1498)	Prec@1 99.219 (98.701)	
Epoch: [29][311/391]	LR: 0.0001	Loss 0.1346 (0.1504)	Prec@1 100.000 (98.678)	
Epoch: [29][389/391]	LR: 0.0001	Loss 0.1544 (0.1504)	Prec@1 98.438 (98.706)	
Total train loss: 0.1504

 * Prec@1 65.290 Prec@5 86.960 Loss 1.4443
Best acc: 65.720
--------------------------------------------------------------------------------
