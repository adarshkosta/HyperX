
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.210 Prec@5 90.410 Loss 1.1758
Pre-trained Prec@1 with 11 layers frozen: 69.20999908447266 	 Loss: 1.17578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.4563 (0.5621)	Prec@1 85.156 (84.065)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.6055 (0.5638)	Prec@1 85.938 (84.075)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.4880 (0.5676)	Prec@1 85.938 (83.974)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.5566 (0.5656)	Prec@1 88.281 (84.120)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5874 (0.5665)	Prec@1 81.250 (84.062)	
Total train loss: 0.5665

 * Prec@1 69.290 Prec@5 90.460 Loss 1.1611
Best acc: 69.290
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.3853 (0.5670)	Prec@1 92.969 (83.764)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.5410 (0.5640)	Prec@1 85.156 (83.919)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5195 (0.5664)	Prec@1 83.594 (83.911)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.6445 (0.5676)	Prec@1 78.906 (83.812)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.5596 (0.5658)	Prec@1 79.688 (83.842)	
Total train loss: 0.5660

 * Prec@1 69.520 Prec@5 90.360 Loss 1.1602
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.7393 (0.5647)	Prec@1 78.906 (84.054)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.5474 (0.5638)	Prec@1 85.938 (84.255)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.4336 (0.5611)	Prec@1 89.062 (84.255)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.7783 (0.5636)	Prec@1 74.219 (84.142)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.4719 (0.5665)	Prec@1 87.500 (84.018)	
Total train loss: 0.5664

 * Prec@1 69.430 Prec@5 90.380 Loss 1.1621
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.4766 (0.5591)	Prec@1 85.156 (84.255)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.7007 (0.5617)	Prec@1 80.469 (84.085)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.5576 (0.5609)	Prec@1 85.156 (84.115)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5229 (0.5604)	Prec@1 85.156 (84.202)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.5283 (0.5597)	Prec@1 85.938 (84.183)	
Total train loss: 0.5597

 * Prec@1 69.430 Prec@5 90.560 Loss 1.1650
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.5020 (0.5540)	Prec@1 85.938 (84.155)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.7354 (0.5589)	Prec@1 78.125 (83.949)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.5679 (0.5595)	Prec@1 82.812 (84.054)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.6606 (0.5601)	Prec@1 82.031 (84.019)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.6084 (0.5617)	Prec@1 83.594 (84.016)	
Total train loss: 0.5619

 * Prec@1 69.400 Prec@5 90.570 Loss 1.1660
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.6167 (0.5596)	Prec@1 82.812 (84.595)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.5845 (0.5540)	Prec@1 84.375 (84.756)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.5503 (0.5568)	Prec@1 83.594 (84.499)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.5059 (0.5555)	Prec@1 86.719 (84.445)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.5361 (0.5563)	Prec@1 83.594 (84.429)	
Total train loss: 0.5566

 * Prec@1 69.370 Prec@5 90.600 Loss 1.1650
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.8081 (0.5722)	Prec@1 74.219 (83.764)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.6279 (0.5604)	Prec@1 81.250 (84.205)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.6201 (0.5578)	Prec@1 80.469 (84.402)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.4949 (0.5548)	Prec@1 86.719 (84.488)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.5176 (0.5567)	Prec@1 86.719 (84.455)	
Total train loss: 0.5569

 * Prec@1 69.510 Prec@5 90.630 Loss 1.1602
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.5439 (0.5491)	Prec@1 89.062 (84.535)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.4346 (0.5503)	Prec@1 89.844 (84.701)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.6416 (0.5531)	Prec@1 76.562 (84.645)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5010 (0.5559)	Prec@1 86.719 (84.530)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.4575 (0.5578)	Prec@1 85.156 (84.419)	
Total train loss: 0.5581

 * Prec@1 69.490 Prec@5 90.550 Loss 1.1611
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.4609 (0.5679)	Prec@1 86.719 (84.024)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.6240 (0.5627)	Prec@1 84.375 (84.245)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.5801 (0.5664)	Prec@1 82.812 (84.078)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.5259 (0.5611)	Prec@1 83.594 (84.290)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.4973 (0.5598)	Prec@1 89.062 (84.267)	
Total train loss: 0.5599

 * Prec@1 69.380 Prec@5 90.510 Loss 1.1650
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.5806 (0.5687)	Prec@1 87.500 (83.454)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.7065 (0.5641)	Prec@1 80.469 (84.059)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.4824 (0.5597)	Prec@1 86.719 (84.205)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.4592 (0.5578)	Prec@1 88.281 (84.317)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.6338 (0.5591)	Prec@1 85.938 (84.289)	
Total train loss: 0.5592

 * Prec@1 69.390 Prec@5 90.590 Loss 1.1660
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.6323 (0.5593)	Prec@1 80.469 (84.235)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.5869 (0.5598)	Prec@1 82.031 (84.180)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.6592 (0.5565)	Prec@1 79.688 (84.365)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.6333 (0.5580)	Prec@1 78.906 (84.320)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.5562 (0.5597)	Prec@1 88.281 (84.279)	
Total train loss: 0.5597

 * Prec@1 69.320 Prec@5 90.480 Loss 1.1670
Best acc: 69.520
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.4832 (0.5501)	Prec@1 87.500 (84.505)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.6880 (0.5611)	Prec@1 80.469 (84.330)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.6357 (0.5590)	Prec@1 81.250 (84.358)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.5190 (0.5599)	Prec@1 85.938 (84.378)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5747 (0.5604)	Prec@1 82.812 (84.309)	
Total train loss: 0.5607

 * Prec@1 69.560 Prec@5 90.510 Loss 1.1689
Best acc: 69.560
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.7783 (0.5771)	Prec@1 80.469 (84.044)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.6235 (0.5666)	Prec@1 80.469 (84.210)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.6265 (0.5628)	Prec@1 81.250 (84.365)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.3542 (0.5616)	Prec@1 91.406 (84.425)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.5742 (0.5584)	Prec@1 86.719 (84.495)	
Total train loss: 0.5586

 * Prec@1 69.350 Prec@5 90.560 Loss 1.1602
Best acc: 69.560
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.5332 (0.5684)	Prec@1 84.375 (84.044)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.6426 (0.5637)	Prec@1 81.250 (84.130)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.6250 (0.5634)	Prec@1 82.812 (84.098)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.6504 (0.5617)	Prec@1 79.688 (84.142)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.6499 (0.5609)	Prec@1 80.469 (84.207)	
Total train loss: 0.5612

 * Prec@1 69.440 Prec@5 90.360 Loss 1.1611
Best acc: 69.560
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.6597 (0.5623)	Prec@1 80.469 (83.934)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.5708 (0.5610)	Prec@1 78.125 (84.105)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.4614 (0.5643)	Prec@1 84.375 (84.075)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.5479 (0.5625)	Prec@1 82.812 (84.190)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.4470 (0.5590)	Prec@1 89.844 (84.305)	
Total train loss: 0.5590

 * Prec@1 69.340 Prec@5 90.500 Loss 1.1641
Best acc: 69.560
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.6001 (0.5528)	Prec@1 86.719 (84.505)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.6416 (0.5567)	Prec@1 78.125 (84.280)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.5981 (0.5624)	Prec@1 78.125 (84.265)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.4878 (0.5629)	Prec@1 85.156 (84.120)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.5811 (0.5623)	Prec@1 84.375 (84.109)	
Total train loss: 0.5623

 * Prec@1 69.270 Prec@5 90.520 Loss 1.1631
Best acc: 69.560
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.6069 (0.5597)	Prec@1 79.688 (83.964)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.4021 (0.5552)	Prec@1 92.969 (84.415)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.6021 (0.5595)	Prec@1 82.812 (84.201)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.8760 (0.5623)	Prec@1 77.344 (84.185)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.4817 (0.5616)	Prec@1 85.938 (84.173)	
Total train loss: 0.5615

 * Prec@1 69.630 Prec@5 90.680 Loss 1.1621
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.5151 (0.5634)	Prec@1 85.156 (84.075)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.4673 (0.5619)	Prec@1 86.719 (84.175)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.5801 (0.5627)	Prec@1 82.812 (84.111)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.6641 (0.5631)	Prec@1 81.250 (84.100)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.5830 (0.5617)	Prec@1 81.250 (84.137)	
Total train loss: 0.5617

 * Prec@1 69.220 Prec@5 90.530 Loss 1.1611
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.4460 (0.5621)	Prec@1 86.719 (84.485)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.4663 (0.5586)	Prec@1 91.406 (84.380)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.6313 (0.5580)	Prec@1 77.344 (84.422)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.5469 (0.5604)	Prec@1 86.719 (84.257)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.4348 (0.5596)	Prec@1 90.625 (84.313)	
Total train loss: 0.5597

 * Prec@1 69.420 Prec@5 90.470 Loss 1.1602
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.4651 (0.5501)	Prec@1 85.938 (85.116)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.6880 (0.5464)	Prec@1 77.344 (84.951)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.7104 (0.5548)	Prec@1 82.031 (84.719)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.4507 (0.5543)	Prec@1 85.938 (84.633)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.5591 (0.5552)	Prec@1 88.281 (84.555)	
Total train loss: 0.5551

 * Prec@1 69.310 Prec@5 90.680 Loss 1.1582
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.5366 (0.5440)	Prec@1 82.031 (84.906)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.5723 (0.5553)	Prec@1 83.594 (84.560)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.4329 (0.5554)	Prec@1 89.062 (84.525)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.5298 (0.5580)	Prec@1 85.156 (84.418)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.5474 (0.5612)	Prec@1 82.031 (84.333)	
Total train loss: 0.5612

 * Prec@1 69.320 Prec@5 90.520 Loss 1.1650
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.5664 (0.5644)	Prec@1 83.594 (84.525)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.4995 (0.5636)	Prec@1 85.156 (84.305)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5811 (0.5586)	Prec@1 82.031 (84.438)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.7617 (0.5592)	Prec@1 78.906 (84.350)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.5430 (0.5611)	Prec@1 88.281 (84.253)	
Total train loss: 0.5613

 * Prec@1 69.710 Prec@5 90.480 Loss 1.1611
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.6489 (0.5618)	Prec@1 83.594 (84.645)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.5889 (0.5698)	Prec@1 85.938 (83.944)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.5713 (0.5613)	Prec@1 84.375 (84.285)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.5723 (0.5591)	Prec@1 81.250 (84.390)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.5488 (0.5574)	Prec@1 84.375 (84.479)	
Total train loss: 0.5573

 * Prec@1 69.300 Prec@5 90.470 Loss 1.1660
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.6807 (0.5615)	Prec@1 78.906 (84.175)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.5103 (0.5586)	Prec@1 85.938 (84.105)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.5630 (0.5612)	Prec@1 85.156 (84.095)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.5737 (0.5593)	Prec@1 82.812 (84.132)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.5068 (0.5592)	Prec@1 85.156 (84.137)	
Total train loss: 0.5589

 * Prec@1 69.570 Prec@5 90.600 Loss 1.1631
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.6079 (0.5611)	Prec@1 84.375 (84.125)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.5806 (0.5598)	Prec@1 82.812 (84.235)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.6875 (0.5548)	Prec@1 81.250 (84.325)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.6484 (0.5557)	Prec@1 80.469 (84.393)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5405 (0.5571)	Prec@1 84.375 (84.367)	
Total train loss: 0.5571

 * Prec@1 69.600 Prec@5 90.520 Loss 1.1660
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.5220 (0.5715)	Prec@1 86.719 (83.874)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.5645 (0.5642)	Prec@1 83.594 (84.070)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.5479 (0.5608)	Prec@1 86.719 (84.061)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.7178 (0.5585)	Prec@1 80.469 (84.157)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.6206 (0.5600)	Prec@1 78.125 (84.099)	
Total train loss: 0.5601

 * Prec@1 69.640 Prec@5 90.550 Loss 1.1680
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.6309 (0.5632)	Prec@1 83.594 (84.024)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.4880 (0.5587)	Prec@1 89.062 (84.120)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.3904 (0.5621)	Prec@1 89.844 (83.951)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.6338 (0.5602)	Prec@1 78.906 (84.145)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.4531 (0.5600)	Prec@1 84.375 (84.207)	
Total train loss: 0.5602

 * Prec@1 69.540 Prec@5 90.650 Loss 1.1592
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.4636 (0.5517)	Prec@1 89.062 (84.625)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.4546 (0.5544)	Prec@1 89.844 (84.470)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.7129 (0.5589)	Prec@1 79.688 (84.408)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.5112 (0.5594)	Prec@1 87.500 (84.463)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.6445 (0.5605)	Prec@1 79.688 (84.409)	
Total train loss: 0.5608

 * Prec@1 69.470 Prec@5 90.650 Loss 1.1611
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.5684 (0.5563)	Prec@1 83.594 (84.315)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.6045 (0.5526)	Prec@1 82.031 (84.460)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.5259 (0.5496)	Prec@1 83.594 (84.665)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.5449 (0.5545)	Prec@1 81.250 (84.428)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.5923 (0.5556)	Prec@1 85.938 (84.383)	
Total train loss: 0.5558

 * Prec@1 69.340 Prec@5 90.540 Loss 1.1660
Best acc: 69.710
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.4104 (0.5574)	Prec@1 88.281 (84.295)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.5151 (0.5622)	Prec@1 82.812 (84.265)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.5200 (0.5626)	Prec@1 83.594 (84.322)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.5596 (0.5600)	Prec@1 80.469 (84.295)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5879 (0.5597)	Prec@1 82.031 (84.313)	
Total train loss: 0.5597

 * Prec@1 69.590 Prec@5 90.630 Loss 1.1621
Best acc: 69.710
--------------------------------------------------------------------------------
