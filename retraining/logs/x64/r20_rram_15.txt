
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.060 Prec@5 90.310 Loss 1.1777
Pre-trained Prec@1 with 15 layers frozen: 69.05999755859375 	 Loss: 1.177734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.4536 (0.5622)	Prec@1 88.281 (84.325)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.5879 (0.5600)	Prec@1 83.594 (84.315)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.5352 (0.5605)	Prec@1 80.469 (84.171)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.5156 (0.5581)	Prec@1 87.500 (84.225)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.6392 (0.5638)	Prec@1 78.125 (84.016)	
Total train loss: 0.5636

 * Prec@1 69.950 Prec@5 90.630 Loss 1.1572
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.6572 (0.5350)	Prec@1 80.469 (85.126)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.4839 (0.5455)	Prec@1 86.719 (84.756)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5142 (0.5469)	Prec@1 84.375 (84.742)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.6182 (0.5484)	Prec@1 78.125 (84.761)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.5112 (0.5475)	Prec@1 86.719 (84.806)	
Total train loss: 0.5476

 * Prec@1 69.630 Prec@5 90.420 Loss 1.1631
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.6069 (0.5385)	Prec@1 82.812 (84.575)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5303 (0.5370)	Prec@1 87.500 (85.001)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.5264 (0.5338)	Prec@1 82.031 (85.146)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.4358 (0.5291)	Prec@1 86.719 (85.357)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.4319 (0.5317)	Prec@1 89.844 (85.351)	
Total train loss: 0.5319

 * Prec@1 69.570 Prec@5 90.470 Loss 1.1582
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.4763 (0.5363)	Prec@1 88.281 (85.056)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.6294 (0.5273)	Prec@1 83.594 (85.482)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.3772 (0.5270)	Prec@1 89.844 (85.460)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.5059 (0.5254)	Prec@1 87.500 (85.607)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.6870 (0.5269)	Prec@1 82.031 (85.499)	
Total train loss: 0.5269

 * Prec@1 69.350 Prec@5 90.340 Loss 1.1787
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.5122 (0.5096)	Prec@1 87.500 (86.418)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.5640 (0.5086)	Prec@1 85.156 (86.268)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.4795 (0.5130)	Prec@1 87.500 (86.088)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.5361 (0.5136)	Prec@1 85.938 (86.048)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5234 (0.5156)	Prec@1 86.719 (85.962)	
Total train loss: 0.5157

 * Prec@1 69.520 Prec@5 90.460 Loss 1.1748
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.5742 (0.5008)	Prec@1 83.594 (86.468)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4604 (0.5011)	Prec@1 85.938 (86.569)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5171 (0.5024)	Prec@1 85.156 (86.482)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.4526 (0.5051)	Prec@1 85.156 (86.368)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.6069 (0.5063)	Prec@1 82.812 (86.300)	
Total train loss: 0.5064

 * Prec@1 69.070 Prec@5 90.220 Loss 1.1836
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0001	Loss 0.4648 (0.4842)	Prec@1 85.938 (86.969)	
Epoch: [6][155/391]	LR: 0.0001	Loss 0.4927 (0.4847)	Prec@1 85.938 (87.139)	
Epoch: [6][233/391]	LR: 0.0001	Loss 0.5273 (0.4871)	Prec@1 85.156 (87.019)	
Epoch: [6][311/391]	LR: 0.0001	Loss 0.4209 (0.4899)	Prec@1 89.062 (86.969)	
Epoch: [6][389/391]	LR: 0.0001	Loss 0.5273 (0.4896)	Prec@1 83.594 (86.987)	
Total train loss: 0.4897

 * Prec@1 69.130 Prec@5 90.190 Loss 1.1846
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0001	Loss 0.5205 (0.4758)	Prec@1 85.156 (87.300)	
Epoch: [7][155/391]	LR: 0.0001	Loss 0.4761 (0.4777)	Prec@1 86.719 (87.230)	
Epoch: [7][233/391]	LR: 0.0001	Loss 0.4900 (0.4816)	Prec@1 84.375 (87.283)	
Epoch: [7][311/391]	LR: 0.0001	Loss 0.5054 (0.4843)	Prec@1 86.719 (87.210)	
Epoch: [7][389/391]	LR: 0.0001	Loss 0.5078 (0.4880)	Prec@1 88.281 (87.085)	
Total train loss: 0.4878

 * Prec@1 69.030 Prec@5 90.140 Loss 1.1904
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0001	Loss 0.3984 (0.4828)	Prec@1 92.969 (87.650)	
Epoch: [8][155/391]	LR: 0.0001	Loss 0.4236 (0.4877)	Prec@1 90.625 (87.240)	
Epoch: [8][233/391]	LR: 0.0001	Loss 0.5894 (0.4862)	Prec@1 85.938 (87.246)	
Epoch: [8][311/391]	LR: 0.0001	Loss 0.4819 (0.4862)	Prec@1 88.281 (87.237)	
Epoch: [8][389/391]	LR: 0.0001	Loss 0.5996 (0.4866)	Prec@1 81.250 (87.202)	
Total train loss: 0.4865

 * Prec@1 69.340 Prec@5 90.290 Loss 1.1826
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0001	Loss 0.5415 (0.4850)	Prec@1 81.250 (87.190)	
Epoch: [9][155/391]	LR: 0.0001	Loss 0.4041 (0.4796)	Prec@1 89.062 (87.390)	
Epoch: [9][233/391]	LR: 0.0001	Loss 0.4524 (0.4815)	Prec@1 86.719 (87.163)	
Epoch: [9][311/391]	LR: 0.0001	Loss 0.5181 (0.4856)	Prec@1 87.500 (87.069)	
Epoch: [9][389/391]	LR: 0.0001	Loss 0.5918 (0.4881)	Prec@1 81.250 (87.061)	
Total train loss: 0.4879

 * Prec@1 69.130 Prec@5 90.240 Loss 1.1836
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0001	Loss 0.5107 (0.4828)	Prec@1 86.719 (87.240)	
Epoch: [10][155/391]	LR: 0.0001	Loss 0.5293 (0.4875)	Prec@1 84.375 (86.864)	
Epoch: [10][233/391]	LR: 0.0001	Loss 0.3486 (0.4890)	Prec@1 92.969 (86.926)	
Epoch: [10][311/391]	LR: 0.0001	Loss 0.4570 (0.4857)	Prec@1 87.500 (87.054)	
Epoch: [10][389/391]	LR: 0.0001	Loss 0.3706 (0.4860)	Prec@1 91.406 (86.943)	
Total train loss: 0.4860

 * Prec@1 69.120 Prec@5 90.310 Loss 1.1787
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0001	Loss 0.4922 (0.4885)	Prec@1 90.625 (86.969)	
Epoch: [11][155/391]	LR: 0.0001	Loss 0.6118 (0.4898)	Prec@1 82.031 (86.889)	
Epoch: [11][233/391]	LR: 0.0001	Loss 0.4731 (0.4913)	Prec@1 88.281 (86.779)	
Epoch: [11][311/391]	LR: 0.0001	Loss 0.4746 (0.4898)	Prec@1 87.500 (86.841)	
Epoch: [11][389/391]	LR: 0.0001	Loss 0.5347 (0.4885)	Prec@1 82.031 (86.923)	
Total train loss: 0.4886

 * Prec@1 69.370 Prec@5 90.180 Loss 1.1777
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1e-05	Loss 0.3943 (0.5014)	Prec@1 92.188 (86.508)	
Epoch: [12][155/391]	LR: 1e-05	Loss 0.5454 (0.4926)	Prec@1 85.156 (86.919)	
Epoch: [12][233/391]	LR: 1e-05	Loss 0.4773 (0.4905)	Prec@1 85.156 (86.996)	
Epoch: [12][311/391]	LR: 1e-05	Loss 0.4673 (0.4871)	Prec@1 86.719 (86.992)	
Epoch: [12][389/391]	LR: 1e-05	Loss 0.4661 (0.4879)	Prec@1 87.500 (87.011)	
Total train loss: 0.4881

 * Prec@1 69.060 Prec@5 90.120 Loss 1.1865
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1e-05	Loss 0.5098 (0.4843)	Prec@1 88.281 (87.590)	
Epoch: [13][155/391]	LR: 1e-05	Loss 0.3440 (0.4898)	Prec@1 92.969 (87.370)	
Epoch: [13][233/391]	LR: 1e-05	Loss 0.4297 (0.4906)	Prec@1 87.500 (87.143)	
Epoch: [13][311/391]	LR: 1e-05	Loss 0.5796 (0.4881)	Prec@1 85.156 (87.154)	
Epoch: [13][389/391]	LR: 1e-05	Loss 0.5571 (0.4886)	Prec@1 86.719 (87.139)	
Total train loss: 0.4889

 * Prec@1 69.520 Prec@5 90.250 Loss 1.1748
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1e-05	Loss 0.4897 (0.4968)	Prec@1 86.719 (86.609)	
Epoch: [14][155/391]	LR: 1e-05	Loss 0.3674 (0.4957)	Prec@1 89.844 (86.624)	
Epoch: [14][233/391]	LR: 1e-05	Loss 0.4595 (0.4890)	Prec@1 87.500 (86.909)	
Epoch: [14][311/391]	LR: 1e-05	Loss 0.5425 (0.4881)	Prec@1 85.938 (86.987)	
Epoch: [14][389/391]	LR: 1e-05	Loss 0.4866 (0.4907)	Prec@1 87.500 (86.789)	
Total train loss: 0.4911

 * Prec@1 69.140 Prec@5 90.190 Loss 1.1885
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1e-05	Loss 0.3779 (0.4898)	Prec@1 86.719 (86.759)	
Epoch: [15][155/391]	LR: 1e-05	Loss 0.4758 (0.4917)	Prec@1 89.062 (86.819)	
Epoch: [15][233/391]	LR: 1e-05	Loss 0.4229 (0.4875)	Prec@1 90.625 (86.942)	
Epoch: [15][311/391]	LR: 1e-05	Loss 0.4023 (0.4842)	Prec@1 89.844 (87.107)	
Epoch: [15][389/391]	LR: 1e-05	Loss 0.5054 (0.4843)	Prec@1 88.281 (87.131)	
Total train loss: 0.4844

 * Prec@1 69.230 Prec@5 90.240 Loss 1.1865
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1e-05	Loss 0.4312 (0.5079)	Prec@1 88.281 (85.777)	
Epoch: [16][155/391]	LR: 1e-05	Loss 0.5854 (0.4959)	Prec@1 81.250 (86.403)	
Epoch: [16][233/391]	LR: 1e-05	Loss 0.3860 (0.4935)	Prec@1 92.188 (86.585)	
Epoch: [16][311/391]	LR: 1e-05	Loss 0.4539 (0.4902)	Prec@1 87.500 (86.794)	
Epoch: [16][389/391]	LR: 1e-05	Loss 0.4595 (0.4890)	Prec@1 90.625 (86.905)	
Total train loss: 0.4889

 * Prec@1 69.090 Prec@5 90.260 Loss 1.1826
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1e-05	Loss 0.4182 (0.4689)	Prec@1 91.406 (87.831)	
Epoch: [17][155/391]	LR: 1e-05	Loss 0.4553 (0.4792)	Prec@1 85.156 (87.245)	
Epoch: [17][233/391]	LR: 1e-05	Loss 0.4849 (0.4824)	Prec@1 87.500 (87.056)	
Epoch: [17][311/391]	LR: 1e-05	Loss 0.5303 (0.4866)	Prec@1 88.281 (86.954)	
Epoch: [17][389/391]	LR: 1e-05	Loss 0.4587 (0.4867)	Prec@1 87.500 (86.997)	
Total train loss: 0.4866

 * Prec@1 68.990 Prec@5 90.290 Loss 1.1875
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1e-05	Loss 0.4058 (0.4801)	Prec@1 89.844 (87.059)	
Epoch: [18][155/391]	LR: 1e-05	Loss 0.4688 (0.4862)	Prec@1 85.938 (86.899)	
Epoch: [18][233/391]	LR: 1e-05	Loss 0.4404 (0.4865)	Prec@1 88.281 (86.882)	
Epoch: [18][311/391]	LR: 1e-05	Loss 0.5635 (0.4842)	Prec@1 85.156 (87.052)	
Epoch: [18][389/391]	LR: 1e-05	Loss 0.5186 (0.4866)	Prec@1 87.500 (86.975)	
Total train loss: 0.4869

 * Prec@1 69.300 Prec@5 90.260 Loss 1.1816
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1e-05	Loss 0.4495 (0.4944)	Prec@1 88.281 (87.019)	
Epoch: [19][155/391]	LR: 1e-05	Loss 0.3967 (0.4955)	Prec@1 87.500 (86.899)	
Epoch: [19][233/391]	LR: 1e-05	Loss 0.5054 (0.4899)	Prec@1 86.719 (87.106)	
Epoch: [19][311/391]	LR: 1e-05	Loss 0.5137 (0.4883)	Prec@1 82.031 (87.137)	
Epoch: [19][389/391]	LR: 1e-05	Loss 0.5615 (0.4878)	Prec@1 81.250 (87.107)	
Total train loss: 0.4877

 * Prec@1 69.180 Prec@5 90.280 Loss 1.1826
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-06	Loss 0.4580 (0.4899)	Prec@1 88.281 (87.149)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-06	Loss 0.4080 (0.4875)	Prec@1 90.625 (86.944)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-06	Loss 0.4441 (0.4877)	Prec@1 85.938 (86.969)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-06	Loss 0.5254 (0.4901)	Prec@1 84.375 (86.859)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-06	Loss 0.3684 (0.4886)	Prec@1 90.625 (86.913)	
Total train loss: 0.4887

 * Prec@1 69.150 Prec@5 90.210 Loss 1.1904
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.4934)	Prec@1 82.031 (86.699)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-06	Loss 0.3921 (0.4850)	Prec@1 91.406 (87.179)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-06	Loss 0.4802 (0.4873)	Prec@1 87.500 (87.073)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-06	Loss 0.4150 (0.4844)	Prec@1 89.844 (87.169)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-06	Loss 0.4746 (0.4856)	Prec@1 87.500 (87.037)	
Total train loss: 0.4854

 * Prec@1 69.170 Prec@5 90.290 Loss 1.1797
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-06	Loss 0.4480 (0.4840)	Prec@1 86.719 (87.129)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-06	Loss 0.6138 (0.4898)	Prec@1 79.688 (86.954)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-06	Loss 0.5366 (0.4890)	Prec@1 86.719 (87.066)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-06	Loss 0.4773 (0.4869)	Prec@1 86.719 (87.017)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-06	Loss 0.5122 (0.4865)	Prec@1 85.938 (87.083)	
Total train loss: 0.4867

 * Prec@1 69.260 Prec@5 90.170 Loss 1.1885
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-06	Loss 0.6323 (0.4952)	Prec@1 83.594 (86.609)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-06	Loss 0.4307 (0.4907)	Prec@1 88.281 (86.964)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-06	Loss 0.4319 (0.4899)	Prec@1 89.062 (86.999)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-06	Loss 0.5518 (0.4900)	Prec@1 83.594 (86.869)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-06	Loss 0.5664 (0.4876)	Prec@1 83.594 (87.013)	
Total train loss: 0.4877

 * Prec@1 69.340 Prec@5 90.150 Loss 1.1777
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-06	Loss 0.6143 (0.4803)	Prec@1 79.688 (87.220)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-06	Loss 0.5747 (0.4946)	Prec@1 82.031 (86.634)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-06	Loss 0.4106 (0.4924)	Prec@1 87.500 (86.649)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-06	Loss 0.5215 (0.4906)	Prec@1 85.156 (86.779)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-06	Loss 0.5518 (0.4918)	Prec@1 88.281 (86.725)	
Total train loss: 0.4917

 * Prec@1 69.340 Prec@5 90.190 Loss 1.1758
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-06	Loss 0.6064 (0.4978)	Prec@1 82.812 (86.779)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-06	Loss 0.4717 (0.4973)	Prec@1 84.375 (86.769)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-06	Loss 0.4905 (0.4961)	Prec@1 83.594 (86.829)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-06	Loss 0.4058 (0.4907)	Prec@1 92.969 (86.977)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-06	Loss 0.6221 (0.4878)	Prec@1 82.031 (87.059)	
Total train loss: 0.4879

 * Prec@1 69.170 Prec@5 90.300 Loss 1.1855
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-06	Loss 0.6499 (0.5011)	Prec@1 84.375 (86.609)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-06	Loss 0.4341 (0.4893)	Prec@1 86.719 (87.149)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-06	Loss 0.4348 (0.4890)	Prec@1 89.844 (87.116)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-06	Loss 0.5396 (0.4843)	Prec@1 84.375 (87.207)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-06	Loss 0.5317 (0.4853)	Prec@1 84.375 (87.089)	
Total train loss: 0.4854

 * Prec@1 69.260 Prec@5 90.220 Loss 1.1826
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-06	Loss 0.4858 (0.4831)	Prec@1 85.156 (86.679)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-06	Loss 0.4998 (0.4948)	Prec@1 89.062 (86.218)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-06	Loss 0.4968 (0.4927)	Prec@1 86.719 (86.599)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-06	Loss 0.6143 (0.4900)	Prec@1 84.375 (86.794)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-06	Loss 0.6025 (0.4897)	Prec@1 84.375 (86.877)	
Total train loss: 0.4897

 * Prec@1 69.400 Prec@5 90.190 Loss 1.1885
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-06	Loss 0.4739 (0.4763)	Prec@1 88.281 (87.821)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-06	Loss 0.4836 (0.4848)	Prec@1 89.062 (87.505)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-06	Loss 0.4880 (0.4837)	Prec@1 92.188 (87.517)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-06	Loss 0.4312 (0.4852)	Prec@1 88.281 (87.220)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-06	Loss 0.3704 (0.4844)	Prec@1 92.188 (87.181)	
Total train loss: 0.4846

 * Prec@1 69.260 Prec@5 90.260 Loss 1.1816
Best acc: 69.950
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-06	Loss 0.3660 (0.4893)	Prec@1 90.625 (86.839)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-06	Loss 0.5615 (0.4859)	Prec@1 85.156 (87.074)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-06	Loss 0.4531 (0.4864)	Prec@1 89.062 (87.063)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-06	Loss 0.4370 (0.4867)	Prec@1 87.500 (87.022)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-06	Loss 0.4001 (0.4861)	Prec@1 91.406 (87.033)	
Total train loss: 0.4860

 * Prec@1 69.290 Prec@5 90.290 Loss 1.1816
Best acc: 69.950
--------------------------------------------------------------------------------
