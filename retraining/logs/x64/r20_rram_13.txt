
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.090 Prec@5 90.330 Loss 1.1768
Pre-trained Prec@1 with 13 layers frozen: 69.08999633789062 	 Loss: 1.1767578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.5361 (0.5635)	Prec@1 85.156 (84.065)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.5684 (0.5565)	Prec@1 82.812 (84.355)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.5776 (0.5663)	Prec@1 82.812 (83.948)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.5181 (0.5699)	Prec@1 82.812 (83.909)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.4724 (0.5685)	Prec@1 86.719 (83.936)	
Total train loss: 0.5688

 * Prec@1 69.230 Prec@5 90.460 Loss 1.1699
Best acc: 69.230
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.4778 (0.5733)	Prec@1 85.938 (83.634)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.5845 (0.5740)	Prec@1 82.812 (83.799)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5767 (0.5715)	Prec@1 85.938 (83.801)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.6138 (0.5703)	Prec@1 80.469 (83.844)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.7036 (0.5677)	Prec@1 81.250 (83.972)	
Total train loss: 0.5678

 * Prec@1 69.220 Prec@5 90.430 Loss 1.1709
Best acc: 69.230
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.5693 (0.5785)	Prec@1 89.062 (83.864)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.5879 (0.5667)	Prec@1 85.156 (84.080)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.7319 (0.5667)	Prec@1 78.906 (84.058)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.5137 (0.5664)	Prec@1 85.938 (84.160)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.5542 (0.5659)	Prec@1 82.031 (84.135)	
Total train loss: 0.5659

 * Prec@1 69.670 Prec@5 90.660 Loss 1.1611
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.4873 (0.5685)	Prec@1 83.594 (83.734)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.5977 (0.5607)	Prec@1 83.594 (84.145)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.5928 (0.5641)	Prec@1 83.594 (84.178)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5059 (0.5635)	Prec@1 85.156 (84.195)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.6138 (0.5613)	Prec@1 82.812 (84.255)	
Total train loss: 0.5613

 * Prec@1 69.290 Prec@5 90.390 Loss 1.1719
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.6260 (0.5511)	Prec@1 85.156 (84.455)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.6006 (0.5625)	Prec@1 78.906 (84.115)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.6729 (0.5682)	Prec@1 78.906 (83.961)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.5864 (0.5658)	Prec@1 81.250 (83.952)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.5459 (0.5646)	Prec@1 85.156 (84.073)	
Total train loss: 0.5645

 * Prec@1 69.430 Prec@5 90.610 Loss 1.1572
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.5869 (0.5582)	Prec@1 82.031 (84.375)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.5190 (0.5602)	Prec@1 85.938 (84.240)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.4575 (0.5610)	Prec@1 85.156 (84.288)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.6050 (0.5628)	Prec@1 81.250 (84.215)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.5317 (0.5620)	Prec@1 80.469 (84.269)	
Total train loss: 0.5622

 * Prec@1 69.410 Prec@5 90.680 Loss 1.1611
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.5918 (0.5692)	Prec@1 79.688 (84.215)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.6118 (0.5675)	Prec@1 82.812 (84.120)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.6245 (0.5606)	Prec@1 82.812 (84.288)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.5244 (0.5617)	Prec@1 85.156 (84.202)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.4270 (0.5592)	Prec@1 90.625 (84.347)	
Total train loss: 0.5592

 * Prec@1 69.680 Prec@5 90.640 Loss 1.1592
Best acc: 69.680
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.5361 (0.5469)	Prec@1 86.719 (84.585)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.6704 (0.5563)	Prec@1 80.469 (84.340)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.4883 (0.5611)	Prec@1 86.719 (84.278)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5684 (0.5591)	Prec@1 82.031 (84.335)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.5771 (0.5581)	Prec@1 85.938 (84.345)	
Total train loss: 0.5582

 * Prec@1 69.430 Prec@5 90.450 Loss 1.1621
Best acc: 69.680
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.4856 (0.5621)	Prec@1 88.281 (84.275)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.5210 (0.5627)	Prec@1 84.375 (84.085)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.4861 (0.5601)	Prec@1 85.938 (84.228)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.6240 (0.5592)	Prec@1 82.031 (84.277)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.5894 (0.5619)	Prec@1 82.031 (84.169)	
Total train loss: 0.5620

 * Prec@1 69.230 Prec@5 90.460 Loss 1.1641
Best acc: 69.680
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.4990 (0.5572)	Prec@1 87.500 (84.065)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.5483 (0.5552)	Prec@1 85.938 (84.225)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.5840 (0.5588)	Prec@1 82.812 (84.145)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.5073 (0.5593)	Prec@1 87.500 (84.142)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.5479 (0.5592)	Prec@1 85.156 (84.257)	
Total train loss: 0.5595

 * Prec@1 69.700 Prec@5 90.490 Loss 1.1592
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.7388 (0.5686)	Prec@1 79.688 (84.285)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.6353 (0.5597)	Prec@1 80.469 (84.510)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.5356 (0.5572)	Prec@1 89.062 (84.442)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.5386 (0.5590)	Prec@1 84.375 (84.448)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.5483 (0.5607)	Prec@1 85.938 (84.357)	
Total train loss: 0.5609

 * Prec@1 69.370 Prec@5 90.620 Loss 1.1611
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.4722 (0.5566)	Prec@1 89.062 (84.275)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.6011 (0.5574)	Prec@1 78.906 (84.090)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.4783 (0.5654)	Prec@1 86.719 (84.001)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.5869 (0.5610)	Prec@1 85.156 (84.202)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5493 (0.5623)	Prec@1 85.938 (84.129)	
Total train loss: 0.5624

 * Prec@1 69.520 Prec@5 90.620 Loss 1.1641
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.5410 (0.5555)	Prec@1 82.812 (84.265)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.5664 (0.5548)	Prec@1 82.812 (84.355)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.6001 (0.5590)	Prec@1 85.156 (84.181)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.5391 (0.5619)	Prec@1 86.719 (84.160)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.5684 (0.5613)	Prec@1 81.250 (84.163)	
Total train loss: 0.5615

 * Prec@1 69.450 Prec@5 90.450 Loss 1.1611
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.5552 (0.5597)	Prec@1 85.156 (84.425)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.4827 (0.5602)	Prec@1 85.938 (84.150)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.4097 (0.5563)	Prec@1 88.281 (84.368)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.5093 (0.5580)	Prec@1 84.375 (84.355)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.7163 (0.5587)	Prec@1 81.250 (84.275)	
Total train loss: 0.5589

 * Prec@1 69.370 Prec@5 90.480 Loss 1.1660
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.4988 (0.5745)	Prec@1 87.500 (83.734)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.5190 (0.5694)	Prec@1 85.938 (84.054)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.6255 (0.5591)	Prec@1 78.906 (84.345)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.5142 (0.5567)	Prec@1 84.375 (84.390)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.4285 (0.5575)	Prec@1 86.719 (84.333)	
Total train loss: 0.5578

 * Prec@1 69.430 Prec@5 90.500 Loss 1.1650
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.6606 (0.5727)	Prec@1 82.812 (83.914)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.6313 (0.5641)	Prec@1 79.688 (84.105)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.4768 (0.5630)	Prec@1 86.719 (84.098)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.5054 (0.5612)	Prec@1 87.500 (84.187)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.4302 (0.5596)	Prec@1 89.844 (84.335)	
Total train loss: 0.5595

 * Prec@1 69.640 Prec@5 90.520 Loss 1.1602
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.4456 (0.5393)	Prec@1 89.062 (84.846)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.4536 (0.5471)	Prec@1 90.625 (84.500)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.5566 (0.5489)	Prec@1 84.375 (84.448)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.5259 (0.5518)	Prec@1 88.281 (84.470)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.4482 (0.5570)	Prec@1 88.281 (84.283)	
Total train loss: 0.5570

 * Prec@1 69.530 Prec@5 90.550 Loss 1.1611
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.5679 (0.5706)	Prec@1 80.469 (83.784)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.5762 (0.5622)	Prec@1 81.250 (83.964)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.4878 (0.5568)	Prec@1 87.500 (84.241)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.6138 (0.5583)	Prec@1 81.250 (84.247)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.5757 (0.5613)	Prec@1 83.594 (84.187)	
Total train loss: 0.5615

 * Prec@1 69.500 Prec@5 90.450 Loss 1.1650
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.5825 (0.5631)	Prec@1 83.594 (84.335)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.6152 (0.5619)	Prec@1 80.469 (84.390)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.5015 (0.5615)	Prec@1 84.375 (84.181)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.5210 (0.5593)	Prec@1 87.500 (84.230)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.5376 (0.5595)	Prec@1 83.594 (84.219)	
Total train loss: 0.5601

 * Prec@1 69.500 Prec@5 90.390 Loss 1.1641
Best acc: 69.700
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.4099 (0.5534)	Prec@1 91.406 (84.295)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.5889 (0.5558)	Prec@1 84.375 (84.310)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.4702 (0.5617)	Prec@1 87.500 (84.078)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.5708 (0.5620)	Prec@1 84.375 (84.145)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.5371 (0.5607)	Prec@1 87.500 (84.183)	
Total train loss: 0.5606

 * Prec@1 69.720 Prec@5 90.640 Loss 1.1621
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.6792 (0.5524)	Prec@1 81.250 (84.615)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.6089 (0.5518)	Prec@1 83.594 (84.435)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.4497 (0.5538)	Prec@1 89.844 (84.435)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.5454 (0.5599)	Prec@1 88.281 (84.255)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.6787 (0.5587)	Prec@1 83.594 (84.227)	
Total train loss: 0.5589

 * Prec@1 69.330 Prec@5 90.460 Loss 1.1660
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.5283 (0.5552)	Prec@1 85.938 (84.495)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.5327 (0.5600)	Prec@1 86.719 (84.370)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5874 (0.5611)	Prec@1 82.812 (84.395)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.6177 (0.5585)	Prec@1 85.156 (84.458)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.6250 (0.5592)	Prec@1 80.469 (84.413)	
Total train loss: 0.5591

 * Prec@1 69.540 Prec@5 90.500 Loss 1.1611
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.5649 (0.5617)	Prec@1 85.938 (84.455)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.6025 (0.5536)	Prec@1 81.250 (84.380)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.6099 (0.5607)	Prec@1 85.938 (84.148)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.5308 (0.5620)	Prec@1 82.031 (84.155)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.7085 (0.5623)	Prec@1 76.562 (84.117)	
Total train loss: 0.5623

 * Prec@1 69.340 Prec@5 90.620 Loss 1.1641
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.5400 (0.5738)	Prec@1 89.062 (83.944)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.5376 (0.5657)	Prec@1 85.156 (84.110)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.4456 (0.5664)	Prec@1 89.062 (84.051)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.6333 (0.5652)	Prec@1 85.156 (84.047)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.4343 (0.5630)	Prec@1 92.188 (84.093)	
Total train loss: 0.5628

 * Prec@1 69.510 Prec@5 90.490 Loss 1.1650
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.5239 (0.5592)	Prec@1 85.156 (84.225)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.5405 (0.5594)	Prec@1 82.031 (83.954)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5337 (0.5557)	Prec@1 85.938 (84.261)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.4426 (0.5545)	Prec@1 89.062 (84.380)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5039 (0.5583)	Prec@1 86.719 (84.279)	
Total train loss: 0.5583

 * Prec@1 69.510 Prec@5 90.580 Loss 1.1611
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.5112 (0.5669)	Prec@1 83.594 (83.834)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.5449 (0.5691)	Prec@1 82.812 (83.689)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.4553 (0.5655)	Prec@1 85.156 (83.894)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.4209 (0.5595)	Prec@1 89.062 (84.147)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.5225 (0.5596)	Prec@1 82.812 (84.137)	
Total train loss: 0.5595

 * Prec@1 69.290 Prec@5 90.590 Loss 1.1670
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.5811 (0.5512)	Prec@1 83.594 (84.195)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.6123 (0.5534)	Prec@1 84.375 (84.275)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.4507 (0.5533)	Prec@1 86.719 (84.221)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.5698 (0.5573)	Prec@1 85.156 (84.137)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.5830 (0.5578)	Prec@1 83.594 (84.163)	
Total train loss: 0.5578

 * Prec@1 69.500 Prec@5 90.570 Loss 1.1641
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.4395 (0.5521)	Prec@1 88.281 (84.355)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.5098 (0.5608)	Prec@1 88.281 (84.115)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.5391 (0.5632)	Prec@1 85.156 (84.048)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.4832 (0.5604)	Prec@1 88.281 (84.160)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.6465 (0.5592)	Prec@1 80.469 (84.203)	
Total train loss: 0.5592

 * Prec@1 69.530 Prec@5 90.520 Loss 1.1631
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.5869 (0.5488)	Prec@1 81.250 (84.615)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.5835 (0.5541)	Prec@1 83.594 (84.405)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.5708 (0.5570)	Prec@1 85.156 (84.335)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.5425 (0.5595)	Prec@1 84.375 (84.205)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.6567 (0.5600)	Prec@1 81.250 (84.245)	
Total train loss: 0.5604

 * Prec@1 69.420 Prec@5 90.440 Loss 1.1631
Best acc: 69.720
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.5586 (0.5576)	Prec@1 85.156 (84.355)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.4377 (0.5575)	Prec@1 89.062 (84.440)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.6758 (0.5604)	Prec@1 81.250 (84.318)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.5195 (0.5597)	Prec@1 85.156 (84.322)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5312 (0.5586)	Prec@1 84.375 (84.313)	
Total train loss: 0.5586

 * Prec@1 69.360 Prec@5 90.610 Loss 1.1592
Best acc: 69.720
--------------------------------------------------------------------------------
