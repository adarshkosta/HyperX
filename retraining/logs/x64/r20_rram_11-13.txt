
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.210 Prec@5 90.410 Loss 1.1758
Pre-trained Prec@1 with 11 layers frozen: 69.20999908447266 	 Loss: 1.17578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.1	Loss 1.5684 (1.8101)	Prec@1 59.375 (51.192)	
Epoch: [0][155/391]	LR: 0.1	Loss 1.5938 (1.7329)	Prec@1 56.250 (52.449)	
Epoch: [0][233/391]	LR: 0.1	Loss 1.3027 (1.6719)	Prec@1 61.719 (53.819)	
Epoch: [0][311/391]	LR: 0.1	Loss 1.4736 (1.6142)	Prec@1 53.906 (55.198)	
Epoch: [0][389/391]	LR: 0.1	Loss 1.1592 (1.5738)	Prec@1 62.500 (56.148)	
Total train loss: 1.5732

 * Prec@1 54.540 Prec@5 82.820 Loss 1.7041
Best acc: 54.540
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.1	Loss 0.9375 (1.2078)	Prec@1 73.438 (65.184)	
Epoch: [1][155/391]	LR: 0.1	Loss 1.3057 (1.2234)	Prec@1 61.719 (64.789)	
Epoch: [1][233/391]	LR: 0.1	Loss 1.2559 (1.2350)	Prec@1 64.844 (64.300)	
Epoch: [1][311/391]	LR: 0.1	Loss 1.2695 (1.2336)	Prec@1 61.719 (64.295)	
Epoch: [1][389/391]	LR: 0.1	Loss 1.1729 (1.2348)	Prec@1 64.844 (64.261)	
Total train loss: 1.2350

 * Prec@1 55.970 Prec@5 83.390 Loss 1.7617
Best acc: 55.970
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.1	Loss 1.2285 (1.0453)	Prec@1 64.062 (69.621)	
Epoch: [2][155/391]	LR: 0.1	Loss 1.2510 (1.0580)	Prec@1 65.625 (69.176)	
Epoch: [2][233/391]	LR: 0.1	Loss 1.1074 (1.0699)	Prec@1 61.719 (68.747)	
Epoch: [2][311/391]	LR: 0.1	Loss 1.3447 (1.0814)	Prec@1 64.844 (68.389)	
Epoch: [2][389/391]	LR: 0.1	Loss 0.7900 (1.0879)	Prec@1 76.562 (68.141)	
Total train loss: 1.0878

 * Prec@1 59.480 Prec@5 86.220 Loss 1.5693
Best acc: 59.480
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.1	Loss 0.7725 (0.9022)	Prec@1 74.219 (73.518)	
Epoch: [3][155/391]	LR: 0.1	Loss 1.1377 (0.9258)	Prec@1 63.281 (72.681)	
Epoch: [3][233/391]	LR: 0.1	Loss 0.9614 (0.9544)	Prec@1 70.312 (71.731)	
Epoch: [3][311/391]	LR: 0.1	Loss 0.8882 (0.9744)	Prec@1 74.219 (71.221)	
Epoch: [3][389/391]	LR: 0.1	Loss 1.0537 (0.9879)	Prec@1 65.625 (70.827)	
Total train loss: 0.9878

 * Prec@1 60.250 Prec@5 86.530 Loss 1.5391
Best acc: 60.250
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.1	Loss 0.6841 (0.8219)	Prec@1 82.812 (75.701)	
Epoch: [4][155/391]	LR: 0.1	Loss 1.0137 (0.8522)	Prec@1 71.094 (74.349)	
Epoch: [4][233/391]	LR: 0.1	Loss 0.8604 (0.8756)	Prec@1 73.438 (73.628)	
Epoch: [4][311/391]	LR: 0.1	Loss 0.9214 (0.8896)	Prec@1 71.094 (73.277)	
Epoch: [4][389/391]	LR: 0.1	Loss 1.1309 (0.9057)	Prec@1 67.188 (72.815)	
Total train loss: 0.9057

 * Prec@1 59.270 Prec@5 86.200 Loss 1.5840
Best acc: 60.250
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.1	Loss 0.8403 (0.7563)	Prec@1 75.781 (76.733)	
Epoch: [5][155/391]	LR: 0.1	Loss 0.7866 (0.7755)	Prec@1 76.562 (76.392)	
Epoch: [5][233/391]	LR: 0.1	Loss 0.9219 (0.8036)	Prec@1 72.656 (75.601)	
Epoch: [5][311/391]	LR: 0.1	Loss 0.7070 (0.8227)	Prec@1 79.688 (75.013)	
Epoch: [5][389/391]	LR: 0.1	Loss 0.8896 (0.8398)	Prec@1 75.781 (74.497)	
Total train loss: 0.8400

 * Prec@1 58.670 Prec@5 85.570 Loss 1.6592
Best acc: 60.250
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.010000000000000002	Loss 0.7041 (0.6470)	Prec@1 77.344 (80.950)	
Epoch: [6][155/391]	LR: 0.010000000000000002	Loss 0.5234 (0.6003)	Prec@1 84.375 (82.352)	
Epoch: [6][233/391]	LR: 0.010000000000000002	Loss 0.5483 (0.5768)	Prec@1 83.594 (82.779)	
Epoch: [6][311/391]	LR: 0.010000000000000002	Loss 0.4478 (0.5594)	Prec@1 87.500 (83.286)	
Epoch: [6][389/391]	LR: 0.010000000000000002	Loss 0.5015 (0.5455)	Prec@1 85.938 (83.786)	
Total train loss: 0.5456

 * Prec@1 66.060 Prec@5 89.410 Loss 1.3174
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.010000000000000002	Loss 0.3879 (0.4364)	Prec@1 87.500 (87.831)	
Epoch: [7][155/391]	LR: 0.010000000000000002	Loss 0.3958 (0.4419)	Prec@1 85.938 (87.595)	
Epoch: [7][233/391]	LR: 0.010000000000000002	Loss 0.4578 (0.4415)	Prec@1 84.375 (87.593)	
Epoch: [7][311/391]	LR: 0.010000000000000002	Loss 0.4089 (0.4413)	Prec@1 87.500 (87.462)	
Epoch: [7][389/391]	LR: 0.010000000000000002	Loss 0.4016 (0.4397)	Prec@1 88.281 (87.434)	
Total train loss: 0.4402

 * Prec@1 65.920 Prec@5 89.150 Loss 1.3496
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.010000000000000002	Loss 0.3066 (0.3837)	Prec@1 92.188 (89.393)	
Epoch: [8][155/391]	LR: 0.010000000000000002	Loss 0.4849 (0.3943)	Prec@1 87.500 (89.158)	
Epoch: [8][233/391]	LR: 0.010000000000000002	Loss 0.4287 (0.3946)	Prec@1 85.938 (89.129)	
Epoch: [8][311/391]	LR: 0.010000000000000002	Loss 0.3105 (0.3946)	Prec@1 92.188 (89.098)	
Epoch: [8][389/391]	LR: 0.010000000000000002	Loss 0.4082 (0.3938)	Prec@1 88.281 (89.111)	
Total train loss: 0.3941

 * Prec@1 65.990 Prec@5 89.140 Loss 1.3564
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.010000000000000002	Loss 0.3984 (0.3539)	Prec@1 89.844 (90.535)	
Epoch: [9][155/391]	LR: 0.010000000000000002	Loss 0.4453 (0.3582)	Prec@1 90.625 (90.405)	
Epoch: [9][233/391]	LR: 0.010000000000000002	Loss 0.2346 (0.3558)	Prec@1 96.094 (90.451)	
Epoch: [9][311/391]	LR: 0.010000000000000002	Loss 0.2998 (0.3571)	Prec@1 92.188 (90.355)	
Epoch: [9][389/391]	LR: 0.010000000000000002	Loss 0.3884 (0.3596)	Prec@1 91.406 (90.262)	
Total train loss: 0.3597

 * Prec@1 65.670 Prec@5 88.690 Loss 1.3750
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.010000000000000002	Loss 0.3755 (0.3191)	Prec@1 90.625 (92.097)	
Epoch: [10][155/391]	LR: 0.010000000000000002	Loss 0.3582 (0.3240)	Prec@1 89.062 (91.827)	
Epoch: [10][233/391]	LR: 0.010000000000000002	Loss 0.3826 (0.3285)	Prec@1 88.281 (91.550)	
Epoch: [10][311/391]	LR: 0.010000000000000002	Loss 0.4451 (0.3320)	Prec@1 90.625 (91.389)	
Epoch: [10][389/391]	LR: 0.010000000000000002	Loss 0.3088 (0.3362)	Prec@1 92.188 (91.202)	
Total train loss: 0.3362

 * Prec@1 65.320 Prec@5 88.880 Loss 1.3975
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.010000000000000002	Loss 0.2905 (0.2996)	Prec@1 94.531 (92.598)	
Epoch: [11][155/391]	LR: 0.010000000000000002	Loss 0.3689 (0.3069)	Prec@1 89.844 (92.413)	
Epoch: [11][233/391]	LR: 0.010000000000000002	Loss 0.3325 (0.3092)	Prec@1 90.625 (92.258)	
Epoch: [11][311/391]	LR: 0.010000000000000002	Loss 0.2720 (0.3114)	Prec@1 94.531 (92.170)	
Epoch: [11][389/391]	LR: 0.010000000000000002	Loss 0.3762 (0.3145)	Prec@1 86.719 (92.039)	
Total train loss: 0.3147

 * Prec@1 64.920 Prec@5 88.490 Loss 1.4365
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0010000000000000002	Loss 0.3767 (0.2829)	Prec@1 92.188 (93.590)	
Epoch: [12][155/391]	LR: 0.0010000000000000002	Loss 0.2700 (0.2822)	Prec@1 92.969 (93.600)	
Epoch: [12][233/391]	LR: 0.0010000000000000002	Loss 0.2981 (0.2797)	Prec@1 93.750 (93.650)	
Epoch: [12][311/391]	LR: 0.0010000000000000002	Loss 0.1693 (0.2792)	Prec@1 99.219 (93.607)	
Epoch: [12][389/391]	LR: 0.0010000000000000002	Loss 0.2981 (0.2791)	Prec@1 92.969 (93.574)	
Total train loss: 0.2792

 * Prec@1 65.110 Prec@5 88.460 Loss 1.4160
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0010000000000000002	Loss 0.2788 (0.2903)	Prec@1 92.969 (93.339)	
Epoch: [13][155/391]	LR: 0.0010000000000000002	Loss 0.3093 (0.2832)	Prec@1 92.969 (93.480)	
Epoch: [13][233/391]	LR: 0.0010000000000000002	Loss 0.3489 (0.2823)	Prec@1 92.188 (93.570)	
Epoch: [13][311/391]	LR: 0.0010000000000000002	Loss 0.3232 (0.2802)	Prec@1 90.625 (93.557)	
Epoch: [13][389/391]	LR: 0.0010000000000000002	Loss 0.4346 (0.2794)	Prec@1 87.500 (93.560)	
Total train loss: 0.2797

 * Prec@1 64.950 Prec@5 88.320 Loss 1.4219
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0010000000000000002	Loss 0.3210 (0.2777)	Prec@1 92.188 (93.620)	
Epoch: [14][155/391]	LR: 0.0010000000000000002	Loss 0.3574 (0.2772)	Prec@1 88.281 (93.560)	
Epoch: [14][233/391]	LR: 0.0010000000000000002	Loss 0.2664 (0.2798)	Prec@1 92.188 (93.453)	
Epoch: [14][311/391]	LR: 0.0010000000000000002	Loss 0.2512 (0.2788)	Prec@1 96.094 (93.540)	
Epoch: [14][389/391]	LR: 0.0010000000000000002	Loss 0.1965 (0.2773)	Prec@1 97.656 (93.594)	
Total train loss: 0.2773

 * Prec@1 65.070 Prec@5 88.340 Loss 1.4268
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0010000000000000002	Loss 0.3027 (0.2761)	Prec@1 94.531 (93.710)	
Epoch: [15][155/391]	LR: 0.0010000000000000002	Loss 0.3049 (0.2782)	Prec@1 92.188 (93.485)	
Epoch: [15][233/391]	LR: 0.0010000000000000002	Loss 0.2678 (0.2791)	Prec@1 95.312 (93.446)	
Epoch: [15][311/391]	LR: 0.0010000000000000002	Loss 0.2173 (0.2788)	Prec@1 95.312 (93.510)	
Epoch: [15][389/391]	LR: 0.0010000000000000002	Loss 0.3203 (0.2786)	Prec@1 91.406 (93.526)	
Total train loss: 0.2787

 * Prec@1 65.080 Prec@5 88.430 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0010000000000000002	Loss 0.3337 (0.2821)	Prec@1 89.844 (93.369)	
Epoch: [16][155/391]	LR: 0.0010000000000000002	Loss 0.2133 (0.2776)	Prec@1 96.094 (93.475)	
Epoch: [16][233/391]	LR: 0.0010000000000000002	Loss 0.3425 (0.2786)	Prec@1 91.406 (93.490)	
Epoch: [16][311/391]	LR: 0.0010000000000000002	Loss 0.3777 (0.2785)	Prec@1 91.406 (93.515)	
Epoch: [16][389/391]	LR: 0.0010000000000000002	Loss 0.2433 (0.2776)	Prec@1 94.531 (93.598)	
Total train loss: 0.2777

 * Prec@1 64.870 Prec@5 88.510 Loss 1.4219
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0010000000000000002	Loss 0.2639 (0.2775)	Prec@1 96.094 (93.790)	
Epoch: [17][155/391]	LR: 0.0010000000000000002	Loss 0.2269 (0.2752)	Prec@1 95.312 (93.900)	
Epoch: [17][233/391]	LR: 0.0010000000000000002	Loss 0.3076 (0.2764)	Prec@1 92.969 (93.807)	
Epoch: [17][311/391]	LR: 0.0010000000000000002	Loss 0.3379 (0.2771)	Prec@1 90.625 (93.805)	
Epoch: [17][389/391]	LR: 0.0010000000000000002	Loss 0.2886 (0.2761)	Prec@1 94.531 (93.770)	
Total train loss: 0.2762

 * Prec@1 65.000 Prec@5 88.430 Loss 1.4316
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0010000000000000002	Loss 0.2119 (0.2782)	Prec@1 95.312 (93.890)	
Epoch: [18][155/391]	LR: 0.0010000000000000002	Loss 0.2766 (0.2733)	Prec@1 92.969 (93.915)	
Epoch: [18][233/391]	LR: 0.0010000000000000002	Loss 0.3159 (0.2723)	Prec@1 94.531 (93.890)	
Epoch: [18][311/391]	LR: 0.0010000000000000002	Loss 0.2979 (0.2736)	Prec@1 92.969 (93.773)	
Epoch: [18][389/391]	LR: 0.0010000000000000002	Loss 0.2428 (0.2725)	Prec@1 94.531 (93.854)	
Total train loss: 0.2725

 * Prec@1 65.220 Prec@5 88.540 Loss 1.4209
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0010000000000000002	Loss 0.2202 (0.2682)	Prec@1 96.875 (93.970)	
Epoch: [19][155/391]	LR: 0.0010000000000000002	Loss 0.3137 (0.2671)	Prec@1 90.625 (93.950)	
Epoch: [19][233/391]	LR: 0.0010000000000000002	Loss 0.2917 (0.2717)	Prec@1 92.969 (93.857)	
Epoch: [19][311/391]	LR: 0.0010000000000000002	Loss 0.2140 (0.2708)	Prec@1 96.875 (93.930)	
Epoch: [19][389/391]	LR: 0.0010000000000000002	Loss 0.2191 (0.2704)	Prec@1 95.312 (93.880)	
Total train loss: 0.2704

 * Prec@1 65.160 Prec@5 88.390 Loss 1.4189
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.00010000000000000003	Loss 0.2544 (0.2710)	Prec@1 96.094 (93.930)	
Epoch: [20][155/391]	LR: 0.00010000000000000003	Loss 0.2957 (0.2728)	Prec@1 93.750 (93.800)	
Epoch: [20][233/391]	LR: 0.00010000000000000003	Loss 0.2142 (0.2695)	Prec@1 95.312 (93.880)	
Epoch: [20][311/391]	LR: 0.00010000000000000003	Loss 0.2478 (0.2725)	Prec@1 94.531 (93.725)	
Epoch: [20][389/391]	LR: 0.00010000000000000003	Loss 0.2761 (0.2737)	Prec@1 92.188 (93.694)	
Total train loss: 0.2738

 * Prec@1 65.190 Prec@5 88.380 Loss 1.4277
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.00010000000000000003	Loss 0.3057 (0.2785)	Prec@1 92.188 (93.289)	
Epoch: [21][155/391]	LR: 0.00010000000000000003	Loss 0.2524 (0.2782)	Prec@1 95.312 (93.555)	
Epoch: [21][233/391]	LR: 0.00010000000000000003	Loss 0.1970 (0.2732)	Prec@1 96.094 (93.620)	
Epoch: [21][311/391]	LR: 0.00010000000000000003	Loss 0.4021 (0.2728)	Prec@1 86.719 (93.675)	
Epoch: [21][389/391]	LR: 0.00010000000000000003	Loss 0.2764 (0.2742)	Prec@1 95.312 (93.646)	
Total train loss: 0.2743

 * Prec@1 64.990 Prec@5 88.500 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.00010000000000000003	Loss 0.3179 (0.2701)	Prec@1 93.750 (93.960)	
Epoch: [22][155/391]	LR: 0.00010000000000000003	Loss 0.3254 (0.2736)	Prec@1 94.531 (93.785)	
Epoch: [22][233/391]	LR: 0.00010000000000000003	Loss 0.2917 (0.2723)	Prec@1 94.531 (93.894)	
Epoch: [22][311/391]	LR: 0.00010000000000000003	Loss 0.2900 (0.2720)	Prec@1 94.531 (93.865)	
Epoch: [22][389/391]	LR: 0.00010000000000000003	Loss 0.2776 (0.2720)	Prec@1 92.188 (93.880)	
Total train loss: 0.2721

 * Prec@1 64.980 Prec@5 88.290 Loss 1.4268
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.00010000000000000003	Loss 0.3447 (0.2767)	Prec@1 90.625 (93.349)	
Epoch: [23][155/391]	LR: 0.00010000000000000003	Loss 0.2316 (0.2707)	Prec@1 96.875 (93.900)	
Epoch: [23][233/391]	LR: 0.00010000000000000003	Loss 0.2644 (0.2743)	Prec@1 95.312 (93.760)	
Epoch: [23][311/391]	LR: 0.00010000000000000003	Loss 0.2024 (0.2746)	Prec@1 97.656 (93.750)	
Epoch: [23][389/391]	LR: 0.00010000000000000003	Loss 0.2747 (0.2728)	Prec@1 92.969 (93.850)	
Total train loss: 0.2728

 * Prec@1 65.020 Prec@5 88.660 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.00010000000000000003	Loss 0.3198 (0.2748)	Prec@1 94.531 (93.910)	
Epoch: [24][155/391]	LR: 0.00010000000000000003	Loss 0.2944 (0.2771)	Prec@1 91.406 (93.685)	
Epoch: [24][233/391]	LR: 0.00010000000000000003	Loss 0.3342 (0.2748)	Prec@1 92.969 (93.753)	
Epoch: [24][311/391]	LR: 0.00010000000000000003	Loss 0.2979 (0.2732)	Prec@1 94.531 (93.825)	
Epoch: [24][389/391]	LR: 0.00010000000000000003	Loss 0.3127 (0.2724)	Prec@1 91.406 (93.876)	
Total train loss: 0.2725

 * Prec@1 65.000 Prec@5 88.360 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.00010000000000000003	Loss 0.2445 (0.2804)	Prec@1 95.312 (93.540)	
Epoch: [25][155/391]	LR: 0.00010000000000000003	Loss 0.1935 (0.2773)	Prec@1 99.219 (93.675)	
Epoch: [25][233/391]	LR: 0.00010000000000000003	Loss 0.2324 (0.2739)	Prec@1 92.969 (93.703)	
Epoch: [25][311/391]	LR: 0.00010000000000000003	Loss 0.3413 (0.2725)	Prec@1 89.062 (93.715)	
Epoch: [25][389/391]	LR: 0.00010000000000000003	Loss 0.3154 (0.2732)	Prec@1 92.969 (93.732)	
Total train loss: 0.2732

 * Prec@1 65.130 Prec@5 88.420 Loss 1.4316
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.00010000000000000003	Loss 0.3008 (0.2777)	Prec@1 92.969 (93.500)	
Epoch: [26][155/391]	LR: 0.00010000000000000003	Loss 0.2151 (0.2718)	Prec@1 95.312 (93.680)	
Epoch: [26][233/391]	LR: 0.00010000000000000003	Loss 0.2178 (0.2739)	Prec@1 96.094 (93.573)	
Epoch: [26][311/391]	LR: 0.00010000000000000003	Loss 0.3367 (0.2736)	Prec@1 89.844 (93.567)	
Epoch: [26][389/391]	LR: 0.00010000000000000003	Loss 0.2174 (0.2739)	Prec@1 96.094 (93.612)	
Total train loss: 0.2740

 * Prec@1 65.240 Prec@5 88.440 Loss 1.4209
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.00010000000000000003	Loss 0.2054 (0.2662)	Prec@1 96.094 (93.680)	
Epoch: [27][155/391]	LR: 0.00010000000000000003	Loss 0.2279 (0.2707)	Prec@1 96.875 (93.715)	
Epoch: [27][233/391]	LR: 0.00010000000000000003	Loss 0.3708 (0.2745)	Prec@1 89.062 (93.626)	
Epoch: [27][311/391]	LR: 0.00010000000000000003	Loss 0.2467 (0.2743)	Prec@1 94.531 (93.640)	
Epoch: [27][389/391]	LR: 0.00010000000000000003	Loss 0.2439 (0.2741)	Prec@1 93.750 (93.682)	
Total train loss: 0.2743

 * Prec@1 65.310 Prec@5 88.660 Loss 1.4199
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.00010000000000000003	Loss 0.3137 (0.2697)	Prec@1 91.406 (93.890)	
Epoch: [28][155/391]	LR: 0.00010000000000000003	Loss 0.3020 (0.2692)	Prec@1 95.312 (94.035)	
Epoch: [28][233/391]	LR: 0.00010000000000000003	Loss 0.3428 (0.2676)	Prec@1 89.062 (94.014)	
Epoch: [28][311/391]	LR: 0.00010000000000000003	Loss 0.2729 (0.2697)	Prec@1 94.531 (93.908)	
Epoch: [28][389/391]	LR: 0.00010000000000000003	Loss 0.2610 (0.2693)	Prec@1 92.969 (93.928)	
Total train loss: 0.2696

 * Prec@1 65.220 Prec@5 88.580 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.00010000000000000003	Loss 0.2125 (0.2750)	Prec@1 96.094 (93.590)	
Epoch: [29][155/391]	LR: 0.00010000000000000003	Loss 0.2307 (0.2714)	Prec@1 93.750 (93.930)	
Epoch: [29][233/391]	LR: 0.00010000000000000003	Loss 0.2556 (0.2715)	Prec@1 95.312 (93.877)	
Epoch: [29][311/391]	LR: 0.00010000000000000003	Loss 0.2761 (0.2723)	Prec@1 90.625 (93.840)	
Epoch: [29][389/391]	LR: 0.00010000000000000003	Loss 0.3005 (0.2716)	Prec@1 93.750 (93.846)	
Total train loss: 0.2716

 * Prec@1 64.940 Prec@5 88.510 Loss 1.4229
Best acc: 66.060
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.090 Prec@5 90.330 Loss 1.1768
Pre-trained Prec@1 with 13 layers frozen: 69.08999633789062 	 Loss: 1.1767578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.1	Loss 1.5820 (1.6938)	Prec@1 58.594 (53.636)	
Epoch: [0][155/391]	LR: 0.1	Loss 1.4043 (1.6137)	Prec@1 57.031 (55.188)	
Epoch: [0][233/391]	LR: 0.1	Loss 1.4590 (1.5628)	Prec@1 57.812 (56.510)	
Epoch: [0][311/391]	LR: 0.1	Loss 1.4209 (1.5171)	Prec@1 60.156 (57.535)	
Epoch: [0][389/391]	LR: 0.1	Loss 1.3496 (1.4828)	Prec@1 63.281 (58.379)	
Total train loss: 1.4828

 * Prec@1 55.530 Prec@5 83.900 Loss 1.6885
Best acc: 55.530
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.1	Loss 1.1104 (1.1590)	Prec@1 64.844 (66.627)	
Epoch: [1][155/391]	LR: 0.1	Loss 1.2285 (1.1736)	Prec@1 65.625 (66.176)	
Epoch: [1][233/391]	LR: 0.1	Loss 1.2227 (1.1794)	Prec@1 67.188 (65.999)	
Epoch: [1][311/391]	LR: 0.1	Loss 0.9932 (1.1795)	Prec@1 72.656 (66.006)	
Epoch: [1][389/391]	LR: 0.1	Loss 1.4668 (1.1745)	Prec@1 57.031 (66.076)	
Total train loss: 1.1746

 * Prec@1 58.860 Prec@5 86.390 Loss 1.5449
Best acc: 58.860
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.1	Loss 0.9229 (0.9799)	Prec@1 74.219 (71.054)	
Epoch: [2][155/391]	LR: 0.1	Loss 1.0791 (0.9897)	Prec@1 71.875 (70.848)	
Epoch: [2][233/391]	LR: 0.1	Loss 1.2871 (1.0056)	Prec@1 59.375 (70.399)	
Epoch: [2][311/391]	LR: 0.1	Loss 0.9272 (1.0184)	Prec@1 71.094 (70.082)	
Epoch: [2][389/391]	LR: 0.1	Loss 0.8311 (1.0250)	Prec@1 77.344 (69.836)	
Total train loss: 1.0254

 * Prec@1 60.800 Prec@5 86.200 Loss 1.5059
Best acc: 60.800
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.1	Loss 0.8491 (0.8606)	Prec@1 75.781 (74.529)	
Epoch: [3][155/391]	LR: 0.1	Loss 0.9019 (0.8747)	Prec@1 72.656 (73.993)	
Epoch: [3][233/391]	LR: 0.1	Loss 0.9570 (0.9011)	Prec@1 72.656 (73.024)	
Epoch: [3][311/391]	LR: 0.1	Loss 1.0264 (0.9166)	Prec@1 67.188 (72.619)	
Epoch: [3][389/391]	LR: 0.1	Loss 1.0615 (0.9239)	Prec@1 67.188 (72.440)	
Total train loss: 0.9239

 * Prec@1 60.120 Prec@5 86.080 Loss 1.5342
Best acc: 60.800
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.1	Loss 0.8379 (0.7663)	Prec@1 79.688 (76.853)	
Epoch: [4][155/391]	LR: 0.1	Loss 0.9629 (0.8000)	Prec@1 72.656 (75.871)	
Epoch: [4][233/391]	LR: 0.1	Loss 0.8687 (0.8279)	Prec@1 76.562 (75.107)	
Epoch: [4][311/391]	LR: 0.1	Loss 0.9985 (0.8441)	Prec@1 68.750 (74.735)	
Epoch: [4][389/391]	LR: 0.1	Loss 0.8174 (0.8566)	Prec@1 75.000 (74.377)	
Total train loss: 0.8566

 * Prec@1 58.560 Prec@5 85.280 Loss 1.6816
Best acc: 60.800
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.1	Loss 0.7764 (0.6985)	Prec@1 74.219 (78.486)	
Epoch: [5][155/391]	LR: 0.1	Loss 0.9150 (0.7312)	Prec@1 74.219 (77.734)	
Epoch: [5][233/391]	LR: 0.1	Loss 0.6743 (0.7511)	Prec@1 80.469 (77.017)	
Epoch: [5][311/391]	LR: 0.1	Loss 0.9009 (0.7711)	Prec@1 75.000 (76.445)	
Epoch: [5][389/391]	LR: 0.1	Loss 0.9561 (0.7850)	Prec@1 71.094 (75.956)	
Total train loss: 0.7852

 * Prec@1 59.390 Prec@5 85.800 Loss 1.6641
Best acc: 60.800
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.010000000000000002	Loss 0.5410 (0.5971)	Prec@1 82.031 (82.362)	
Epoch: [6][155/391]	LR: 0.010000000000000002	Loss 0.5229 (0.5635)	Prec@1 81.250 (83.243)	
Epoch: [6][233/391]	LR: 0.010000000000000002	Loss 0.5532 (0.5370)	Prec@1 78.906 (84.008)	
Epoch: [6][311/391]	LR: 0.010000000000000002	Loss 0.4531 (0.5230)	Prec@1 83.594 (84.450)	
Epoch: [6][389/391]	LR: 0.010000000000000002	Loss 0.4436 (0.5102)	Prec@1 87.500 (84.992)	
Total train loss: 0.5100

 * Prec@1 66.350 Prec@5 89.400 Loss 1.3301
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.010000000000000002	Loss 0.4380 (0.4040)	Prec@1 87.500 (88.502)	
Epoch: [7][155/391]	LR: 0.010000000000000002	Loss 0.5532 (0.4073)	Prec@1 82.031 (88.351)	
Epoch: [7][233/391]	LR: 0.010000000000000002	Loss 0.3684 (0.4097)	Prec@1 89.844 (88.375)	
Epoch: [7][311/391]	LR: 0.010000000000000002	Loss 0.3347 (0.4089)	Prec@1 91.406 (88.371)	
Epoch: [7][389/391]	LR: 0.010000000000000002	Loss 0.4775 (0.4091)	Prec@1 86.719 (88.355)	
Total train loss: 0.4092

 * Prec@1 65.760 Prec@5 89.220 Loss 1.3662
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.010000000000000002	Loss 0.3132 (0.3641)	Prec@1 92.969 (90.044)	
Epoch: [8][155/391]	LR: 0.010000000000000002	Loss 0.3347 (0.3703)	Prec@1 89.844 (89.914)	
Epoch: [8][233/391]	LR: 0.010000000000000002	Loss 0.2920 (0.3672)	Prec@1 91.406 (90.051)	
Epoch: [8][311/391]	LR: 0.010000000000000002	Loss 0.3826 (0.3682)	Prec@1 92.969 (90.014)	
Epoch: [8][389/391]	LR: 0.010000000000000002	Loss 0.2551 (0.3713)	Prec@1 93.750 (89.902)	
Total train loss: 0.3713

 * Prec@1 65.400 Prec@5 88.950 Loss 1.3965
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.010000000000000002	Loss 0.2917 (0.3364)	Prec@1 92.188 (91.306)	
Epoch: [9][155/391]	LR: 0.010000000000000002	Loss 0.3501 (0.3376)	Prec@1 91.406 (91.186)	
Epoch: [9][233/391]	LR: 0.010000000000000002	Loss 0.4111 (0.3411)	Prec@1 88.281 (91.039)	
Epoch: [9][311/391]	LR: 0.010000000000000002	Loss 0.3057 (0.3426)	Prec@1 93.750 (90.946)	
Epoch: [9][389/391]	LR: 0.010000000000000002	Loss 0.3130 (0.3436)	Prec@1 93.750 (90.853)	
Total train loss: 0.3437

 * Prec@1 65.660 Prec@5 88.910 Loss 1.3984
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.010000000000000002	Loss 0.5029 (0.3137)	Prec@1 84.375 (92.087)	
Epoch: [10][155/391]	LR: 0.010000000000000002	Loss 0.3328 (0.3112)	Prec@1 91.406 (92.218)	
Epoch: [10][233/391]	LR: 0.010000000000000002	Loss 0.2788 (0.3132)	Prec@1 93.750 (91.977)	
Epoch: [10][311/391]	LR: 0.010000000000000002	Loss 0.3196 (0.3164)	Prec@1 93.750 (91.945)	
Epoch: [10][389/391]	LR: 0.010000000000000002	Loss 0.2206 (0.3203)	Prec@1 97.656 (91.759)	
Total train loss: 0.3205

 * Prec@1 64.890 Prec@5 88.600 Loss 1.4297
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.010000000000000002	Loss 0.2361 (0.2874)	Prec@1 92.969 (93.119)	
Epoch: [11][155/391]	LR: 0.010000000000000002	Loss 0.2737 (0.2903)	Prec@1 95.312 (92.954)	
Epoch: [11][233/391]	LR: 0.010000000000000002	Loss 0.2788 (0.2995)	Prec@1 91.406 (92.601)	
Epoch: [11][311/391]	LR: 0.010000000000000002	Loss 0.2944 (0.2979)	Prec@1 92.188 (92.588)	
Epoch: [11][389/391]	LR: 0.010000000000000002	Loss 0.3127 (0.2994)	Prec@1 93.750 (92.514)	
Total train loss: 0.2996

 * Prec@1 65.080 Prec@5 88.290 Loss 1.4355
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0010000000000000002	Loss 0.2363 (0.2609)	Prec@1 95.312 (93.950)	
Epoch: [12][155/391]	LR: 0.0010000000000000002	Loss 0.2812 (0.2612)	Prec@1 92.188 (94.020)	
Epoch: [12][233/391]	LR: 0.0010000000000000002	Loss 0.3032 (0.2663)	Prec@1 92.188 (93.840)	
Epoch: [12][311/391]	LR: 0.0010000000000000002	Loss 0.2822 (0.2676)	Prec@1 92.188 (93.795)	
Epoch: [12][389/391]	LR: 0.0010000000000000002	Loss 0.2874 (0.2690)	Prec@1 96.094 (93.756)	
Total train loss: 0.2691

 * Prec@1 65.070 Prec@5 88.290 Loss 1.4404
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0010000000000000002	Loss 0.2825 (0.2665)	Prec@1 92.969 (94.000)	
Epoch: [13][155/391]	LR: 0.0010000000000000002	Loss 0.2159 (0.2664)	Prec@1 96.094 (93.845)	
Epoch: [13][233/391]	LR: 0.0010000000000000002	Loss 0.1997 (0.2647)	Prec@1 96.094 (93.847)	
Epoch: [13][311/391]	LR: 0.0010000000000000002	Loss 0.2971 (0.2653)	Prec@1 90.625 (93.838)	
Epoch: [13][389/391]	LR: 0.0010000000000000002	Loss 0.3582 (0.2656)	Prec@1 91.406 (93.862)	
Total train loss: 0.2658

 * Prec@1 65.030 Prec@5 88.530 Loss 1.4443
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0010000000000000002	Loss 0.1895 (0.2722)	Prec@1 95.312 (93.660)	
Epoch: [14][155/391]	LR: 0.0010000000000000002	Loss 0.2094 (0.2703)	Prec@1 97.656 (93.825)	
Epoch: [14][233/391]	LR: 0.0010000000000000002	Loss 0.2382 (0.2657)	Prec@1 94.531 (93.900)	
Epoch: [14][311/391]	LR: 0.0010000000000000002	Loss 0.2013 (0.2656)	Prec@1 96.875 (93.843)	
Epoch: [14][389/391]	LR: 0.0010000000000000002	Loss 0.2112 (0.2654)	Prec@1 96.875 (93.876)	
Total train loss: 0.2655

 * Prec@1 65.150 Prec@5 88.330 Loss 1.4404
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0010000000000000002	Loss 0.3225 (0.2680)	Prec@1 90.625 (93.800)	
Epoch: [15][155/391]	LR: 0.0010000000000000002	Loss 0.3215 (0.2663)	Prec@1 91.406 (93.885)	
Epoch: [15][233/391]	LR: 0.0010000000000000002	Loss 0.2793 (0.2677)	Prec@1 92.969 (93.770)	
Epoch: [15][311/391]	LR: 0.0010000000000000002	Loss 0.2888 (0.2674)	Prec@1 93.750 (93.763)	
Epoch: [15][389/391]	LR: 0.0010000000000000002	Loss 0.2438 (0.2664)	Prec@1 93.750 (93.796)	
Total train loss: 0.2664

 * Prec@1 65.130 Prec@5 88.420 Loss 1.4375
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0010000000000000002	Loss 0.2201 (0.2576)	Prec@1 94.531 (93.990)	
Epoch: [16][155/391]	LR: 0.0010000000000000002	Loss 0.2079 (0.2605)	Prec@1 96.094 (94.091)	
Epoch: [16][233/391]	LR: 0.0010000000000000002	Loss 0.2610 (0.2600)	Prec@1 96.094 (94.137)	
Epoch: [16][311/391]	LR: 0.0010000000000000002	Loss 0.3074 (0.2610)	Prec@1 94.531 (94.076)	
Epoch: [16][389/391]	LR: 0.0010000000000000002	Loss 0.2087 (0.2625)	Prec@1 96.094 (94.065)	
Total train loss: 0.2627

 * Prec@1 64.960 Prec@5 88.390 Loss 1.4443
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0010000000000000002	Loss 0.2496 (0.2666)	Prec@1 94.531 (93.900)	
Epoch: [17][155/391]	LR: 0.0010000000000000002	Loss 0.2332 (0.2636)	Prec@1 94.531 (94.071)	
Epoch: [17][233/391]	LR: 0.0010000000000000002	Loss 0.2515 (0.2623)	Prec@1 94.531 (94.127)	
Epoch: [17][311/391]	LR: 0.0010000000000000002	Loss 0.2756 (0.2631)	Prec@1 93.750 (94.131)	
Epoch: [17][389/391]	LR: 0.0010000000000000002	Loss 0.2666 (0.2647)	Prec@1 94.531 (94.111)	
Total train loss: 0.2650

 * Prec@1 64.940 Prec@5 88.200 Loss 1.4414
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0010000000000000002	Loss 0.3098 (0.2637)	Prec@1 90.625 (93.740)	
Epoch: [18][155/391]	LR: 0.0010000000000000002	Loss 0.2206 (0.2633)	Prec@1 95.312 (93.765)	
Epoch: [18][233/391]	LR: 0.0010000000000000002	Loss 0.2642 (0.2643)	Prec@1 93.750 (93.867)	
Epoch: [18][311/391]	LR: 0.0010000000000000002	Loss 0.2939 (0.2627)	Prec@1 93.750 (93.885)	
Epoch: [18][389/391]	LR: 0.0010000000000000002	Loss 0.2460 (0.2629)	Prec@1 91.406 (93.850)	
Total train loss: 0.2632

 * Prec@1 65.070 Prec@5 88.340 Loss 1.4473
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0010000000000000002	Loss 0.2252 (0.2573)	Prec@1 97.656 (94.141)	
Epoch: [19][155/391]	LR: 0.0010000000000000002	Loss 0.2866 (0.2603)	Prec@1 92.188 (93.985)	
Epoch: [19][233/391]	LR: 0.0010000000000000002	Loss 0.1940 (0.2621)	Prec@1 96.094 (93.940)	
Epoch: [19][311/391]	LR: 0.0010000000000000002	Loss 0.2233 (0.2620)	Prec@1 96.875 (93.970)	
Epoch: [19][389/391]	LR: 0.0010000000000000002	Loss 0.2406 (0.2615)	Prec@1 94.531 (93.966)	
Total train loss: 0.2614

 * Prec@1 65.090 Prec@5 88.210 Loss 1.4502
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.00010000000000000003	Loss 0.2739 (0.2569)	Prec@1 93.750 (94.251)	
Epoch: [20][155/391]	LR: 0.00010000000000000003	Loss 0.2800 (0.2550)	Prec@1 94.531 (94.341)	
Epoch: [20][233/391]	LR: 0.00010000000000000003	Loss 0.2412 (0.2583)	Prec@1 93.750 (94.114)	
Epoch: [20][311/391]	LR: 0.00010000000000000003	Loss 0.2529 (0.2619)	Prec@1 93.750 (94.018)	
Epoch: [20][389/391]	LR: 0.00010000000000000003	Loss 0.3638 (0.2619)	Prec@1 89.844 (93.982)	
Total train loss: 0.2619

 * Prec@1 64.910 Prec@5 88.210 Loss 1.4492
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.00010000000000000003	Loss 0.2734 (0.2593)	Prec@1 92.188 (94.151)	
Epoch: [21][155/391]	LR: 0.00010000000000000003	Loss 0.2322 (0.2620)	Prec@1 96.094 (94.055)	
Epoch: [21][233/391]	LR: 0.00010000000000000003	Loss 0.2175 (0.2643)	Prec@1 97.656 (93.894)	
Epoch: [21][311/391]	LR: 0.00010000000000000003	Loss 0.3323 (0.2637)	Prec@1 92.188 (93.880)	
Epoch: [21][389/391]	LR: 0.00010000000000000003	Loss 0.3042 (0.2631)	Prec@1 91.406 (93.924)	
Total train loss: 0.2631

 * Prec@1 65.080 Prec@5 88.220 Loss 1.4346
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.00010000000000000003	Loss 0.2952 (0.2673)	Prec@1 92.969 (93.690)	
Epoch: [22][155/391]	LR: 0.00010000000000000003	Loss 0.3018 (0.2632)	Prec@1 91.406 (93.920)	
Epoch: [22][233/391]	LR: 0.00010000000000000003	Loss 0.2230 (0.2642)	Prec@1 96.875 (93.840)	
Epoch: [22][311/391]	LR: 0.00010000000000000003	Loss 0.2566 (0.2630)	Prec@1 92.969 (93.920)	
Epoch: [22][389/391]	LR: 0.00010000000000000003	Loss 0.2930 (0.2629)	Prec@1 92.188 (93.962)	
Total train loss: 0.2630

 * Prec@1 65.030 Prec@5 88.180 Loss 1.4404
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.00010000000000000003	Loss 0.3208 (0.2688)	Prec@1 92.969 (93.820)	
Epoch: [23][155/391]	LR: 0.00010000000000000003	Loss 0.2678 (0.2656)	Prec@1 93.750 (93.805)	
Epoch: [23][233/391]	LR: 0.00010000000000000003	Loss 0.2156 (0.2652)	Prec@1 94.531 (93.874)	
Epoch: [23][311/391]	LR: 0.00010000000000000003	Loss 0.2935 (0.2652)	Prec@1 91.406 (93.843)	
Epoch: [23][389/391]	LR: 0.00010000000000000003	Loss 0.2139 (0.2636)	Prec@1 95.312 (93.910)	
Total train loss: 0.2636

 * Prec@1 65.080 Prec@5 88.400 Loss 1.4473
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.00010000000000000003	Loss 0.3167 (0.2595)	Prec@1 94.531 (94.241)	
Epoch: [24][155/391]	LR: 0.00010000000000000003	Loss 0.1951 (0.2607)	Prec@1 96.094 (94.271)	
Epoch: [24][233/391]	LR: 0.00010000000000000003	Loss 0.2378 (0.2588)	Prec@1 93.750 (94.274)	
Epoch: [24][311/391]	LR: 0.00010000000000000003	Loss 0.2235 (0.2583)	Prec@1 93.750 (94.191)	
Epoch: [24][389/391]	LR: 0.00010000000000000003	Loss 0.1980 (0.2607)	Prec@1 97.656 (94.060)	
Total train loss: 0.2607

 * Prec@1 64.680 Prec@5 88.340 Loss 1.4443
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.00010000000000000003	Loss 0.2749 (0.2671)	Prec@1 91.406 (93.960)	
Epoch: [25][155/391]	LR: 0.00010000000000000003	Loss 0.3142 (0.2638)	Prec@1 92.188 (94.076)	
Epoch: [25][233/391]	LR: 0.00010000000000000003	Loss 0.2021 (0.2637)	Prec@1 96.094 (94.034)	
Epoch: [25][311/391]	LR: 0.00010000000000000003	Loss 0.2058 (0.2602)	Prec@1 96.094 (94.136)	
Epoch: [25][389/391]	LR: 0.00010000000000000003	Loss 0.2208 (0.2617)	Prec@1 96.094 (94.073)	
Total train loss: 0.2617

 * Prec@1 64.920 Prec@5 88.360 Loss 1.4453
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.00010000000000000003	Loss 0.2286 (0.2599)	Prec@1 95.312 (94.121)	
Epoch: [26][155/391]	LR: 0.00010000000000000003	Loss 0.2705 (0.2605)	Prec@1 94.531 (94.201)	
Epoch: [26][233/391]	LR: 0.00010000000000000003	Loss 0.2239 (0.2588)	Prec@1 95.312 (94.267)	
Epoch: [26][311/391]	LR: 0.00010000000000000003	Loss 0.2986 (0.2598)	Prec@1 92.969 (94.218)	
Epoch: [26][389/391]	LR: 0.00010000000000000003	Loss 0.2898 (0.2602)	Prec@1 92.188 (94.143)	
Total train loss: 0.2604

 * Prec@1 65.010 Prec@5 88.350 Loss 1.4434
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.00010000000000000003	Loss 0.2347 (0.2677)	Prec@1 94.531 (93.740)	
Epoch: [27][155/391]	LR: 0.00010000000000000003	Loss 0.2583 (0.2618)	Prec@1 94.531 (94.010)	
Epoch: [27][233/391]	LR: 0.00010000000000000003	Loss 0.3035 (0.2618)	Prec@1 92.188 (93.997)	
Epoch: [27][311/391]	LR: 0.00010000000000000003	Loss 0.2849 (0.2611)	Prec@1 92.969 (93.995)	
Epoch: [27][389/391]	LR: 0.00010000000000000003	Loss 0.2512 (0.2612)	Prec@1 96.875 (94.014)	
Total train loss: 0.2613

 * Prec@1 64.950 Prec@5 88.320 Loss 1.4473
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.00010000000000000003	Loss 0.2673 (0.2554)	Prec@1 95.312 (94.271)	
Epoch: [28][155/391]	LR: 0.00010000000000000003	Loss 0.2629 (0.2592)	Prec@1 93.750 (94.156)	
Epoch: [28][233/391]	LR: 0.00010000000000000003	Loss 0.2598 (0.2617)	Prec@1 92.969 (94.004)	
Epoch: [28][311/391]	LR: 0.00010000000000000003	Loss 0.3123 (0.2633)	Prec@1 91.406 (93.895)	
Epoch: [28][389/391]	LR: 0.00010000000000000003	Loss 0.2205 (0.2629)	Prec@1 96.875 (93.954)	
Total train loss: 0.2631

 * Prec@1 64.910 Prec@5 88.290 Loss 1.4502
Best acc: 66.350
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.00010000000000000003	Loss 0.2025 (0.2603)	Prec@1 96.094 (94.181)	
Epoch: [29][155/391]	LR: 0.00010000000000000003	Loss 0.1582 (0.2614)	Prec@1 98.438 (93.970)	
Epoch: [29][233/391]	LR: 0.00010000000000000003	Loss 0.2625 (0.2627)	Prec@1 95.312 (93.977)	
Epoch: [29][311/391]	LR: 0.00010000000000000003	Loss 0.1930 (0.2626)	Prec@1 96.875 (93.990)	
Epoch: [29][389/391]	LR: 0.00010000000000000003	Loss 0.2461 (0.2626)	Prec@1 93.750 (93.966)	
Total train loss: 0.2628

 * Prec@1 65.020 Prec@5 88.490 Loss 1.4424
Best acc: 66.350
--------------------------------------------------------------------------------
