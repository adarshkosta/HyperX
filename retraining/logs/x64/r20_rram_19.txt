
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.090 Prec@5 90.330 Loss 1.1777
Pre-trained Prec@1 with 19 layers frozen: 69.08999633789062 	 Loss: 1.177734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.001	Loss 0.4785 (0.5633)	Prec@1 89.844 (84.054)	
Epoch: [0][155/391]	LR: 0.001	Loss 0.4705 (0.5593)	Prec@1 88.281 (84.345)	
Epoch: [0][233/391]	LR: 0.001	Loss 0.6187 (0.5621)	Prec@1 83.594 (84.328)	
Epoch: [0][311/391]	LR: 0.001	Loss 0.6274 (0.5589)	Prec@1 82.031 (84.360)	
Epoch: [0][389/391]	LR: 0.001	Loss 0.5391 (0.5603)	Prec@1 82.812 (84.363)	
Total train loss: 0.5602

 * Prec@1 69.530 Prec@5 90.560 Loss 1.1562
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.001	Loss 0.4866 (0.5605)	Prec@1 89.062 (84.355)	
Epoch: [1][155/391]	LR: 0.001	Loss 0.5527 (0.5600)	Prec@1 81.250 (84.280)	
Epoch: [1][233/391]	LR: 0.001	Loss 0.5356 (0.5616)	Prec@1 85.156 (84.255)	
Epoch: [1][311/391]	LR: 0.001	Loss 0.4294 (0.5631)	Prec@1 89.844 (84.165)	
Epoch: [1][389/391]	LR: 0.001	Loss 0.4692 (0.5639)	Prec@1 90.625 (84.103)	
Total train loss: 0.5637

 * Prec@1 69.490 Prec@5 90.560 Loss 1.1582
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.001	Loss 0.5708 (0.5683)	Prec@1 81.250 (84.145)	
Epoch: [2][155/391]	LR: 0.001	Loss 0.5151 (0.5563)	Prec@1 85.156 (84.630)	
Epoch: [2][233/391]	LR: 0.001	Loss 0.4883 (0.5601)	Prec@1 87.500 (84.412)	
Epoch: [2][311/391]	LR: 0.001	Loss 0.6084 (0.5599)	Prec@1 83.594 (84.445)	
Epoch: [2][389/391]	LR: 0.001	Loss 0.6440 (0.5616)	Prec@1 82.031 (84.281)	
Total train loss: 0.5615

 * Prec@1 69.450 Prec@5 90.520 Loss 1.1572
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.001	Loss 0.5425 (0.5576)	Prec@1 84.375 (84.345)	
Epoch: [3][155/391]	LR: 0.001	Loss 0.5298 (0.5556)	Prec@1 83.594 (84.460)	
Epoch: [3][233/391]	LR: 0.001	Loss 0.5762 (0.5602)	Prec@1 82.031 (84.315)	
Epoch: [3][311/391]	LR: 0.001	Loss 0.4719 (0.5618)	Prec@1 86.719 (84.245)	
Epoch: [3][389/391]	LR: 0.001	Loss 0.5776 (0.5616)	Prec@1 84.375 (84.211)	
Total train loss: 0.5615

 * Prec@1 69.280 Prec@5 90.540 Loss 1.1592
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.001	Loss 0.6411 (0.5425)	Prec@1 85.156 (84.706)	
Epoch: [4][155/391]	LR: 0.001	Loss 0.4260 (0.5492)	Prec@1 87.500 (84.445)	
Epoch: [4][233/391]	LR: 0.001	Loss 0.5000 (0.5545)	Prec@1 84.375 (84.342)	
Epoch: [4][311/391]	LR: 0.001	Loss 0.8843 (0.5562)	Prec@1 70.312 (84.290)	
Epoch: [4][389/391]	LR: 0.001	Loss 0.5576 (0.5573)	Prec@1 82.031 (84.275)	
Total train loss: 0.5575

 * Prec@1 69.440 Prec@5 90.620 Loss 1.1562
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.6216 (0.5525)	Prec@1 83.594 (84.746)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.5220 (0.5565)	Prec@1 85.156 (84.555)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.6079 (0.5561)	Prec@1 84.375 (84.448)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.5430 (0.5533)	Prec@1 85.156 (84.448)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.5034 (0.5545)	Prec@1 88.281 (84.407)	
Total train loss: 0.5546

 * Prec@1 69.340 Prec@5 90.420 Loss 1.1621
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.0001	Loss 0.5664 (0.5567)	Prec@1 83.594 (84.435)	
Epoch: [6][155/391]	LR: 0.0001	Loss 0.6362 (0.5551)	Prec@1 85.938 (84.410)	
Epoch: [6][233/391]	LR: 0.0001	Loss 0.4744 (0.5480)	Prec@1 85.938 (84.565)	
Epoch: [6][311/391]	LR: 0.0001	Loss 0.6572 (0.5495)	Prec@1 83.594 (84.625)	
Epoch: [6][389/391]	LR: 0.0001	Loss 0.5654 (0.5538)	Prec@1 85.156 (84.509)	
Total train loss: 0.5538

 * Prec@1 69.220 Prec@5 90.510 Loss 1.1582
Best acc: 69.530
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.0001	Loss 0.6216 (0.5509)	Prec@1 82.031 (84.495)	
Epoch: [7][155/391]	LR: 0.0001	Loss 0.5229 (0.5535)	Prec@1 86.719 (84.570)	
Epoch: [7][233/391]	LR: 0.0001	Loss 0.4658 (0.5511)	Prec@1 86.719 (84.612)	
Epoch: [7][311/391]	LR: 0.0001	Loss 0.5234 (0.5492)	Prec@1 82.031 (84.618)	
Epoch: [7][389/391]	LR: 0.0001	Loss 0.4561 (0.5528)	Prec@1 87.500 (84.505)	
Total train loss: 0.5527

 * Prec@1 69.810 Prec@5 90.490 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.0001	Loss 0.6050 (0.5457)	Prec@1 81.250 (84.726)	
Epoch: [8][155/391]	LR: 0.0001	Loss 0.6084 (0.5535)	Prec@1 82.812 (84.575)	
Epoch: [8][233/391]	LR: 0.0001	Loss 0.4292 (0.5559)	Prec@1 86.719 (84.555)	
Epoch: [8][311/391]	LR: 0.0001	Loss 0.4521 (0.5537)	Prec@1 89.844 (84.610)	
Epoch: [8][389/391]	LR: 0.0001	Loss 0.6113 (0.5542)	Prec@1 84.375 (84.555)	
Total train loss: 0.5543

 * Prec@1 69.450 Prec@5 90.440 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.0001	Loss 0.6724 (0.5604)	Prec@1 81.250 (84.495)	
Epoch: [9][155/391]	LR: 0.0001	Loss 0.4839 (0.5620)	Prec@1 86.719 (84.365)	
Epoch: [9][233/391]	LR: 0.0001	Loss 0.4121 (0.5553)	Prec@1 90.625 (84.592)	
Epoch: [9][311/391]	LR: 0.0001	Loss 0.4512 (0.5531)	Prec@1 86.719 (84.653)	
Epoch: [9][389/391]	LR: 0.0001	Loss 0.6523 (0.5541)	Prec@1 81.250 (84.585)	
Total train loss: 0.5543

 * Prec@1 69.410 Prec@5 90.540 Loss 1.1592
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0001	Loss 0.6299 (0.5577)	Prec@1 82.031 (84.415)	
Epoch: [10][155/391]	LR: 0.0001	Loss 0.5381 (0.5518)	Prec@1 86.719 (84.480)	
Epoch: [10][233/391]	LR: 0.0001	Loss 0.5146 (0.5540)	Prec@1 84.375 (84.505)	
Epoch: [10][311/391]	LR: 0.0001	Loss 0.5215 (0.5564)	Prec@1 82.031 (84.410)	
Epoch: [10][389/391]	LR: 0.0001	Loss 0.4407 (0.5538)	Prec@1 86.719 (84.513)	
Total train loss: 0.5540

 * Prec@1 69.590 Prec@5 90.440 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0001	Loss 0.6382 (0.5603)	Prec@1 82.031 (84.365)	
Epoch: [11][155/391]	LR: 0.0001	Loss 0.4976 (0.5533)	Prec@1 84.375 (84.595)	
Epoch: [11][233/391]	LR: 0.0001	Loss 0.4949 (0.5485)	Prec@1 84.375 (84.709)	
Epoch: [11][311/391]	LR: 0.0001	Loss 0.5942 (0.5536)	Prec@1 83.594 (84.550)	
Epoch: [11][389/391]	LR: 0.0001	Loss 0.5176 (0.5538)	Prec@1 83.594 (84.473)	
Total train loss: 0.5540

 * Prec@1 69.740 Prec@5 90.530 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1e-05	Loss 0.6143 (0.5513)	Prec@1 84.375 (84.215)	
Epoch: [12][155/391]	LR: 1e-05	Loss 0.5122 (0.5526)	Prec@1 86.719 (84.395)	
Epoch: [12][233/391]	LR: 1e-05	Loss 0.6714 (0.5513)	Prec@1 82.031 (84.422)	
Epoch: [12][311/391]	LR: 1e-05	Loss 0.5391 (0.5538)	Prec@1 85.156 (84.433)	
Epoch: [12][389/391]	LR: 1e-05	Loss 0.6035 (0.5538)	Prec@1 78.125 (84.445)	
Total train loss: 0.5538

 * Prec@1 69.410 Prec@5 90.500 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1e-05	Loss 0.5156 (0.5546)	Prec@1 86.719 (84.625)	
Epoch: [13][155/391]	LR: 1e-05	Loss 0.7314 (0.5639)	Prec@1 78.125 (84.070)	
Epoch: [13][233/391]	LR: 1e-05	Loss 0.5669 (0.5599)	Prec@1 82.031 (84.248)	
Epoch: [13][311/391]	LR: 1e-05	Loss 0.5898 (0.5564)	Prec@1 82.812 (84.260)	
Epoch: [13][389/391]	LR: 1e-05	Loss 0.6362 (0.5561)	Prec@1 82.812 (84.247)	
Total train loss: 0.5563

 * Prec@1 69.450 Prec@5 90.440 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1e-05	Loss 0.6099 (0.5605)	Prec@1 78.906 (84.145)	
Epoch: [14][155/391]	LR: 1e-05	Loss 0.5444 (0.5563)	Prec@1 88.281 (84.260)	
Epoch: [14][233/391]	LR: 1e-05	Loss 0.5132 (0.5542)	Prec@1 82.812 (84.458)	
Epoch: [14][311/391]	LR: 1e-05	Loss 0.5688 (0.5541)	Prec@1 85.156 (84.535)	
Epoch: [14][389/391]	LR: 1e-05	Loss 0.5845 (0.5550)	Prec@1 83.594 (84.447)	
Total train loss: 0.5548

 * Prec@1 69.210 Prec@5 90.550 Loss 1.1611
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1e-05	Loss 0.4429 (0.5569)	Prec@1 85.156 (84.315)	
Epoch: [15][155/391]	LR: 1e-05	Loss 0.4934 (0.5586)	Prec@1 89.844 (84.320)	
Epoch: [15][233/391]	LR: 1e-05	Loss 0.6660 (0.5556)	Prec@1 79.688 (84.515)	
Epoch: [15][311/391]	LR: 1e-05	Loss 0.6079 (0.5532)	Prec@1 79.688 (84.598)	
Epoch: [15][389/391]	LR: 1e-05	Loss 0.5239 (0.5545)	Prec@1 84.375 (84.589)	
Total train loss: 0.5546

 * Prec@1 69.510 Prec@5 90.580 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1e-05	Loss 0.5361 (0.5505)	Prec@1 86.719 (84.395)	
Epoch: [16][155/391]	LR: 1e-05	Loss 0.5952 (0.5558)	Prec@1 80.469 (84.580)	
Epoch: [16][233/391]	LR: 1e-05	Loss 0.4858 (0.5604)	Prec@1 86.719 (84.288)	
Epoch: [16][311/391]	LR: 1e-05	Loss 0.6899 (0.5576)	Prec@1 76.562 (84.443)	
Epoch: [16][389/391]	LR: 1e-05	Loss 0.6509 (0.5563)	Prec@1 80.469 (84.417)	
Total train loss: 0.5563

 * Prec@1 69.420 Prec@5 90.500 Loss 1.1602
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1e-05	Loss 0.6157 (0.5535)	Prec@1 81.250 (84.415)	
Epoch: [17][155/391]	LR: 1e-05	Loss 0.5986 (0.5584)	Prec@1 82.812 (84.150)	
Epoch: [17][233/391]	LR: 1e-05	Loss 0.7173 (0.5587)	Prec@1 81.250 (84.225)	
Epoch: [17][311/391]	LR: 1e-05	Loss 0.5425 (0.5564)	Prec@1 82.031 (84.340)	
Epoch: [17][389/391]	LR: 1e-05	Loss 0.6948 (0.5563)	Prec@1 81.250 (84.395)	
Total train loss: 0.5562

 * Prec@1 69.470 Prec@5 90.510 Loss 1.1553
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1e-05	Loss 0.6021 (0.5546)	Prec@1 81.250 (84.806)	
Epoch: [18][155/391]	LR: 1e-05	Loss 0.5537 (0.5579)	Prec@1 84.375 (84.540)	
Epoch: [18][233/391]	LR: 1e-05	Loss 0.5361 (0.5602)	Prec@1 86.719 (84.372)	
Epoch: [18][311/391]	LR: 1e-05	Loss 0.5889 (0.5580)	Prec@1 80.469 (84.347)	
Epoch: [18][389/391]	LR: 1e-05	Loss 0.4541 (0.5565)	Prec@1 88.281 (84.423)	
Total train loss: 0.5563

 * Prec@1 69.580 Prec@5 90.590 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1e-05	Loss 0.5693 (0.5676)	Prec@1 81.250 (83.874)	
Epoch: [19][155/391]	LR: 1e-05	Loss 0.4429 (0.5541)	Prec@1 86.719 (84.470)	
Epoch: [19][233/391]	LR: 1e-05	Loss 0.5400 (0.5552)	Prec@1 88.281 (84.395)	
Epoch: [19][311/391]	LR: 1e-05	Loss 0.6128 (0.5573)	Prec@1 82.031 (84.347)	
Epoch: [19][389/391]	LR: 1e-05	Loss 0.6133 (0.5528)	Prec@1 82.812 (84.443)	
Total train loss: 0.5529

 * Prec@1 69.360 Prec@5 90.530 Loss 1.1582
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-06	Loss 0.6987 (0.5602)	Prec@1 78.906 (84.415)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-06	Loss 0.4587 (0.5594)	Prec@1 87.500 (84.615)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-06	Loss 0.4575 (0.5599)	Prec@1 88.281 (84.468)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-06	Loss 0.6509 (0.5569)	Prec@1 82.031 (84.503)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-06	Loss 0.5142 (0.5545)	Prec@1 82.031 (84.459)	
Total train loss: 0.5546

 * Prec@1 69.360 Prec@5 90.530 Loss 1.1602
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-06	Loss 0.5034 (0.5507)	Prec@1 87.500 (84.645)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-06	Loss 0.5024 (0.5561)	Prec@1 86.719 (84.480)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-06	Loss 0.5264 (0.5500)	Prec@1 86.719 (84.599)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-06	Loss 0.6333 (0.5522)	Prec@1 82.812 (84.530)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-06	Loss 0.5151 (0.5533)	Prec@1 84.375 (84.537)	
Total train loss: 0.5535

 * Prec@1 69.770 Prec@5 90.640 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-06	Loss 0.5190 (0.5532)	Prec@1 85.156 (84.355)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-06	Loss 0.5366 (0.5643)	Prec@1 83.594 (84.054)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-06	Loss 0.4543 (0.5615)	Prec@1 85.156 (84.115)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-06	Loss 0.5278 (0.5595)	Prec@1 87.500 (84.232)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-06	Loss 0.5952 (0.5566)	Prec@1 78.125 (84.333)	
Total train loss: 0.5568

 * Prec@1 69.500 Prec@5 90.340 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-06	Loss 0.5698 (0.5607)	Prec@1 85.938 (84.445)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-06	Loss 0.6074 (0.5600)	Prec@1 82.031 (84.265)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-06	Loss 0.3594 (0.5587)	Prec@1 89.844 (84.338)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.5562)	Prec@1 85.938 (84.370)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-06	Loss 0.4814 (0.5549)	Prec@1 87.500 (84.403)	
Total train loss: 0.5546

 * Prec@1 69.310 Prec@5 90.670 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-06	Loss 0.6450 (0.5693)	Prec@1 80.469 (84.265)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-06	Loss 0.6255 (0.5597)	Prec@1 83.594 (84.470)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-06	Loss 0.4490 (0.5548)	Prec@1 89.844 (84.489)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-06	Loss 0.5527 (0.5522)	Prec@1 83.594 (84.545)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-06	Loss 0.5801 (0.5533)	Prec@1 82.812 (84.515)	
Total train loss: 0.5534

 * Prec@1 69.320 Prec@5 90.390 Loss 1.1611
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-06	Loss 0.5684 (0.5505)	Prec@1 81.250 (84.475)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-06	Loss 0.5615 (0.5487)	Prec@1 79.688 (84.450)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-06	Loss 0.5879 (0.5518)	Prec@1 81.250 (84.352)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-06	Loss 0.5552 (0.5548)	Prec@1 85.156 (84.317)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-06	Loss 0.4841 (0.5539)	Prec@1 84.375 (84.397)	
Total train loss: 0.5538

 * Prec@1 69.380 Prec@5 90.520 Loss 1.1592
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-06	Loss 0.6504 (0.5536)	Prec@1 80.469 (84.635)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-06	Loss 0.5376 (0.5579)	Prec@1 83.594 (84.340)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-06	Loss 0.5259 (0.5617)	Prec@1 87.500 (84.275)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-06	Loss 0.6621 (0.5600)	Prec@1 78.125 (84.215)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-06	Loss 0.4475 (0.5570)	Prec@1 87.500 (84.323)	
Total train loss: 0.5569

 * Prec@1 69.460 Prec@5 90.550 Loss 1.1582
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-06	Loss 0.5811 (0.5661)	Prec@1 86.719 (83.704)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-06	Loss 0.5796 (0.5566)	Prec@1 82.812 (84.160)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-06	Loss 0.5029 (0.5517)	Prec@1 88.281 (84.505)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-06	Loss 0.5713 (0.5531)	Prec@1 86.719 (84.445)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-06	Loss 0.5679 (0.5538)	Prec@1 85.156 (84.427)	
Total train loss: 0.5540

 * Prec@1 69.370 Prec@5 90.520 Loss 1.1592
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-06	Loss 0.6152 (0.5598)	Prec@1 79.688 (84.195)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-06	Loss 0.6309 (0.5581)	Prec@1 81.250 (84.405)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-06	Loss 0.6069 (0.5579)	Prec@1 87.500 (84.428)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-06	Loss 0.6367 (0.5555)	Prec@1 82.812 (84.495)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-06	Loss 0.5400 (0.5553)	Prec@1 80.469 (84.447)	
Total train loss: 0.5555

 * Prec@1 69.440 Prec@5 90.640 Loss 1.1572
Best acc: 69.810
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-06	Loss 0.5161 (0.5507)	Prec@1 86.719 (84.585)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-06	Loss 0.5444 (0.5570)	Prec@1 82.031 (84.425)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-06	Loss 0.5376 (0.5560)	Prec@1 84.375 (84.615)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-06	Loss 0.6519 (0.5504)	Prec@1 79.688 (84.683)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-06	Loss 0.6001 (0.5526)	Prec@1 83.594 (84.641)	
Total train loss: 0.5528

 * Prec@1 69.410 Prec@5 90.620 Loss 1.1562
Best acc: 69.810
--------------------------------------------------------------------------------
