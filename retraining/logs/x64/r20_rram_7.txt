
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.330 Prec@5 90.450 Loss 1.1748
Pre-trained Prec@1 with 7 layers frozen: 69.33000183105469 	 Loss: 1.1748046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.5439 (0.5500)	Prec@1 85.938 (84.495)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.4768 (0.5599)	Prec@1 87.500 (84.215)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.5425 (0.5606)	Prec@1 85.938 (84.121)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.5415 (0.5647)	Prec@1 82.031 (83.977)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5527 (0.5655)	Prec@1 83.594 (83.918)	
Total train loss: 0.5654

 * Prec@1 69.370 Prec@5 90.660 Loss 1.1641
Best acc: 69.370
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.6221 (0.5588)	Prec@1 84.375 (84.545)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.5347 (0.5593)	Prec@1 86.719 (84.465)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5332 (0.5645)	Prec@1 86.719 (84.325)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.5762 (0.5634)	Prec@1 83.594 (84.310)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.6797 (0.5638)	Prec@1 82.031 (84.203)	
Total train loss: 0.5641

 * Prec@1 69.120 Prec@5 90.600 Loss 1.1758
Best acc: 69.370
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.6362 (0.5633)	Prec@1 83.594 (84.575)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.5859 (0.5653)	Prec@1 82.031 (84.185)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.5571 (0.5655)	Prec@1 84.375 (83.991)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.5288 (0.5627)	Prec@1 86.719 (84.047)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.5063 (0.5621)	Prec@1 90.625 (84.163)	
Total train loss: 0.5622

 * Prec@1 69.390 Prec@5 90.620 Loss 1.1631
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.5688 (0.5645)	Prec@1 85.938 (84.385)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.6816 (0.5625)	Prec@1 82.812 (84.315)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.6597 (0.5606)	Prec@1 81.250 (84.378)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5088 (0.5575)	Prec@1 89.062 (84.433)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.5283 (0.5601)	Prec@1 84.375 (84.353)	
Total train loss: 0.5600

 * Prec@1 69.300 Prec@5 90.700 Loss 1.1699
Best acc: 69.390
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.4001 (0.5529)	Prec@1 92.188 (84.736)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.5142 (0.5597)	Prec@1 83.594 (84.475)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.5957 (0.5560)	Prec@1 79.688 (84.522)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.6558 (0.5584)	Prec@1 82.812 (84.375)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.5439 (0.5605)	Prec@1 85.938 (84.285)	
Total train loss: 0.5607

 * Prec@1 69.480 Prec@5 90.550 Loss 1.1641
Best acc: 69.480
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.5728 (0.5530)	Prec@1 85.156 (84.635)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.6719 (0.5579)	Prec@1 80.469 (84.475)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.6377 (0.5556)	Prec@1 80.469 (84.565)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.6074 (0.5585)	Prec@1 81.250 (84.425)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.5278 (0.5587)	Prec@1 87.500 (84.351)	
Total train loss: 0.5588

 * Prec@1 69.170 Prec@5 90.590 Loss 1.1660
Best acc: 69.480
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.6172 (0.5576)	Prec@1 84.375 (84.325)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.5127 (0.5554)	Prec@1 84.375 (84.425)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.6006 (0.5531)	Prec@1 82.812 (84.482)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.6313 (0.5551)	Prec@1 81.250 (84.435)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.4653 (0.5559)	Prec@1 87.500 (84.443)	
Total train loss: 0.5559

 * Prec@1 69.350 Prec@5 90.650 Loss 1.1602
Best acc: 69.480
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.6025 (0.5615)	Prec@1 81.250 (84.285)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.6484 (0.5596)	Prec@1 81.250 (84.390)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.5039 (0.5581)	Prec@1 88.281 (84.425)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5928 (0.5579)	Prec@1 82.031 (84.400)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.5493 (0.5567)	Prec@1 85.938 (84.433)	
Total train loss: 0.5568

 * Prec@1 69.550 Prec@5 90.610 Loss 1.1680
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.5698 (0.5421)	Prec@1 80.469 (85.136)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.5386 (0.5453)	Prec@1 87.500 (84.841)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.5410 (0.5504)	Prec@1 82.031 (84.519)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.4893 (0.5539)	Prec@1 86.719 (84.388)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.4353 (0.5560)	Prec@1 89.062 (84.391)	
Total train loss: 0.5560

 * Prec@1 69.440 Prec@5 90.600 Loss 1.1670
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.5269 (0.5537)	Prec@1 84.375 (84.515)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.5488 (0.5552)	Prec@1 85.156 (84.540)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.5430 (0.5562)	Prec@1 83.594 (84.395)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.5132 (0.5599)	Prec@1 87.500 (84.200)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.6118 (0.5577)	Prec@1 82.031 (84.269)	
Total train loss: 0.5576

 * Prec@1 69.270 Prec@5 90.690 Loss 1.1680
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.4746 (0.5558)	Prec@1 89.062 (84.475)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.5566 (0.5568)	Prec@1 85.938 (84.320)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.4368 (0.5567)	Prec@1 88.281 (84.292)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.5005 (0.5564)	Prec@1 86.719 (84.270)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.6445 (0.5536)	Prec@1 82.031 (84.349)	
Total train loss: 0.5537

 * Prec@1 69.450 Prec@5 90.680 Loss 1.1699
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.5366 (0.5722)	Prec@1 83.594 (84.024)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.5146 (0.5633)	Prec@1 87.500 (84.285)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.4834 (0.5642)	Prec@1 90.625 (84.282)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.5005 (0.5581)	Prec@1 81.250 (84.460)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5474 (0.5581)	Prec@1 85.156 (84.455)	
Total train loss: 0.5582

 * Prec@1 69.400 Prec@5 90.600 Loss 1.1641
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.5396 (0.5697)	Prec@1 81.250 (83.764)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.6416 (0.5660)	Prec@1 85.938 (84.115)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.5039 (0.5593)	Prec@1 84.375 (84.352)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.5645 (0.5573)	Prec@1 84.375 (84.380)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.4080 (0.5568)	Prec@1 90.625 (84.453)	
Total train loss: 0.5568

 * Prec@1 69.440 Prec@5 90.600 Loss 1.1650
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.6436 (0.5699)	Prec@1 83.594 (83.974)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.4780 (0.5605)	Prec@1 88.281 (84.300)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.6113 (0.5565)	Prec@1 78.906 (84.478)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.5073 (0.5561)	Prec@1 85.938 (84.530)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.6094 (0.5571)	Prec@1 82.812 (84.505)	
Total train loss: 0.5573

 * Prec@1 69.550 Prec@5 90.590 Loss 1.1699
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.5713 (0.5608)	Prec@1 86.719 (84.325)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.3643 (0.5641)	Prec@1 94.531 (84.150)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.5176 (0.5649)	Prec@1 85.938 (84.181)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.5449 (0.5617)	Prec@1 83.594 (84.187)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.4661 (0.5594)	Prec@1 86.719 (84.279)	
Total train loss: 0.5593

 * Prec@1 69.110 Prec@5 90.620 Loss 1.1650
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.6289 (0.5681)	Prec@1 82.031 (84.115)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.7686 (0.5585)	Prec@1 79.688 (84.525)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.5469 (0.5602)	Prec@1 82.812 (84.368)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.5561)	Prec@1 83.594 (84.523)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.5029 (0.5540)	Prec@1 87.500 (84.529)	
Total train loss: 0.5540

 * Prec@1 69.340 Prec@5 90.600 Loss 1.1650
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.6016 (0.5517)	Prec@1 79.688 (84.425)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.5840 (0.5611)	Prec@1 81.250 (84.235)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.4905 (0.5558)	Prec@1 87.500 (84.382)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.6694 (0.5545)	Prec@1 78.125 (84.435)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.7217 (0.5553)	Prec@1 75.000 (84.347)	
Total train loss: 0.5555

 * Prec@1 69.510 Prec@5 90.570 Loss 1.1611
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.4927 (0.5509)	Prec@1 87.500 (84.415)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.4258 (0.5580)	Prec@1 89.062 (84.235)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.7383 (0.5553)	Prec@1 78.906 (84.335)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.5107 (0.5558)	Prec@1 82.812 (84.305)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.6118 (0.5559)	Prec@1 82.031 (84.391)	
Total train loss: 0.5561

 * Prec@1 69.430 Prec@5 90.570 Loss 1.1689
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.7065 (0.5533)	Prec@1 79.688 (84.515)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.6450 (0.5555)	Prec@1 82.812 (84.395)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.5303 (0.5537)	Prec@1 83.594 (84.422)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.6172 (0.5537)	Prec@1 78.906 (84.403)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.4744 (0.5537)	Prec@1 87.500 (84.371)	
Total train loss: 0.5536

 * Prec@1 69.400 Prec@5 90.630 Loss 1.1602
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.6406 (0.5492)	Prec@1 78.906 (84.665)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.4514 (0.5543)	Prec@1 89.062 (84.345)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.5884 (0.5572)	Prec@1 82.812 (84.282)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.4653 (0.5579)	Prec@1 85.938 (84.232)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.5078 (0.5547)	Prec@1 84.375 (84.293)	
Total train loss: 0.5547

 * Prec@1 69.220 Prec@5 90.690 Loss 1.1650
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.5972 (0.5494)	Prec@1 82.031 (84.746)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.5991 (0.5539)	Prec@1 81.250 (84.480)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.5762 (0.5560)	Prec@1 83.594 (84.375)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.7256 (0.5582)	Prec@1 79.688 (84.292)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.6304 (0.5561)	Prec@1 79.688 (84.401)	
Total train loss: 0.5562

 * Prec@1 69.320 Prec@5 90.510 Loss 1.1660
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.6533 (0.5682)	Prec@1 78.906 (83.904)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.5049 (0.5576)	Prec@1 84.375 (84.355)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5459 (0.5558)	Prec@1 83.594 (84.272)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.5420 (0.5545)	Prec@1 85.156 (84.383)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.4631 (0.5552)	Prec@1 89.844 (84.453)	
Total train loss: 0.5551

 * Prec@1 69.340 Prec@5 90.710 Loss 1.1660
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.6201 (0.5538)	Prec@1 84.375 (84.565)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.6768 (0.5581)	Prec@1 83.594 (84.385)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.5649 (0.5584)	Prec@1 82.031 (84.358)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.5293 (0.5593)	Prec@1 85.156 (84.257)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.6211 (0.5581)	Prec@1 81.250 (84.221)	
Total train loss: 0.5583

 * Prec@1 69.250 Prec@5 90.570 Loss 1.1650
Best acc: 69.550
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.6646 (0.5593)	Prec@1 82.812 (84.145)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.6133 (0.5570)	Prec@1 84.375 (84.205)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.6318 (0.5550)	Prec@1 85.156 (84.382)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.4805 (0.5551)	Prec@1 85.156 (84.370)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.5513 (0.5538)	Prec@1 83.594 (84.479)	
Total train loss: 0.5543

 * Prec@1 69.590 Prec@5 90.500 Loss 1.1631
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.5806 (0.5434)	Prec@1 85.938 (84.756)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.5879 (0.5507)	Prec@1 84.375 (84.680)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5518 (0.5556)	Prec@1 85.156 (84.432)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.4836 (0.5556)	Prec@1 86.719 (84.425)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5991 (0.5561)	Prec@1 84.375 (84.407)	
Total train loss: 0.5562

 * Prec@1 69.330 Prec@5 90.500 Loss 1.1689
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.5151 (0.5545)	Prec@1 89.062 (84.365)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.6030 (0.5587)	Prec@1 85.156 (84.195)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.4307 (0.5599)	Prec@1 89.062 (84.151)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.4597 (0.5560)	Prec@1 85.938 (84.370)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.4978 (0.5567)	Prec@1 87.500 (84.357)	
Total train loss: 0.5566

 * Prec@1 69.280 Prec@5 90.510 Loss 1.1670
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.6211 (0.5573)	Prec@1 84.375 (84.305)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.5918 (0.5582)	Prec@1 85.156 (84.415)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.5737 (0.5598)	Prec@1 85.938 (84.285)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.5728 (0.5585)	Prec@1 82.812 (84.342)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.5205 (0.5564)	Prec@1 81.250 (84.379)	
Total train loss: 0.5563

 * Prec@1 69.440 Prec@5 90.730 Loss 1.1641
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.4578 (0.5525)	Prec@1 89.062 (84.465)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.6626 (0.5510)	Prec@1 83.594 (84.575)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.6030 (0.5554)	Prec@1 81.250 (84.328)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.5503 (0.5524)	Prec@1 87.500 (84.485)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.4573 (0.5549)	Prec@1 85.938 (84.375)	
Total train loss: 0.5551

 * Prec@1 69.330 Prec@5 90.480 Loss 1.1680
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.6631 (0.5511)	Prec@1 83.594 (84.445)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.5635 (0.5597)	Prec@1 85.156 (84.360)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.5518 (0.5569)	Prec@1 84.375 (84.355)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.6489 (0.5585)	Prec@1 82.031 (84.322)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.5693 (0.5584)	Prec@1 85.938 (84.267)	
Total train loss: 0.5585

 * Prec@1 69.360 Prec@5 90.620 Loss 1.1641
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.4897 (0.5593)	Prec@1 85.156 (84.044)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.5029 (0.5589)	Prec@1 83.594 (84.049)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.5664 (0.5521)	Prec@1 86.719 (84.282)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.5181 (0.5531)	Prec@1 84.375 (84.388)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5088 (0.5520)	Prec@1 84.375 (84.499)	
Total train loss: 0.5525

 * Prec@1 69.530 Prec@5 90.680 Loss 1.1670
Best acc: 69.590
--------------------------------------------------------------------------------
