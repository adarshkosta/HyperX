
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.310 Prec@5 90.430 Loss 1.1748
Pre-trained Prec@1 with 5 layers frozen: 69.30999755859375 	 Loss: 1.1748046875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.5039 (0.5554)	Prec@1 82.031 (84.706)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.6279 (0.5644)	Prec@1 80.469 (84.075)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.4622 (0.5652)	Prec@1 87.500 (84.098)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.5659 (0.5650)	Prec@1 85.156 (84.057)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5161 (0.5651)	Prec@1 86.719 (84.143)	
Total train loss: 0.5650

 * Prec@1 69.200 Prec@5 90.540 Loss 1.1660
Best acc: 69.200
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.5542 (0.5544)	Prec@1 84.375 (84.034)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.5850 (0.5565)	Prec@1 80.469 (84.095)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5474 (0.5586)	Prec@1 84.375 (84.201)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.4741 (0.5603)	Prec@1 86.719 (84.147)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.5269 (0.5615)	Prec@1 88.281 (84.155)	
Total train loss: 0.5618

 * Prec@1 69.540 Prec@5 90.630 Loss 1.1641
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.5469 (0.5544)	Prec@1 82.812 (84.335)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.5518 (0.5634)	Prec@1 84.375 (83.974)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.5981 (0.5592)	Prec@1 83.594 (84.054)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.5474 (0.5601)	Prec@1 86.719 (84.102)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.6787 (0.5610)	Prec@1 80.469 (84.095)	
Total train loss: 0.5609

 * Prec@1 69.390 Prec@5 90.580 Loss 1.1660
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.5747 (0.5631)	Prec@1 82.812 (84.115)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.5049 (0.5612)	Prec@1 90.625 (84.255)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.5352 (0.5575)	Prec@1 83.594 (84.285)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5332 (0.5615)	Prec@1 81.250 (84.120)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.5845 (0.5607)	Prec@1 82.812 (84.183)	
Total train loss: 0.5609

 * Prec@1 69.440 Prec@5 90.740 Loss 1.1611
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.4668 (0.5482)	Prec@1 85.938 (84.215)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.3420 (0.5480)	Prec@1 89.062 (84.325)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.4888 (0.5550)	Prec@1 85.156 (84.248)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.4387 (0.5547)	Prec@1 91.406 (84.307)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.6113 (0.5555)	Prec@1 81.250 (84.291)	
Total train loss: 0.5557

 * Prec@1 69.190 Prec@5 90.570 Loss 1.1670
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.5083 (0.5524)	Prec@1 87.500 (84.495)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.3689 (0.5491)	Prec@1 90.625 (84.605)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.5176 (0.5585)	Prec@1 84.375 (84.425)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.7085 (0.5546)	Prec@1 80.469 (84.535)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.5107 (0.5577)	Prec@1 85.156 (84.347)	
Total train loss: 0.5576

 * Prec@1 69.350 Prec@5 90.610 Loss 1.1631
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.4534 (0.5494)	Prec@1 88.281 (84.445)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.6143 (0.5510)	Prec@1 87.500 (84.610)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.5142 (0.5514)	Prec@1 85.156 (84.605)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.3269 (0.5524)	Prec@1 92.188 (84.510)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.6787 (0.5544)	Prec@1 78.906 (84.461)	
Total train loss: 0.5547

 * Prec@1 69.470 Prec@5 90.660 Loss 1.1650
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.4260 (0.5459)	Prec@1 89.062 (84.645)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.4836 (0.5472)	Prec@1 86.719 (84.575)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.5522 (0.5515)	Prec@1 78.906 (84.345)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5776 (0.5540)	Prec@1 85.156 (84.242)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.5767 (0.5553)	Prec@1 82.031 (84.289)	
Total train loss: 0.5552

 * Prec@1 69.350 Prec@5 90.400 Loss 1.1729
Best acc: 69.540
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.5410 (0.5520)	Prec@1 84.375 (84.395)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.5977 (0.5516)	Prec@1 84.375 (84.455)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.7197 (0.5539)	Prec@1 78.906 (84.415)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.5791 (0.5550)	Prec@1 85.156 (84.372)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.5371 (0.5542)	Prec@1 85.156 (84.421)	
Total train loss: 0.5545

 * Prec@1 69.670 Prec@5 90.700 Loss 1.1650
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.5557 (0.5535)	Prec@1 84.375 (84.876)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.4468 (0.5505)	Prec@1 87.500 (84.630)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.6836 (0.5537)	Prec@1 81.250 (84.489)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.6733 (0.5555)	Prec@1 80.469 (84.350)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.5854 (0.5539)	Prec@1 86.719 (84.397)	
Total train loss: 0.5540

 * Prec@1 69.510 Prec@5 90.620 Loss 1.1641
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.5659 (0.5495)	Prec@1 85.156 (84.726)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.4888 (0.5522)	Prec@1 86.719 (84.510)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.6855 (0.5571)	Prec@1 81.250 (84.378)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.5537 (0.5563)	Prec@1 82.812 (84.395)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.6094 (0.5559)	Prec@1 85.156 (84.495)	
Total train loss: 0.5561

 * Prec@1 69.420 Prec@5 90.590 Loss 1.1680
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.5430 (0.5600)	Prec@1 87.500 (84.195)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.4658 (0.5617)	Prec@1 87.500 (84.125)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.4880 (0.5616)	Prec@1 85.156 (84.091)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.5059 (0.5583)	Prec@1 86.719 (84.277)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5581 (0.5559)	Prec@1 82.812 (84.403)	
Total train loss: 0.5563

 * Prec@1 69.270 Prec@5 90.550 Loss 1.1660
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.4802 (0.5557)	Prec@1 86.719 (84.435)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.6045 (0.5581)	Prec@1 85.156 (84.295)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.6221 (0.5525)	Prec@1 80.469 (84.405)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.6050 (0.5497)	Prec@1 81.250 (84.435)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.5527 (0.5528)	Prec@1 82.812 (84.391)	
Total train loss: 0.5532

 * Prec@1 69.470 Prec@5 90.680 Loss 1.1670
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.5283 (0.5597)	Prec@1 85.938 (84.205)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.6406 (0.5536)	Prec@1 79.688 (84.395)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.5581 (0.5506)	Prec@1 82.812 (84.468)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.5942 (0.5511)	Prec@1 85.156 (84.503)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.4800 (0.5533)	Prec@1 87.500 (84.429)	
Total train loss: 0.5535

 * Prec@1 69.390 Prec@5 90.580 Loss 1.1699
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.6284 (0.5682)	Prec@1 79.688 (83.954)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.6094 (0.5636)	Prec@1 81.250 (84.125)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.5186 (0.5620)	Prec@1 85.938 (84.121)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.6362 (0.5597)	Prec@1 81.250 (84.240)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.5654 (0.5560)	Prec@1 83.594 (84.325)	
Total train loss: 0.5561

 * Prec@1 69.470 Prec@5 90.560 Loss 1.1660
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.6250 (0.5578)	Prec@1 83.594 (84.275)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.6069 (0.5634)	Prec@1 82.031 (84.145)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.5654 (0.5613)	Prec@1 84.375 (84.188)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.5112 (0.5583)	Prec@1 88.281 (84.252)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.5649 (0.5580)	Prec@1 82.812 (84.263)	
Total train loss: 0.5582

 * Prec@1 69.510 Prec@5 90.450 Loss 1.1631
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.6953 (0.5735)	Prec@1 81.250 (83.994)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.6309 (0.5597)	Prec@1 84.375 (84.210)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.5596 (0.5611)	Prec@1 82.812 (84.282)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.6084 (0.5585)	Prec@1 82.812 (84.290)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.4812 (0.5576)	Prec@1 87.500 (84.277)	
Total train loss: 0.5580

 * Prec@1 69.310 Prec@5 90.510 Loss 1.1709
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.5122 (0.5436)	Prec@1 85.938 (84.806)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.5977 (0.5551)	Prec@1 82.031 (84.390)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.7021 (0.5608)	Prec@1 78.906 (84.225)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.4878 (0.5574)	Prec@1 85.938 (84.370)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.4917 (0.5569)	Prec@1 84.375 (84.315)	
Total train loss: 0.5570

 * Prec@1 69.250 Prec@5 90.540 Loss 1.1611
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.5396 (0.5382)	Prec@1 85.938 (84.675)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.4778 (0.5443)	Prec@1 86.719 (84.746)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.7393 (0.5552)	Prec@1 78.906 (84.345)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.5737 (0.5559)	Prec@1 79.688 (84.385)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.5137 (0.5564)	Prec@1 87.500 (84.325)	
Total train loss: 0.5567

 * Prec@1 69.360 Prec@5 90.740 Loss 1.1631
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.4700 (0.5340)	Prec@1 85.938 (85.176)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.5312 (0.5434)	Prec@1 85.938 (84.946)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.6221 (0.5495)	Prec@1 84.375 (84.752)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.5669 (0.5519)	Prec@1 81.250 (84.753)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.6304 (0.5525)	Prec@1 78.906 (84.647)	
Total train loss: 0.5526

 * Prec@1 69.210 Prec@5 90.690 Loss 1.1689
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.6406 (0.5588)	Prec@1 78.906 (84.345)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.5928 (0.5516)	Prec@1 79.688 (84.575)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.6865 (0.5524)	Prec@1 75.781 (84.505)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.4512 (0.5530)	Prec@1 85.156 (84.530)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.4607 (0.5536)	Prec@1 89.844 (84.525)	
Total train loss: 0.5538

 * Prec@1 69.470 Prec@5 90.610 Loss 1.1611
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.6138 (0.5676)	Prec@1 79.688 (83.784)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.4365 (0.5518)	Prec@1 88.281 (84.465)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5723 (0.5503)	Prec@1 82.812 (84.478)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.4617 (0.5550)	Prec@1 86.719 (84.330)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.4888 (0.5558)	Prec@1 87.500 (84.381)	
Total train loss: 0.5560

 * Prec@1 69.440 Prec@5 90.620 Loss 1.1631
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.4597 (0.5485)	Prec@1 87.500 (84.515)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.5356 (0.5494)	Prec@1 82.812 (84.370)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.4797 (0.5467)	Prec@1 85.938 (84.545)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.4612 (0.5510)	Prec@1 85.156 (84.418)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.5806 (0.5523)	Prec@1 82.031 (84.441)	
Total train loss: 0.5522

 * Prec@1 69.340 Prec@5 90.670 Loss 1.1719
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.5181 (0.5610)	Prec@1 88.281 (84.535)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.6387 (0.5586)	Prec@1 83.594 (84.465)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.5620 (0.5604)	Prec@1 85.938 (84.258)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.6196 (0.5569)	Prec@1 85.156 (84.405)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.6836 (0.5544)	Prec@1 80.469 (84.451)	
Total train loss: 0.5545

 * Prec@1 69.320 Prec@5 90.460 Loss 1.1699
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.5615 (0.5523)	Prec@1 83.594 (84.145)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.4995 (0.5547)	Prec@1 87.500 (84.395)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5635 (0.5534)	Prec@1 85.156 (84.465)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.4741 (0.5551)	Prec@1 86.719 (84.400)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5620 (0.5557)	Prec@1 82.031 (84.393)	
Total train loss: 0.5559

 * Prec@1 69.160 Prec@5 90.650 Loss 1.1670
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.4246 (0.5503)	Prec@1 89.844 (84.535)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.5547 (0.5526)	Prec@1 86.719 (84.385)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.4978 (0.5544)	Prec@1 82.812 (84.342)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.5200 (0.5531)	Prec@1 87.500 (84.460)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.6650 (0.5550)	Prec@1 75.781 (84.347)	
Total train loss: 0.5550

 * Prec@1 69.380 Prec@5 90.680 Loss 1.1660
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.5742 (0.5463)	Prec@1 85.156 (84.625)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.4458 (0.5487)	Prec@1 86.719 (84.791)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.6274 (0.5523)	Prec@1 81.250 (84.532)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.5635 (0.5513)	Prec@1 85.156 (84.535)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.6646 (0.5528)	Prec@1 79.688 (84.503)	
Total train loss: 0.5531

 * Prec@1 69.390 Prec@5 90.550 Loss 1.1660
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.5918 (0.5611)	Prec@1 85.156 (84.425)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.4934 (0.5624)	Prec@1 88.281 (84.245)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.4963 (0.5601)	Prec@1 85.156 (84.215)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.6055 (0.5593)	Prec@1 85.156 (84.325)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.5649 (0.5560)	Prec@1 83.594 (84.423)	
Total train loss: 0.5561

 * Prec@1 69.280 Prec@5 90.560 Loss 1.1641
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5352)	Prec@1 85.938 (85.046)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.4807 (0.5417)	Prec@1 89.844 (84.941)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.4817 (0.5492)	Prec@1 88.281 (84.752)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.7119 (0.5539)	Prec@1 82.031 (84.588)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.7285 (0.5537)	Prec@1 81.250 (84.473)	
Total train loss: 0.5540

 * Prec@1 69.400 Prec@5 90.480 Loss 1.1660
Best acc: 69.670
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.4751 (0.5454)	Prec@1 85.938 (84.615)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.6113 (0.5565)	Prec@1 82.812 (84.460)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.5063 (0.5569)	Prec@1 84.375 (84.268)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.5845 (0.5581)	Prec@1 80.469 (84.182)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5327 (0.5558)	Prec@1 83.594 (84.233)	
Total train loss: 0.5557

 * Prec@1 69.390 Prec@5 90.670 Loss 1.1631
Best acc: 69.670
--------------------------------------------------------------------------------
