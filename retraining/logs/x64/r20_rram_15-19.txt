
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.060 Prec@5 90.310 Loss 1.1777
Pre-trained Prec@1 with 15 layers frozen: 69.05999755859375 	 Loss: 1.177734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.1	Loss 1.1777 (1.3331)	Prec@1 62.500 (62.810)	
Epoch: [0][155/391]	LR: 0.1	Loss 1.2402 (1.3207)	Prec@1 59.375 (62.535)	
Epoch: [0][233/391]	LR: 0.1	Loss 0.9668 (1.2883)	Prec@1 71.094 (63.355)	
Epoch: [0][311/391]	LR: 0.1	Loss 1.0996 (1.2613)	Prec@1 64.844 (63.962)	
Epoch: [0][389/391]	LR: 0.1	Loss 1.2383 (1.2491)	Prec@1 62.500 (64.143)	
Total train loss: 1.2485

 * Prec@1 57.650 Prec@5 85.280 Loss 1.6289
Best acc: 57.650
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.1	Loss 1.1934 (0.9995)	Prec@1 64.844 (70.613)	
Epoch: [1][155/391]	LR: 0.1	Loss 1.0430 (1.0113)	Prec@1 69.531 (70.237)	
Epoch: [1][233/391]	LR: 0.1	Loss 0.8936 (1.0125)	Prec@1 76.562 (70.152)	
Epoch: [1][311/391]	LR: 0.1	Loss 1.2002 (1.0209)	Prec@1 64.844 (70.037)	
Epoch: [1][389/391]	LR: 0.1	Loss 1.0156 (1.0181)	Prec@1 70.312 (70.028)	
Total train loss: 1.0182

 * Prec@1 60.280 Prec@5 86.370 Loss 1.5576
Best acc: 60.280
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.1	Loss 0.8696 (0.8645)	Prec@1 71.094 (74.008)	
Epoch: [2][155/391]	LR: 0.1	Loss 0.9229 (0.8884)	Prec@1 70.312 (73.387)	
Epoch: [2][233/391]	LR: 0.1	Loss 0.8477 (0.8917)	Prec@1 74.219 (73.274)	
Epoch: [2][311/391]	LR: 0.1	Loss 0.8159 (0.8922)	Prec@1 75.000 (73.270)	
Epoch: [2][389/391]	LR: 0.1	Loss 0.7661 (0.9020)	Prec@1 75.000 (73.001)	
Total train loss: 0.9020

 * Prec@1 61.020 Prec@5 86.830 Loss 1.5234
Best acc: 61.020
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.1	Loss 0.6504 (0.7884)	Prec@1 78.906 (76.352)	
Epoch: [3][155/391]	LR: 0.1	Loss 0.8623 (0.7995)	Prec@1 75.000 (76.012)	
Epoch: [3][233/391]	LR: 0.1	Loss 0.6597 (0.8177)	Prec@1 76.562 (75.404)	
Epoch: [3][311/391]	LR: 0.1	Loss 0.9287 (0.8242)	Prec@1 71.875 (75.218)	
Epoch: [3][389/391]	LR: 0.1	Loss 1.1064 (0.8341)	Prec@1 68.750 (74.844)	
Total train loss: 0.8340

 * Prec@1 61.440 Prec@5 87.160 Loss 1.4980
Best acc: 61.440
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.1	Loss 0.6982 (0.7111)	Prec@1 78.906 (78.496)	
Epoch: [4][155/391]	LR: 0.1	Loss 0.7451 (0.7234)	Prec@1 78.125 (77.915)	
Epoch: [4][233/391]	LR: 0.1	Loss 0.7583 (0.7467)	Prec@1 76.562 (77.237)	
Epoch: [4][311/391]	LR: 0.1	Loss 0.8384 (0.7622)	Prec@1 70.312 (76.743)	
Epoch: [4][389/391]	LR: 0.1	Loss 0.8589 (0.7733)	Prec@1 74.219 (76.372)	
Total train loss: 0.7735

 * Prec@1 61.650 Prec@5 87.300 Loss 1.5449
Best acc: 61.650
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.1	Loss 0.7793 (0.6518)	Prec@1 76.562 (79.968)	
Epoch: [5][155/391]	LR: 0.1	Loss 0.7134 (0.6737)	Prec@1 77.344 (79.347)	
Epoch: [5][233/391]	LR: 0.1	Loss 0.7549 (0.6902)	Prec@1 75.781 (78.846)	
Epoch: [5][311/391]	LR: 0.1	Loss 0.6309 (0.7086)	Prec@1 79.688 (78.323)	
Epoch: [5][389/391]	LR: 0.1	Loss 0.8047 (0.7194)	Prec@1 76.562 (77.855)	
Total train loss: 0.7195

 * Prec@1 59.020 Prec@5 85.260 Loss 1.7451
Best acc: 61.650
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.010000000000000002	Loss 0.4922 (0.5536)	Prec@1 85.156 (83.073)	
Epoch: [6][155/391]	LR: 0.010000000000000002	Loss 0.5000 (0.5218)	Prec@1 84.375 (84.100)	
Epoch: [6][233/391]	LR: 0.010000000000000002	Loss 0.5664 (0.5074)	Prec@1 82.812 (84.739)	
Epoch: [6][311/391]	LR: 0.010000000000000002	Loss 0.4246 (0.4970)	Prec@1 87.500 (85.166)	
Epoch: [6][389/391]	LR: 0.010000000000000002	Loss 0.4812 (0.4876)	Prec@1 82.812 (85.459)	
Total train loss: 0.4876

 * Prec@1 66.720 Prec@5 89.740 Loss 1.3193
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.010000000000000002	Loss 0.4043 (0.4001)	Prec@1 88.281 (88.502)	
Epoch: [7][155/391]	LR: 0.010000000000000002	Loss 0.3193 (0.3995)	Prec@1 88.281 (88.502)	
Epoch: [7][233/391]	LR: 0.010000000000000002	Loss 0.3774 (0.4031)	Prec@1 90.625 (88.472)	
Epoch: [7][311/391]	LR: 0.010000000000000002	Loss 0.4309 (0.4041)	Prec@1 86.719 (88.429)	
Epoch: [7][389/391]	LR: 0.010000000000000002	Loss 0.4761 (0.4061)	Prec@1 87.500 (88.369)	
Total train loss: 0.4060

 * Prec@1 66.620 Prec@5 89.020 Loss 1.3496
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.010000000000000002	Loss 0.3350 (0.3739)	Prec@1 89.062 (89.423)	
Epoch: [8][155/391]	LR: 0.010000000000000002	Loss 0.2781 (0.3752)	Prec@1 93.750 (89.378)	
Epoch: [8][233/391]	LR: 0.010000000000000002	Loss 0.3772 (0.3719)	Prec@1 87.500 (89.567)	
Epoch: [8][311/391]	LR: 0.010000000000000002	Loss 0.4495 (0.3719)	Prec@1 85.938 (89.603)	
Epoch: [8][389/391]	LR: 0.010000000000000002	Loss 0.4651 (0.3720)	Prec@1 86.719 (89.591)	
Total train loss: 0.3721

 * Prec@1 66.590 Prec@5 89.400 Loss 1.3545
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.010000000000000002	Loss 0.3958 (0.3491)	Prec@1 89.062 (90.585)	
Epoch: [9][155/391]	LR: 0.010000000000000002	Loss 0.3000 (0.3426)	Prec@1 90.625 (90.650)	
Epoch: [9][233/391]	LR: 0.010000000000000002	Loss 0.2798 (0.3425)	Prec@1 91.406 (90.588)	
Epoch: [9][311/391]	LR: 0.010000000000000002	Loss 0.3921 (0.3465)	Prec@1 88.281 (90.402)	
Epoch: [9][389/391]	LR: 0.010000000000000002	Loss 0.4841 (0.3504)	Prec@1 85.938 (90.325)	
Total train loss: 0.3503

 * Prec@1 66.040 Prec@5 89.080 Loss 1.3818
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.010000000000000002	Loss 0.3782 (0.3222)	Prec@1 89.062 (91.386)	
Epoch: [10][155/391]	LR: 0.010000000000000002	Loss 0.4048 (0.3255)	Prec@1 86.719 (91.366)	
Epoch: [10][233/391]	LR: 0.010000000000000002	Loss 0.2849 (0.3278)	Prec@1 95.312 (91.223)	
Epoch: [10][311/391]	LR: 0.010000000000000002	Loss 0.3118 (0.3290)	Prec@1 91.406 (91.181)	
Epoch: [10][389/391]	LR: 0.010000000000000002	Loss 0.2372 (0.3300)	Prec@1 95.312 (91.176)	
Total train loss: 0.3301

 * Prec@1 65.860 Prec@5 89.020 Loss 1.3916
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.010000000000000002	Loss 0.3203 (0.3019)	Prec@1 90.625 (92.238)	
Epoch: [11][155/391]	LR: 0.010000000000000002	Loss 0.3667 (0.3079)	Prec@1 89.062 (92.082)	
Epoch: [11][233/391]	LR: 0.010000000000000002	Loss 0.2612 (0.3106)	Prec@1 95.312 (91.877)	
Epoch: [11][311/391]	LR: 0.010000000000000002	Loss 0.2620 (0.3129)	Prec@1 92.188 (91.794)	
Epoch: [11][389/391]	LR: 0.010000000000000002	Loss 0.3550 (0.3157)	Prec@1 89.062 (91.641)	
Total train loss: 0.3158

 * Prec@1 66.350 Prec@5 88.930 Loss 1.4004
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0010000000000000002	Loss 0.2465 (0.2986)	Prec@1 93.750 (92.648)	
Epoch: [12][155/391]	LR: 0.0010000000000000002	Loss 0.2803 (0.2931)	Prec@1 93.750 (92.663)	
Epoch: [12][233/391]	LR: 0.0010000000000000002	Loss 0.3301 (0.2925)	Prec@1 89.062 (92.648)	
Epoch: [12][311/391]	LR: 0.0010000000000000002	Loss 0.3049 (0.2904)	Prec@1 92.969 (92.618)	
Epoch: [12][389/391]	LR: 0.0010000000000000002	Loss 0.2681 (0.2900)	Prec@1 94.531 (92.650)	
Total train loss: 0.2903

 * Prec@1 65.890 Prec@5 88.820 Loss 1.4180
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0010000000000000002	Loss 0.2520 (0.2882)	Prec@1 93.750 (92.498)	
Epoch: [13][155/391]	LR: 0.0010000000000000002	Loss 0.2012 (0.2854)	Prec@1 95.312 (92.829)	
Epoch: [13][233/391]	LR: 0.0010000000000000002	Loss 0.2542 (0.2871)	Prec@1 93.750 (92.758)	
Epoch: [13][311/391]	LR: 0.0010000000000000002	Loss 0.3455 (0.2861)	Prec@1 89.062 (92.753)	
Epoch: [13][389/391]	LR: 0.0010000000000000002	Loss 0.3762 (0.2871)	Prec@1 91.406 (92.758)	
Total train loss: 0.2873

 * Prec@1 66.580 Prec@5 88.810 Loss 1.3965
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0010000000000000002	Loss 0.3169 (0.2954)	Prec@1 94.531 (92.538)	
Epoch: [14][155/391]	LR: 0.0010000000000000002	Loss 0.2031 (0.2959)	Prec@1 96.875 (92.563)	
Epoch: [14][233/391]	LR: 0.0010000000000000002	Loss 0.2998 (0.2911)	Prec@1 91.406 (92.685)	
Epoch: [14][311/391]	LR: 0.0010000000000000002	Loss 0.3323 (0.2883)	Prec@1 90.625 (92.751)	
Epoch: [14][389/391]	LR: 0.0010000000000000002	Loss 0.2864 (0.2903)	Prec@1 92.188 (92.648)	
Total train loss: 0.2905

 * Prec@1 66.090 Prec@5 88.720 Loss 1.4141
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0010000000000000002	Loss 0.2294 (0.2843)	Prec@1 92.188 (93.129)	
Epoch: [15][155/391]	LR: 0.0010000000000000002	Loss 0.2910 (0.2859)	Prec@1 91.406 (92.979)	
Epoch: [15][233/391]	LR: 0.0010000000000000002	Loss 0.2612 (0.2831)	Prec@1 94.531 (93.022)	
Epoch: [15][311/391]	LR: 0.0010000000000000002	Loss 0.2273 (0.2815)	Prec@1 94.531 (93.034)	
Epoch: [15][389/391]	LR: 0.0010000000000000002	Loss 0.2944 (0.2824)	Prec@1 95.312 (93.001)	
Total train loss: 0.2824

 * Prec@1 66.020 Prec@5 88.720 Loss 1.4150
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0010000000000000002	Loss 0.3044 (0.2976)	Prec@1 90.625 (92.478)	
Epoch: [16][155/391]	LR: 0.0010000000000000002	Loss 0.3298 (0.2890)	Prec@1 89.062 (92.798)	
Epoch: [16][233/391]	LR: 0.0010000000000000002	Loss 0.1831 (0.2876)	Prec@1 97.656 (92.915)	
Epoch: [16][311/391]	LR: 0.0010000000000000002	Loss 0.1897 (0.2859)	Prec@1 97.656 (92.936)	
Epoch: [16][389/391]	LR: 0.0010000000000000002	Loss 0.2476 (0.2848)	Prec@1 95.312 (92.977)	
Total train loss: 0.2846

 * Prec@1 66.260 Prec@5 88.850 Loss 1.4033
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0010000000000000002	Loss 0.2191 (0.2714)	Prec@1 94.531 (93.640)	
Epoch: [17][155/391]	LR: 0.0010000000000000002	Loss 0.2922 (0.2783)	Prec@1 91.406 (93.224)	
Epoch: [17][233/391]	LR: 0.0010000000000000002	Loss 0.2637 (0.2794)	Prec@1 92.969 (93.182)	
Epoch: [17][311/391]	LR: 0.0010000000000000002	Loss 0.3408 (0.2826)	Prec@1 92.969 (93.059)	
Epoch: [17][389/391]	LR: 0.0010000000000000002	Loss 0.2715 (0.2825)	Prec@1 93.750 (93.097)	
Total train loss: 0.2826

 * Prec@1 66.230 Prec@5 88.790 Loss 1.4180
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0010000000000000002	Loss 0.2389 (0.2803)	Prec@1 96.875 (92.979)	
Epoch: [18][155/391]	LR: 0.0010000000000000002	Loss 0.2522 (0.2852)	Prec@1 93.750 (92.904)	
Epoch: [18][233/391]	LR: 0.0010000000000000002	Loss 0.2125 (0.2845)	Prec@1 95.312 (92.965)	
Epoch: [18][311/391]	LR: 0.0010000000000000002	Loss 0.3340 (0.2813)	Prec@1 89.062 (93.104)	
Epoch: [18][389/391]	LR: 0.0010000000000000002	Loss 0.2898 (0.2825)	Prec@1 92.969 (93.011)	
Total train loss: 0.2828

 * Prec@1 65.960 Prec@5 88.870 Loss 1.4092
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0010000000000000002	Loss 0.2825 (0.2874)	Prec@1 93.750 (92.808)	
Epoch: [19][155/391]	LR: 0.0010000000000000002	Loss 0.2390 (0.2864)	Prec@1 94.531 (92.914)	
Epoch: [19][233/391]	LR: 0.0010000000000000002	Loss 0.2893 (0.2832)	Prec@1 92.188 (93.005)	
Epoch: [19][311/391]	LR: 0.0010000000000000002	Loss 0.2576 (0.2826)	Prec@1 92.969 (92.986)	
Epoch: [19][389/391]	LR: 0.0010000000000000002	Loss 0.3450 (0.2839)	Prec@1 91.406 (92.931)	
Total train loss: 0.2840

 * Prec@1 66.260 Prec@5 88.870 Loss 1.4111
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.00010000000000000003	Loss 0.3110 (0.2776)	Prec@1 92.188 (93.059)	
Epoch: [20][155/391]	LR: 0.00010000000000000003	Loss 0.2656 (0.2807)	Prec@1 92.969 (92.894)	
Epoch: [20][233/391]	LR: 0.00010000000000000003	Loss 0.2458 (0.2809)	Prec@1 93.750 (92.932)	
Epoch: [20][311/391]	LR: 0.00010000000000000003	Loss 0.2744 (0.2831)	Prec@1 91.406 (92.941)	
Epoch: [20][389/391]	LR: 0.00010000000000000003	Loss 0.2371 (0.2823)	Prec@1 93.750 (92.971)	
Total train loss: 0.2824

 * Prec@1 65.570 Prec@5 88.760 Loss 1.4189
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.00010000000000000003	Loss 0.3613 (0.2886)	Prec@1 89.062 (92.819)	
Epoch: [21][155/391]	LR: 0.00010000000000000003	Loss 0.2008 (0.2806)	Prec@1 96.094 (93.144)	
Epoch: [21][233/391]	LR: 0.00010000000000000003	Loss 0.2590 (0.2847)	Prec@1 94.531 (93.032)	
Epoch: [21][311/391]	LR: 0.00010000000000000003	Loss 0.2141 (0.2821)	Prec@1 96.875 (93.111)	
Epoch: [21][389/391]	LR: 0.00010000000000000003	Loss 0.2686 (0.2820)	Prec@1 94.531 (93.117)	
Total train loss: 0.2819

 * Prec@1 66.320 Prec@5 88.920 Loss 1.4053
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.00010000000000000003	Loss 0.2751 (0.2862)	Prec@1 93.750 (92.929)	
Epoch: [22][155/391]	LR: 0.00010000000000000003	Loss 0.3652 (0.2857)	Prec@1 89.062 (92.909)	
Epoch: [22][233/391]	LR: 0.00010000000000000003	Loss 0.3276 (0.2873)	Prec@1 93.750 (92.802)	
Epoch: [22][311/391]	LR: 0.00010000000000000003	Loss 0.2283 (0.2842)	Prec@1 95.312 (92.899)	
Epoch: [22][389/391]	LR: 0.00010000000000000003	Loss 0.2764 (0.2817)	Prec@1 93.750 (93.031)	
Total train loss: 0.2820

 * Prec@1 66.210 Prec@5 88.860 Loss 1.4072
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.00010000000000000003	Loss 0.3750 (0.2847)	Prec@1 89.062 (92.879)	
Epoch: [23][155/391]	LR: 0.00010000000000000003	Loss 0.2983 (0.2808)	Prec@1 92.188 (93.114)	
Epoch: [23][233/391]	LR: 0.00010000000000000003	Loss 0.2568 (0.2820)	Prec@1 94.531 (93.042)	
Epoch: [23][311/391]	LR: 0.00010000000000000003	Loss 0.3450 (0.2816)	Prec@1 92.188 (93.081)	
Epoch: [23][389/391]	LR: 0.00010000000000000003	Loss 0.3579 (0.2812)	Prec@1 89.844 (93.073)	
Total train loss: 0.2812

 * Prec@1 66.540 Prec@5 88.970 Loss 1.4082
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.00010000000000000003	Loss 0.3054 (0.2796)	Prec@1 90.625 (92.748)	
Epoch: [24][155/391]	LR: 0.00010000000000000003	Loss 0.3740 (0.2861)	Prec@1 90.625 (92.643)	
Epoch: [24][233/391]	LR: 0.00010000000000000003	Loss 0.2332 (0.2849)	Prec@1 92.188 (92.715)	
Epoch: [24][311/391]	LR: 0.00010000000000000003	Loss 0.2971 (0.2831)	Prec@1 92.188 (92.773)	
Epoch: [24][389/391]	LR: 0.00010000000000000003	Loss 0.3818 (0.2845)	Prec@1 91.406 (92.752)	
Total train loss: 0.2845

 * Prec@1 66.320 Prec@5 88.910 Loss 1.4023
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.00010000000000000003	Loss 0.3672 (0.2871)	Prec@1 91.406 (92.949)	
Epoch: [25][155/391]	LR: 0.00010000000000000003	Loss 0.3167 (0.2855)	Prec@1 92.969 (93.024)	
Epoch: [25][233/391]	LR: 0.00010000000000000003	Loss 0.3005 (0.2856)	Prec@1 90.625 (93.002)	
Epoch: [25][311/391]	LR: 0.00010000000000000003	Loss 0.2474 (0.2831)	Prec@1 92.969 (93.059)	
Epoch: [25][389/391]	LR: 0.00010000000000000003	Loss 0.3586 (0.2817)	Prec@1 92.969 (93.103)	
Total train loss: 0.2818

 * Prec@1 66.170 Prec@5 88.820 Loss 1.4111
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.00010000000000000003	Loss 0.3831 (0.2831)	Prec@1 88.281 (93.069)	
Epoch: [26][155/391]	LR: 0.00010000000000000003	Loss 0.2788 (0.2779)	Prec@1 92.969 (93.364)	
Epoch: [26][233/391]	LR: 0.00010000000000000003	Loss 0.2175 (0.2801)	Prec@1 92.969 (93.216)	
Epoch: [26][311/391]	LR: 0.00010000000000000003	Loss 0.2776 (0.2788)	Prec@1 95.312 (93.299)	
Epoch: [26][389/391]	LR: 0.00010000000000000003	Loss 0.3054 (0.2801)	Prec@1 92.188 (93.233)	
Total train loss: 0.2802

 * Prec@1 66.130 Prec@5 88.870 Loss 1.4092
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.00010000000000000003	Loss 0.2666 (0.2782)	Prec@1 94.531 (93.329)	
Epoch: [27][155/391]	LR: 0.00010000000000000003	Loss 0.3372 (0.2842)	Prec@1 92.188 (93.099)	
Epoch: [27][233/391]	LR: 0.00010000000000000003	Loss 0.2839 (0.2848)	Prec@1 91.406 (92.982)	
Epoch: [27][311/391]	LR: 0.00010000000000000003	Loss 0.2651 (0.2839)	Prec@1 93.750 (93.034)	
Epoch: [27][389/391]	LR: 0.00010000000000000003	Loss 0.3748 (0.2833)	Prec@1 89.062 (93.051)	
Total train loss: 0.2833

 * Prec@1 66.240 Prec@5 88.720 Loss 1.4141
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.00010000000000000003	Loss 0.2854 (0.2741)	Prec@1 93.750 (93.119)	
Epoch: [28][155/391]	LR: 0.00010000000000000003	Loss 0.2571 (0.2801)	Prec@1 95.312 (92.994)	
Epoch: [28][233/391]	LR: 0.00010000000000000003	Loss 0.3152 (0.2806)	Prec@1 92.969 (93.056)	
Epoch: [28][311/391]	LR: 0.00010000000000000003	Loss 0.2600 (0.2808)	Prec@1 92.188 (93.009)	
Epoch: [28][389/391]	LR: 0.00010000000000000003	Loss 0.2664 (0.2802)	Prec@1 92.969 (93.077)	
Total train loss: 0.2803

 * Prec@1 66.150 Prec@5 88.870 Loss 1.4072
Best acc: 66.720
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.00010000000000000003	Loss 0.1869 (0.2782)	Prec@1 96.875 (93.359)	
Epoch: [29][155/391]	LR: 0.00010000000000000003	Loss 0.3931 (0.2808)	Prec@1 87.500 (93.274)	
Epoch: [29][233/391]	LR: 0.00010000000000000003	Loss 0.2155 (0.2792)	Prec@1 94.531 (93.229)	
Epoch: [29][311/391]	LR: 0.00010000000000000003	Loss 0.2394 (0.2789)	Prec@1 95.312 (93.247)	
Epoch: [29][389/391]	LR: 0.00010000000000000003	Loss 0.2136 (0.2798)	Prec@1 95.312 (93.209)	
Total train loss: 0.2798

 * Prec@1 66.200 Prec@5 88.950 Loss 1.4082
Best acc: 66.720
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.050 Prec@5 90.320 Loss 1.1768
Pre-trained Prec@1 with 17 layers frozen: 69.04999542236328 	 Loss: 1.1767578125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.1	Loss 0.8667 (1.0044)	Prec@1 75.781 (71.114)	
Epoch: [0][155/391]	LR: 0.1	Loss 1.0420 (1.0094)	Prec@1 71.094 (70.954)	
Epoch: [0][233/391]	LR: 0.1	Loss 0.9727 (0.9838)	Prec@1 71.094 (71.334)	
Epoch: [0][311/391]	LR: 0.1	Loss 0.8638 (0.9691)	Prec@1 71.094 (71.625)	
Epoch: [0][389/391]	LR: 0.1	Loss 0.8779 (0.9537)	Prec@1 73.438 (71.997)	
Total train loss: 0.9536

 * Prec@1 65.040 Prec@5 88.830 Loss 1.3213
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.1	Loss 0.6831 (0.7591)	Prec@1 76.562 (76.953)	
Epoch: [1][155/391]	LR: 0.1	Loss 0.9365 (0.7691)	Prec@1 71.094 (76.813)	
Epoch: [1][233/391]	LR: 0.1	Loss 0.7124 (0.7719)	Prec@1 75.781 (76.746)	
Epoch: [1][311/391]	LR: 0.1	Loss 0.8086 (0.7767)	Prec@1 75.781 (76.718)	
Epoch: [1][389/391]	LR: 0.1	Loss 0.8569 (0.7835)	Prec@1 73.438 (76.514)	
Total train loss: 0.7837

 * Prec@1 63.930 Prec@5 88.690 Loss 1.3633
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.1	Loss 0.6440 (0.6537)	Prec@1 78.125 (80.599)	
Epoch: [2][155/391]	LR: 0.1	Loss 0.7632 (0.6771)	Prec@1 76.562 (79.838)	
Epoch: [2][233/391]	LR: 0.1	Loss 0.8311 (0.6929)	Prec@1 74.219 (79.240)	
Epoch: [2][311/391]	LR: 0.1	Loss 0.6201 (0.7038)	Prec@1 80.469 (78.839)	
Epoch: [2][389/391]	LR: 0.1	Loss 0.6230 (0.7119)	Prec@1 77.344 (78.536)	
Total train loss: 0.7123

 * Prec@1 63.720 Prec@5 87.860 Loss 1.4805
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.1	Loss 0.5732 (0.6070)	Prec@1 78.906 (81.731)	
Epoch: [3][155/391]	LR: 0.1	Loss 0.7197 (0.6276)	Prec@1 76.562 (80.854)	
Epoch: [3][233/391]	LR: 0.1	Loss 0.7329 (0.6410)	Prec@1 78.906 (80.352)	
Epoch: [3][311/391]	LR: 0.1	Loss 0.6855 (0.6541)	Prec@1 81.250 (79.920)	
Epoch: [3][389/391]	LR: 0.1	Loss 0.6274 (0.6591)	Prec@1 82.031 (79.864)	
Total train loss: 0.6593

 * Prec@1 63.990 Prec@5 88.560 Loss 1.4404
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.1	Loss 0.6489 (0.5784)	Prec@1 75.000 (82.181)	
Epoch: [4][155/391]	LR: 0.1	Loss 0.5630 (0.5976)	Prec@1 81.250 (81.545)	
Epoch: [4][233/391]	LR: 0.1	Loss 0.6250 (0.6042)	Prec@1 84.375 (81.207)	
Epoch: [4][311/391]	LR: 0.1	Loss 0.5464 (0.6147)	Prec@1 83.594 (80.857)	
Epoch: [4][389/391]	LR: 0.1	Loss 0.6978 (0.6226)	Prec@1 75.000 (80.617)	
Total train loss: 0.6229

 * Prec@1 64.220 Prec@5 88.170 Loss 1.4580
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.1	Loss 0.5142 (0.5330)	Prec@1 83.594 (83.273)	
Epoch: [5][155/391]	LR: 0.1	Loss 0.5298 (0.5509)	Prec@1 83.594 (82.732)	
Epoch: [5][233/391]	LR: 0.1	Loss 0.7056 (0.5679)	Prec@1 75.781 (82.178)	
Epoch: [5][311/391]	LR: 0.1	Loss 0.5122 (0.5754)	Prec@1 82.031 (82.036)	
Epoch: [5][389/391]	LR: 0.1	Loss 0.7832 (0.5855)	Prec@1 74.219 (81.827)	
Total train loss: 0.5859

 * Prec@1 64.130 Prec@5 87.660 Loss 1.5020
Best acc: 65.040
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.010000000000000002	Loss 0.4358 (0.4787)	Prec@1 85.938 (85.477)	
Epoch: [6][155/391]	LR: 0.010000000000000002	Loss 0.4426 (0.4577)	Prec@1 84.375 (86.323)	
Epoch: [6][233/391]	LR: 0.010000000000000002	Loss 0.4553 (0.4485)	Prec@1 84.375 (86.535)	
Epoch: [6][311/391]	LR: 0.010000000000000002	Loss 0.4949 (0.4423)	Prec@1 85.156 (86.666)	
Epoch: [6][389/391]	LR: 0.010000000000000002	Loss 0.4573 (0.4356)	Prec@1 85.156 (86.913)	
Total train loss: 0.4358

 * Prec@1 67.360 Prec@5 89.510 Loss 1.3320
Best acc: 67.360
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.010000000000000002	Loss 0.3000 (0.3854)	Prec@1 89.844 (88.622)	
Epoch: [7][155/391]	LR: 0.010000000000000002	Loss 0.3672 (0.3867)	Prec@1 87.500 (88.426)	
Epoch: [7][233/391]	LR: 0.010000000000000002	Loss 0.3364 (0.3854)	Prec@1 90.625 (88.522)	
Epoch: [7][311/391]	LR: 0.010000000000000002	Loss 0.4153 (0.3844)	Prec@1 87.500 (88.642)	
Epoch: [7][389/391]	LR: 0.010000000000000002	Loss 0.4324 (0.3837)	Prec@1 85.156 (88.726)	
Total train loss: 0.3838

 * Prec@1 67.440 Prec@5 89.750 Loss 1.3564
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.010000000000000002	Loss 0.3298 (0.3601)	Prec@1 90.625 (89.613)	
Epoch: [8][155/391]	LR: 0.010000000000000002	Loss 0.3757 (0.3582)	Prec@1 89.844 (89.678)	
Epoch: [8][233/391]	LR: 0.010000000000000002	Loss 0.3823 (0.3599)	Prec@1 89.062 (89.690)	
Epoch: [8][311/391]	LR: 0.010000000000000002	Loss 0.3865 (0.3636)	Prec@1 88.281 (89.528)	
Epoch: [8][389/391]	LR: 0.010000000000000002	Loss 0.3130 (0.3650)	Prec@1 91.406 (89.507)	
Total train loss: 0.3651

 * Prec@1 67.140 Prec@5 89.370 Loss 1.3711
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.010000000000000002	Loss 0.3694 (0.3524)	Prec@1 92.188 (89.994)	
Epoch: [9][155/391]	LR: 0.010000000000000002	Loss 0.4043 (0.3488)	Prec@1 88.281 (90.004)	
Epoch: [9][233/391]	LR: 0.010000000000000002	Loss 0.4331 (0.3528)	Prec@1 88.281 (89.874)	
Epoch: [9][311/391]	LR: 0.010000000000000002	Loss 0.4082 (0.3544)	Prec@1 85.156 (89.751)	
Epoch: [9][389/391]	LR: 0.010000000000000002	Loss 0.3386 (0.3551)	Prec@1 89.062 (89.768)	
Total train loss: 0.3551

 * Prec@1 67.160 Prec@5 89.250 Loss 1.3828
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.010000000000000002	Loss 0.2020 (0.3329)	Prec@1 96.094 (90.725)	
Epoch: [10][155/391]	LR: 0.010000000000000002	Loss 0.3752 (0.3351)	Prec@1 88.281 (90.585)	
Epoch: [10][233/391]	LR: 0.010000000000000002	Loss 0.3542 (0.3356)	Prec@1 89.062 (90.565)	
Epoch: [10][311/391]	LR: 0.010000000000000002	Loss 0.3740 (0.3429)	Prec@1 87.500 (90.322)	
Epoch: [10][389/391]	LR: 0.010000000000000002	Loss 0.4287 (0.3449)	Prec@1 89.062 (90.238)	
Total train loss: 0.3451

 * Prec@1 67.180 Prec@5 89.260 Loss 1.3945
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.010000000000000002	Loss 0.2546 (0.3240)	Prec@1 91.406 (90.805)	
Epoch: [11][155/391]	LR: 0.010000000000000002	Loss 0.3640 (0.3262)	Prec@1 89.844 (90.865)	
Epoch: [11][233/391]	LR: 0.010000000000000002	Loss 0.3364 (0.3307)	Prec@1 92.188 (90.685)	
Epoch: [11][311/391]	LR: 0.010000000000000002	Loss 0.3147 (0.3318)	Prec@1 92.969 (90.617)	
Epoch: [11][389/391]	LR: 0.010000000000000002	Loss 0.3286 (0.3329)	Prec@1 92.188 (90.671)	
Total train loss: 0.3327

 * Prec@1 67.200 Prec@5 89.140 Loss 1.4062
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0010000000000000002	Loss 0.2632 (0.3023)	Prec@1 92.969 (91.847)	
Epoch: [12][155/391]	LR: 0.0010000000000000002	Loss 0.2278 (0.3116)	Prec@1 93.750 (91.587)	
Epoch: [12][233/391]	LR: 0.0010000000000000002	Loss 0.3083 (0.3148)	Prec@1 92.188 (91.466)	
Epoch: [12][311/391]	LR: 0.0010000000000000002	Loss 0.3105 (0.3137)	Prec@1 95.312 (91.521)	
Epoch: [12][389/391]	LR: 0.0010000000000000002	Loss 0.4324 (0.3128)	Prec@1 88.281 (91.528)	
Total train loss: 0.3129

 * Prec@1 67.040 Prec@5 89.060 Loss 1.4121
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0010000000000000002	Loss 0.4817 (0.3166)	Prec@1 86.719 (91.466)	
Epoch: [13][155/391]	LR: 0.0010000000000000002	Loss 0.3484 (0.3126)	Prec@1 88.281 (91.561)	
Epoch: [13][233/391]	LR: 0.0010000000000000002	Loss 0.4846 (0.3128)	Prec@1 85.938 (91.486)	
Epoch: [13][311/391]	LR: 0.0010000000000000002	Loss 0.3879 (0.3139)	Prec@1 86.719 (91.416)	
Epoch: [13][389/391]	LR: 0.0010000000000000002	Loss 0.2394 (0.3125)	Prec@1 96.094 (91.470)	
Total train loss: 0.3125

 * Prec@1 67.320 Prec@5 89.000 Loss 1.4092
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0010000000000000002	Loss 0.2632 (0.3147)	Prec@1 93.750 (91.246)	
Epoch: [14][155/391]	LR: 0.0010000000000000002	Loss 0.2827 (0.3151)	Prec@1 91.406 (91.241)	
Epoch: [14][233/391]	LR: 0.0010000000000000002	Loss 0.2732 (0.3149)	Prec@1 94.531 (91.239)	
Epoch: [14][311/391]	LR: 0.0010000000000000002	Loss 0.3237 (0.3155)	Prec@1 90.625 (91.206)	
Epoch: [14][389/391]	LR: 0.0010000000000000002	Loss 0.3293 (0.3156)	Prec@1 91.406 (91.234)	
Total train loss: 0.3158

 * Prec@1 67.180 Prec@5 89.090 Loss 1.4043
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0010000000000000002	Loss 0.1855 (0.3221)	Prec@1 97.656 (91.156)	
Epoch: [15][155/391]	LR: 0.0010000000000000002	Loss 0.2434 (0.3153)	Prec@1 94.531 (91.306)	
Epoch: [15][233/391]	LR: 0.0010000000000000002	Loss 0.3164 (0.3110)	Prec@1 92.969 (91.473)	
Epoch: [15][311/391]	LR: 0.0010000000000000002	Loss 0.3333 (0.3101)	Prec@1 89.844 (91.597)	
Epoch: [15][389/391]	LR: 0.0010000000000000002	Loss 0.3777 (0.3107)	Prec@1 88.281 (91.575)	
Total train loss: 0.3107

 * Prec@1 67.120 Prec@5 88.880 Loss 1.4043
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0010000000000000002	Loss 0.2905 (0.3099)	Prec@1 91.406 (91.777)	
Epoch: [16][155/391]	LR: 0.0010000000000000002	Loss 0.2839 (0.3147)	Prec@1 91.406 (91.471)	
Epoch: [16][233/391]	LR: 0.0010000000000000002	Loss 0.3052 (0.3111)	Prec@1 92.188 (91.567)	
Epoch: [16][311/391]	LR: 0.0010000000000000002	Loss 0.3030 (0.3112)	Prec@1 92.969 (91.564)	
Epoch: [16][389/391]	LR: 0.0010000000000000002	Loss 0.3193 (0.3119)	Prec@1 90.625 (91.583)	
Total train loss: 0.3120

 * Prec@1 67.240 Prec@5 89.080 Loss 1.4053
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0010000000000000002	Loss 0.2678 (0.3124)	Prec@1 90.625 (91.386)	
Epoch: [17][155/391]	LR: 0.0010000000000000002	Loss 0.2378 (0.3130)	Prec@1 93.750 (91.391)	
Epoch: [17][233/391]	LR: 0.0010000000000000002	Loss 0.3301 (0.3125)	Prec@1 91.406 (91.440)	
Epoch: [17][311/391]	LR: 0.0010000000000000002	Loss 0.3313 (0.3146)	Prec@1 89.844 (91.416)	
Epoch: [17][389/391]	LR: 0.0010000000000000002	Loss 0.2469 (0.3129)	Prec@1 92.969 (91.516)	
Total train loss: 0.3131

 * Prec@1 67.150 Prec@5 89.150 Loss 1.3945
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0010000000000000002	Loss 0.3350 (0.3093)	Prec@1 89.062 (91.637)	
Epoch: [18][155/391]	LR: 0.0010000000000000002	Loss 0.3110 (0.3104)	Prec@1 90.625 (91.607)	
Epoch: [18][233/391]	LR: 0.0010000000000000002	Loss 0.3613 (0.3091)	Prec@1 91.406 (91.610)	
Epoch: [18][311/391]	LR: 0.0010000000000000002	Loss 0.3247 (0.3091)	Prec@1 90.625 (91.624)	
Epoch: [18][389/391]	LR: 0.0010000000000000002	Loss 0.3774 (0.3105)	Prec@1 88.281 (91.556)	
Total train loss: 0.3107

 * Prec@1 67.300 Prec@5 89.130 Loss 1.3926
Best acc: 67.440
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0010000000000000002	Loss 0.3325 (0.3035)	Prec@1 91.406 (92.017)	
Epoch: [19][155/391]	LR: 0.0010000000000000002	Loss 0.2573 (0.3014)	Prec@1 92.188 (91.862)	
Epoch: [19][233/391]	LR: 0.0010000000000000002	Loss 0.3796 (0.3084)	Prec@1 88.281 (91.553)	
Epoch: [19][311/391]	LR: 0.0010000000000000002	Loss 0.3606 (0.3094)	Prec@1 90.625 (91.574)	
Epoch: [19][389/391]	LR: 0.0010000000000000002	Loss 0.2888 (0.3096)	Prec@1 92.188 (91.593)	
Total train loss: 0.3096

 * Prec@1 67.460 Prec@5 89.070 Loss 1.3994
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.00010000000000000003	Loss 0.3306 (0.3087)	Prec@1 90.625 (91.567)	
Epoch: [20][155/391]	LR: 0.00010000000000000003	Loss 0.3062 (0.3118)	Prec@1 87.500 (91.627)	
Epoch: [20][233/391]	LR: 0.00010000000000000003	Loss 0.3625 (0.3109)	Prec@1 90.625 (91.627)	
Epoch: [20][311/391]	LR: 0.00010000000000000003	Loss 0.2761 (0.3088)	Prec@1 92.969 (91.689)	
Epoch: [20][389/391]	LR: 0.00010000000000000003	Loss 0.2090 (0.3110)	Prec@1 94.531 (91.617)	
Total train loss: 0.3110

 * Prec@1 67.140 Prec@5 89.020 Loss 1.4023
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.00010000000000000003	Loss 0.4221 (0.3029)	Prec@1 85.938 (92.027)	
Epoch: [21][155/391]	LR: 0.00010000000000000003	Loss 0.2761 (0.3025)	Prec@1 92.969 (92.017)	
Epoch: [21][233/391]	LR: 0.00010000000000000003	Loss 0.3369 (0.3050)	Prec@1 92.188 (91.914)	
Epoch: [21][311/391]	LR: 0.00010000000000000003	Loss 0.2578 (0.3048)	Prec@1 94.531 (91.917)	
Epoch: [21][389/391]	LR: 0.00010000000000000003	Loss 0.3567 (0.3076)	Prec@1 89.062 (91.765)	
Total train loss: 0.3077

 * Prec@1 67.090 Prec@5 89.040 Loss 1.3955
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.00010000000000000003	Loss 0.2979 (0.3094)	Prec@1 89.844 (91.486)	
Epoch: [22][155/391]	LR: 0.00010000000000000003	Loss 0.3132 (0.3079)	Prec@1 89.062 (91.707)	
Epoch: [22][233/391]	LR: 0.00010000000000000003	Loss 0.2695 (0.3076)	Prec@1 89.844 (91.663)	
Epoch: [22][311/391]	LR: 0.00010000000000000003	Loss 0.2944 (0.3078)	Prec@1 91.406 (91.599)	
Epoch: [22][389/391]	LR: 0.00010000000000000003	Loss 0.3210 (0.3090)	Prec@1 92.188 (91.560)	
Total train loss: 0.3091

 * Prec@1 67.010 Prec@5 89.180 Loss 1.4043
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.00010000000000000003	Loss 0.2751 (0.3101)	Prec@1 92.969 (91.807)	
Epoch: [23][155/391]	LR: 0.00010000000000000003	Loss 0.2983 (0.3148)	Prec@1 94.531 (91.667)	
Epoch: [23][233/391]	LR: 0.00010000000000000003	Loss 0.2399 (0.3131)	Prec@1 93.750 (91.630)	
Epoch: [23][311/391]	LR: 0.00010000000000000003	Loss 0.2974 (0.3129)	Prec@1 92.188 (91.597)	
Epoch: [23][389/391]	LR: 0.00010000000000000003	Loss 0.2979 (0.3133)	Prec@1 92.188 (91.538)	
Total train loss: 0.3135

 * Prec@1 67.200 Prec@5 89.160 Loss 1.3984
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.00010000000000000003	Loss 0.3242 (0.3165)	Prec@1 90.625 (91.256)	
Epoch: [24][155/391]	LR: 0.00010000000000000003	Loss 0.3140 (0.3089)	Prec@1 91.406 (91.622)	
Epoch: [24][233/391]	LR: 0.00010000000000000003	Loss 0.3767 (0.3081)	Prec@1 90.625 (91.683)	
Epoch: [24][311/391]	LR: 0.00010000000000000003	Loss 0.2152 (0.3083)	Prec@1 94.531 (91.662)	
Epoch: [24][389/391]	LR: 0.00010000000000000003	Loss 0.4182 (0.3082)	Prec@1 89.844 (91.667)	
Total train loss: 0.3083

 * Prec@1 67.270 Prec@5 89.210 Loss 1.4004
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.00010000000000000003	Loss 0.2830 (0.3129)	Prec@1 90.625 (91.556)	
Epoch: [25][155/391]	LR: 0.00010000000000000003	Loss 0.3228 (0.3138)	Prec@1 92.188 (91.541)	
Epoch: [25][233/391]	LR: 0.00010000000000000003	Loss 0.2356 (0.3166)	Prec@1 93.750 (91.456)	
Epoch: [25][311/391]	LR: 0.00010000000000000003	Loss 0.3174 (0.3145)	Prec@1 92.188 (91.539)	
Epoch: [25][389/391]	LR: 0.00010000000000000003	Loss 0.3525 (0.3126)	Prec@1 89.844 (91.629)	
Total train loss: 0.3125

 * Prec@1 67.180 Prec@5 89.000 Loss 1.3984
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.00010000000000000003	Loss 0.3374 (0.3140)	Prec@1 92.969 (91.356)	
Epoch: [26][155/391]	LR: 0.00010000000000000003	Loss 0.3291 (0.3100)	Prec@1 89.844 (91.421)	
Epoch: [26][233/391]	LR: 0.00010000000000000003	Loss 0.2325 (0.3094)	Prec@1 95.312 (91.460)	
Epoch: [26][311/391]	LR: 0.00010000000000000003	Loss 0.3484 (0.3098)	Prec@1 89.062 (91.519)	
Epoch: [26][389/391]	LR: 0.00010000000000000003	Loss 0.3113 (0.3101)	Prec@1 92.188 (91.562)	
Total train loss: 0.3103

 * Prec@1 66.770 Prec@5 88.980 Loss 1.4102
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.00010000000000000003	Loss 0.2766 (0.3115)	Prec@1 92.188 (91.526)	
Epoch: [27][155/391]	LR: 0.00010000000000000003	Loss 0.3376 (0.3134)	Prec@1 87.500 (91.406)	
Epoch: [27][233/391]	LR: 0.00010000000000000003	Loss 0.2998 (0.3130)	Prec@1 93.750 (91.453)	
Epoch: [27][311/391]	LR: 0.00010000000000000003	Loss 0.2771 (0.3109)	Prec@1 93.750 (91.549)	
Epoch: [27][389/391]	LR: 0.00010000000000000003	Loss 0.2620 (0.3104)	Prec@1 95.312 (91.575)	
Total train loss: 0.3105

 * Prec@1 67.170 Prec@5 89.060 Loss 1.3975
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.00010000000000000003	Loss 0.2039 (0.3064)	Prec@1 96.094 (91.817)	
Epoch: [28][155/391]	LR: 0.00010000000000000003	Loss 0.2434 (0.3050)	Prec@1 93.750 (91.737)	
Epoch: [28][233/391]	LR: 0.00010000000000000003	Loss 0.2786 (0.3092)	Prec@1 90.625 (91.663)	
Epoch: [28][311/391]	LR: 0.00010000000000000003	Loss 0.2593 (0.3106)	Prec@1 96.094 (91.554)	
Epoch: [28][389/391]	LR: 0.00010000000000000003	Loss 0.2849 (0.3104)	Prec@1 93.750 (91.619)	
Total train loss: 0.3106

 * Prec@1 67.220 Prec@5 89.020 Loss 1.4014
Best acc: 67.460
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.00010000000000000003	Loss 0.2620 (0.3067)	Prec@1 93.750 (91.697)	
Epoch: [29][155/391]	LR: 0.00010000000000000003	Loss 0.3767 (0.3083)	Prec@1 89.844 (91.627)	
Epoch: [29][233/391]	LR: 0.00010000000000000003	Loss 0.3052 (0.3090)	Prec@1 90.625 (91.630)	
Epoch: [29][311/391]	LR: 0.00010000000000000003	Loss 0.2308 (0.3084)	Prec@1 92.969 (91.669)	
Epoch: [29][389/391]	LR: 0.00010000000000000003	Loss 0.3730 (0.3107)	Prec@1 89.062 (91.571)	
Total train loss: 0.3106

 * Prec@1 67.160 Prec@5 88.960 Loss 1.3955
Best acc: 67.460
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 3
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 3
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.090 Prec@5 90.330 Loss 1.1777
Pre-trained Prec@1 with 19 layers frozen: 69.08999633789062 	 Loss: 1.177734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.1	Loss 0.5664 (0.6585)	Prec@1 83.594 (81.010)	
Epoch: [0][155/391]	LR: 0.1	Loss 0.5698 (0.6574)	Prec@1 83.594 (80.764)	
Epoch: [0][233/391]	LR: 0.1	Loss 0.7070 (0.6560)	Prec@1 81.250 (80.746)	
Epoch: [0][311/391]	LR: 0.1	Loss 0.7051 (0.6470)	Prec@1 76.562 (80.922)	
Epoch: [0][389/391]	LR: 0.1	Loss 0.6313 (0.6429)	Prec@1 78.906 (80.976)	
Total train loss: 0.6428

 * Prec@1 68.350 Prec@5 89.980 Loss 1.2490
Best acc: 68.350
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.1	Loss 0.4963 (0.5805)	Prec@1 86.719 (82.823)	
Epoch: [1][155/391]	LR: 0.1	Loss 0.5156 (0.5846)	Prec@1 83.594 (82.712)	
Epoch: [1][233/391]	LR: 0.1	Loss 0.5288 (0.5876)	Prec@1 82.031 (82.502)	
Epoch: [1][311/391]	LR: 0.1	Loss 0.4417 (0.5889)	Prec@1 86.719 (82.434)	
Epoch: [1][389/391]	LR: 0.1	Loss 0.4956 (0.5910)	Prec@1 83.594 (82.300)	
Total train loss: 0.5909

 * Prec@1 68.450 Prec@5 90.060 Loss 1.2676
Best acc: 68.450
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.1	Loss 0.5547 (0.5612)	Prec@1 78.906 (82.923)	
Epoch: [2][155/391]	LR: 0.1	Loss 0.4934 (0.5508)	Prec@1 83.594 (83.343)	
Epoch: [2][233/391]	LR: 0.1	Loss 0.5117 (0.5602)	Prec@1 79.688 (82.993)	
Epoch: [2][311/391]	LR: 0.1	Loss 0.6313 (0.5628)	Prec@1 82.031 (82.905)	
Epoch: [2][389/391]	LR: 0.1	Loss 0.6455 (0.5665)	Prec@1 83.594 (82.684)	
Total train loss: 0.5663

 * Prec@1 68.450 Prec@5 90.020 Loss 1.2734
Best acc: 68.450
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.1	Loss 0.5767 (0.5335)	Prec@1 82.031 (83.644)	
Epoch: [3][155/391]	LR: 0.1	Loss 0.5366 (0.5359)	Prec@1 85.938 (83.584)	
Epoch: [3][233/391]	LR: 0.1	Loss 0.5200 (0.5482)	Prec@1 81.250 (83.350)	
Epoch: [3][311/391]	LR: 0.1	Loss 0.4951 (0.5524)	Prec@1 83.594 (83.186)	
Epoch: [3][389/391]	LR: 0.1	Loss 0.5400 (0.5543)	Prec@1 84.375 (83.039)	
Total train loss: 0.5542

 * Prec@1 68.390 Prec@5 89.960 Loss 1.2891
Best acc: 68.450
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.1	Loss 0.5986 (0.5092)	Prec@1 82.812 (83.934)	
Epoch: [4][155/391]	LR: 0.1	Loss 0.4141 (0.5249)	Prec@1 86.719 (83.564)	
Epoch: [4][233/391]	LR: 0.1	Loss 0.4829 (0.5358)	Prec@1 80.469 (83.303)	
Epoch: [4][311/391]	LR: 0.1	Loss 0.8843 (0.5412)	Prec@1 67.188 (83.146)	
Epoch: [4][389/391]	LR: 0.1	Loss 0.5771 (0.5432)	Prec@1 81.250 (83.039)	
Total train loss: 0.5435

 * Prec@1 68.470 Prec@5 90.300 Loss 1.2861
Best acc: 68.470
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.1	Loss 0.5957 (0.5168)	Prec@1 85.156 (84.585)	
Epoch: [5][155/391]	LR: 0.1	Loss 0.5146 (0.5267)	Prec@1 85.156 (83.944)	
Epoch: [5][233/391]	LR: 0.1	Loss 0.5508 (0.5328)	Prec@1 78.906 (83.587)	
Epoch: [5][311/391]	LR: 0.1	Loss 0.5547 (0.5318)	Prec@1 85.938 (83.566)	
Epoch: [5][389/391]	LR: 0.1	Loss 0.4851 (0.5356)	Prec@1 85.938 (83.440)	
Total train loss: 0.5357

 * Prec@1 68.550 Prec@5 90.190 Loss 1.2988
Best acc: 68.550
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.010000000000000002	Loss 0.5347 (0.5067)	Prec@1 82.031 (84.135)	
Epoch: [6][155/391]	LR: 0.010000000000000002	Loss 0.5654 (0.5031)	Prec@1 85.938 (84.325)	
Epoch: [6][233/391]	LR: 0.010000000000000002	Loss 0.3911 (0.4932)	Prec@1 85.156 (84.812)	
Epoch: [6][311/391]	LR: 0.010000000000000002	Loss 0.5713 (0.4939)	Prec@1 82.812 (84.828)	
Epoch: [6][389/391]	LR: 0.010000000000000002	Loss 0.4692 (0.4973)	Prec@1 85.156 (84.730)	
Total train loss: 0.4972

 * Prec@1 68.970 Prec@5 90.200 Loss 1.2812
Best acc: 68.970
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.010000000000000002	Loss 0.5322 (0.4820)	Prec@1 82.812 (85.236)	
Epoch: [7][155/391]	LR: 0.010000000000000002	Loss 0.4280 (0.4870)	Prec@1 86.719 (85.151)	
Epoch: [7][233/391]	LR: 0.010000000000000002	Loss 0.3823 (0.4847)	Prec@1 87.500 (85.196)	
Epoch: [7][311/391]	LR: 0.010000000000000002	Loss 0.4587 (0.4834)	Prec@1 82.812 (85.234)	
Epoch: [7][389/391]	LR: 0.010000000000000002	Loss 0.3970 (0.4865)	Prec@1 83.594 (85.092)	
Total train loss: 0.4864

 * Prec@1 69.130 Prec@5 90.270 Loss 1.2676
Best acc: 69.130
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.010000000000000002	Loss 0.5254 (0.4714)	Prec@1 83.594 (85.337)	
Epoch: [8][155/391]	LR: 0.010000000000000002	Loss 0.5415 (0.4821)	Prec@1 84.375 (85.111)	
Epoch: [8][233/391]	LR: 0.010000000000000002	Loss 0.3909 (0.4856)	Prec@1 85.938 (85.110)	
Epoch: [8][311/391]	LR: 0.010000000000000002	Loss 0.3687 (0.4839)	Prec@1 89.844 (85.141)	
Epoch: [8][389/391]	LR: 0.010000000000000002	Loss 0.5542 (0.4852)	Prec@1 83.594 (85.088)	
Total train loss: 0.4854

 * Prec@1 69.180 Prec@5 90.290 Loss 1.2666
Best acc: 69.180
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.010000000000000002	Loss 0.5786 (0.4874)	Prec@1 84.375 (85.276)	
Epoch: [9][155/391]	LR: 0.010000000000000002	Loss 0.4004 (0.4873)	Prec@1 85.938 (85.161)	
Epoch: [9][233/391]	LR: 0.010000000000000002	Loss 0.3557 (0.4825)	Prec@1 88.281 (85.226)	
Epoch: [9][311/391]	LR: 0.010000000000000002	Loss 0.4019 (0.4807)	Prec@1 87.500 (85.309)	
Epoch: [9][389/391]	LR: 0.010000000000000002	Loss 0.6113 (0.4822)	Prec@1 80.469 (85.226)	
Total train loss: 0.4825

 * Prec@1 69.300 Prec@5 90.250 Loss 1.2783
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.010000000000000002	Loss 0.5127 (0.4794)	Prec@1 83.594 (85.146)	
Epoch: [10][155/391]	LR: 0.010000000000000002	Loss 0.4822 (0.4758)	Prec@1 85.938 (85.357)	
Epoch: [10][233/391]	LR: 0.010000000000000002	Loss 0.4788 (0.4799)	Prec@1 82.812 (85.213)	
Epoch: [10][311/391]	LR: 0.010000000000000002	Loss 0.4158 (0.4832)	Prec@1 85.156 (85.136)	
Epoch: [10][389/391]	LR: 0.010000000000000002	Loss 0.3884 (0.4813)	Prec@1 86.719 (85.182)	
Total train loss: 0.4815

 * Prec@1 69.110 Prec@5 90.230 Loss 1.2754
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.010000000000000002	Loss 0.5615 (0.4813)	Prec@1 85.156 (85.086)	
Epoch: [11][155/391]	LR: 0.010000000000000002	Loss 0.4490 (0.4758)	Prec@1 85.156 (85.186)	
Epoch: [11][233/391]	LR: 0.010000000000000002	Loss 0.4126 (0.4721)	Prec@1 85.938 (85.417)	
Epoch: [11][311/391]	LR: 0.010000000000000002	Loss 0.4858 (0.4790)	Prec@1 83.594 (85.261)	
Epoch: [11][389/391]	LR: 0.010000000000000002	Loss 0.4688 (0.4798)	Prec@1 83.594 (85.154)	
Total train loss: 0.4801

 * Prec@1 69.190 Prec@5 90.250 Loss 1.2725
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0010000000000000002	Loss 0.5630 (0.4726)	Prec@1 84.375 (85.347)	
Epoch: [12][155/391]	LR: 0.0010000000000000002	Loss 0.4277 (0.4733)	Prec@1 88.281 (85.387)	
Epoch: [12][233/391]	LR: 0.0010000000000000002	Loss 0.6021 (0.4733)	Prec@1 83.594 (85.423)	
Epoch: [12][311/391]	LR: 0.0010000000000000002	Loss 0.4653 (0.4755)	Prec@1 85.938 (85.417)	
Epoch: [12][389/391]	LR: 0.0010000000000000002	Loss 0.5483 (0.4756)	Prec@1 81.250 (85.461)	
Total train loss: 0.4755

 * Prec@1 68.970 Prec@5 90.160 Loss 1.2695
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0010000000000000002	Loss 0.4343 (0.4775)	Prec@1 85.938 (85.487)	
Epoch: [13][155/391]	LR: 0.0010000000000000002	Loss 0.6284 (0.4866)	Prec@1 78.125 (84.931)	
Epoch: [13][233/391]	LR: 0.0010000000000000002	Loss 0.4832 (0.4832)	Prec@1 82.812 (85.063)	
Epoch: [13][311/391]	LR: 0.0010000000000000002	Loss 0.4709 (0.4785)	Prec@1 83.594 (85.124)	
Epoch: [13][389/391]	LR: 0.0010000000000000002	Loss 0.5371 (0.4776)	Prec@1 83.594 (85.138)	
Total train loss: 0.4779

 * Prec@1 69.160 Prec@5 90.120 Loss 1.2725
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0010000000000000002	Loss 0.5464 (0.4794)	Prec@1 81.250 (85.286)	
Epoch: [14][155/391]	LR: 0.0010000000000000002	Loss 0.4697 (0.4759)	Prec@1 87.500 (85.362)	
Epoch: [14][233/391]	LR: 0.0010000000000000002	Loss 0.4182 (0.4741)	Prec@1 84.375 (85.460)	
Epoch: [14][311/391]	LR: 0.0010000000000000002	Loss 0.5015 (0.4741)	Prec@1 85.938 (85.492)	
Epoch: [14][389/391]	LR: 0.0010000000000000002	Loss 0.4646 (0.4751)	Prec@1 87.500 (85.431)	
Total train loss: 0.4750

 * Prec@1 68.950 Prec@5 90.300 Loss 1.2803
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.0010000000000000002	Loss 0.3523 (0.4727)	Prec@1 86.719 (85.317)	
Epoch: [15][155/391]	LR: 0.0010000000000000002	Loss 0.4258 (0.4775)	Prec@1 91.406 (85.266)	
Epoch: [15][233/391]	LR: 0.0010000000000000002	Loss 0.6138 (0.4762)	Prec@1 82.031 (85.410)	
Epoch: [15][311/391]	LR: 0.0010000000000000002	Loss 0.5127 (0.4741)	Prec@1 82.031 (85.459)	
Epoch: [15][389/391]	LR: 0.0010000000000000002	Loss 0.4331 (0.4755)	Prec@1 84.375 (85.403)	
Total train loss: 0.4756

 * Prec@1 69.110 Prec@5 90.210 Loss 1.2695
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.0010000000000000002	Loss 0.4741 (0.4701)	Prec@1 85.938 (85.266)	
Epoch: [16][155/391]	LR: 0.0010000000000000002	Loss 0.5117 (0.4750)	Prec@1 81.250 (85.477)	
Epoch: [16][233/391]	LR: 0.0010000000000000002	Loss 0.3779 (0.4791)	Prec@1 89.844 (85.220)	
Epoch: [16][311/391]	LR: 0.0010000000000000002	Loss 0.6118 (0.4772)	Prec@1 79.688 (85.379)	
Epoch: [16][389/391]	LR: 0.0010000000000000002	Loss 0.5854 (0.4763)	Prec@1 80.469 (85.298)	
Total train loss: 0.4764

 * Prec@1 69.180 Prec@5 90.160 Loss 1.2725
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.0010000000000000002	Loss 0.5361 (0.4731)	Prec@1 85.156 (85.327)	
Epoch: [17][155/391]	LR: 0.0010000000000000002	Loss 0.5337 (0.4798)	Prec@1 82.031 (85.046)	
Epoch: [17][233/391]	LR: 0.0010000000000000002	Loss 0.6689 (0.4793)	Prec@1 81.250 (85.186)	
Epoch: [17][311/391]	LR: 0.0010000000000000002	Loss 0.4541 (0.4768)	Prec@1 84.375 (85.281)	
Epoch: [17][389/391]	LR: 0.0010000000000000002	Loss 0.6436 (0.4773)	Prec@1 81.250 (85.312)	
Total train loss: 0.4771

 * Prec@1 69.100 Prec@5 90.320 Loss 1.2666
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.0010000000000000002	Loss 0.5303 (0.4770)	Prec@1 84.375 (85.747)	
Epoch: [18][155/391]	LR: 0.0010000000000000002	Loss 0.4758 (0.4799)	Prec@1 85.938 (85.472)	
Epoch: [18][233/391]	LR: 0.0010000000000000002	Loss 0.4500 (0.4816)	Prec@1 88.281 (85.283)	
Epoch: [18][311/391]	LR: 0.0010000000000000002	Loss 0.5122 (0.4789)	Prec@1 80.469 (85.219)	
Epoch: [18][389/391]	LR: 0.0010000000000000002	Loss 0.3696 (0.4772)	Prec@1 89.844 (85.335)	
Total train loss: 0.4770

 * Prec@1 68.930 Prec@5 90.300 Loss 1.2695
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.0010000000000000002	Loss 0.4797 (0.4867)	Prec@1 83.594 (84.946)	
Epoch: [19][155/391]	LR: 0.0010000000000000002	Loss 0.3582 (0.4741)	Prec@1 88.281 (85.477)	
Epoch: [19][233/391]	LR: 0.0010000000000000002	Loss 0.4382 (0.4756)	Prec@1 89.844 (85.387)	
Epoch: [19][311/391]	LR: 0.0010000000000000002	Loss 0.5156 (0.4775)	Prec@1 81.250 (85.309)	
Epoch: [19][389/391]	LR: 0.0010000000000000002	Loss 0.5186 (0.4732)	Prec@1 83.594 (85.419)	
Total train loss: 0.4732

 * Prec@1 69.100 Prec@5 90.190 Loss 1.2744
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.00010000000000000003	Loss 0.6274 (0.4814)	Prec@1 78.125 (85.367)	
Epoch: [20][155/391]	LR: 0.00010000000000000003	Loss 0.3696 (0.4797)	Prec@1 87.500 (85.637)	
Epoch: [20][233/391]	LR: 0.00010000000000000003	Loss 0.3889 (0.4802)	Prec@1 88.281 (85.450)	
Epoch: [20][311/391]	LR: 0.00010000000000000003	Loss 0.5435 (0.4769)	Prec@1 85.156 (85.507)	
Epoch: [20][389/391]	LR: 0.00010000000000000003	Loss 0.4119 (0.4746)	Prec@1 83.594 (85.447)	
Total train loss: 0.4746

 * Prec@1 69.060 Prec@5 90.390 Loss 1.2764
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.00010000000000000003	Loss 0.4429 (0.4735)	Prec@1 88.281 (85.517)	
Epoch: [21][155/391]	LR: 0.00010000000000000003	Loss 0.4036 (0.4759)	Prec@1 88.281 (85.462)	
Epoch: [21][233/391]	LR: 0.00010000000000000003	Loss 0.4319 (0.4694)	Prec@1 88.281 (85.530)	
Epoch: [21][311/391]	LR: 0.00010000000000000003	Loss 0.5596 (0.4720)	Prec@1 83.594 (85.442)	
Epoch: [21][389/391]	LR: 0.00010000000000000003	Loss 0.4348 (0.4733)	Prec@1 85.938 (85.443)	
Total train loss: 0.4735

 * Prec@1 69.160 Prec@5 90.380 Loss 1.2695
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.00010000000000000003	Loss 0.4319 (0.4752)	Prec@1 86.719 (85.357)	
Epoch: [22][155/391]	LR: 0.00010000000000000003	Loss 0.4355 (0.4859)	Prec@1 85.938 (85.031)	
Epoch: [22][233/391]	LR: 0.00010000000000000003	Loss 0.3811 (0.4823)	Prec@1 85.938 (85.066)	
Epoch: [22][311/391]	LR: 0.00010000000000000003	Loss 0.4646 (0.4798)	Prec@1 88.281 (85.184)	
Epoch: [22][389/391]	LR: 0.00010000000000000003	Loss 0.4902 (0.4770)	Prec@1 79.688 (85.266)	
Total train loss: 0.4772

 * Prec@1 69.040 Prec@5 90.230 Loss 1.2705
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.00010000000000000003	Loss 0.4736 (0.4804)	Prec@1 85.156 (85.347)	
Epoch: [23][155/391]	LR: 0.00010000000000000003	Loss 0.5679 (0.4803)	Prec@1 84.375 (85.246)	
Epoch: [23][233/391]	LR: 0.00010000000000000003	Loss 0.2910 (0.4795)	Prec@1 89.844 (85.216)	
Epoch: [23][311/391]	LR: 0.00010000000000000003	Loss 0.4988 (0.4778)	Prec@1 85.938 (85.256)	
Epoch: [23][389/391]	LR: 0.00010000000000000003	Loss 0.3848 (0.4764)	Prec@1 87.500 (85.278)	
Total train loss: 0.4761

 * Prec@1 69.160 Prec@5 90.360 Loss 1.2734
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.00010000000000000003	Loss 0.5771 (0.4909)	Prec@1 82.812 (85.146)	
Epoch: [24][155/391]	LR: 0.00010000000000000003	Loss 0.5322 (0.4813)	Prec@1 85.156 (85.407)	
Epoch: [24][233/391]	LR: 0.00010000000000000003	Loss 0.3940 (0.4753)	Prec@1 89.844 (85.480)	
Epoch: [24][311/391]	LR: 0.00010000000000000003	Loss 0.4766 (0.4728)	Prec@1 84.375 (85.589)	
Epoch: [24][389/391]	LR: 0.00010000000000000003	Loss 0.4634 (0.4738)	Prec@1 85.156 (85.555)	
Total train loss: 0.4740

 * Prec@1 68.990 Prec@5 90.210 Loss 1.2734
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.00010000000000000003	Loss 0.4929 (0.4722)	Prec@1 82.812 (85.637)	
Epoch: [25][155/391]	LR: 0.00010000000000000003	Loss 0.4875 (0.4700)	Prec@1 82.031 (85.632)	
Epoch: [25][233/391]	LR: 0.00010000000000000003	Loss 0.4924 (0.4724)	Prec@1 85.938 (85.487)	
Epoch: [25][311/391]	LR: 0.00010000000000000003	Loss 0.4558 (0.4757)	Prec@1 84.375 (85.434)	
Epoch: [25][389/391]	LR: 0.00010000000000000003	Loss 0.4053 (0.4746)	Prec@1 85.938 (85.411)	
Total train loss: 0.4745

 * Prec@1 68.970 Prec@5 90.170 Loss 1.2725
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.00010000000000000003	Loss 0.5552 (0.4697)	Prec@1 83.594 (85.507)	
Epoch: [26][155/391]	LR: 0.00010000000000000003	Loss 0.4583 (0.4769)	Prec@1 85.938 (85.322)	
Epoch: [26][233/391]	LR: 0.00010000000000000003	Loss 0.4478 (0.4818)	Prec@1 86.719 (85.166)	
Epoch: [26][311/391]	LR: 0.00010000000000000003	Loss 0.5522 (0.4805)	Prec@1 80.469 (85.166)	
Epoch: [26][389/391]	LR: 0.00010000000000000003	Loss 0.3940 (0.4776)	Prec@1 85.938 (85.290)	
Total train loss: 0.4775

 * Prec@1 69.280 Prec@5 90.280 Loss 1.2686
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.00010000000000000003	Loss 0.5195 (0.4868)	Prec@1 84.375 (84.836)	
Epoch: [27][155/391]	LR: 0.00010000000000000003	Loss 0.4824 (0.4762)	Prec@1 85.156 (85.332)	
Epoch: [27][233/391]	LR: 0.00010000000000000003	Loss 0.4302 (0.4701)	Prec@1 87.500 (85.654)	
Epoch: [27][311/391]	LR: 0.00010000000000000003	Loss 0.4890 (0.4721)	Prec@1 85.938 (85.532)	
Epoch: [27][389/391]	LR: 0.00010000000000000003	Loss 0.5054 (0.4734)	Prec@1 83.594 (85.519)	
Total train loss: 0.4736

 * Prec@1 69.020 Prec@5 90.200 Loss 1.2754
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.00010000000000000003	Loss 0.5317 (0.4797)	Prec@1 79.688 (84.856)	
Epoch: [28][155/391]	LR: 0.00010000000000000003	Loss 0.5186 (0.4798)	Prec@1 84.375 (85.091)	
Epoch: [28][233/391]	LR: 0.00010000000000000003	Loss 0.5562 (0.4804)	Prec@1 85.938 (85.096)	
Epoch: [28][311/391]	LR: 0.00010000000000000003	Loss 0.5459 (0.4775)	Prec@1 82.031 (85.204)	
Epoch: [28][389/391]	LR: 0.00010000000000000003	Loss 0.4446 (0.4774)	Prec@1 84.375 (85.252)	
Total train loss: 0.4775

 * Prec@1 69.230 Prec@5 90.230 Loss 1.2695
Best acc: 69.300
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.00010000000000000003	Loss 0.4426 (0.4706)	Prec@1 88.281 (85.747)	
Epoch: [29][155/391]	LR: 0.00010000000000000003	Loss 0.4480 (0.4785)	Prec@1 85.938 (85.332)	
Epoch: [29][233/391]	LR: 0.00010000000000000003	Loss 0.4141 (0.4767)	Prec@1 85.938 (85.437)	
Epoch: [29][311/391]	LR: 0.00010000000000000003	Loss 0.5859 (0.4708)	Prec@1 82.031 (85.630)	
Epoch: [29][389/391]	LR: 0.00010000000000000003	Loss 0.5303 (0.4728)	Prec@1 83.594 (85.593)	
Total train loss: 0.4730

 * Prec@1 69.210 Prec@5 90.250 Loss 1.2773
Best acc: 69.300
--------------------------------------------------------------------------------
