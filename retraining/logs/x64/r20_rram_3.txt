
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 1
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 1
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.360 Prec@5 90.430 Loss 1.1738
Pre-trained Prec@1 with 3 layers frozen: 69.36000061035156 	 Loss: 1.173828125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.6606 (0.5576)	Prec@1 81.250 (84.515)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.5562 (0.5646)	Prec@1 86.719 (84.245)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.6094 (0.5645)	Prec@1 83.594 (84.115)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.5356 (0.5601)	Prec@1 84.375 (84.245)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5127 (0.5605)	Prec@1 83.594 (84.223)	
Total train loss: 0.5607

 * Prec@1 69.490 Prec@5 90.720 Loss 1.1660
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.5771 (0.5439)	Prec@1 82.031 (85.126)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.4146 (0.5520)	Prec@1 89.062 (84.731)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.4817 (0.5540)	Prec@1 88.281 (84.659)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.6543 (0.5577)	Prec@1 78.125 (84.513)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.5781 (0.5595)	Prec@1 85.156 (84.389)	
Total train loss: 0.5598

 * Prec@1 69.380 Prec@5 90.680 Loss 1.1631
Best acc: 69.490
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.5620 (0.5669)	Prec@1 83.594 (84.235)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.5225 (0.5626)	Prec@1 86.719 (84.009)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.6240 (0.5582)	Prec@1 82.812 (84.268)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.5337 (0.5587)	Prec@1 83.594 (84.255)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.5107 (0.5573)	Prec@1 86.719 (84.329)	
Total train loss: 0.5574

 * Prec@1 69.580 Prec@5 90.560 Loss 1.1650
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.7261 (0.5552)	Prec@1 75.000 (84.255)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.5737 (0.5561)	Prec@1 84.375 (84.210)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.6050 (0.5564)	Prec@1 83.594 (84.298)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5449 (0.5593)	Prec@1 87.500 (84.177)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.6016 (0.5578)	Prec@1 84.375 (84.171)	
Total train loss: 0.5577

 * Prec@1 69.130 Prec@5 90.520 Loss 1.1689
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.6860 (0.5494)	Prec@1 81.250 (84.685)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.7690 (0.5498)	Prec@1 74.219 (84.580)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.5366 (0.5518)	Prec@1 85.156 (84.515)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.4658 (0.5536)	Prec@1 88.281 (84.385)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.5269 (0.5553)	Prec@1 87.500 (84.365)	
Total train loss: 0.5554

 * Prec@1 69.350 Prec@5 90.600 Loss 1.1621
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.6450 (0.5606)	Prec@1 81.250 (84.065)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.4878 (0.5587)	Prec@1 86.719 (84.029)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.5498 (0.5567)	Prec@1 85.156 (84.185)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.6035 (0.5552)	Prec@1 84.375 (84.332)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.4980 (0.5517)	Prec@1 84.375 (84.437)	
Total train loss: 0.5518

 * Prec@1 69.470 Prec@5 90.770 Loss 1.1660
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.6406 (0.5554)	Prec@1 82.812 (84.575)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.4866 (0.5560)	Prec@1 86.719 (84.290)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.6592 (0.5571)	Prec@1 84.375 (84.318)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.5195 (0.5564)	Prec@1 82.812 (84.438)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.6504 (0.5548)	Prec@1 82.031 (84.429)	
Total train loss: 0.5548

 * Prec@1 69.330 Prec@5 90.600 Loss 1.1689
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.5869 (0.5529)	Prec@1 84.375 (84.455)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.6255 (0.5538)	Prec@1 82.031 (84.445)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.5049 (0.5558)	Prec@1 89.844 (84.191)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5825 (0.5539)	Prec@1 82.031 (84.315)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.4509 (0.5526)	Prec@1 85.156 (84.399)	
Total train loss: 0.5529

 * Prec@1 69.580 Prec@5 90.650 Loss 1.1602
Best acc: 69.580
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.4602 (0.5489)	Prec@1 89.062 (84.706)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.4741 (0.5449)	Prec@1 87.500 (84.741)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.5435 (0.5488)	Prec@1 85.938 (84.732)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.5942 (0.5520)	Prec@1 85.938 (84.696)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.7012 (0.5543)	Prec@1 78.125 (84.627)	
Total train loss: 0.5547

 * Prec@1 69.590 Prec@5 90.710 Loss 1.1631
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.4141 (0.5539)	Prec@1 89.062 (84.826)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.7778 (0.5499)	Prec@1 76.562 (84.761)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.5049 (0.5495)	Prec@1 85.938 (84.822)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.5527 (0.5486)	Prec@1 85.938 (84.778)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.5381 (0.5514)	Prec@1 87.500 (84.609)	
Total train loss: 0.5515

 * Prec@1 69.430 Prec@5 90.600 Loss 1.1641
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.6436 (0.5407)	Prec@1 80.469 (84.946)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.6079 (0.5429)	Prec@1 80.469 (84.706)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.5283 (0.5467)	Prec@1 82.812 (84.612)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.4697 (0.5492)	Prec@1 87.500 (84.605)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.5366 (0.5536)	Prec@1 84.375 (84.357)	
Total train loss: 0.5537

 * Prec@1 69.500 Prec@5 90.700 Loss 1.1611
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.5356 (0.5374)	Prec@1 83.594 (85.126)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.4619 (0.5447)	Prec@1 87.500 (84.816)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.5488 (0.5499)	Prec@1 82.031 (84.652)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.6030 (0.5529)	Prec@1 83.594 (84.588)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5181 (0.5529)	Prec@1 82.812 (84.561)	
Total train loss: 0.5529

 * Prec@1 69.520 Prec@5 90.590 Loss 1.1631
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.5415 (0.5423)	Prec@1 82.812 (85.036)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.5347 (0.5466)	Prec@1 85.156 (84.756)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.5254 (0.5468)	Prec@1 86.719 (84.866)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.5815 (0.5521)	Prec@1 81.250 (84.623)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.5049 (0.5539)	Prec@1 86.719 (84.495)	
Total train loss: 0.5540

 * Prec@1 69.440 Prec@5 90.580 Loss 1.1719
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.5469 (0.5441)	Prec@1 85.156 (84.736)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.5029 (0.5485)	Prec@1 85.938 (84.635)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.5068 (0.5491)	Prec@1 84.375 (84.649)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.5703 (0.5498)	Prec@1 82.031 (84.675)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.4109 (0.5536)	Prec@1 91.406 (84.629)	
Total train loss: 0.5537

 * Prec@1 69.570 Prec@5 90.570 Loss 1.1621
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.6782 (0.5513)	Prec@1 79.688 (84.225)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.5796 (0.5456)	Prec@1 82.031 (84.470)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.6846 (0.5542)	Prec@1 81.250 (84.325)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.4883 (0.5518)	Prec@1 85.156 (84.362)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.6958 (0.5522)	Prec@1 80.469 (84.381)	
Total train loss: 0.5521

 * Prec@1 69.150 Prec@5 90.740 Loss 1.1611
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.5205 (0.5612)	Prec@1 85.938 (84.305)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.5684 (0.5508)	Prec@1 84.375 (84.706)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.6636 (0.5551)	Prec@1 81.250 (84.565)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.5337 (0.5558)	Prec@1 82.812 (84.398)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.5625 (0.5545)	Prec@1 82.812 (84.417)	
Total train loss: 0.5544

 * Prec@1 69.420 Prec@5 90.730 Loss 1.1602
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.5801 (0.5614)	Prec@1 81.250 (83.994)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.4775 (0.5589)	Prec@1 88.281 (84.115)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.4822 (0.5591)	Prec@1 89.062 (84.161)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.5981 (0.5559)	Prec@1 82.031 (84.267)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.5405 (0.5537)	Prec@1 84.375 (84.419)	
Total train loss: 0.5536

 * Prec@1 69.360 Prec@5 90.600 Loss 1.1611
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.4912 (0.5542)	Prec@1 84.375 (84.655)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.5215 (0.5534)	Prec@1 83.594 (84.680)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.3945 (0.5526)	Prec@1 91.406 (84.679)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.4155 (0.5547)	Prec@1 89.844 (84.543)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.5244 (0.5512)	Prec@1 84.375 (84.635)	
Total train loss: 0.5512

 * Prec@1 69.270 Prec@5 90.740 Loss 1.1699
Best acc: 69.590
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.5029 (0.5519)	Prec@1 86.719 (84.425)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.5527 (0.5537)	Prec@1 85.938 (84.420)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.6147 (0.5514)	Prec@1 82.031 (84.525)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.7378 (0.5521)	Prec@1 78.125 (84.535)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.6128 (0.5535)	Prec@1 79.688 (84.513)	
Total train loss: 0.5537

 * Prec@1 69.630 Prec@5 90.560 Loss 1.1670
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.4253 (0.5602)	Prec@1 88.281 (84.205)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.5796 (0.5584)	Prec@1 82.031 (84.240)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.6138 (0.5551)	Prec@1 84.375 (84.181)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.4475 (0.5551)	Prec@1 88.281 (84.210)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.6006 (0.5525)	Prec@1 85.156 (84.413)	
Total train loss: 0.5526

 * Prec@1 69.390 Prec@5 90.580 Loss 1.1631
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.5986 (0.5507)	Prec@1 82.812 (84.475)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.5767 (0.5532)	Prec@1 82.812 (84.455)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.4990 (0.5515)	Prec@1 86.719 (84.415)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.4309 (0.5554)	Prec@1 89.062 (84.310)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.5171 (0.5525)	Prec@1 86.719 (84.395)	
Total train loss: 0.5526

 * Prec@1 69.580 Prec@5 90.610 Loss 1.1650
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.6450 (0.5535)	Prec@1 78.906 (84.325)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.5796 (0.5548)	Prec@1 85.938 (84.270)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5425 (0.5555)	Prec@1 83.594 (84.395)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.4023 (0.5517)	Prec@1 89.844 (84.425)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.6274 (0.5510)	Prec@1 82.031 (84.521)	
Total train loss: 0.5512

 * Prec@1 69.430 Prec@5 90.550 Loss 1.1719
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.6826 (0.5493)	Prec@1 78.125 (84.425)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.5220 (0.5543)	Prec@1 85.156 (84.330)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.6387 (0.5535)	Prec@1 79.688 (84.275)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.6934 (0.5528)	Prec@1 77.344 (84.362)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.5869 (0.5534)	Prec@1 84.375 (84.367)	
Total train loss: 0.5536

 * Prec@1 69.510 Prec@5 90.670 Loss 1.1680
Best acc: 69.630
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.5444 (0.5524)	Prec@1 82.812 (84.355)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.7129 (0.5524)	Prec@1 79.688 (84.465)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.5493 (0.5550)	Prec@1 84.375 (84.405)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.4812 (0.5550)	Prec@1 87.500 (84.460)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.5371 (0.5522)	Prec@1 81.250 (84.515)	
Total train loss: 0.5524

 * Prec@1 69.840 Prec@5 90.730 Loss 1.1523
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.4836 (0.5572)	Prec@1 83.594 (84.105)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.6528 (0.5539)	Prec@1 78.906 (84.430)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5752 (0.5553)	Prec@1 80.469 (84.251)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.5610 (0.5520)	Prec@1 84.375 (84.420)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5533)	Prec@1 87.500 (84.437)	
Total train loss: 0.5534

 * Prec@1 69.370 Prec@5 90.650 Loss 1.1660
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.5957 (0.5560)	Prec@1 84.375 (84.886)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.6782 (0.5527)	Prec@1 80.469 (84.650)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.5771 (0.5475)	Prec@1 80.469 (84.756)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.4702 (0.5515)	Prec@1 85.156 (84.580)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.4961 (0.5509)	Prec@1 86.719 (84.627)	
Total train loss: 0.5512

 * Prec@1 69.310 Prec@5 90.760 Loss 1.1621
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.5977 (0.5430)	Prec@1 79.688 (84.806)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.7119 (0.5453)	Prec@1 78.906 (84.655)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.5938 (0.5493)	Prec@1 81.250 (84.589)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.4924 (0.5517)	Prec@1 84.375 (84.470)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.6875 (0.5530)	Prec@1 80.469 (84.367)	
Total train loss: 0.5533

 * Prec@1 69.530 Prec@5 90.510 Loss 1.1699
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.5898 (0.5630)	Prec@1 81.250 (84.415)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.6294 (0.5615)	Prec@1 82.812 (84.475)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.5254 (0.5561)	Prec@1 81.250 (84.509)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.5200 (0.5556)	Prec@1 83.594 (84.565)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.5322 (0.5550)	Prec@1 85.938 (84.531)	
Total train loss: 0.5548

 * Prec@1 69.450 Prec@5 90.800 Loss 1.1641
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.4666 (0.5479)	Prec@1 85.156 (84.685)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.5703 (0.5496)	Prec@1 80.469 (84.560)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.4812 (0.5472)	Prec@1 88.281 (84.706)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.5127 (0.5512)	Prec@1 86.719 (84.706)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.4976 (0.5521)	Prec@1 87.500 (84.601)	
Total train loss: 0.5524

 * Prec@1 69.550 Prec@5 90.780 Loss 1.1592
Best acc: 69.840
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.5625 (0.5537)	Prec@1 85.156 (84.736)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.5562 (0.5488)	Prec@1 86.719 (84.826)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.6006 (0.5523)	Prec@1 84.375 (84.572)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.6338 (0.5496)	Prec@1 81.250 (84.663)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.8179 (0.5519)	Prec@1 73.438 (84.523)	
Total train loss: 0.5519

 * Prec@1 69.130 Prec@5 90.700 Loss 1.1660
Best acc: 69.840
--------------------------------------------------------------------------------
