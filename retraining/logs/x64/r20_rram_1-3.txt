
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [10, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.380 Prec@5 90.440 Loss 1.1680
Pre-trained Prec@1 with 1 layers frozen: 69.37999725341797 	 Loss: 1.16796875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	Loss 0.8433 (0.6879)	Prec@1 77.344 (80.118)	
Epoch: [0][155/391]	LR: 0.01	Loss 0.8647 (0.7325)	Prec@1 72.656 (78.631)	
Epoch: [0][233/391]	LR: 0.01	Loss 0.8418 (0.7572)	Prec@1 74.219 (77.654)	
Epoch: [0][311/391]	LR: 0.01	Loss 1.0410 (0.7760)	Prec@1 68.750 (77.166)	
Epoch: [0][389/391]	LR: 0.01	Loss 0.8677 (0.7914)	Prec@1 70.312 (76.783)	
Total train loss: 0.7916

 * Prec@1 64.190 Prec@5 88.490 Loss 1.3535
Best acc: 64.190
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.01	Loss 0.5703 (0.6546)	Prec@1 83.594 (81.911)	
Epoch: [1][155/391]	LR: 0.01	Loss 0.7500 (0.6763)	Prec@1 77.344 (80.839)	
Epoch: [1][233/391]	LR: 0.01	Loss 0.7236 (0.6873)	Prec@1 82.812 (80.402)	
Epoch: [1][311/391]	LR: 0.01	Loss 0.8994 (0.7006)	Prec@1 71.094 (79.958)	
Epoch: [1][389/391]	LR: 0.01	Loss 0.7998 (0.7092)	Prec@1 79.688 (79.583)	
Total train loss: 0.7097

 * Prec@1 63.570 Prec@5 87.730 Loss 1.4277
Best acc: 64.190
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.01	Loss 0.6016 (0.5686)	Prec@1 85.156 (84.325)	
Epoch: [2][155/391]	LR: 0.01	Loss 0.6733 (0.5921)	Prec@1 75.781 (83.263)	
Epoch: [2][233/391]	LR: 0.01	Loss 0.6479 (0.6178)	Prec@1 82.031 (82.429)	
Epoch: [2][311/391]	LR: 0.01	Loss 0.6250 (0.6319)	Prec@1 84.375 (81.914)	
Epoch: [2][389/391]	LR: 0.01	Loss 0.7090 (0.6452)	Prec@1 78.125 (81.448)	
Total train loss: 0.6455

 * Prec@1 63.860 Prec@5 87.860 Loss 1.3975
Best acc: 64.190
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.01	Loss 0.4253 (0.5311)	Prec@1 87.500 (85.517)	
Epoch: [3][155/391]	LR: 0.01	Loss 0.6582 (0.5531)	Prec@1 83.594 (84.776)	
Epoch: [3][233/391]	LR: 0.01	Loss 0.4651 (0.5715)	Prec@1 84.375 (84.054)	
Epoch: [3][311/391]	LR: 0.01	Loss 0.5488 (0.5803)	Prec@1 85.938 (83.764)	
Epoch: [3][389/391]	LR: 0.01	Loss 0.6396 (0.5921)	Prec@1 84.375 (83.303)	
Total train loss: 0.5927

 * Prec@1 65.400 Prec@5 88.300 Loss 1.3418
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.01	Loss 0.3962 (0.4948)	Prec@1 89.062 (86.879)	
Epoch: [4][155/391]	LR: 0.01	Loss 0.5566 (0.5107)	Prec@1 81.250 (86.183)	
Epoch: [4][233/391]	LR: 0.01	Loss 0.4727 (0.5205)	Prec@1 88.281 (85.787)	
Epoch: [4][311/391]	LR: 0.01	Loss 0.6943 (0.5339)	Prec@1 78.906 (85.264)	
Epoch: [4][389/391]	LR: 0.01	Loss 0.5859 (0.5465)	Prec@1 83.594 (84.776)	
Total train loss: 0.5466

 * Prec@1 63.630 Prec@5 87.680 Loss 1.4375
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.01	Loss 0.3948 (0.4436)	Prec@1 89.062 (88.612)	
Epoch: [5][155/391]	LR: 0.01	Loss 0.4829 (0.4628)	Prec@1 89.844 (87.921)	
Epoch: [5][233/391]	LR: 0.01	Loss 0.4839 (0.4705)	Prec@1 86.719 (87.550)	
Epoch: [5][311/391]	LR: 0.01	Loss 0.6401 (0.4858)	Prec@1 85.156 (86.989)	
Epoch: [5][389/391]	LR: 0.01	Loss 0.5654 (0.4981)	Prec@1 82.031 (86.526)	
Total train loss: 0.4982

 * Prec@1 64.430 Prec@5 87.690 Loss 1.4229
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.01	Loss 0.3945 (0.4079)	Prec@1 86.719 (89.533)	
Epoch: [6][155/391]	LR: 0.01	Loss 0.4834 (0.4260)	Prec@1 88.281 (88.822)	
Epoch: [6][233/391]	LR: 0.01	Loss 0.4119 (0.4401)	Prec@1 92.969 (88.321)	
Epoch: [6][311/391]	LR: 0.01	Loss 0.4827 (0.4508)	Prec@1 87.500 (87.943)	
Epoch: [6][389/391]	LR: 0.01	Loss 0.4519 (0.4637)	Prec@1 86.719 (87.408)	
Total train loss: 0.4644

 * Prec@1 63.400 Prec@5 86.960 Loss 1.4639
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.01	Loss 0.3809 (0.4015)	Prec@1 89.062 (89.623)	
Epoch: [7][155/391]	LR: 0.01	Loss 0.4622 (0.4027)	Prec@1 85.938 (89.689)	
Epoch: [7][233/391]	LR: 0.01	Loss 0.4707 (0.4091)	Prec@1 83.594 (89.537)	
Epoch: [7][311/391]	LR: 0.01	Loss 0.3792 (0.4172)	Prec@1 90.625 (89.313)	
Epoch: [7][389/391]	LR: 0.01	Loss 0.5200 (0.4253)	Prec@1 82.812 (88.992)	
Total train loss: 0.4254

 * Prec@1 64.200 Prec@5 87.130 Loss 1.4795
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.01	Loss 0.3440 (0.3516)	Prec@1 92.188 (91.897)	
Epoch: [8][155/391]	LR: 0.01	Loss 0.3984 (0.3542)	Prec@1 90.625 (91.582)	
Epoch: [8][233/391]	LR: 0.01	Loss 0.4763 (0.3693)	Prec@1 87.500 (90.966)	
Epoch: [8][311/391]	LR: 0.01	Loss 0.4265 (0.3814)	Prec@1 92.188 (90.557)	
Epoch: [8][389/391]	LR: 0.01	Loss 0.4746 (0.3907)	Prec@1 85.156 (90.242)	
Total train loss: 0.3910

 * Prec@1 63.420 Prec@5 86.990 Loss 1.4893
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.01	Loss 0.3564 (0.3172)	Prec@1 91.406 (92.869)	
Epoch: [9][155/391]	LR: 0.01	Loss 0.2524 (0.3268)	Prec@1 94.531 (92.568)	
Epoch: [9][233/391]	LR: 0.01	Loss 0.3003 (0.3397)	Prec@1 92.188 (92.141)	
Epoch: [9][311/391]	LR: 0.01	Loss 0.3447 (0.3512)	Prec@1 91.406 (91.709)	
Epoch: [9][389/391]	LR: 0.01	Loss 0.3523 (0.3618)	Prec@1 91.406 (91.314)	
Total train loss: 0.3621

 * Prec@1 61.490 Prec@5 85.890 Loss 1.5732
Best acc: 65.400
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.001	Loss 0.2227 (0.2805)	Prec@1 96.875 (94.361)	
Epoch: [10][155/391]	LR: 0.001	Loss 0.2705 (0.2647)	Prec@1 95.312 (94.762)	
Epoch: [10][233/391]	LR: 0.001	Loss 0.2788 (0.2549)	Prec@1 93.750 (95.132)	
Epoch: [10][311/391]	LR: 0.001	Loss 0.2722 (0.2489)	Prec@1 95.312 (95.297)	
Epoch: [10][389/391]	LR: 0.001	Loss 0.1967 (0.2438)	Prec@1 97.656 (95.465)	
Total train loss: 0.2437

 * Prec@1 65.890 Prec@5 87.820 Loss 1.3887
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.001	Loss 0.1688 (0.2062)	Prec@1 97.656 (96.855)	
Epoch: [11][155/391]	LR: 0.001	Loss 0.3018 (0.2059)	Prec@1 92.188 (96.930)	
Epoch: [11][233/391]	LR: 0.001	Loss 0.1444 (0.2029)	Prec@1 97.656 (96.985)	
Epoch: [11][311/391]	LR: 0.001	Loss 0.1974 (0.2032)	Prec@1 96.875 (97.030)	
Epoch: [11][389/391]	LR: 0.001	Loss 0.1698 (0.2028)	Prec@1 96.875 (97.041)	
Total train loss: 0.2029

 * Prec@1 65.800 Prec@5 87.770 Loss 1.3877
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.001	Loss 0.1681 (0.1930)	Prec@1 99.219 (97.526)	
Epoch: [12][155/391]	LR: 0.001	Loss 0.2288 (0.1937)	Prec@1 95.312 (97.381)	
Epoch: [12][233/391]	LR: 0.001	Loss 0.1539 (0.1928)	Prec@1 97.656 (97.346)	
Epoch: [12][311/391]	LR: 0.001	Loss 0.1583 (0.1918)	Prec@1 98.438 (97.356)	
Epoch: [12][389/391]	LR: 0.001	Loss 0.1587 (0.1911)	Prec@1 99.219 (97.364)	
Total train loss: 0.1912

 * Prec@1 65.730 Prec@5 87.620 Loss 1.3984
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.001	Loss 0.2373 (0.1800)	Prec@1 96.875 (97.676)	
Epoch: [13][155/391]	LR: 0.001	Loss 0.2583 (0.1799)	Prec@1 96.094 (97.751)	
Epoch: [13][233/391]	LR: 0.001	Loss 0.1641 (0.1797)	Prec@1 97.656 (97.803)	
Epoch: [13][311/391]	LR: 0.001	Loss 0.1515 (0.1804)	Prec@1 98.438 (97.794)	
Epoch: [13][389/391]	LR: 0.001	Loss 0.2502 (0.1803)	Prec@1 96.094 (97.794)	
Total train loss: 0.1804

 * Prec@1 65.510 Prec@5 87.520 Loss 1.4111
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.001	Loss 0.1763 (0.1770)	Prec@1 97.656 (97.997)	
Epoch: [14][155/391]	LR: 0.001	Loss 0.2316 (0.1778)	Prec@1 93.750 (97.887)	
Epoch: [14][233/391]	LR: 0.001	Loss 0.1573 (0.1772)	Prec@1 99.219 (97.857)	
Epoch: [14][311/391]	LR: 0.001	Loss 0.2192 (0.1773)	Prec@1 96.094 (97.854)	
Epoch: [14][389/391]	LR: 0.001	Loss 0.1360 (0.1772)	Prec@1 99.219 (97.871)	
Total train loss: 0.1773

 * Prec@1 65.610 Prec@5 87.590 Loss 1.4014
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.001	Loss 0.1807 (0.1694)	Prec@1 96.094 (97.987)	
Epoch: [15][155/391]	LR: 0.001	Loss 0.1732 (0.1698)	Prec@1 98.438 (98.027)	
Epoch: [15][233/391]	LR: 0.001	Loss 0.1727 (0.1705)	Prec@1 97.656 (98.010)	
Epoch: [15][311/391]	LR: 0.001	Loss 0.1265 (0.1704)	Prec@1 100.000 (98.082)	
Epoch: [15][389/391]	LR: 0.001	Loss 0.1643 (0.1707)	Prec@1 97.656 (98.051)	
Total train loss: 0.1709

 * Prec@1 65.720 Prec@5 87.530 Loss 1.4004
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.001	Loss 0.1565 (0.1669)	Prec@1 100.000 (98.147)	
Epoch: [16][155/391]	LR: 0.001	Loss 0.1388 (0.1680)	Prec@1 100.000 (98.227)	
Epoch: [16][233/391]	LR: 0.001	Loss 0.1226 (0.1663)	Prec@1 99.219 (98.287)	
Epoch: [16][311/391]	LR: 0.001	Loss 0.1864 (0.1660)	Prec@1 96.094 (98.357)	
Epoch: [16][389/391]	LR: 0.001	Loss 0.1547 (0.1657)	Prec@1 96.875 (98.339)	
Total train loss: 0.1658

 * Prec@1 65.580 Prec@5 87.390 Loss 1.4092
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.001	Loss 0.2094 (0.1612)	Prec@1 96.875 (98.297)	
Epoch: [17][155/391]	LR: 0.001	Loss 0.2024 (0.1621)	Prec@1 97.656 (98.312)	
Epoch: [17][233/391]	LR: 0.001	Loss 0.1724 (0.1624)	Prec@1 97.656 (98.314)	
Epoch: [17][311/391]	LR: 0.001	Loss 0.1814 (0.1630)	Prec@1 96.094 (98.265)	
Epoch: [17][389/391]	LR: 0.001	Loss 0.1802 (0.1637)	Prec@1 97.656 (98.247)	
Total train loss: 0.1639

 * Prec@1 65.630 Prec@5 87.450 Loss 1.4072
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.001	Loss 0.1554 (0.1532)	Prec@1 97.656 (98.598)	
Epoch: [18][155/391]	LR: 0.001	Loss 0.1721 (0.1577)	Prec@1 97.656 (98.463)	
Epoch: [18][233/391]	LR: 0.001	Loss 0.1572 (0.1594)	Prec@1 98.438 (98.427)	
Epoch: [18][311/391]	LR: 0.001	Loss 0.1342 (0.1594)	Prec@1 98.438 (98.402)	
Epoch: [18][389/391]	LR: 0.001	Loss 0.1404 (0.1592)	Prec@1 99.219 (98.405)	
Total train loss: 0.1593

 * Prec@1 65.500 Prec@5 87.290 Loss 1.4180
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.001	Loss 0.1309 (0.1534)	Prec@1 99.219 (98.498)	
Epoch: [19][155/391]	LR: 0.001	Loss 0.1238 (0.1535)	Prec@1 100.000 (98.518)	
Epoch: [19][233/391]	LR: 0.001	Loss 0.2008 (0.1561)	Prec@1 97.656 (98.481)	
Epoch: [19][311/391]	LR: 0.001	Loss 0.1516 (0.1570)	Prec@1 98.438 (98.483)	
Epoch: [19][389/391]	LR: 0.001	Loss 0.1448 (0.1569)	Prec@1 99.219 (98.496)	
Total train loss: 0.1568

 * Prec@1 65.630 Prec@5 87.340 Loss 1.4150
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0001	Loss 0.1378 (0.1544)	Prec@1 99.219 (98.548)	
Epoch: [20][155/391]	LR: 0.0001	Loss 0.1920 (0.1550)	Prec@1 97.656 (98.533)	
Epoch: [20][233/391]	LR: 0.0001	Loss 0.1232 (0.1545)	Prec@1 98.438 (98.521)	
Epoch: [20][311/391]	LR: 0.0001	Loss 0.1807 (0.1534)	Prec@1 98.438 (98.575)	
Epoch: [20][389/391]	LR: 0.0001	Loss 0.1815 (0.1538)	Prec@1 99.219 (98.584)	
Total train loss: 0.1539

 * Prec@1 65.690 Prec@5 87.500 Loss 1.4180
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0001	Loss 0.1603 (0.1562)	Prec@1 97.656 (98.387)	
Epoch: [21][155/391]	LR: 0.0001	Loss 0.1698 (0.1543)	Prec@1 96.875 (98.448)	
Epoch: [21][233/391]	LR: 0.0001	Loss 0.1307 (0.1558)	Prec@1 99.219 (98.458)	
Epoch: [21][311/391]	LR: 0.0001	Loss 0.1609 (0.1549)	Prec@1 99.219 (98.518)	
Epoch: [21][389/391]	LR: 0.0001	Loss 0.1648 (0.1556)	Prec@1 99.219 (98.524)	
Total train loss: 0.1557

 * Prec@1 65.670 Prec@5 87.410 Loss 1.4170
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0001	Loss 0.1422 (0.1505)	Prec@1 98.438 (98.708)	
Epoch: [22][155/391]	LR: 0.0001	Loss 0.1487 (0.1511)	Prec@1 96.875 (98.598)	
Epoch: [22][233/391]	LR: 0.0001	Loss 0.1478 (0.1528)	Prec@1 100.000 (98.591)	
Epoch: [22][311/391]	LR: 0.0001	Loss 0.1525 (0.1532)	Prec@1 97.656 (98.583)	
Epoch: [22][389/391]	LR: 0.0001	Loss 0.1323 (0.1523)	Prec@1 98.438 (98.580)	
Total train loss: 0.1524

 * Prec@1 65.710 Prec@5 87.410 Loss 1.4121
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0001	Loss 0.1523 (0.1587)	Prec@1 100.000 (98.468)	
Epoch: [23][155/391]	LR: 0.0001	Loss 0.1504 (0.1595)	Prec@1 99.219 (98.402)	
Epoch: [23][233/391]	LR: 0.0001	Loss 0.1191 (0.1574)	Prec@1 99.219 (98.464)	
Epoch: [23][311/391]	LR: 0.0001	Loss 0.1335 (0.1553)	Prec@1 98.438 (98.500)	
Epoch: [23][389/391]	LR: 0.0001	Loss 0.1685 (0.1553)	Prec@1 98.438 (98.492)	
Total train loss: 0.1553

 * Prec@1 65.670 Prec@5 87.430 Loss 1.4141
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0001	Loss 0.1381 (0.1516)	Prec@1 100.000 (98.578)	
Epoch: [24][155/391]	LR: 0.0001	Loss 0.1284 (0.1526)	Prec@1 99.219 (98.518)	
Epoch: [24][233/391]	LR: 0.0001	Loss 0.1450 (0.1533)	Prec@1 99.219 (98.541)	
Epoch: [24][311/391]	LR: 0.0001	Loss 0.1736 (0.1535)	Prec@1 97.656 (98.533)	
Epoch: [24][389/391]	LR: 0.0001	Loss 0.1545 (0.1529)	Prec@1 99.219 (98.564)	
Total train loss: 0.1529

 * Prec@1 65.630 Prec@5 87.380 Loss 1.4121
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0001	Loss 0.1030 (0.1539)	Prec@1 100.000 (98.558)	
Epoch: [25][155/391]	LR: 0.0001	Loss 0.1333 (0.1503)	Prec@1 100.000 (98.668)	
Epoch: [25][233/391]	LR: 0.0001	Loss 0.1169 (0.1516)	Prec@1 99.219 (98.638)	
Epoch: [25][311/391]	LR: 0.0001	Loss 0.0955 (0.1521)	Prec@1 99.219 (98.628)	
Epoch: [25][389/391]	LR: 0.0001	Loss 0.1473 (0.1529)	Prec@1 99.219 (98.610)	
Total train loss: 0.1530

 * Prec@1 65.700 Prec@5 87.340 Loss 1.4102
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0001	Loss 0.1763 (0.1507)	Prec@1 96.875 (98.668)	
Epoch: [26][155/391]	LR: 0.0001	Loss 0.1592 (0.1510)	Prec@1 98.438 (98.663)	
Epoch: [26][233/391]	LR: 0.0001	Loss 0.1250 (0.1513)	Prec@1 99.219 (98.638)	
Epoch: [26][311/391]	LR: 0.0001	Loss 0.1522 (0.1536)	Prec@1 98.438 (98.625)	
Epoch: [26][389/391]	LR: 0.0001	Loss 0.1555 (0.1540)	Prec@1 98.438 (98.626)	
Total train loss: 0.1540

 * Prec@1 65.620 Prec@5 87.310 Loss 1.4092
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0001	Loss 0.1722 (0.1557)	Prec@1 98.438 (98.708)	
Epoch: [27][155/391]	LR: 0.0001	Loss 0.1478 (0.1551)	Prec@1 99.219 (98.588)	
Epoch: [27][233/391]	LR: 0.0001	Loss 0.1416 (0.1529)	Prec@1 98.438 (98.604)	
Epoch: [27][311/391]	LR: 0.0001	Loss 0.1370 (0.1520)	Prec@1 100.000 (98.650)	
Epoch: [27][389/391]	LR: 0.0001	Loss 0.1785 (0.1523)	Prec@1 96.875 (98.616)	
Total train loss: 0.1524

 * Prec@1 65.100 Prec@5 87.320 Loss 1.4229
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0001	Loss 0.1534 (0.1553)	Prec@1 98.438 (98.387)	
Epoch: [28][155/391]	LR: 0.0001	Loss 0.1809 (0.1545)	Prec@1 96.875 (98.453)	
Epoch: [28][233/391]	LR: 0.0001	Loss 0.1370 (0.1555)	Prec@1 99.219 (98.427)	
Epoch: [28][311/391]	LR: 0.0001	Loss 0.1454 (0.1543)	Prec@1 99.219 (98.500)	
Epoch: [28][389/391]	LR: 0.0001	Loss 0.2067 (0.1542)	Prec@1 96.875 (98.508)	
Total train loss: 0.1542

 * Prec@1 65.580 Prec@5 87.310 Loss 1.4170
Best acc: 65.890
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0001	Loss 0.1692 (0.1549)	Prec@1 96.875 (98.508)	
Epoch: [29][155/391]	LR: 0.0001	Loss 0.1711 (0.1567)	Prec@1 98.438 (98.503)	
Epoch: [29][233/391]	LR: 0.0001	Loss 0.1289 (0.1559)	Prec@1 99.219 (98.614)	
Epoch: [29][311/391]	LR: 0.0001	Loss 0.1578 (0.1549)	Prec@1 98.438 (98.610)	
Epoch: [29][389/391]	LR: 0.0001	Loss 0.1378 (0.1537)	Prec@1 100.000 (98.620)	
Total train loss: 0.1539

 * Prec@1 65.710 Prec@5 87.300 Loss 1.4180
Best acc: 65.890
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.01
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [10, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.360 Prec@5 90.430 Loss 1.1738
Pre-trained Prec@1 with 3 layers frozen: 69.36000061035156 	 Loss: 1.173828125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.01	Loss 0.8286 (0.6799)	Prec@1 75.000 (80.359)	
Epoch: [0][155/391]	LR: 0.01	Loss 0.7617 (0.7305)	Prec@1 82.812 (78.791)	
Epoch: [0][233/391]	LR: 0.01	Loss 0.8359 (0.7570)	Prec@1 73.438 (77.855)	
Epoch: [0][311/391]	LR: 0.01	Loss 0.8320 (0.7740)	Prec@1 72.656 (77.361)	
Epoch: [0][389/391]	LR: 0.01	Loss 0.7744 (0.7873)	Prec@1 77.344 (77.017)	
Total train loss: 0.7877

 * Prec@1 65.280 Prec@5 88.950 Loss 1.3047
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.01	Loss 0.7183 (0.6299)	Prec@1 80.469 (82.051)	
Epoch: [1][155/391]	LR: 0.01	Loss 0.5361 (0.6555)	Prec@1 86.719 (81.365)	
Epoch: [1][233/391]	LR: 0.01	Loss 0.6846 (0.6702)	Prec@1 82.031 (80.940)	
Epoch: [1][311/391]	LR: 0.01	Loss 0.8438 (0.6896)	Prec@1 75.781 (80.248)	
Epoch: [1][389/391]	LR: 0.01	Loss 0.8804 (0.7022)	Prec@1 75.000 (79.794)	
Total train loss: 0.7027

 * Prec@1 63.500 Prec@5 88.260 Loss 1.3691
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.01	Loss 0.5767 (0.5934)	Prec@1 85.156 (83.393)	
Epoch: [2][155/391]	LR: 0.01	Loss 0.5771 (0.6057)	Prec@1 83.594 (82.802)	
Epoch: [2][233/391]	LR: 0.01	Loss 0.7393 (0.6181)	Prec@1 79.688 (82.338)	
Epoch: [2][311/391]	LR: 0.01	Loss 0.7710 (0.6293)	Prec@1 77.344 (81.964)	
Epoch: [2][389/391]	LR: 0.01	Loss 0.7056 (0.6393)	Prec@1 83.594 (81.645)	
Total train loss: 0.6396

 * Prec@1 64.250 Prec@5 87.880 Loss 1.3994
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.01	Loss 0.6665 (0.5258)	Prec@1 80.469 (85.927)	
Epoch: [3][155/391]	LR: 0.01	Loss 0.6128 (0.5470)	Prec@1 80.469 (84.971)	
Epoch: [3][233/391]	LR: 0.01	Loss 0.7173 (0.5583)	Prec@1 80.469 (84.532)	
Epoch: [3][311/391]	LR: 0.01	Loss 0.5830 (0.5752)	Prec@1 81.250 (83.919)	
Epoch: [3][389/391]	LR: 0.01	Loss 0.7090 (0.5845)	Prec@1 78.906 (83.530)	
Total train loss: 0.5846

 * Prec@1 63.160 Prec@5 87.350 Loss 1.4561
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.01	Loss 0.6143 (0.4681)	Prec@1 82.031 (87.800)	
Epoch: [4][155/391]	LR: 0.01	Loss 0.6787 (0.4871)	Prec@1 75.781 (87.134)	
Epoch: [4][233/391]	LR: 0.01	Loss 0.4927 (0.5045)	Prec@1 87.500 (86.345)	
Epoch: [4][311/391]	LR: 0.01	Loss 0.4985 (0.5183)	Prec@1 82.812 (85.820)	
Epoch: [4][389/391]	LR: 0.01	Loss 0.7109 (0.5346)	Prec@1 78.906 (85.232)	
Total train loss: 0.5348

 * Prec@1 63.430 Prec@5 87.660 Loss 1.4473
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.01	Loss 0.4709 (0.4469)	Prec@1 86.719 (88.121)	
Epoch: [5][155/391]	LR: 0.01	Loss 0.4641 (0.4560)	Prec@1 84.375 (88.051)	
Epoch: [5][233/391]	LR: 0.01	Loss 0.5312 (0.4680)	Prec@1 83.594 (87.600)	
Epoch: [5][311/391]	LR: 0.01	Loss 0.6499 (0.4794)	Prec@1 81.250 (87.072)	
Epoch: [5][389/391]	LR: 0.01	Loss 0.4761 (0.4890)	Prec@1 87.500 (86.627)	
Total train loss: 0.4891

 * Prec@1 63.700 Prec@5 87.530 Loss 1.4316
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.01	Loss 0.5181 (0.4055)	Prec@1 86.719 (89.744)	
Epoch: [6][155/391]	LR: 0.01	Loss 0.4832 (0.4134)	Prec@1 85.156 (89.458)	
Epoch: [6][233/391]	LR: 0.01	Loss 0.5400 (0.4257)	Prec@1 82.031 (89.022)	
Epoch: [6][311/391]	LR: 0.01	Loss 0.4717 (0.4398)	Prec@1 86.719 (88.549)	
Epoch: [6][389/391]	LR: 0.01	Loss 0.5957 (0.4513)	Prec@1 81.250 (88.027)	
Total train loss: 0.4516

 * Prec@1 63.600 Prec@5 87.340 Loss 1.4648
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.01	Loss 0.4158 (0.3680)	Prec@1 88.281 (91.456)	
Epoch: [7][155/391]	LR: 0.01	Loss 0.3994 (0.3765)	Prec@1 91.406 (90.910)	
Epoch: [7][233/391]	LR: 0.01	Loss 0.3914 (0.3929)	Prec@1 89.844 (90.328)	
Epoch: [7][311/391]	LR: 0.01	Loss 0.5347 (0.4039)	Prec@1 87.500 (89.894)	
Epoch: [7][389/391]	LR: 0.01	Loss 0.3115 (0.4154)	Prec@1 92.969 (89.409)	
Total train loss: 0.4157

 * Prec@1 62.870 Prec@5 86.980 Loss 1.4980
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.01	Loss 0.3010 (0.3475)	Prec@1 96.094 (91.827)	
Epoch: [8][155/391]	LR: 0.01	Loss 0.4080 (0.3496)	Prec@1 87.500 (91.802)	
Epoch: [8][233/391]	LR: 0.01	Loss 0.3879 (0.3620)	Prec@1 89.844 (91.276)	
Epoch: [8][311/391]	LR: 0.01	Loss 0.4568 (0.3763)	Prec@1 87.500 (90.715)	
Epoch: [8][389/391]	LR: 0.01	Loss 0.4302 (0.3899)	Prec@1 88.281 (90.218)	
Total train loss: 0.3905

 * Prec@1 62.760 Prec@5 86.790 Loss 1.4844
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.01	Loss 0.2120 (0.3137)	Prec@1 96.875 (93.450)	
Epoch: [9][155/391]	LR: 0.01	Loss 0.3682 (0.3189)	Prec@1 89.062 (93.239)	
Epoch: [9][233/391]	LR: 0.01	Loss 0.4062 (0.3318)	Prec@1 89.844 (92.615)	
Epoch: [9][311/391]	LR: 0.01	Loss 0.3237 (0.3398)	Prec@1 93.750 (92.208)	
Epoch: [9][389/391]	LR: 0.01	Loss 0.4324 (0.3515)	Prec@1 87.500 (91.717)	
Total train loss: 0.3518

 * Prec@1 62.570 Prec@5 86.280 Loss 1.5547
Best acc: 65.280
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.001	Loss 0.2820 (0.2752)	Prec@1 94.531 (94.411)	
Epoch: [10][155/391]	LR: 0.001	Loss 0.2394 (0.2574)	Prec@1 95.312 (95.052)	
Epoch: [10][233/391]	LR: 0.001	Loss 0.1992 (0.2508)	Prec@1 96.875 (95.289)	
Epoch: [10][311/391]	LR: 0.001	Loss 0.1559 (0.2433)	Prec@1 98.438 (95.628)	
Epoch: [10][389/391]	LR: 0.001	Loss 0.1909 (0.2396)	Prec@1 98.438 (95.737)	
Total train loss: 0.2397

 * Prec@1 65.930 Prec@5 87.870 Loss 1.4004
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.001	Loss 0.2397 (0.2012)	Prec@1 97.656 (97.055)	
Epoch: [11][155/391]	LR: 0.001	Loss 0.2327 (0.2029)	Prec@1 95.312 (96.945)	
Epoch: [11][233/391]	LR: 0.001	Loss 0.1836 (0.2026)	Prec@1 95.312 (96.992)	
Epoch: [11][311/391]	LR: 0.001	Loss 0.2144 (0.2023)	Prec@1 96.094 (96.985)	
Epoch: [11][389/391]	LR: 0.001	Loss 0.1937 (0.2013)	Prec@1 96.875 (97.005)	
Total train loss: 0.2013

 * Prec@1 65.520 Prec@5 87.830 Loss 1.4102
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.001	Loss 0.1725 (0.1885)	Prec@1 98.438 (97.236)	
Epoch: [12][155/391]	LR: 0.001	Loss 0.1799 (0.1893)	Prec@1 98.438 (97.261)	
Epoch: [12][233/391]	LR: 0.001	Loss 0.1599 (0.1893)	Prec@1 97.656 (97.322)	
Epoch: [12][311/391]	LR: 0.001	Loss 0.2231 (0.1893)	Prec@1 96.094 (97.373)	
Epoch: [12][389/391]	LR: 0.001	Loss 0.1746 (0.1887)	Prec@1 96.875 (97.396)	
Total train loss: 0.1889

 * Prec@1 65.160 Prec@5 87.670 Loss 1.4229
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.001	Loss 0.1388 (0.1743)	Prec@1 98.438 (97.987)	
Epoch: [13][155/391]	LR: 0.001	Loss 0.1910 (0.1793)	Prec@1 97.656 (97.786)	
Epoch: [13][233/391]	LR: 0.001	Loss 0.1735 (0.1788)	Prec@1 100.000 (97.823)	
Epoch: [13][311/391]	LR: 0.001	Loss 0.1787 (0.1792)	Prec@1 97.656 (97.829)	
Epoch: [13][389/391]	LR: 0.001	Loss 0.1565 (0.1800)	Prec@1 100.000 (97.768)	
Total train loss: 0.1802

 * Prec@1 65.110 Prec@5 87.820 Loss 1.4170
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.001	Loss 0.1893 (0.1704)	Prec@1 96.875 (97.917)	
Epoch: [14][155/391]	LR: 0.001	Loss 0.2084 (0.1679)	Prec@1 97.656 (98.027)	
Epoch: [14][233/391]	LR: 0.001	Loss 0.1782 (0.1718)	Prec@1 96.875 (97.977)	
Epoch: [14][311/391]	LR: 0.001	Loss 0.1866 (0.1722)	Prec@1 97.656 (97.937)	
Epoch: [14][389/391]	LR: 0.001	Loss 0.1855 (0.1719)	Prec@1 98.438 (97.955)	
Total train loss: 0.1720

 * Prec@1 65.180 Prec@5 87.780 Loss 1.4229
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 0.001	Loss 0.1451 (0.1695)	Prec@1 98.438 (98.257)	
Epoch: [15][155/391]	LR: 0.001	Loss 0.2172 (0.1694)	Prec@1 96.094 (98.152)	
Epoch: [15][233/391]	LR: 0.001	Loss 0.1473 (0.1697)	Prec@1 99.219 (98.090)	
Epoch: [15][311/391]	LR: 0.001	Loss 0.1670 (0.1694)	Prec@1 97.656 (98.122)	
Epoch: [15][389/391]	LR: 0.001	Loss 0.1384 (0.1699)	Prec@1 98.438 (98.139)	
Total train loss: 0.1700

 * Prec@1 65.450 Prec@5 87.810 Loss 1.4141
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 0.001	Loss 0.1616 (0.1617)	Prec@1 100.000 (98.197)	
Epoch: [16][155/391]	LR: 0.001	Loss 0.1638 (0.1629)	Prec@1 96.094 (98.252)	
Epoch: [16][233/391]	LR: 0.001	Loss 0.1494 (0.1640)	Prec@1 98.438 (98.254)	
Epoch: [16][311/391]	LR: 0.001	Loss 0.2042 (0.1644)	Prec@1 98.438 (98.240)	
Epoch: [16][389/391]	LR: 0.001	Loss 0.1935 (0.1654)	Prec@1 96.094 (98.233)	
Total train loss: 0.1654

 * Prec@1 65.080 Prec@5 87.780 Loss 1.4209
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 0.001	Loss 0.1484 (0.1600)	Prec@1 97.656 (98.478)	
Epoch: [17][155/391]	LR: 0.001	Loss 0.1636 (0.1601)	Prec@1 97.656 (98.367)	
Epoch: [17][233/391]	LR: 0.001	Loss 0.1460 (0.1599)	Prec@1 98.438 (98.367)	
Epoch: [17][311/391]	LR: 0.001	Loss 0.1600 (0.1609)	Prec@1 96.094 (98.380)	
Epoch: [17][389/391]	LR: 0.001	Loss 0.1434 (0.1600)	Prec@1 99.219 (98.417)	
Total train loss: 0.1601

 * Prec@1 64.930 Prec@5 87.370 Loss 1.4346
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 0.001	Loss 0.1508 (0.1579)	Prec@1 98.438 (98.478)	
Epoch: [18][155/391]	LR: 0.001	Loss 0.1621 (0.1591)	Prec@1 99.219 (98.478)	
Epoch: [18][233/391]	LR: 0.001	Loss 0.1676 (0.1581)	Prec@1 98.438 (98.491)	
Epoch: [18][311/391]	LR: 0.001	Loss 0.2135 (0.1595)	Prec@1 96.875 (98.420)	
Epoch: [18][389/391]	LR: 0.001	Loss 0.2035 (0.1595)	Prec@1 98.438 (98.454)	
Total train loss: 0.1596

 * Prec@1 65.100 Prec@5 87.520 Loss 1.4268
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 0.001	Loss 0.1307 (0.1563)	Prec@1 99.219 (98.498)	
Epoch: [19][155/391]	LR: 0.001	Loss 0.1301 (0.1541)	Prec@1 100.000 (98.633)	
Epoch: [19][233/391]	LR: 0.001	Loss 0.1407 (0.1542)	Prec@1 98.438 (98.571)	
Epoch: [19][311/391]	LR: 0.001	Loss 0.1223 (0.1557)	Prec@1 98.438 (98.543)	
Epoch: [19][389/391]	LR: 0.001	Loss 0.1274 (0.1559)	Prec@1 99.219 (98.520)	
Total train loss: 0.1561

 * Prec@1 64.960 Prec@5 87.650 Loss 1.4277
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 0.0001	Loss 0.1377 (0.1519)	Prec@1 99.219 (98.307)	
Epoch: [20][155/391]	LR: 0.0001	Loss 0.1389 (0.1511)	Prec@1 100.000 (98.448)	
Epoch: [20][233/391]	LR: 0.0001	Loss 0.1674 (0.1506)	Prec@1 99.219 (98.474)	
Epoch: [20][311/391]	LR: 0.0001	Loss 0.1503 (0.1528)	Prec@1 97.656 (98.470)	
Epoch: [20][389/391]	LR: 0.0001	Loss 0.1404 (0.1520)	Prec@1 97.656 (98.454)	
Total train loss: 0.1521

 * Prec@1 64.980 Prec@5 87.550 Loss 1.4268
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 0.0001	Loss 0.1949 (0.1518)	Prec@1 95.312 (98.598)	
Epoch: [21][155/391]	LR: 0.0001	Loss 0.1918 (0.1539)	Prec@1 99.219 (98.498)	
Epoch: [21][233/391]	LR: 0.0001	Loss 0.1436 (0.1521)	Prec@1 98.438 (98.544)	
Epoch: [21][311/391]	LR: 0.0001	Loss 0.1134 (0.1513)	Prec@1 98.438 (98.600)	
Epoch: [21][389/391]	LR: 0.0001	Loss 0.1835 (0.1511)	Prec@1 96.875 (98.612)	
Total train loss: 0.1513

 * Prec@1 64.730 Prec@5 87.480 Loss 1.4385
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 0.0001	Loss 0.1708 (0.1515)	Prec@1 96.875 (98.618)	
Epoch: [22][155/391]	LR: 0.0001	Loss 0.1591 (0.1527)	Prec@1 98.438 (98.613)	
Epoch: [22][233/391]	LR: 0.0001	Loss 0.1832 (0.1523)	Prec@1 99.219 (98.655)	
Epoch: [22][311/391]	LR: 0.0001	Loss 0.1997 (0.1531)	Prec@1 97.656 (98.635)	
Epoch: [22][389/391]	LR: 0.0001	Loss 0.1570 (0.1519)	Prec@1 100.000 (98.644)	
Total train loss: 0.1522

 * Prec@1 65.030 Prec@5 87.500 Loss 1.4307
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 0.0001	Loss 0.1456 (0.1500)	Prec@1 98.438 (98.788)	
Epoch: [23][155/391]	LR: 0.0001	Loss 0.1176 (0.1489)	Prec@1 100.000 (98.793)	
Epoch: [23][233/391]	LR: 0.0001	Loss 0.1304 (0.1494)	Prec@1 99.219 (98.735)	
Epoch: [23][311/391]	LR: 0.0001	Loss 0.1241 (0.1501)	Prec@1 100.000 (98.678)	
Epoch: [23][389/391]	LR: 0.0001	Loss 0.1515 (0.1507)	Prec@1 99.219 (98.666)	
Total train loss: 0.1508

 * Prec@1 65.120 Prec@5 87.580 Loss 1.4229
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 0.0001	Loss 0.1345 (0.1497)	Prec@1 98.438 (98.628)	
Epoch: [24][155/391]	LR: 0.0001	Loss 0.1577 (0.1506)	Prec@1 98.438 (98.678)	
Epoch: [24][233/391]	LR: 0.0001	Loss 0.1544 (0.1521)	Prec@1 97.656 (98.621)	
Epoch: [24][311/391]	LR: 0.0001	Loss 0.1720 (0.1515)	Prec@1 97.656 (98.575)	
Epoch: [24][389/391]	LR: 0.0001	Loss 0.1354 (0.1520)	Prec@1 99.219 (98.548)	
Total train loss: 0.1521

 * Prec@1 64.750 Prec@5 87.570 Loss 1.4365
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 0.0001	Loss 0.1514 (0.1500)	Prec@1 98.438 (98.518)	
Epoch: [25][155/391]	LR: 0.0001	Loss 0.1976 (0.1493)	Prec@1 98.438 (98.583)	
Epoch: [25][233/391]	LR: 0.0001	Loss 0.1637 (0.1498)	Prec@1 97.656 (98.574)	
Epoch: [25][311/391]	LR: 0.0001	Loss 0.1313 (0.1510)	Prec@1 99.219 (98.598)	
Epoch: [25][389/391]	LR: 0.0001	Loss 0.1545 (0.1510)	Prec@1 97.656 (98.608)	
Total train loss: 0.1512

 * Prec@1 65.020 Prec@5 87.500 Loss 1.4268
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 0.0001	Loss 0.1870 (0.1504)	Prec@1 98.438 (98.718)	
Epoch: [26][155/391]	LR: 0.0001	Loss 0.1593 (0.1493)	Prec@1 100.000 (98.733)	
Epoch: [26][233/391]	LR: 0.0001	Loss 0.1782 (0.1499)	Prec@1 97.656 (98.705)	
Epoch: [26][311/391]	LR: 0.0001	Loss 0.1340 (0.1502)	Prec@1 99.219 (98.703)	
Epoch: [26][389/391]	LR: 0.0001	Loss 0.1669 (0.1515)	Prec@1 100.000 (98.642)	
Total train loss: 0.1517

 * Prec@1 64.900 Prec@5 87.360 Loss 1.4434
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 0.0001	Loss 0.1638 (0.1523)	Prec@1 98.438 (98.618)	
Epoch: [27][155/391]	LR: 0.0001	Loss 0.1682 (0.1540)	Prec@1 99.219 (98.618)	
Epoch: [27][233/391]	LR: 0.0001	Loss 0.1396 (0.1527)	Prec@1 99.219 (98.688)	
Epoch: [27][311/391]	LR: 0.0001	Loss 0.1393 (0.1530)	Prec@1 98.438 (98.650)	
Epoch: [27][389/391]	LR: 0.0001	Loss 0.1862 (0.1521)	Prec@1 97.656 (98.692)	
Total train loss: 0.1522

 * Prec@1 64.990 Prec@5 87.640 Loss 1.4277
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 0.0001	Loss 0.1191 (0.1514)	Prec@1 98.438 (98.558)	
Epoch: [28][155/391]	LR: 0.0001	Loss 0.1583 (0.1523)	Prec@1 99.219 (98.573)	
Epoch: [28][233/391]	LR: 0.0001	Loss 0.1459 (0.1512)	Prec@1 99.219 (98.641)	
Epoch: [28][311/391]	LR: 0.0001	Loss 0.1312 (0.1510)	Prec@1 99.219 (98.633)	
Epoch: [28][389/391]	LR: 0.0001	Loss 0.1390 (0.1511)	Prec@1 99.219 (98.628)	
Total train loss: 0.1514

 * Prec@1 65.070 Prec@5 87.660 Loss 1.4307
Best acc: 65.930
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 0.0001	Loss 0.1696 (0.1539)	Prec@1 99.219 (98.618)	
Epoch: [29][155/391]	LR: 0.0001	Loss 0.1554 (0.1519)	Prec@1 97.656 (98.663)	
Epoch: [29][233/391]	LR: 0.0001	Loss 0.1448 (0.1522)	Prec@1 98.438 (98.638)	
Epoch: [29][311/391]	LR: 0.0001	Loss 0.1747 (0.1516)	Prec@1 96.875 (98.645)	
Epoch: [29][389/391]	LR: 0.0001	Loss 0.1974 (0.1520)	Prec@1 96.875 (98.600)	
Total train loss: 0.1520

 * Prec@1 64.890 Prec@5 87.520 Loss 1.4385
Best acc: 65.930
--------------------------------------------------------------------------------
