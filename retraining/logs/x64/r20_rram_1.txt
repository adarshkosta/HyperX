
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/x64/rram/one_batch/
          savedir: ../pretrained_models/frozen/x64/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 30
          start_epoch: 0
          batch_size: 128
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.1
          milestones: [6, 12, 20]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 69.380 Prec@5 90.440 Loss 1.1680
Pre-trained Prec@1 with 1 layers frozen: 69.37999725341797 	 Loss: 1.16796875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.0001	Loss 0.5454 (0.5540)	Prec@1 85.156 (84.355)	
Epoch: [0][155/391]	LR: 0.0001	Loss 0.6172 (0.5505)	Prec@1 82.031 (84.450)	
Epoch: [0][233/391]	LR: 0.0001	Loss 0.6021 (0.5502)	Prec@1 78.906 (84.489)	
Epoch: [0][311/391]	LR: 0.0001	Loss 0.6870 (0.5524)	Prec@1 75.000 (84.483)	
Epoch: [0][389/391]	LR: 0.0001	Loss 0.5088 (0.5562)	Prec@1 85.938 (84.407)	
Total train loss: 0.5563

 * Prec@1 69.770 Prec@5 90.550 Loss 1.1611
Best acc: 69.770
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.0001	Loss 0.4568 (0.5638)	Prec@1 88.281 (84.365)	
Epoch: [1][155/391]	LR: 0.0001	Loss 0.4963 (0.5597)	Prec@1 87.500 (84.480)	
Epoch: [1][233/391]	LR: 0.0001	Loss 0.5532 (0.5583)	Prec@1 82.812 (84.422)	
Epoch: [1][311/391]	LR: 0.0001	Loss 0.6392 (0.5566)	Prec@1 78.906 (84.495)	
Epoch: [1][389/391]	LR: 0.0001	Loss 0.6362 (0.5540)	Prec@1 82.031 (84.483)	
Total train loss: 0.5543

 * Prec@1 69.540 Prec@5 90.590 Loss 1.1660
Best acc: 69.770
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.0001	Loss 0.5708 (0.5302)	Prec@1 89.062 (85.467)	
Epoch: [2][155/391]	LR: 0.0001	Loss 0.4739 (0.5419)	Prec@1 89.062 (84.786)	
Epoch: [2][233/391]	LR: 0.0001	Loss 0.6094 (0.5518)	Prec@1 82.031 (84.425)	
Epoch: [2][311/391]	LR: 0.0001	Loss 0.5723 (0.5520)	Prec@1 82.031 (84.493)	
Epoch: [2][389/391]	LR: 0.0001	Loss 0.5625 (0.5539)	Prec@1 83.594 (84.429)	
Total train loss: 0.5541

 * Prec@1 69.580 Prec@5 90.690 Loss 1.1660
Best acc: 69.770
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.0001	Loss 0.4536 (0.5577)	Prec@1 89.062 (84.305)	
Epoch: [3][155/391]	LR: 0.0001	Loss 0.6782 (0.5576)	Prec@1 78.906 (84.225)	
Epoch: [3][233/391]	LR: 0.0001	Loss 0.4211 (0.5605)	Prec@1 90.625 (84.135)	
Epoch: [3][311/391]	LR: 0.0001	Loss 0.5547 (0.5548)	Prec@1 85.156 (84.413)	
Epoch: [3][389/391]	LR: 0.0001	Loss 0.5269 (0.5553)	Prec@1 85.938 (84.357)	
Total train loss: 0.5556

 * Prec@1 69.500 Prec@5 90.640 Loss 1.1631
Best acc: 69.770
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.0001	Loss 0.4795 (0.5486)	Prec@1 86.719 (84.766)	
Epoch: [4][155/391]	LR: 0.0001	Loss 0.4927 (0.5578)	Prec@1 83.594 (84.275)	
Epoch: [4][233/391]	LR: 0.0001	Loss 0.5117 (0.5529)	Prec@1 87.500 (84.422)	
Epoch: [4][311/391]	LR: 0.0001	Loss 0.7031 (0.5538)	Prec@1 80.469 (84.317)	
Epoch: [4][389/391]	LR: 0.0001	Loss 0.5474 (0.5535)	Prec@1 83.594 (84.341)	
Total train loss: 0.5535

 * Prec@1 69.790 Prec@5 90.560 Loss 1.1602
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.0001	Loss 0.5264 (0.5400)	Prec@1 86.719 (84.515)	
Epoch: [5][155/391]	LR: 0.0001	Loss 0.6431 (0.5516)	Prec@1 82.031 (84.355)	
Epoch: [5][233/391]	LR: 0.0001	Loss 0.4600 (0.5491)	Prec@1 87.500 (84.509)	
Epoch: [5][311/391]	LR: 0.0001	Loss 0.5679 (0.5503)	Prec@1 83.594 (84.428)	
Epoch: [5][389/391]	LR: 0.0001	Loss 0.6016 (0.5494)	Prec@1 78.906 (84.449)	
Total train loss: 0.5495

 * Prec@1 69.550 Prec@5 90.610 Loss 1.1641
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 1e-05	Loss 0.4316 (0.5347)	Prec@1 88.281 (84.846)	
Epoch: [6][155/391]	LR: 1e-05	Loss 0.5718 (0.5518)	Prec@1 85.938 (84.520)	
Epoch: [6][233/391]	LR: 1e-05	Loss 0.4907 (0.5543)	Prec@1 85.156 (84.292)	
Epoch: [6][311/391]	LR: 1e-05	Loss 0.5132 (0.5498)	Prec@1 85.938 (84.585)	
Epoch: [6][389/391]	LR: 1e-05	Loss 0.4214 (0.5496)	Prec@1 86.719 (84.673)	
Total train loss: 0.5502

 * Prec@1 69.390 Prec@5 90.720 Loss 1.1582
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 1e-05	Loss 0.5693 (0.5570)	Prec@1 80.469 (84.445)	
Epoch: [7][155/391]	LR: 1e-05	Loss 0.5522 (0.5534)	Prec@1 86.719 (84.285)	
Epoch: [7][233/391]	LR: 1e-05	Loss 0.5869 (0.5531)	Prec@1 82.812 (84.428)	
Epoch: [7][311/391]	LR: 1e-05	Loss 0.5532 (0.5507)	Prec@1 84.375 (84.510)	
Epoch: [7][389/391]	LR: 1e-05	Loss 0.5044 (0.5480)	Prec@1 86.719 (84.595)	
Total train loss: 0.5479

 * Prec@1 69.640 Prec@5 90.660 Loss 1.1602
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 1e-05	Loss 0.5552 (0.5655)	Prec@1 83.594 (84.024)	
Epoch: [8][155/391]	LR: 1e-05	Loss 0.5786 (0.5515)	Prec@1 85.156 (84.530)	
Epoch: [8][233/391]	LR: 1e-05	Loss 0.5786 (0.5553)	Prec@1 82.031 (84.295)	
Epoch: [8][311/391]	LR: 1e-05	Loss 0.5342 (0.5551)	Prec@1 85.938 (84.315)	
Epoch: [8][389/391]	LR: 1e-05	Loss 0.6660 (0.5508)	Prec@1 82.031 (84.495)	
Total train loss: 0.5510

 * Prec@1 69.730 Prec@5 90.590 Loss 1.1572
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 1e-05	Loss 0.6831 (0.5562)	Prec@1 82.031 (84.295)	
Epoch: [9][155/391]	LR: 1e-05	Loss 0.4456 (0.5472)	Prec@1 86.719 (84.801)	
Epoch: [9][233/391]	LR: 1e-05	Loss 0.4783 (0.5474)	Prec@1 88.281 (84.786)	
Epoch: [9][311/391]	LR: 1e-05	Loss 0.5571 (0.5503)	Prec@1 85.156 (84.560)	
Epoch: [9][389/391]	LR: 1e-05	Loss 0.5972 (0.5506)	Prec@1 80.469 (84.545)	
Total train loss: 0.5509

 * Prec@1 69.770 Prec@5 90.530 Loss 1.1621
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 1e-05	Loss 0.3860 (0.5558)	Prec@1 91.406 (84.565)	
Epoch: [10][155/391]	LR: 1e-05	Loss 0.5732 (0.5513)	Prec@1 83.594 (84.786)	
Epoch: [10][233/391]	LR: 1e-05	Loss 0.6880 (0.5512)	Prec@1 80.469 (84.782)	
Epoch: [10][311/391]	LR: 1e-05	Loss 0.5405 (0.5506)	Prec@1 89.062 (84.701)	
Epoch: [10][389/391]	LR: 1e-05	Loss 0.5342 (0.5484)	Prec@1 85.938 (84.706)	
Total train loss: 0.5484

 * Prec@1 69.720 Prec@5 90.620 Loss 1.1611
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 1e-05	Loss 0.5303 (0.5501)	Prec@1 85.156 (84.435)	
Epoch: [11][155/391]	LR: 1e-05	Loss 0.5444 (0.5443)	Prec@1 84.375 (84.721)	
Epoch: [11][233/391]	LR: 1e-05	Loss 0.3589 (0.5435)	Prec@1 91.406 (84.739)	
Epoch: [11][311/391]	LR: 1e-05	Loss 0.5459 (0.5501)	Prec@1 83.594 (84.493)	
Epoch: [11][389/391]	LR: 1e-05	Loss 0.5410 (0.5498)	Prec@1 82.812 (84.519)	
Total train loss: 0.5495

 * Prec@1 69.500 Prec@5 90.610 Loss 1.1641
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 1.0000000000000002e-06	Loss 0.5444 (0.5614)	Prec@1 85.938 (84.415)	
Epoch: [12][155/391]	LR: 1.0000000000000002e-06	Loss 0.5059 (0.5576)	Prec@1 85.156 (84.370)	
Epoch: [12][233/391]	LR: 1.0000000000000002e-06	Loss 0.4817 (0.5532)	Prec@1 85.938 (84.555)	
Epoch: [12][311/391]	LR: 1.0000000000000002e-06	Loss 0.4927 (0.5505)	Prec@1 86.719 (84.655)	
Epoch: [12][389/391]	LR: 1.0000000000000002e-06	Loss 0.5259 (0.5483)	Prec@1 85.156 (84.738)	
Total train loss: 0.5487

 * Prec@1 69.660 Prec@5 90.590 Loss 1.1631
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 1.0000000000000002e-06	Loss 0.6313 (0.5523)	Prec@1 83.594 (84.555)	
Epoch: [13][155/391]	LR: 1.0000000000000002e-06	Loss 0.5771 (0.5456)	Prec@1 84.375 (84.741)	
Epoch: [13][233/391]	LR: 1.0000000000000002e-06	Loss 0.4998 (0.5453)	Prec@1 84.375 (84.665)	
Epoch: [13][311/391]	LR: 1.0000000000000002e-06	Loss 0.5396 (0.5476)	Prec@1 89.062 (84.578)	
Epoch: [13][389/391]	LR: 1.0000000000000002e-06	Loss 0.6904 (0.5497)	Prec@1 78.906 (84.497)	
Total train loss: 0.5499

 * Prec@1 69.480 Prec@5 90.450 Loss 1.1709
Best acc: 69.790
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 1.0000000000000002e-06	Loss 0.4849 (0.5605)	Prec@1 89.844 (84.034)	
Epoch: [14][155/391]	LR: 1.0000000000000002e-06	Loss 0.5977 (0.5557)	Prec@1 84.375 (84.245)	
Epoch: [14][233/391]	LR: 1.0000000000000002e-06	Loss 0.5781 (0.5527)	Prec@1 84.375 (84.408)	
Epoch: [14][311/391]	LR: 1.0000000000000002e-06	Loss 0.5933 (0.5523)	Prec@1 81.250 (84.375)	
Epoch: [14][389/391]	LR: 1.0000000000000002e-06	Loss 0.4626 (0.5513)	Prec@1 84.375 (84.401)	
Total train loss: 0.5512

 * Prec@1 69.850 Prec@5 90.640 Loss 1.1611
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 1.0000000000000002e-06	Loss 0.5454 (0.5481)	Prec@1 85.938 (84.445)	
Epoch: [15][155/391]	LR: 1.0000000000000002e-06	Loss 0.5127 (0.5501)	Prec@1 83.594 (84.400)	
Epoch: [15][233/391]	LR: 1.0000000000000002e-06	Loss 0.6235 (0.5520)	Prec@1 83.594 (84.418)	
Epoch: [15][311/391]	LR: 1.0000000000000002e-06	Loss 0.3965 (0.5508)	Prec@1 91.406 (84.470)	
Epoch: [15][389/391]	LR: 1.0000000000000002e-06	Loss 0.4683 (0.5499)	Prec@1 86.719 (84.495)	
Total train loss: 0.5499

 * Prec@1 69.490 Prec@5 90.690 Loss 1.1650
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 1.0000000000000002e-06	Loss 0.5010 (0.5445)	Prec@1 85.938 (85.096)	
Epoch: [16][155/391]	LR: 1.0000000000000002e-06	Loss 0.6265 (0.5509)	Prec@1 85.156 (84.610)	
Epoch: [16][233/391]	LR: 1.0000000000000002e-06	Loss 0.4509 (0.5495)	Prec@1 87.500 (84.659)	
Epoch: [16][311/391]	LR: 1.0000000000000002e-06	Loss 0.5054 (0.5502)	Prec@1 86.719 (84.620)	
Epoch: [16][389/391]	LR: 1.0000000000000002e-06	Loss 0.5347 (0.5486)	Prec@1 87.500 (84.617)	
Total train loss: 0.5489

 * Prec@1 69.340 Prec@5 90.720 Loss 1.1631
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 1.0000000000000002e-06	Loss 0.6982 (0.5516)	Prec@1 79.688 (84.105)	
Epoch: [17][155/391]	LR: 1.0000000000000002e-06	Loss 0.5073 (0.5503)	Prec@1 87.500 (84.365)	
Epoch: [17][233/391]	LR: 1.0000000000000002e-06	Loss 0.4417 (0.5504)	Prec@1 89.844 (84.412)	
Epoch: [17][311/391]	LR: 1.0000000000000002e-06	Loss 0.6089 (0.5515)	Prec@1 86.719 (84.458)	
Epoch: [17][389/391]	LR: 1.0000000000000002e-06	Loss 0.6133 (0.5508)	Prec@1 82.812 (84.475)	
Total train loss: 0.5508

 * Prec@1 69.630 Prec@5 90.780 Loss 1.1572
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 1.0000000000000002e-06	Loss 0.6455 (0.5397)	Prec@1 81.250 (84.836)	
Epoch: [18][155/391]	LR: 1.0000000000000002e-06	Loss 0.6191 (0.5431)	Prec@1 83.594 (84.721)	
Epoch: [18][233/391]	LR: 1.0000000000000002e-06	Loss 0.6895 (0.5497)	Prec@1 79.688 (84.499)	
Epoch: [18][311/391]	LR: 1.0000000000000002e-06	Loss 0.4761 (0.5496)	Prec@1 83.594 (84.540)	
Epoch: [18][389/391]	LR: 1.0000000000000002e-06	Loss 0.4614 (0.5470)	Prec@1 87.500 (84.649)	
Total train loss: 0.5473

 * Prec@1 69.340 Prec@5 90.690 Loss 1.1670
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 1.0000000000000002e-06	Loss 0.5723 (0.5446)	Prec@1 85.156 (84.625)	
Epoch: [19][155/391]	LR: 1.0000000000000002e-06	Loss 0.5601 (0.5392)	Prec@1 85.156 (85.041)	
Epoch: [19][233/391]	LR: 1.0000000000000002e-06	Loss 0.6255 (0.5456)	Prec@1 78.906 (84.759)	
Epoch: [19][311/391]	LR: 1.0000000000000002e-06	Loss 0.5698 (0.5476)	Prec@1 82.812 (84.736)	
Epoch: [19][389/391]	LR: 1.0000000000000002e-06	Loss 0.5454 (0.5464)	Prec@1 82.031 (84.704)	
Total train loss: 0.5462

 * Prec@1 69.750 Prec@5 90.560 Loss 1.1660
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [20][77/391]	LR: 1.0000000000000002e-07	Loss 0.6182 (0.5487)	Prec@1 82.031 (84.746)	
Epoch: [20][155/391]	LR: 1.0000000000000002e-07	Loss 0.6865 (0.5476)	Prec@1 78.906 (84.811)	
Epoch: [20][233/391]	LR: 1.0000000000000002e-07	Loss 0.4573 (0.5490)	Prec@1 87.500 (84.762)	
Epoch: [20][311/391]	LR: 1.0000000000000002e-07	Loss 0.6499 (0.5485)	Prec@1 82.031 (84.628)	
Epoch: [20][389/391]	LR: 1.0000000000000002e-07	Loss 0.6201 (0.5489)	Prec@1 81.250 (84.611)	
Total train loss: 0.5492

 * Prec@1 69.340 Prec@5 90.490 Loss 1.1680
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [21][77/391]	LR: 1.0000000000000002e-07	Loss 0.5386 (0.5491)	Prec@1 83.594 (84.515)	
Epoch: [21][155/391]	LR: 1.0000000000000002e-07	Loss 0.5327 (0.5509)	Prec@1 87.500 (84.370)	
Epoch: [21][233/391]	LR: 1.0000000000000002e-07	Loss 0.5205 (0.5541)	Prec@1 84.375 (84.335)	
Epoch: [21][311/391]	LR: 1.0000000000000002e-07	Loss 0.6543 (0.5516)	Prec@1 80.469 (84.433)	
Epoch: [21][389/391]	LR: 1.0000000000000002e-07	Loss 0.5464 (0.5529)	Prec@1 86.719 (84.417)	
Total train loss: 0.5529

 * Prec@1 69.590 Prec@5 90.590 Loss 1.1641
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [22][77/391]	LR: 1.0000000000000002e-07	Loss 0.4553 (0.5421)	Prec@1 86.719 (84.706)	
Epoch: [22][155/391]	LR: 1.0000000000000002e-07	Loss 0.5952 (0.5462)	Prec@1 84.375 (84.505)	
Epoch: [22][233/391]	LR: 1.0000000000000002e-07	Loss 0.5410 (0.5509)	Prec@1 82.812 (84.422)	
Epoch: [22][311/391]	LR: 1.0000000000000002e-07	Loss 0.5190 (0.5497)	Prec@1 87.500 (84.510)	
Epoch: [22][389/391]	LR: 1.0000000000000002e-07	Loss 0.5522 (0.5471)	Prec@1 82.812 (84.565)	
Total train loss: 0.5471

 * Prec@1 69.650 Prec@5 90.600 Loss 1.1582
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [23][77/391]	LR: 1.0000000000000002e-07	Loss 0.5967 (0.5521)	Prec@1 83.594 (84.555)	
Epoch: [23][155/391]	LR: 1.0000000000000002e-07	Loss 0.6187 (0.5565)	Prec@1 85.156 (84.365)	
Epoch: [23][233/391]	LR: 1.0000000000000002e-07	Loss 0.3643 (0.5580)	Prec@1 93.750 (84.338)	
Epoch: [23][311/391]	LR: 1.0000000000000002e-07	Loss 0.4683 (0.5529)	Prec@1 89.062 (84.618)	
Epoch: [23][389/391]	LR: 1.0000000000000002e-07	Loss 0.5625 (0.5523)	Prec@1 85.156 (84.585)	
Total train loss: 0.5524

 * Prec@1 69.630 Prec@5 90.730 Loss 1.1592
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [24][77/391]	LR: 1.0000000000000002e-07	Loss 0.5005 (0.5490)	Prec@1 82.031 (84.716)	
Epoch: [24][155/391]	LR: 1.0000000000000002e-07	Loss 0.5454 (0.5500)	Prec@1 82.812 (84.565)	
Epoch: [24][233/391]	LR: 1.0000000000000002e-07	Loss 0.5386 (0.5516)	Prec@1 87.500 (84.475)	
Epoch: [24][311/391]	LR: 1.0000000000000002e-07	Loss 0.6445 (0.5475)	Prec@1 75.781 (84.638)	
Epoch: [24][389/391]	LR: 1.0000000000000002e-07	Loss 0.5181 (0.5489)	Prec@1 86.719 (84.563)	
Total train loss: 0.5488

 * Prec@1 69.550 Prec@5 90.830 Loss 1.1572
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [25][77/391]	LR: 1.0000000000000002e-07	Loss 0.3652 (0.5437)	Prec@1 89.062 (84.966)	
Epoch: [25][155/391]	LR: 1.0000000000000002e-07	Loss 0.5601 (0.5451)	Prec@1 84.375 (84.851)	
Epoch: [25][233/391]	LR: 1.0000000000000002e-07	Loss 0.5527 (0.5489)	Prec@1 83.594 (84.739)	
Epoch: [25][311/391]	LR: 1.0000000000000002e-07	Loss 0.4592 (0.5491)	Prec@1 90.625 (84.683)	
Epoch: [25][389/391]	LR: 1.0000000000000002e-07	Loss 0.4526 (0.5483)	Prec@1 89.062 (84.708)	
Total train loss: 0.5482

 * Prec@1 69.680 Prec@5 90.640 Loss 1.1582
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [26][77/391]	LR: 1.0000000000000002e-07	Loss 0.5884 (0.5401)	Prec@1 83.594 (84.605)	
Epoch: [26][155/391]	LR: 1.0000000000000002e-07	Loss 0.5146 (0.5423)	Prec@1 82.812 (84.766)	
Epoch: [26][233/391]	LR: 1.0000000000000002e-07	Loss 0.5063 (0.5465)	Prec@1 82.812 (84.689)	
Epoch: [26][311/391]	LR: 1.0000000000000002e-07	Loss 0.5229 (0.5537)	Prec@1 82.812 (84.352)	
Epoch: [26][389/391]	LR: 1.0000000000000002e-07	Loss 0.5894 (0.5518)	Prec@1 81.250 (84.389)	
Total train loss: 0.5516

 * Prec@1 69.710 Prec@5 90.690 Loss 1.1592
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [27][77/391]	LR: 1.0000000000000002e-07	Loss 0.5820 (0.5507)	Prec@1 80.469 (84.555)	
Epoch: [27][155/391]	LR: 1.0000000000000002e-07	Loss 0.7056 (0.5501)	Prec@1 78.125 (84.585)	
Epoch: [27][233/391]	LR: 1.0000000000000002e-07	Loss 0.5317 (0.5466)	Prec@1 87.500 (84.782)	
Epoch: [27][311/391]	LR: 1.0000000000000002e-07	Loss 0.5713 (0.5457)	Prec@1 82.031 (84.726)	
Epoch: [27][389/391]	LR: 1.0000000000000002e-07	Loss 0.6719 (0.5492)	Prec@1 79.688 (84.493)	
Total train loss: 0.5490

 * Prec@1 69.640 Prec@5 90.510 Loss 1.1641
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [28][77/391]	LR: 1.0000000000000002e-07	Loss 0.4763 (0.5654)	Prec@1 87.500 (84.044)	
Epoch: [28][155/391]	LR: 1.0000000000000002e-07	Loss 0.5498 (0.5558)	Prec@1 84.375 (84.140)	
Epoch: [28][233/391]	LR: 1.0000000000000002e-07	Loss 0.5464 (0.5502)	Prec@1 85.938 (84.355)	
Epoch: [28][311/391]	LR: 1.0000000000000002e-07	Loss 0.5830 (0.5498)	Prec@1 82.812 (84.435)	
Epoch: [28][389/391]	LR: 1.0000000000000002e-07	Loss 0.6890 (0.5510)	Prec@1 81.250 (84.441)	
Total train loss: 0.5510

 * Prec@1 69.790 Prec@5 90.620 Loss 1.1592
Best acc: 69.850
--------------------------------------------------------------------------------
Epoch: [29][77/391]	LR: 1.0000000000000002e-07	Loss 0.4980 (0.5584)	Prec@1 82.812 (84.195)	
Epoch: [29][155/391]	LR: 1.0000000000000002e-07	Loss 0.6011 (0.5533)	Prec@1 79.688 (84.500)	
Epoch: [29][233/391]	LR: 1.0000000000000002e-07	Loss 0.6006 (0.5561)	Prec@1 80.469 (84.442)	
Epoch: [29][311/391]	LR: 1.0000000000000002e-07	Loss 0.5303 (0.5516)	Prec@1 85.938 (84.525)	
Epoch: [29][389/391]	LR: 1.0000000000000002e-07	Loss 0.5596 (0.5513)	Prec@1 82.812 (84.605)	
Total train loss: 0.5514

 * Prec@1 69.590 Prec@5 90.580 Loss 1.1621
Best acc: 69.850
--------------------------------------------------------------------------------
