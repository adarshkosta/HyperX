
      ==> Arguments:
          dataset: cifar100
          model: resnet18
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          workers: 8
          epochs: 40
          start_epoch: 0
          batch_size: 256
          lr: 0.002
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [10, 20, 30]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 2
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 2
==> Building model for resnet18 ...
==> Initializing model with pre-trained parameters (except classifier)...
==> Load pretrained model form ../pretrained_models/ideal/resnet18fp_imnet.pth.tar ...
Original model accuracy on ImageNet: 69.93189239501953
 * Prec@1 1.040 Prec@5 5.620 Loss 4.6172
Pre-trained Prec@1 with 13 layers frozen: 1.0399999618530273 	 Loss: 4.6171875

Starting training on SRAM layers...
Epoch: [0][38/196]	LR: 0.002	Loss 4.3125 (4.6883)	Prec@1 7.812 (3.726)	
Epoch: [0][77/196]	LR: 0.002	Loss 3.7656 (4.3257)	Prec@1 16.797 (7.988)	
Epoch: [0][116/196]	LR: 0.002	Loss 3.4336 (4.0795)	Prec@1 21.094 (11.732)	
Epoch: [0][155/196]	LR: 0.002	Loss 3.2070 (3.8950)	Prec@1 29.688 (14.901)	
Epoch: [0][194/196]	LR: 0.002	Loss 3.1719 (3.7573)	Prec@1 29.297 (17.422)	
Total train loss: 3.7561

 * Prec@1 30.320 Prec@5 59.490 Loss 3.0820
Best acc: 30.320
--------------------------------------------------------------------------------
Epoch: [1][38/196]	LR: 0.002	Loss 2.8750 (2.9339)	Prec@1 33.594 (35.266)	
Epoch: [1][77/196]	LR: 0.002	Loss 2.9160 (2.9039)	Prec@1 33.594 (35.973)	
Epoch: [1][116/196]	LR: 0.002	Loss 2.7695 (2.8724)	Prec@1 33.984 (36.472)	
Epoch: [1][155/196]	LR: 0.002	Loss 2.7637 (2.8422)	Prec@1 36.328 (37.032)	
Epoch: [1][194/196]	LR: 0.002	Loss 2.7969 (2.8104)	Prec@1 39.844 (37.821)	
Total train loss: 2.8101

 * Prec@1 38.440 Prec@5 69.390 Loss 2.7129
Best acc: 38.440
--------------------------------------------------------------------------------
Epoch: [2][38/196]	LR: 0.002	Loss 2.5312 (2.5398)	Prec@1 43.750 (45.012)	
Epoch: [2][77/196]	LR: 0.002	Loss 2.5059 (2.5203)	Prec@1 39.844 (44.922)	
Epoch: [2][116/196]	LR: 0.002	Loss 2.3027 (2.5002)	Prec@1 52.344 (45.359)	
Epoch: [2][155/196]	LR: 0.002	Loss 2.5156 (2.4874)	Prec@1 43.750 (45.565)	
Epoch: [2][194/196]	LR: 0.002	Loss 2.4062 (2.4711)	Prec@1 46.484 (45.923)	
Total train loss: 2.4714

 * Prec@1 43.220 Prec@5 74.120 Loss 2.4941
Best acc: 43.220
--------------------------------------------------------------------------------
Epoch: [3][38/196]	LR: 0.002	Loss 2.4395 (2.2857)	Prec@1 46.094 (50.921)	
Epoch: [3][77/196]	LR: 0.002	Loss 2.4199 (2.2794)	Prec@1 44.531 (51.077)	
Epoch: [3][116/196]	LR: 0.002	Loss 2.2617 (2.2748)	Prec@1 48.828 (51.092)	
Epoch: [3][155/196]	LR: 0.002	Loss 2.0449 (2.2590)	Prec@1 56.250 (51.247)	
Epoch: [3][194/196]	LR: 0.002	Loss 2.2031 (2.2481)	Prec@1 50.781 (51.344)	
Total train loss: 2.2489

 * Prec@1 46.340 Prec@5 76.540 Loss 2.3555
Best acc: 46.340
--------------------------------------------------------------------------------
Epoch: [4][38/196]	LR: 0.002	Loss 2.1445 (2.1066)	Prec@1 52.344 (55.158)	
Epoch: [4][77/196]	LR: 0.002	Loss 2.2031 (2.0995)	Prec@1 53.906 (55.258)	
Epoch: [4][116/196]	LR: 0.002	Loss 2.0781 (2.0936)	Prec@1 53.906 (55.342)	
Epoch: [4][155/196]	LR: 0.002	Loss 2.1133 (2.0880)	Prec@1 57.031 (55.401)	
Epoch: [4][194/196]	LR: 0.002	Loss 2.1543 (2.0805)	Prec@1 51.953 (55.551)	
Total train loss: 2.0806

 * Prec@1 48.120 Prec@5 77.940 Loss 2.2656
Best acc: 48.120
--------------------------------------------------------------------------------
Epoch: [5][38/196]	LR: 0.002	Loss 1.9951 (1.9488)	Prec@1 57.812 (59.615)	
Epoch: [5][77/196]	LR: 0.002	Loss 1.8848 (1.9517)	Prec@1 62.109 (59.380)	
Epoch: [5][116/196]	LR: 0.002	Loss 1.8486 (1.9495)	Prec@1 60.547 (59.211)	
Epoch: [5][155/196]	LR: 0.002	Loss 1.8818 (1.9460)	Prec@1 62.500 (59.255)	
Epoch: [5][194/196]	LR: 0.002	Loss 1.8389 (1.9414)	Prec@1 63.281 (59.199)	
Total train loss: 1.9418

 * Prec@1 49.660 Prec@5 79.210 Loss 2.1914
Best acc: 49.660
--------------------------------------------------------------------------------
Epoch: [6][38/196]	LR: 0.002	Loss 1.9121 (1.8255)	Prec@1 60.156 (62.240)	
Epoch: [6][77/196]	LR: 0.002	Loss 1.7393 (1.8230)	Prec@1 66.406 (62.069)	
Epoch: [6][116/196]	LR: 0.002	Loss 1.8506 (1.8274)	Prec@1 63.281 (61.876)	
Epoch: [6][155/196]	LR: 0.002	Loss 1.8477 (1.8244)	Prec@1 61.719 (62.019)	
Epoch: [6][194/196]	LR: 0.002	Loss 1.8096 (1.8240)	Prec@1 62.500 (61.975)	
Total train loss: 1.8243

 * Prec@1 50.840 Prec@5 79.960 Loss 2.1387
Best acc: 50.840
--------------------------------------------------------------------------------
Epoch: [7][38/196]	LR: 0.002	Loss 1.5977 (1.7204)	Prec@1 72.266 (64.994)	
Epoch: [7][77/196]	LR: 0.002	Loss 1.6318 (1.7184)	Prec@1 68.359 (65.219)	
Epoch: [7][116/196]	LR: 0.002	Loss 1.6836 (1.7210)	Prec@1 66.016 (65.188)	
Epoch: [7][155/196]	LR: 0.002	Loss 1.6182 (1.7209)	Prec@1 69.922 (65.024)	
Epoch: [7][194/196]	LR: 0.002	Loss 1.6992 (1.7199)	Prec@1 65.625 (64.946)	
Total train loss: 1.7201

 * Prec@1 51.830 Prec@5 80.560 Loss 2.0957
Best acc: 51.830
--------------------------------------------------------------------------------
Epoch: [8][38/196]	LR: 0.002	Loss 1.6143 (1.6319)	Prec@1 69.531 (67.859)	
Epoch: [8][77/196]	LR: 0.002	Loss 1.5283 (1.6257)	Prec@1 66.797 (67.748)	
Epoch: [8][116/196]	LR: 0.002	Loss 1.7285 (1.6297)	Prec@1 65.625 (67.475)	
Epoch: [8][155/196]	LR: 0.002	Loss 1.6299 (1.6261)	Prec@1 69.531 (67.553)	
Epoch: [8][194/196]	LR: 0.002	Loss 1.5791 (1.6248)	Prec@1 67.578 (67.556)	
Total train loss: 1.6252

 * Prec@1 52.960 Prec@5 81.180 Loss 2.0684
Best acc: 52.960
--------------------------------------------------------------------------------
Epoch: [9][38/196]	LR: 0.002	Loss 1.4043 (1.5319)	Prec@1 75.391 (70.312)	
Epoch: [9][77/196]	LR: 0.002	Loss 1.5527 (1.5359)	Prec@1 65.234 (70.292)	
Epoch: [9][116/196]	LR: 0.002	Loss 1.5293 (1.5307)	Prec@1 67.188 (70.162)	
Epoch: [9][155/196]	LR: 0.002	Loss 1.5918 (1.5314)	Prec@1 64.844 (70.102)	
Epoch: [9][194/196]	LR: 0.002	Loss 1.6006 (1.5332)	Prec@1 64.062 (69.950)	
Total train loss: 1.5337

 * Prec@1 53.320 Prec@5 81.350 Loss 2.0410
Best acc: 53.320
--------------------------------------------------------------------------------
Epoch: [10][38/196]	LR: 0.0004	Loss 1.4404 (1.4402)	Prec@1 71.094 (72.696)	
Epoch: [10][77/196]	LR: 0.0004	Loss 1.3652 (1.4371)	Prec@1 74.609 (73.187)	
Epoch: [10][116/196]	LR: 0.0004	Loss 1.3271 (1.4351)	Prec@1 76.953 (73.451)	
Epoch: [10][155/196]	LR: 0.0004	Loss 1.4238 (1.4302)	Prec@1 75.000 (73.530)	
Epoch: [10][194/196]	LR: 0.0004	Loss 1.4766 (1.4323)	Prec@1 74.609 (73.480)	
Total train loss: 1.4326

 * Prec@1 53.480 Prec@5 81.380 Loss 2.0352
Best acc: 53.480
--------------------------------------------------------------------------------
