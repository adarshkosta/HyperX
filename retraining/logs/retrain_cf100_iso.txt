
      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 1
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 68.690 Prec@5 90.190 Loss 1.1846
Pre-trained Prec@1 with 1 layers frozen: 68.68999481201172 	 Loss: 1.1845703125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.7383 (1.0936)	Prec@1 82.031 (68.910)	
Epoch: [0][155/391]	LR: 0.005	Loss 1.1211 (1.0573)	Prec@1 67.188 (69.737)	
Epoch: [0][233/391]	LR: 0.005	Loss 1.1377 (1.0338)	Prec@1 68.750 (70.409)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.8140 (1.0281)	Prec@1 77.344 (70.518)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.7710 (1.0150)	Prec@1 77.344 (70.815)	
Total train loss: 1.0149

 * Prec@1 66.450 Prec@5 89.940 Loss 1.2158
Best acc: 66.450
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.8901 (0.8624)	Prec@1 75.000 (75.501)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.8940 (0.8720)	Prec@1 75.000 (75.185)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6934 (0.8756)	Prec@1 78.906 (74.993)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.8867 (0.8775)	Prec@1 75.000 (74.922)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.8506 (0.8827)	Prec@1 71.094 (74.782)	
Total train loss: 0.8828

 * Prec@1 66.940 Prec@5 89.770 Loss 1.2139
Best acc: 66.940
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.7778 (0.7900)	Prec@1 78.906 (77.804)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.9292 (0.7976)	Prec@1 75.781 (77.454)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.8271 (0.8020)	Prec@1 75.781 (77.267)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.9072 (0.8093)	Prec@1 71.094 (77.123)	
Epoch: [2][389/391]	LR: 0.005	Loss 1.0029 (0.8174)	Prec@1 64.844 (76.859)	
Total train loss: 0.8175

 * Prec@1 66.830 Prec@5 89.780 Loss 1.2217
Best acc: 66.940
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.7422 (0.7454)	Prec@1 78.125 (79.217)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.8633 (0.7594)	Prec@1 73.438 (78.636)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.7026 (0.7696)	Prec@1 78.906 (78.292)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.9565 (0.7745)	Prec@1 70.312 (78.130)	
Epoch: [3][389/391]	LR: 0.005	Loss 1.0869 (0.7758)	Prec@1 71.094 (78.059)	
Total train loss: 0.7761

 * Prec@1 66.900 Prec@5 89.580 Loss 1.2217
Best acc: 66.940
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.7373 (0.6999)	Prec@1 76.562 (80.278)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.7480 (0.7122)	Prec@1 81.250 (79.818)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.7075 (0.7184)	Prec@1 77.344 (79.714)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.8418 (0.7296)	Prec@1 82.031 (79.420)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.8359 (0.7357)	Prec@1 74.219 (79.241)	
Total train loss: 0.7356

 * Prec@1 66.640 Prec@5 89.150 Loss 1.2510
Best acc: 66.940
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.6255 (0.6446)	Prec@1 83.594 (82.312)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.5498 (0.6341)	Prec@1 88.281 (82.687)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5713 (0.6279)	Prec@1 82.031 (83.090)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.5776 (0.6248)	Prec@1 82.031 (83.211)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.6011 (0.6217)	Prec@1 81.250 (83.323)	
Total train loss: 0.6215

 * Prec@1 67.660 Prec@5 90.050 Loss 1.1943
Best acc: 67.660
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.5703 (0.5889)	Prec@1 84.375 (84.305)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.5786 (0.5874)	Prec@1 85.156 (84.460)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.5396 (0.5938)	Prec@1 82.031 (84.288)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4424 (0.5931)	Prec@1 91.406 (84.260)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.5801 (0.5946)	Prec@1 87.500 (84.257)	
Total train loss: 0.5947

 * Prec@1 67.880 Prec@5 90.010 Loss 1.1934
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.5317 (0.5857)	Prec@1 91.406 (84.555)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.6226 (0.5853)	Prec@1 80.469 (84.325)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.4871 (0.5782)	Prec@1 89.062 (84.649)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.6831 (0.5795)	Prec@1 83.594 (84.696)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.5171 (0.5815)	Prec@1 89.062 (84.631)	
Total train loss: 0.5818

 * Prec@1 67.490 Prec@5 89.920 Loss 1.2051
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.5288 (0.5593)	Prec@1 89.844 (85.527)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.4873 (0.5636)	Prec@1 89.844 (85.497)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.6455 (0.5673)	Prec@1 81.250 (85.323)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.5869 (0.5733)	Prec@1 84.375 (84.921)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.5342 (0.5726)	Prec@1 85.938 (84.912)	
Total train loss: 0.5725

 * Prec@1 67.500 Prec@5 89.820 Loss 1.2061
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.4409 (0.5566)	Prec@1 87.500 (85.547)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.6821 (0.5573)	Prec@1 83.594 (85.462)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.5527 (0.5582)	Prec@1 88.281 (85.597)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.4590 (0.5606)	Prec@1 86.719 (85.487)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.6650 (0.5591)	Prec@1 80.469 (85.599)	
Total train loss: 0.5594

 * Prec@1 67.490 Prec@5 89.890 Loss 1.2158
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.5200 (0.5414)	Prec@1 87.500 (86.198)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.5054 (0.5427)	Prec@1 86.719 (86.113)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.5098 (0.5424)	Prec@1 85.938 (86.165)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.4744 (0.5415)	Prec@1 85.938 (86.178)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.5361 (0.5412)	Prec@1 86.719 (86.218)	
Total train loss: 0.5413

 * Prec@1 67.700 Prec@5 89.730 Loss 1.2061
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.4973 (0.5444)	Prec@1 88.281 (86.138)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4773 (0.5453)	Prec@1 91.406 (86.058)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.5117 (0.5427)	Prec@1 86.719 (86.144)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.5615 (0.5434)	Prec@1 84.375 (86.118)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.6230 (0.5450)	Prec@1 82.031 (86.078)	
Total train loss: 0.5450

 * Prec@1 67.530 Prec@5 89.740 Loss 1.2129
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.7241 (0.5469)	Prec@1 79.688 (85.737)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.5835 (0.5399)	Prec@1 87.500 (86.148)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.6016 (0.5393)	Prec@1 85.938 (86.171)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.6060 (0.5409)	Prec@1 88.281 (86.080)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.4729 (0.5413)	Prec@1 87.500 (86.160)	
Total train loss: 0.5414

 * Prec@1 67.810 Prec@5 89.710 Loss 1.2129
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.5078 (0.5303)	Prec@1 89.062 (86.869)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.5039 (0.5382)	Prec@1 89.844 (86.388)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4600 (0.5381)	Prec@1 87.500 (86.462)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.6533 (0.5379)	Prec@1 79.688 (86.353)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.4688 (0.5400)	Prec@1 84.375 (86.224)	
Total train loss: 0.5400

 * Prec@1 67.640 Prec@5 89.950 Loss 1.2080
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.5122 (0.5513)	Prec@1 85.938 (85.557)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.5908 (0.5431)	Prec@1 82.031 (85.872)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.5337 (0.5447)	Prec@1 85.938 (85.894)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.5015 (0.5390)	Prec@1 85.156 (86.078)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.6929 (0.5373)	Prec@1 79.688 (86.192)	
Total train loss: 0.5376

 * Prec@1 67.510 Prec@5 89.800 Loss 1.2080
Best acc: 67.880
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.4663 (0.5297)	Prec@1 88.281 (86.558)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.7031 (0.5343)	Prec@1 81.250 (86.398)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.5957 (0.5395)	Prec@1 81.250 (86.171)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.4980 (0.5385)	Prec@1 89.062 (86.316)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.6772 (0.5389)	Prec@1 80.469 (86.242)	
Total train loss: 0.5390

 * Prec@1 67.900 Prec@5 89.660 Loss 1.2100
Best acc: 67.900
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.4006 (0.5279)	Prec@1 92.188 (86.929)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.6133 (0.5319)	Prec@1 83.594 (86.624)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.5200 (0.5338)	Prec@1 86.719 (86.612)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.6235 (0.5350)	Prec@1 82.031 (86.551)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.6304 (0.5357)	Prec@1 85.156 (86.430)	
Total train loss: 0.5357

 * Prec@1 67.860 Prec@5 89.860 Loss 1.2080
Best acc: 67.900
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.5479 (0.5446)	Prec@1 85.156 (85.887)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.5259 (0.5336)	Prec@1 86.719 (86.438)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.5181 (0.5401)	Prec@1 85.938 (86.258)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.3755 (0.5367)	Prec@1 92.969 (86.278)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.4780 (0.5376)	Prec@1 89.062 (86.214)	
Total train loss: 0.5377

 * Prec@1 67.600 Prec@5 89.730 Loss 1.2090
Best acc: 67.900
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.4712 (0.5469)	Prec@1 87.500 (85.978)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.5054 (0.5421)	Prec@1 87.500 (86.063)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.7065 (0.5416)	Prec@1 75.781 (86.068)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.5547 (0.5408)	Prec@1 85.938 (86.178)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.6079 (0.5398)	Prec@1 84.375 (86.206)	
Total train loss: 0.5401

 * Prec@1 67.540 Prec@5 89.810 Loss 1.2109
Best acc: 67.900
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.3987 (0.5366)	Prec@1 94.531 (86.158)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.5342 (0.5353)	Prec@1 85.938 (86.398)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.5640 (0.5376)	Prec@1 84.375 (86.318)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.5562 (0.5385)	Prec@1 87.500 (86.283)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.4221 (0.5374)	Prec@1 91.406 (86.332)	
Total train loss: 0.5376

 * Prec@1 67.670 Prec@5 89.890 Loss 1.2070
Best acc: 67.900
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 3
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 67.640 Prec@5 89.480 Loss 1.2285
Pre-trained Prec@1 with 3 layers frozen: 67.63999938964844 	 Loss: 1.228515625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.5859 (0.6273)	Prec@1 82.812 (82.161)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.6738 (0.6379)	Prec@1 83.594 (81.661)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.7056 (0.6396)	Prec@1 83.594 (81.631)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.6802 (0.6539)	Prec@1 78.906 (81.212)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.6768 (0.6623)	Prec@1 79.688 (80.931)	
Total train loss: 0.6625

 * Prec@1 67.190 Prec@5 89.530 Loss 1.2559
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.4705 (0.5462)	Prec@1 87.500 (84.766)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.4949 (0.5573)	Prec@1 89.062 (84.330)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6699 (0.5685)	Prec@1 81.250 (83.958)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.4785 (0.5812)	Prec@1 85.938 (83.544)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.6465 (0.5888)	Prec@1 78.906 (83.361)	
Total train loss: 0.5890

 * Prec@1 66.700 Prec@5 89.420 Loss 1.2686
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.4998 (0.5019)	Prec@1 85.938 (86.508)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.6440 (0.5061)	Prec@1 82.812 (86.398)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.3904 (0.5164)	Prec@1 88.281 (85.944)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.5586 (0.5279)	Prec@1 85.938 (85.519)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.6133 (0.5360)	Prec@1 80.469 (85.250)	
Total train loss: 0.5364

 * Prec@1 65.810 Prec@5 89.020 Loss 1.3066
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.4470 (0.4495)	Prec@1 89.844 (88.532)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.6318 (0.4629)	Prec@1 80.469 (88.196)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.5039 (0.4729)	Prec@1 87.500 (87.787)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.3625 (0.4778)	Prec@1 93.750 (87.678)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.6152 (0.4862)	Prec@1 85.938 (87.302)	
Total train loss: 0.4863

 * Prec@1 65.990 Prec@5 88.820 Loss 1.3242
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.4189 (0.4194)	Prec@1 92.188 (90.004)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.4082 (0.4213)	Prec@1 87.500 (89.648)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.4446 (0.4308)	Prec@1 88.281 (89.149)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.5146 (0.4403)	Prec@1 84.375 (88.742)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.3999 (0.4492)	Prec@1 89.844 (88.494)	
Total train loss: 0.4492

 * Prec@1 66.170 Prec@5 88.690 Loss 1.3301
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.3535 (0.3540)	Prec@1 92.969 (92.228)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3276 (0.3460)	Prec@1 94.531 (92.513)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.4104 (0.3404)	Prec@1 85.938 (92.725)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.2703 (0.3365)	Prec@1 95.312 (92.819)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3015 (0.3346)	Prec@1 94.531 (92.917)	
Total train loss: 0.3346

 * Prec@1 66.920 Prec@5 89.220 Loss 1.2832
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.2493 (0.3098)	Prec@1 96.875 (93.790)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3123 (0.3093)	Prec@1 95.312 (93.960)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3079 (0.3089)	Prec@1 94.531 (94.007)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.2974 (0.3086)	Prec@1 94.531 (93.985)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.3696 (0.3075)	Prec@1 91.406 (94.008)	
Total train loss: 0.3075

 * Prec@1 66.800 Prec@5 88.950 Loss 1.2959
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.1887 (0.2903)	Prec@1 99.219 (94.581)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.2979 (0.2936)	Prec@1 96.094 (94.306)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.2844 (0.2966)	Prec@1 93.750 (94.311)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.3396 (0.2970)	Prec@1 92.969 (94.323)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2983 (0.2973)	Prec@1 93.750 (94.335)	
Total train loss: 0.2973

 * Prec@1 66.650 Prec@5 88.900 Loss 1.3086
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.3289 (0.2869)	Prec@1 93.750 (94.942)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.2269 (0.2823)	Prec@1 97.656 (95.122)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.2520 (0.2882)	Prec@1 96.094 (94.755)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.2927 (0.2898)	Prec@1 95.312 (94.694)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2163 (0.2909)	Prec@1 96.875 (94.708)	
Total train loss: 0.2910

 * Prec@1 66.510 Prec@5 88.930 Loss 1.3125
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.2583 (0.2782)	Prec@1 95.312 (95.052)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.2935 (0.2771)	Prec@1 95.312 (95.127)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.2374 (0.2765)	Prec@1 100.000 (95.242)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.2344 (0.2801)	Prec@1 98.438 (95.092)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.2891 (0.2814)	Prec@1 93.750 (95.038)	
Total train loss: 0.2815

 * Prec@1 66.340 Prec@5 88.750 Loss 1.3291
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.2932 (0.2626)	Prec@1 92.969 (96.084)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.2515 (0.2672)	Prec@1 97.656 (95.833)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2253 (0.2681)	Prec@1 97.656 (95.777)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.2317 (0.2674)	Prec@1 96.094 (95.746)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3142 (0.2673)	Prec@1 97.656 (95.699)	
Total train loss: 0.2674

 * Prec@1 66.260 Prec@5 88.670 Loss 1.3271
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.2527 (0.2590)	Prec@1 95.312 (95.863)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.2367 (0.2646)	Prec@1 96.875 (95.653)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2583 (0.2663)	Prec@1 95.312 (95.626)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.3186 (0.2660)	Prec@1 91.406 (95.603)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.2445 (0.2660)	Prec@1 97.656 (95.641)	
Total train loss: 0.2661

 * Prec@1 66.520 Prec@5 88.650 Loss 1.3105
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2566 (0.2643)	Prec@1 96.094 (95.583)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.2448 (0.2638)	Prec@1 96.875 (95.558)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2983 (0.2647)	Prec@1 94.531 (95.576)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.2559 (0.2669)	Prec@1 95.312 (95.503)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3455 (0.2683)	Prec@1 89.844 (95.493)	
Total train loss: 0.2683

 * Prec@1 66.110 Prec@5 88.730 Loss 1.3193
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3064 (0.2649)	Prec@1 95.312 (95.543)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.2197 (0.2655)	Prec@1 98.438 (95.663)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.2505 (0.2659)	Prec@1 97.656 (95.656)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.2462 (0.2650)	Prec@1 98.438 (95.663)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.2634 (0.2662)	Prec@1 94.531 (95.631)	
Total train loss: 0.2663

 * Prec@1 66.310 Prec@5 88.650 Loss 1.3164
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2329 (0.2575)	Prec@1 97.656 (95.994)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2805 (0.2601)	Prec@1 92.969 (95.843)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.3040 (0.2621)	Prec@1 94.531 (95.700)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.2771 (0.2636)	Prec@1 94.531 (95.678)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3367 (0.2647)	Prec@1 95.312 (95.663)	
Total train loss: 0.2647

 * Prec@1 66.240 Prec@5 88.720 Loss 1.3174
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2683 (0.2628)	Prec@1 96.094 (95.693)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.2573 (0.2636)	Prec@1 95.312 (95.628)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.3115 (0.2639)	Prec@1 92.188 (95.656)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.2411 (0.2637)	Prec@1 96.094 (95.691)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.2847 (0.2642)	Prec@1 97.656 (95.727)	
Total train loss: 0.2644

 * Prec@1 66.440 Prec@5 88.680 Loss 1.3281
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.2106 (0.2667)	Prec@1 96.875 (95.813)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.2466 (0.2708)	Prec@1 94.531 (95.658)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2465 (0.2689)	Prec@1 96.094 (95.750)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2576 (0.2665)	Prec@1 95.312 (95.716)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.3171 (0.2660)	Prec@1 95.312 (95.721)	
Total train loss: 0.2662

 * Prec@1 66.350 Prec@5 88.680 Loss 1.3213
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.2693 (0.2721)	Prec@1 94.531 (95.513)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.2737 (0.2683)	Prec@1 94.531 (95.568)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.2932 (0.2663)	Prec@1 95.312 (95.590)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.3186 (0.2667)	Prec@1 90.625 (95.600)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.2854 (0.2667)	Prec@1 96.875 (95.599)	
Total train loss: 0.2667

 * Prec@1 66.070 Prec@5 88.490 Loss 1.3232
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.2937 (0.2730)	Prec@1 98.438 (95.443)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3223 (0.2683)	Prec@1 93.750 (95.613)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.3162 (0.2686)	Prec@1 94.531 (95.506)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.2450 (0.2681)	Prec@1 97.656 (95.540)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.2651 (0.2667)	Prec@1 96.094 (95.595)	
Total train loss: 0.2667

 * Prec@1 66.270 Prec@5 88.800 Loss 1.3223
Best acc: 67.190
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.3035 (0.2635)	Prec@1 95.312 (95.312)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.2715 (0.2628)	Prec@1 94.531 (95.518)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.2437 (0.2625)	Prec@1 98.438 (95.563)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.3140 (0.2628)	Prec@1 93.750 (95.600)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.2729 (0.2646)	Prec@1 97.656 (95.547)	
Total train loss: 0.2648

 * Prec@1 66.250 Prec@5 88.660 Loss 1.3203
Best acc: 67.190
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 5
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 66.330 Prec@5 88.910 Loss 1.2773
Pre-trained Prec@1 with 5 layers frozen: 66.33000183105469 	 Loss: 1.27734375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.6802 (0.6327)	Prec@1 79.688 (82.061)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.9683 (0.6513)	Prec@1 74.219 (81.340)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.6470 (0.6613)	Prec@1 82.812 (81.073)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.7930 (0.6691)	Prec@1 72.656 (80.729)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.8384 (0.6734)	Prec@1 75.781 (80.609)	
Total train loss: 0.6733

 * Prec@1 67.310 Prec@5 89.450 Loss 1.2588
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.6006 (0.5766)	Prec@1 82.031 (83.323)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.5640 (0.5822)	Prec@1 82.812 (83.484)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6782 (0.5863)	Prec@1 77.344 (83.330)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.5430 (0.5904)	Prec@1 83.594 (83.281)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.7021 (0.5961)	Prec@1 76.562 (83.103)	
Total train loss: 0.5959

 * Prec@1 66.980 Prec@5 89.530 Loss 1.2646
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.3618 (0.5019)	Prec@1 94.531 (86.789)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.4653 (0.5183)	Prec@1 86.719 (86.033)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.5811 (0.5312)	Prec@1 82.031 (85.524)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.6323 (0.5344)	Prec@1 80.469 (85.269)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.5537 (0.5426)	Prec@1 89.062 (85.026)	
Total train loss: 0.5426

 * Prec@1 66.320 Prec@5 88.970 Loss 1.3115
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.4326 (0.4547)	Prec@1 90.625 (88.622)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.3865 (0.4684)	Prec@1 92.188 (88.061)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.4690 (0.4780)	Prec@1 88.281 (87.670)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.5161 (0.4875)	Prec@1 85.938 (87.277)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.4387 (0.4950)	Prec@1 88.281 (86.941)	
Total train loss: 0.4952

 * Prec@1 66.770 Prec@5 89.030 Loss 1.3066
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.5469 (0.4134)	Prec@1 83.594 (89.704)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.3625 (0.4240)	Prec@1 94.531 (89.463)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.4231 (0.4336)	Prec@1 89.062 (89.006)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.5288 (0.4421)	Prec@1 85.938 (88.797)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.3799 (0.4514)	Prec@1 92.188 (88.407)	
Total train loss: 0.4515

 * Prec@1 65.760 Prec@5 88.640 Loss 1.3525
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.3013 (0.3668)	Prec@1 96.094 (91.887)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3330 (0.3509)	Prec@1 92.188 (92.483)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.3611 (0.3442)	Prec@1 89.844 (92.688)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.2964 (0.3431)	Prec@1 95.312 (92.681)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3022 (0.3401)	Prec@1 95.312 (92.744)	
Total train loss: 0.3402

 * Prec@1 67.020 Prec@5 88.830 Loss 1.3086
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.3052 (0.3187)	Prec@1 89.844 (93.830)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3364 (0.3127)	Prec@1 91.406 (93.900)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.2507 (0.3134)	Prec@1 95.312 (93.847)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4333 (0.3144)	Prec@1 89.062 (93.720)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.3284 (0.3150)	Prec@1 94.531 (93.708)	
Total train loss: 0.3152

 * Prec@1 66.710 Prec@5 88.710 Loss 1.3252
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2837 (0.2931)	Prec@1 93.750 (94.601)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.2793 (0.2961)	Prec@1 94.531 (94.396)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.3284 (0.2981)	Prec@1 92.969 (94.408)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.2959 (0.3004)	Prec@1 96.875 (94.291)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.3372 (0.3028)	Prec@1 92.969 (94.207)	
Total train loss: 0.3029

 * Prec@1 66.610 Prec@5 88.690 Loss 1.3203
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.2396 (0.2844)	Prec@1 95.312 (95.012)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.3491 (0.2889)	Prec@1 92.969 (94.686)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.2991 (0.2890)	Prec@1 92.969 (94.625)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.2549 (0.2914)	Prec@1 94.531 (94.504)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2634 (0.2944)	Prec@1 93.750 (94.417)	
Total train loss: 0.2943

 * Prec@1 66.420 Prec@5 88.610 Loss 1.3301
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.2469 (0.2787)	Prec@1 96.875 (95.272)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3193 (0.2819)	Prec@1 95.312 (95.277)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.2642 (0.2844)	Prec@1 95.312 (95.126)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.3362 (0.2865)	Prec@1 93.750 (95.007)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3489 (0.2879)	Prec@1 92.188 (94.938)	
Total train loss: 0.2880

 * Prec@1 66.230 Prec@5 88.760 Loss 1.3291
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.2688 (0.2751)	Prec@1 96.094 (95.483)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3560 (0.2710)	Prec@1 92.188 (95.388)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2008 (0.2733)	Prec@1 95.312 (95.299)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.2585 (0.2752)	Prec@1 97.656 (95.290)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.2769 (0.2747)	Prec@1 95.312 (95.331)	
Total train loss: 0.2746

 * Prec@1 66.580 Prec@5 88.690 Loss 1.3330
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.3213 (0.2773)	Prec@1 93.750 (95.343)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.2874 (0.2726)	Prec@1 96.094 (95.448)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2512 (0.2716)	Prec@1 96.875 (95.403)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.2203 (0.2735)	Prec@1 96.875 (95.318)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.2456 (0.2734)	Prec@1 95.312 (95.341)	
Total train loss: 0.2736

 * Prec@1 66.390 Prec@5 88.660 Loss 1.3408
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2654 (0.2688)	Prec@1 95.312 (95.703)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3362 (0.2688)	Prec@1 91.406 (95.428)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2661 (0.2710)	Prec@1 96.094 (95.453)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3018 (0.2705)	Prec@1 93.750 (95.503)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.2559 (0.2716)	Prec@1 96.094 (95.481)	
Total train loss: 0.2718

 * Prec@1 66.490 Prec@5 88.700 Loss 1.3350
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.2969 (0.2686)	Prec@1 91.406 (95.543)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.2510 (0.2713)	Prec@1 96.875 (95.368)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.2617 (0.2674)	Prec@1 95.312 (95.600)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.2847 (0.2682)	Prec@1 96.094 (95.525)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.2351 (0.2697)	Prec@1 95.312 (95.437)	
Total train loss: 0.2699

 * Prec@1 66.650 Prec@5 88.620 Loss 1.3311
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2808 (0.2693)	Prec@1 95.312 (95.713)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2542 (0.2720)	Prec@1 96.875 (95.598)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.2515 (0.2698)	Prec@1 96.875 (95.640)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.2686 (0.2699)	Prec@1 94.531 (95.600)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.2759 (0.2691)	Prec@1 95.312 (95.577)	
Total train loss: 0.2691

 * Prec@1 66.290 Prec@5 88.460 Loss 1.3350
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2556 (0.2733)	Prec@1 97.656 (95.833)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.3042 (0.2688)	Prec@1 92.969 (95.808)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.2344 (0.2723)	Prec@1 96.875 (95.603)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.2378 (0.2708)	Prec@1 96.094 (95.618)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.2957 (0.2711)	Prec@1 94.531 (95.613)	
Total train loss: 0.2713

 * Prec@1 66.460 Prec@5 88.690 Loss 1.3359
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.2007 (0.2742)	Prec@1 98.438 (95.423)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.2847 (0.2728)	Prec@1 92.969 (95.408)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2581 (0.2708)	Prec@1 93.750 (95.479)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2207 (0.2697)	Prec@1 100.000 (95.570)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.3076 (0.2694)	Prec@1 94.531 (95.605)	
Total train loss: 0.2695

 * Prec@1 66.250 Prec@5 88.510 Loss 1.3418
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.2925 (0.2696)	Prec@1 93.750 (95.723)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.2771 (0.2667)	Prec@1 95.312 (95.858)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.3906 (0.2693)	Prec@1 90.625 (95.606)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.2527 (0.2685)	Prec@1 94.531 (95.633)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.3469 (0.2690)	Prec@1 92.188 (95.609)	
Total train loss: 0.2689

 * Prec@1 66.360 Prec@5 88.630 Loss 1.3320
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.3582 (0.2686)	Prec@1 92.969 (95.543)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.2351 (0.2699)	Prec@1 98.438 (95.508)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.3054 (0.2713)	Prec@1 96.094 (95.436)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.3127 (0.2707)	Prec@1 94.531 (95.435)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.2656 (0.2689)	Prec@1 97.656 (95.525)	
Total train loss: 0.2689

 * Prec@1 66.420 Prec@5 88.540 Loss 1.3379
Best acc: 67.310
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.2751 (0.2628)	Prec@1 96.094 (95.763)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.3118 (0.2654)	Prec@1 96.094 (95.623)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.2898 (0.2664)	Prec@1 94.531 (95.603)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.2532 (0.2675)	Prec@1 96.875 (95.580)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.3179 (0.2698)	Prec@1 92.188 (95.531)	
Total train loss: 0.2699

 * Prec@1 66.490 Prec@5 88.380 Loss 1.3350
Best acc: 67.310
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 7
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 65.160 Prec@5 88.110 Loss 1.3447
Pre-trained Prec@1 with 7 layers frozen: 65.15999603271484 	 Loss: 1.3447265625

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.6904 (0.6634)	Prec@1 79.688 (80.919)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.8267 (0.6659)	Prec@1 77.344 (80.839)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.6172 (0.6735)	Prec@1 81.250 (80.702)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.6436 (0.6820)	Prec@1 82.812 (80.334)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.8345 (0.6869)	Prec@1 72.656 (80.170)	
Total train loss: 0.6870

 * Prec@1 66.630 Prec@5 89.450 Loss 1.2627
Best acc: 66.630
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.5576 (0.5665)	Prec@1 80.469 (84.255)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.5967 (0.5806)	Prec@1 83.594 (83.829)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6113 (0.5895)	Prec@1 80.469 (83.413)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.7188 (0.5984)	Prec@1 78.125 (83.166)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.5913 (0.6062)	Prec@1 85.156 (82.889)	
Total train loss: 0.6061

 * Prec@1 67.010 Prec@5 89.450 Loss 1.2559
Best acc: 67.010
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.5864 (0.5193)	Prec@1 86.719 (85.857)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.4758 (0.5228)	Prec@1 86.719 (85.802)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.5220 (0.5328)	Prec@1 87.500 (85.430)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.4543 (0.5399)	Prec@1 90.625 (85.239)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.5137 (0.5469)	Prec@1 83.594 (84.982)	
Total train loss: 0.5474

 * Prec@1 66.200 Prec@5 89.110 Loss 1.2969
Best acc: 67.010
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.3938 (0.4523)	Prec@1 91.406 (88.061)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.4001 (0.4678)	Prec@1 89.844 (87.831)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.4670 (0.4826)	Prec@1 84.375 (87.186)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.6372 (0.4905)	Prec@1 82.031 (86.917)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.6333 (0.4993)	Prec@1 81.250 (86.683)	
Total train loss: 0.4996

 * Prec@1 66.440 Prec@5 88.810 Loss 1.3047
Best acc: 67.010
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.4329 (0.4169)	Prec@1 85.938 (89.854)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.3750 (0.4303)	Prec@1 90.625 (89.433)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.3953 (0.4427)	Prec@1 92.188 (88.912)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.5771 (0.4532)	Prec@1 83.594 (88.574)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.4812 (0.4604)	Prec@1 88.281 (88.301)	
Total train loss: 0.4608

 * Prec@1 66.190 Prec@5 88.510 Loss 1.3438
Best acc: 67.010
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.3452 (0.3577)	Prec@1 94.531 (91.967)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3403 (0.3516)	Prec@1 93.750 (92.293)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.3306 (0.3475)	Prec@1 92.969 (92.461)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.4199 (0.3460)	Prec@1 86.719 (92.551)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3101 (0.3451)	Prec@1 92.969 (92.578)	
Total train loss: 0.3452

 * Prec@1 67.040 Prec@5 88.910 Loss 1.2979
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.3403 (0.3115)	Prec@1 95.312 (93.810)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.2957 (0.3145)	Prec@1 93.750 (93.610)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3450 (0.3172)	Prec@1 89.844 (93.513)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.3057 (0.3180)	Prec@1 96.875 (93.572)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.3130 (0.3181)	Prec@1 94.531 (93.580)	
Total train loss: 0.3182

 * Prec@1 66.310 Prec@5 88.610 Loss 1.3174
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2832 (0.3109)	Prec@1 94.531 (93.970)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3467 (0.3067)	Prec@1 93.750 (94.116)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.2487 (0.3077)	Prec@1 96.875 (93.987)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.2695 (0.3071)	Prec@1 95.312 (94.028)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2854 (0.3092)	Prec@1 95.312 (93.968)	
Total train loss: 0.3095

 * Prec@1 66.520 Prec@5 88.950 Loss 1.3135
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.2207 (0.2992)	Prec@1 96.094 (94.581)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.2598 (0.2941)	Prec@1 97.656 (94.671)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.3215 (0.2951)	Prec@1 95.312 (94.595)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.3008 (0.2963)	Prec@1 95.312 (94.581)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.3335 (0.2971)	Prec@1 92.969 (94.517)	
Total train loss: 0.2973

 * Prec@1 66.360 Prec@5 88.580 Loss 1.3320
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3772 (0.2815)	Prec@1 86.719 (94.932)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.2842 (0.2881)	Prec@1 95.312 (94.787)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.2345 (0.2914)	Prec@1 96.094 (94.615)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.3284 (0.2940)	Prec@1 94.531 (94.531)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.2693 (0.2945)	Prec@1 99.219 (94.567)	
Total train loss: 0.2945

 * Prec@1 66.230 Prec@5 88.400 Loss 1.3389
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.2927 (0.2763)	Prec@1 96.875 (95.713)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.2754 (0.2799)	Prec@1 94.531 (95.378)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2688 (0.2797)	Prec@1 94.531 (95.266)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.2710 (0.2786)	Prec@1 92.969 (95.260)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3372 (0.2799)	Prec@1 94.531 (95.232)	
Total train loss: 0.2801

 * Prec@1 65.810 Prec@5 88.430 Loss 1.3350
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.2693 (0.2844)	Prec@1 96.094 (95.142)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.2467 (0.2824)	Prec@1 96.094 (95.027)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2754 (0.2799)	Prec@1 93.750 (95.176)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.2817 (0.2809)	Prec@1 92.969 (95.130)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.2874 (0.2789)	Prec@1 94.531 (95.236)	
Total train loss: 0.2790

 * Prec@1 66.300 Prec@5 88.650 Loss 1.3359
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2505 (0.2750)	Prec@1 95.312 (95.122)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3267 (0.2776)	Prec@1 93.750 (95.217)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2393 (0.2791)	Prec@1 96.875 (95.182)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.2595 (0.2782)	Prec@1 94.531 (95.285)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.2590 (0.2784)	Prec@1 96.094 (95.234)	
Total train loss: 0.2785

 * Prec@1 66.130 Prec@5 88.530 Loss 1.3369
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.2632 (0.2805)	Prec@1 95.312 (95.132)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3347 (0.2744)	Prec@1 91.406 (95.358)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.2424 (0.2764)	Prec@1 96.094 (95.406)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.2416 (0.2753)	Prec@1 96.094 (95.428)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.2659 (0.2758)	Prec@1 95.312 (95.407)	
Total train loss: 0.2759

 * Prec@1 66.110 Prec@5 88.440 Loss 1.3359
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2476 (0.2749)	Prec@1 96.875 (95.443)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2576 (0.2728)	Prec@1 96.094 (95.413)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.2786 (0.2741)	Prec@1 93.750 (95.386)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.3018 (0.2749)	Prec@1 93.750 (95.353)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.2949 (0.2767)	Prec@1 93.750 (95.300)	
Total train loss: 0.2769

 * Prec@1 66.080 Prec@5 88.400 Loss 1.3379
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2073 (0.2780)	Prec@1 98.438 (95.323)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.2578 (0.2742)	Prec@1 94.531 (95.378)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.2593 (0.2735)	Prec@1 96.875 (95.409)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.1936 (0.2749)	Prec@1 98.438 (95.400)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.3005 (0.2748)	Prec@1 91.406 (95.367)	
Total train loss: 0.2751

 * Prec@1 66.320 Prec@5 88.400 Loss 1.3311
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.2910 (0.2725)	Prec@1 96.094 (95.413)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.2279 (0.2720)	Prec@1 97.656 (95.373)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2401 (0.2740)	Prec@1 96.094 (95.302)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2429 (0.2741)	Prec@1 95.312 (95.363)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.2522 (0.2735)	Prec@1 94.531 (95.381)	
Total train loss: 0.2737

 * Prec@1 66.340 Prec@5 88.580 Loss 1.3320
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.2810 (0.2730)	Prec@1 95.312 (95.523)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.2515 (0.2748)	Prec@1 94.531 (95.603)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.2507 (0.2725)	Prec@1 97.656 (95.613)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.2456 (0.2734)	Prec@1 96.094 (95.508)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.2456 (0.2735)	Prec@1 95.312 (95.495)	
Total train loss: 0.2735

 * Prec@1 66.300 Prec@5 88.650 Loss 1.3369
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.2391 (0.2759)	Prec@1 95.312 (95.343)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3352 (0.2763)	Prec@1 92.188 (95.297)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.2859 (0.2750)	Prec@1 93.750 (95.423)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.2087 (0.2746)	Prec@1 96.875 (95.393)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.3147 (0.2756)	Prec@1 93.750 (95.365)	
Total train loss: 0.2757

 * Prec@1 66.340 Prec@5 88.440 Loss 1.3330
Best acc: 67.040
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.1927 (0.2682)	Prec@1 99.219 (95.573)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.2242 (0.2689)	Prec@1 99.219 (95.678)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.3774 (0.2724)	Prec@1 92.188 (95.523)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.3542 (0.2744)	Prec@1 93.750 (95.488)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.2455 (0.2739)	Prec@1 95.312 (95.531)	
Total train loss: 0.2740

 * Prec@1 66.170 Prec@5 88.460 Loss 1.3320
Best acc: 67.040
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 9
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 57.720 Prec@5 83.750 Loss 1.7061
Pre-trained Prec@1 with 9 layers frozen: 57.71999740600586 	 Loss: 1.7060546875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.6689 (0.7144)	Prec@1 82.031 (79.117)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.6577 (0.7117)	Prec@1 85.156 (79.447)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.8521 (0.7117)	Prec@1 73.438 (79.374)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.7168 (0.7121)	Prec@1 76.562 (79.224)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.6499 (0.7181)	Prec@1 81.250 (79.065)	
Total train loss: 0.7180

 * Prec@1 67.320 Prec@5 89.580 Loss 1.2383
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.7158 (0.5718)	Prec@1 77.344 (84.225)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.7295 (0.5893)	Prec@1 81.250 (83.674)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.7295 (0.6008)	Prec@1 77.344 (83.253)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.4998 (0.6078)	Prec@1 87.500 (82.903)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.7666 (0.6195)	Prec@1 77.344 (82.492)	
Total train loss: 0.6197

 * Prec@1 66.080 Prec@5 89.160 Loss 1.2861
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.4380 (0.5294)	Prec@1 92.969 (85.667)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.3987 (0.5412)	Prec@1 93.750 (85.076)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.5371 (0.5543)	Prec@1 83.594 (84.535)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.6465 (0.5621)	Prec@1 81.250 (84.262)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.5288 (0.5652)	Prec@1 82.812 (84.109)	
Total train loss: 0.5654

 * Prec@1 66.790 Prec@5 89.000 Loss 1.2852
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.4263 (0.4809)	Prec@1 87.500 (87.300)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.4170 (0.4872)	Prec@1 90.625 (87.019)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.5161 (0.4982)	Prec@1 87.500 (86.605)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.5981 (0.5062)	Prec@1 85.156 (86.318)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.5117 (0.5148)	Prec@1 88.281 (86.016)	
Total train loss: 0.5150

 * Prec@1 66.360 Prec@5 88.820 Loss 1.3154
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.4192 (0.4254)	Prec@1 89.844 (89.924)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.5254 (0.4415)	Prec@1 84.375 (89.133)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.4331 (0.4512)	Prec@1 89.062 (88.615)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.5073 (0.4639)	Prec@1 89.844 (88.113)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.4155 (0.4736)	Prec@1 89.062 (87.756)	
Total train loss: 0.4737

 * Prec@1 66.190 Prec@5 88.310 Loss 1.3428
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.3169 (0.3828)	Prec@1 95.312 (91.316)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.3096 (0.3729)	Prec@1 92.188 (91.637)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.4624 (0.3677)	Prec@1 88.281 (91.847)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.2852 (0.3649)	Prec@1 91.406 (91.927)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3755 (0.3645)	Prec@1 89.844 (91.939)	
Total train loss: 0.3647

 * Prec@1 66.860 Prec@5 88.800 Loss 1.3135
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.3274 (0.3311)	Prec@1 93.750 (93.179)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3728 (0.3330)	Prec@1 89.844 (93.134)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3938 (0.3354)	Prec@1 88.281 (93.022)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.4519 (0.3365)	Prec@1 89.844 (92.991)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.3784 (0.3363)	Prec@1 90.625 (93.031)	
Total train loss: 0.3363

 * Prec@1 66.430 Prec@5 88.580 Loss 1.3252
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.2859 (0.3167)	Prec@1 94.531 (94.081)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3079 (0.3212)	Prec@1 94.531 (93.845)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.3401 (0.3244)	Prec@1 96.094 (93.596)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.2947 (0.3258)	Prec@1 96.875 (93.552)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.3044 (0.3262)	Prec@1 96.094 (93.524)	
Total train loss: 0.3264

 * Prec@1 66.220 Prec@5 88.410 Loss 1.3398
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.2949 (0.3090)	Prec@1 95.312 (94.050)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.3420 (0.3119)	Prec@1 92.969 (93.940)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.3325 (0.3124)	Prec@1 93.750 (93.910)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.2252 (0.3139)	Prec@1 99.219 (93.818)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.3362 (0.3164)	Prec@1 92.969 (93.686)	
Total train loss: 0.3166

 * Prec@1 66.420 Prec@5 88.500 Loss 1.3330
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.2394 (0.2973)	Prec@1 97.656 (94.661)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3787 (0.3032)	Prec@1 87.500 (94.481)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3530 (0.3059)	Prec@1 92.188 (94.304)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.3210 (0.3065)	Prec@1 94.531 (94.248)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.2739 (0.3088)	Prec@1 96.094 (94.101)	
Total train loss: 0.3088

 * Prec@1 66.470 Prec@5 88.420 Loss 1.3291
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.2756 (0.2894)	Prec@1 95.312 (94.762)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3303 (0.2887)	Prec@1 93.750 (95.027)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.2277 (0.2942)	Prec@1 97.656 (94.768)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.3042 (0.2970)	Prec@1 93.750 (94.689)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3672 (0.2958)	Prec@1 94.531 (94.756)	
Total train loss: 0.2960

 * Prec@1 66.460 Prec@5 88.300 Loss 1.3369
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.3076 (0.2935)	Prec@1 95.312 (94.692)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3242 (0.2924)	Prec@1 92.969 (94.717)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2976 (0.2947)	Prec@1 93.750 (94.661)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.3525 (0.2953)	Prec@1 91.406 (94.719)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.2737 (0.2937)	Prec@1 95.312 (94.794)	
Total train loss: 0.2938

 * Prec@1 66.330 Prec@5 88.480 Loss 1.3301
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3298 (0.2880)	Prec@1 96.094 (94.972)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3181 (0.2897)	Prec@1 92.969 (94.742)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.3396 (0.2910)	Prec@1 92.188 (94.798)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.2898 (0.2912)	Prec@1 96.094 (94.772)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3247 (0.2912)	Prec@1 92.188 (94.812)	
Total train loss: 0.2912

 * Prec@1 66.240 Prec@5 88.200 Loss 1.3418
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3062 (0.2926)	Prec@1 96.875 (94.732)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3235 (0.2932)	Prec@1 94.531 (94.561)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3140 (0.2954)	Prec@1 94.531 (94.501)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.2269 (0.2935)	Prec@1 95.312 (94.571)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.2512 (0.2937)	Prec@1 94.531 (94.657)	
Total train loss: 0.2936

 * Prec@1 66.320 Prec@5 88.270 Loss 1.3369
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2788 (0.2939)	Prec@1 94.531 (94.752)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2700 (0.2918)	Prec@1 92.188 (94.827)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.2888 (0.2912)	Prec@1 95.312 (94.872)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.2607 (0.2924)	Prec@1 95.312 (94.839)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.2512 (0.2919)	Prec@1 93.750 (94.894)	
Total train loss: 0.2920

 * Prec@1 66.350 Prec@5 88.300 Loss 1.3379
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2966 (0.2871)	Prec@1 95.312 (94.982)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.3340 (0.2921)	Prec@1 92.188 (94.932)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.3357 (0.2909)	Prec@1 92.969 (94.969)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.2915 (0.2916)	Prec@1 93.750 (94.937)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.2263 (0.2915)	Prec@1 96.875 (94.944)	
Total train loss: 0.2914

 * Prec@1 66.270 Prec@5 88.230 Loss 1.3408
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.2546 (0.2967)	Prec@1 96.094 (94.822)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.3787 (0.2953)	Prec@1 89.844 (94.737)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.2487 (0.2920)	Prec@1 98.438 (94.845)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2253 (0.2887)	Prec@1 96.875 (94.942)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.3120 (0.2880)	Prec@1 93.750 (94.962)	
Total train loss: 0.2883

 * Prec@1 66.350 Prec@5 88.270 Loss 1.3486
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.3364 (0.2899)	Prec@1 91.406 (94.681)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.3059 (0.2889)	Prec@1 93.750 (94.932)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.3020 (0.2865)	Prec@1 95.312 (95.039)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.2732 (0.2884)	Prec@1 95.312 (94.952)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.1868 (0.2886)	Prec@1 97.656 (94.926)	
Total train loss: 0.2888

 * Prec@1 66.190 Prec@5 88.400 Loss 1.3398
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.2959 (0.2873)	Prec@1 94.531 (94.832)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3496 (0.2870)	Prec@1 92.969 (94.982)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.2274 (0.2883)	Prec@1 97.656 (94.979)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.1959 (0.2901)	Prec@1 97.656 (94.884)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.3252 (0.2896)	Prec@1 95.312 (94.938)	
Total train loss: 0.2899

 * Prec@1 66.270 Prec@5 88.310 Loss 1.3398
Best acc: 67.320
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.2683 (0.2866)	Prec@1 96.094 (95.142)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.2712 (0.2872)	Prec@1 93.750 (95.052)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.3098 (0.2885)	Prec@1 92.188 (94.895)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.2764 (0.2886)	Prec@1 94.531 (94.887)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.2710 (0.2904)	Prec@1 95.312 (94.828)	
Total train loss: 0.2904

 * Prec@1 66.050 Prec@5 88.390 Loss 1.3398
Best acc: 67.320
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 11
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 52.200 Prec@5 79.060 Loss 2.0078
Pre-trained Prec@1 with 11 layers frozen: 52.19999694824219 	 Loss: 2.0078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.8403 (0.7646)	Prec@1 73.438 (78.095)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.6460 (0.7609)	Prec@1 80.469 (77.980)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.8047 (0.7496)	Prec@1 75.000 (78.269)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.8018 (0.7492)	Prec@1 81.250 (78.315)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.7266 (0.7487)	Prec@1 78.125 (78.267)	
Total train loss: 0.7490

 * Prec@1 67.090 Prec@5 89.470 Loss 1.2520
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.5713 (0.6095)	Prec@1 85.156 (82.572)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.7300 (0.6166)	Prec@1 77.344 (82.807)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6670 (0.6293)	Prec@1 83.594 (82.228)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.6836 (0.6385)	Prec@1 81.250 (81.831)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.4856 (0.6430)	Prec@1 85.156 (81.651)	
Total train loss: 0.6431

 * Prec@1 66.590 Prec@5 89.230 Loss 1.2734
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.6465 (0.5443)	Prec@1 78.906 (85.747)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.6055 (0.5503)	Prec@1 82.031 (85.241)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.6016 (0.5660)	Prec@1 82.812 (84.622)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.4863 (0.5748)	Prec@1 87.500 (84.195)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.5596 (0.5827)	Prec@1 82.812 (83.918)	
Total train loss: 0.5828

 * Prec@1 66.700 Prec@5 88.870 Loss 1.2822
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.4463 (0.4886)	Prec@1 88.281 (87.089)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.5264 (0.5074)	Prec@1 86.719 (86.308)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.5952 (0.5189)	Prec@1 82.812 (85.891)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.5757 (0.5261)	Prec@1 85.156 (85.750)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.6211 (0.5342)	Prec@1 82.812 (85.393)	
Total train loss: 0.5346

 * Prec@1 66.360 Prec@5 88.610 Loss 1.3105
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.4146 (0.4638)	Prec@1 89.844 (87.871)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.5723 (0.4698)	Prec@1 86.719 (87.911)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.4893 (0.4778)	Prec@1 87.500 (87.640)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.6250 (0.4885)	Prec@1 83.594 (87.307)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.5493 (0.4938)	Prec@1 83.594 (87.109)	
Total train loss: 0.4940

 * Prec@1 65.740 Prec@5 88.090 Loss 1.3545
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4033 (0.4079)	Prec@1 90.625 (90.615)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.4629 (0.3971)	Prec@1 88.281 (90.875)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.4739 (0.3897)	Prec@1 88.281 (91.153)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.3550 (0.3869)	Prec@1 93.750 (91.259)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3032 (0.3858)	Prec@1 96.094 (91.268)	
Total train loss: 0.3860

 * Prec@1 66.940 Prec@5 88.400 Loss 1.3135
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.4065 (0.3610)	Prec@1 92.188 (92.298)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.4180 (0.3556)	Prec@1 90.625 (92.458)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.4170 (0.3566)	Prec@1 92.188 (92.458)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.3594 (0.3576)	Prec@1 89.062 (92.373)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.4028 (0.3578)	Prec@1 92.188 (92.390)	
Total train loss: 0.3579

 * Prec@1 66.880 Prec@5 88.210 Loss 1.3164
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.3223 (0.3407)	Prec@1 92.969 (92.879)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3599 (0.3444)	Prec@1 91.406 (92.808)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.3896 (0.3471)	Prec@1 89.844 (92.778)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.4268 (0.3470)	Prec@1 89.062 (92.798)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.2988 (0.3472)	Prec@1 94.531 (92.802)	
Total train loss: 0.3473

 * Prec@1 66.660 Prec@5 88.130 Loss 1.3320
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.3416 (0.3369)	Prec@1 92.188 (93.089)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.2830 (0.3357)	Prec@1 95.312 (93.359)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.3271 (0.3400)	Prec@1 92.188 (93.046)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.3987 (0.3409)	Prec@1 90.625 (92.984)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.2979 (0.3434)	Prec@1 96.094 (92.839)	
Total train loss: 0.3433

 * Prec@1 66.860 Prec@5 88.200 Loss 1.3340
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3599 (0.3241)	Prec@1 92.969 (93.530)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3782 (0.3284)	Prec@1 92.188 (93.515)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.4514 (0.3322)	Prec@1 88.281 (93.293)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.3572 (0.3315)	Prec@1 94.531 (93.327)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.3992 (0.3342)	Prec@1 92.188 (93.303)	
Total train loss: 0.3343

 * Prec@1 66.610 Prec@5 88.030 Loss 1.3438
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.3999 (0.3189)	Prec@1 92.188 (93.810)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3357 (0.3214)	Prec@1 94.531 (93.780)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.3894 (0.3203)	Prec@1 89.844 (93.857)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.3103 (0.3205)	Prec@1 95.312 (93.883)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3108 (0.3182)	Prec@1 92.969 (93.944)	
Total train loss: 0.3183

 * Prec@1 66.670 Prec@5 88.050 Loss 1.3379
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.2922 (0.3184)	Prec@1 96.875 (93.800)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3191 (0.3152)	Prec@1 91.406 (94.005)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.2734 (0.3145)	Prec@1 96.875 (94.214)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.2590 (0.3160)	Prec@1 96.094 (94.133)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.3787 (0.3168)	Prec@1 91.406 (94.060)	
Total train loss: 0.3168

 * Prec@1 66.520 Prec@5 87.920 Loss 1.3438
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.2998 (0.3136)	Prec@1 95.312 (94.101)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.2791 (0.3115)	Prec@1 94.531 (94.035)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2705 (0.3126)	Prec@1 93.750 (94.097)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3066 (0.3137)	Prec@1 95.312 (94.066)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.2417 (0.3128)	Prec@1 97.656 (94.099)	
Total train loss: 0.3128

 * Prec@1 66.490 Prec@5 88.110 Loss 1.3389
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3267 (0.3140)	Prec@1 95.312 (94.171)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.2847 (0.3149)	Prec@1 93.750 (94.055)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3057 (0.3160)	Prec@1 92.969 (93.997)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3374 (0.3157)	Prec@1 90.625 (93.925)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.3279 (0.3156)	Prec@1 95.312 (93.982)	
Total train loss: 0.3157

 * Prec@1 66.280 Prec@5 87.980 Loss 1.3486
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.2893 (0.3167)	Prec@1 94.531 (94.251)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2764 (0.3123)	Prec@1 94.531 (94.261)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.3892 (0.3151)	Prec@1 92.969 (94.104)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.2761 (0.3151)	Prec@1 96.875 (94.093)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3779 (0.3127)	Prec@1 94.531 (94.187)	
Total train loss: 0.3128

 * Prec@1 66.630 Prec@5 88.110 Loss 1.3398
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.2571 (0.3151)	Prec@1 96.094 (94.391)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.2886 (0.3115)	Prec@1 96.875 (94.386)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.2343 (0.3115)	Prec@1 97.656 (94.267)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.2839 (0.3133)	Prec@1 96.875 (94.191)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.3293 (0.3126)	Prec@1 93.750 (94.163)	
Total train loss: 0.3126

 * Prec@1 66.170 Prec@5 87.910 Loss 1.3457
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.3494 (0.3070)	Prec@1 93.750 (94.351)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.3997 (0.3092)	Prec@1 90.625 (94.296)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.3254 (0.3107)	Prec@1 92.188 (94.231)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.2778 (0.3107)	Prec@1 96.094 (94.218)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.2847 (0.3115)	Prec@1 94.531 (94.205)	
Total train loss: 0.3116

 * Prec@1 66.500 Prec@5 88.070 Loss 1.3428
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.3081 (0.3097)	Prec@1 92.188 (94.291)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.3196 (0.3149)	Prec@1 96.875 (94.141)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.2568 (0.3119)	Prec@1 96.875 (94.288)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.3145 (0.3097)	Prec@1 92.188 (94.341)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.3752 (0.3124)	Prec@1 92.188 (94.241)	
Total train loss: 0.3126

 * Prec@1 66.190 Prec@5 87.910 Loss 1.3486
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.2357 (0.3091)	Prec@1 96.875 (94.481)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3276 (0.3104)	Prec@1 95.312 (94.411)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.2396 (0.3098)	Prec@1 97.656 (94.378)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.3020 (0.3104)	Prec@1 93.750 (94.331)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.3647 (0.3121)	Prec@1 92.969 (94.213)	
Total train loss: 0.3122

 * Prec@1 66.080 Prec@5 87.950 Loss 1.3525
Best acc: 67.090
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.3562 (0.3048)	Prec@1 94.531 (94.281)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.2817 (0.3101)	Prec@1 94.531 (94.216)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.3435 (0.3088)	Prec@1 92.188 (94.267)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.3289 (0.3088)	Prec@1 95.312 (94.296)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.3098 (0.3087)	Prec@1 92.188 (94.279)	
Total train loss: 0.3089

 * Prec@1 66.420 Prec@5 88.100 Loss 1.3428
Best acc: 67.090
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 13
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 39.440 Prec@5 67.140 Loss 2.8184
Pre-trained Prec@1 with 13 layers frozen: 39.439998626708984 	 Loss: 2.818359375

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 0.9341 (0.8614)	Prec@1 73.438 (75.270)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.6499 (0.8315)	Prec@1 78.906 (75.871)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.6885 (0.8136)	Prec@1 79.688 (76.255)	
Epoch: [0][311/391]	LR: 0.005	Loss 0.7485 (0.8062)	Prec@1 78.906 (76.588)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.8560 (0.8038)	Prec@1 77.344 (76.607)	
Total train loss: 0.8042

 * Prec@1 66.370 Prec@5 89.150 Loss 1.2598
Best acc: 66.370
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.6587 (0.6458)	Prec@1 78.906 (81.711)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.6621 (0.6630)	Prec@1 82.031 (81.150)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.6416 (0.6654)	Prec@1 82.031 (81.100)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.6885 (0.6748)	Prec@1 82.031 (80.679)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.7661 (0.6833)	Prec@1 81.250 (80.463)	
Total train loss: 0.6833

 * Prec@1 66.240 Prec@5 89.270 Loss 1.2842
Best acc: 66.370
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.6348 (0.5738)	Prec@1 80.469 (84.535)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.4766 (0.5983)	Prec@1 88.281 (83.579)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.6318 (0.6049)	Prec@1 82.031 (83.297)	
Epoch: [2][311/391]	LR: 0.005	Loss 0.6040 (0.6109)	Prec@1 81.250 (83.030)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.8315 (0.6181)	Prec@1 75.781 (82.722)	
Total train loss: 0.6181

 * Prec@1 65.510 Prec@5 88.490 Loss 1.3232
Best acc: 66.370
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.6191 (0.5311)	Prec@1 82.812 (85.917)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.6812 (0.5503)	Prec@1 82.812 (85.347)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.7119 (0.5579)	Prec@1 79.688 (85.029)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.5459 (0.5640)	Prec@1 84.375 (84.685)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.5718 (0.5707)	Prec@1 81.250 (84.387)	
Total train loss: 0.5710

 * Prec@1 65.830 Prec@5 88.300 Loss 1.3311
Best acc: 66.370
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.4871 (0.4945)	Prec@1 85.156 (87.310)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.4766 (0.5025)	Prec@1 86.719 (86.774)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.5806 (0.5111)	Prec@1 83.594 (86.538)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.4727 (0.5216)	Prec@1 87.500 (86.190)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.5806 (0.5313)	Prec@1 86.719 (85.811)	
Total train loss: 0.5313

 * Prec@1 65.710 Prec@5 88.260 Loss 1.3525
Best acc: 66.370
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.4395 (0.4444)	Prec@1 85.156 (88.942)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.5850 (0.4353)	Prec@1 82.812 (89.383)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.3794 (0.4290)	Prec@1 91.406 (89.643)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.4729 (0.4262)	Prec@1 89.844 (89.819)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.3870 (0.4240)	Prec@1 91.406 (89.842)	
Total train loss: 0.4241

 * Prec@1 66.540 Prec@5 88.460 Loss 1.3193
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.3096 (0.3875)	Prec@1 95.312 (91.466)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.3564 (0.3883)	Prec@1 92.188 (91.516)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.3862 (0.3854)	Prec@1 88.281 (91.620)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.3506 (0.3890)	Prec@1 94.531 (91.394)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.5029 (0.3924)	Prec@1 88.281 (91.226)	
Total train loss: 0.3926

 * Prec@1 66.270 Prec@5 88.630 Loss 1.3203
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.4658 (0.3840)	Prec@1 87.500 (91.597)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.3623 (0.3829)	Prec@1 92.969 (91.607)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.4292 (0.3831)	Prec@1 89.844 (91.483)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.4143 (0.3842)	Prec@1 91.406 (91.431)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.4556 (0.3861)	Prec@1 86.719 (91.346)	
Total train loss: 0.3862

 * Prec@1 65.960 Prec@5 88.380 Loss 1.3418
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.3875 (0.3594)	Prec@1 92.188 (92.458)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.3901 (0.3660)	Prec@1 91.406 (92.112)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.3455 (0.3708)	Prec@1 89.844 (91.900)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.4307 (0.3741)	Prec@1 89.844 (91.789)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.3374 (0.3768)	Prec@1 95.312 (91.803)	
Total train loss: 0.3768

 * Prec@1 66.170 Prec@5 88.360 Loss 1.3398
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.3735 (0.3606)	Prec@1 89.844 (92.558)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.3909 (0.3603)	Prec@1 92.188 (92.548)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.3804 (0.3616)	Prec@1 93.750 (92.428)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.2578 (0.3671)	Prec@1 95.312 (92.240)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.2341 (0.3665)	Prec@1 99.219 (92.292)	
Total train loss: 0.3666

 * Prec@1 66.070 Prec@5 88.170 Loss 1.3457
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.3037 (0.3481)	Prec@1 92.969 (92.939)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.3296 (0.3499)	Prec@1 92.969 (92.904)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.4448 (0.3493)	Prec@1 88.281 (93.009)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.2986 (0.3497)	Prec@1 94.531 (92.946)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.3281 (0.3491)	Prec@1 95.312 (92.955)	
Total train loss: 0.3490

 * Prec@1 66.040 Prec@5 88.250 Loss 1.3398
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.3618 (0.3428)	Prec@1 93.750 (93.249)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.3533 (0.3433)	Prec@1 92.969 (93.124)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.3623 (0.3458)	Prec@1 92.188 (93.109)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.4211 (0.3473)	Prec@1 87.500 (93.046)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.2986 (0.3483)	Prec@1 95.312 (93.041)	
Total train loss: 0.3486

 * Prec@1 65.820 Prec@5 88.310 Loss 1.3496
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.3369 (0.3511)	Prec@1 95.312 (92.738)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.3169 (0.3468)	Prec@1 93.750 (93.074)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.2820 (0.3463)	Prec@1 96.875 (93.089)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.3169 (0.3479)	Prec@1 94.531 (92.991)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.3828 (0.3485)	Prec@1 90.625 (92.987)	
Total train loss: 0.3485

 * Prec@1 66.150 Prec@5 88.170 Loss 1.3418
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.3594 (0.3443)	Prec@1 94.531 (93.099)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.3103 (0.3451)	Prec@1 92.969 (93.084)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.3735 (0.3444)	Prec@1 89.844 (93.076)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.3865 (0.3451)	Prec@1 89.844 (93.101)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.3850 (0.3457)	Prec@1 88.281 (93.081)	
Total train loss: 0.3459

 * Prec@1 65.960 Prec@5 88.280 Loss 1.3516
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.3281 (0.3577)	Prec@1 92.188 (92.638)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.2869 (0.3519)	Prec@1 96.094 (92.824)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.3445 (0.3475)	Prec@1 93.750 (93.059)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.3816 (0.3475)	Prec@1 92.188 (93.079)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3640 (0.3478)	Prec@1 95.312 (93.111)	
Total train loss: 0.3479

 * Prec@1 66.140 Prec@5 88.360 Loss 1.3457
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.3621 (0.3441)	Prec@1 92.188 (93.600)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.2908 (0.3463)	Prec@1 94.531 (93.364)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.3413 (0.3438)	Prec@1 92.188 (93.333)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.3176 (0.3445)	Prec@1 92.188 (93.212)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.3606 (0.3452)	Prec@1 95.312 (93.215)	
Total train loss: 0.3454

 * Prec@1 65.960 Prec@5 88.260 Loss 1.3486
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.2908 (0.3408)	Prec@1 95.312 (93.219)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.3286 (0.3437)	Prec@1 95.312 (93.189)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.4019 (0.3440)	Prec@1 93.750 (93.233)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.4138 (0.3454)	Prec@1 86.719 (93.121)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.3010 (0.3445)	Prec@1 95.312 (93.115)	
Total train loss: 0.3446

 * Prec@1 66.160 Prec@5 88.260 Loss 1.3398
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.3643 (0.3450)	Prec@1 91.406 (92.979)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.3259 (0.3455)	Prec@1 92.969 (92.889)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.3025 (0.3442)	Prec@1 94.531 (93.049)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.3730 (0.3444)	Prec@1 89.844 (93.076)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.3679 (0.3462)	Prec@1 92.188 (93.125)	
Total train loss: 0.3463

 * Prec@1 65.730 Prec@5 88.150 Loss 1.3486
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.4033 (0.3446)	Prec@1 89.062 (93.399)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.3472 (0.3471)	Prec@1 92.188 (93.179)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.3628 (0.3458)	Prec@1 91.406 (93.162)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.3806 (0.3474)	Prec@1 95.312 (93.086)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.4209 (0.3466)	Prec@1 89.062 (93.077)	
Total train loss: 0.3468

 * Prec@1 65.930 Prec@5 88.260 Loss 1.3516
Best acc: 66.540
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.4443 (0.3551)	Prec@1 89.062 (92.678)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.2676 (0.3495)	Prec@1 96.094 (92.829)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.3181 (0.3492)	Prec@1 95.312 (92.859)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.3088 (0.3494)	Prec@1 96.094 (92.891)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.3591 (0.3482)	Prec@1 92.188 (92.953)	
Total train loss: 0.3484

 * Prec@1 66.200 Prec@5 88.300 Loss 1.3428
Best acc: 66.540
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 15
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 34.890 Prec@5 63.170 Loss 3.1543
Pre-trained Prec@1 with 15 layers frozen: 34.88999938964844 	 Loss: 3.154296875

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 1.0176 (1.2643)	Prec@1 73.438 (64.193)	
Epoch: [0][155/391]	LR: 0.005	Loss 0.9312 (1.1576)	Prec@1 78.125 (67.027)	
Epoch: [0][233/391]	LR: 0.005	Loss 0.8828 (1.1128)	Prec@1 74.219 (68.069)	
Epoch: [0][311/391]	LR: 0.005	Loss 1.0400 (1.0888)	Prec@1 73.438 (68.560)	
Epoch: [0][389/391]	LR: 0.005	Loss 0.9390 (1.0738)	Prec@1 71.875 (69.002)	
Total train loss: 1.0738

 * Prec@1 64.400 Prec@5 88.460 Loss 1.3301
Best acc: 64.400
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 0.7510 (0.8614)	Prec@1 78.906 (74.599)	
Epoch: [1][155/391]	LR: 0.005	Loss 0.9233 (0.8588)	Prec@1 72.656 (74.830)	
Epoch: [1][233/391]	LR: 0.005	Loss 0.9941 (0.8661)	Prec@1 67.969 (74.639)	
Epoch: [1][311/391]	LR: 0.005	Loss 0.8691 (0.8685)	Prec@1 73.438 (74.607)	
Epoch: [1][389/391]	LR: 0.005	Loss 0.6729 (0.8728)	Prec@1 78.125 (74.527)	
Total train loss: 0.8728

 * Prec@1 64.710 Prec@5 88.440 Loss 1.3115
Best acc: 64.710
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 0.6865 (0.7678)	Prec@1 79.688 (78.385)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.8770 (0.7815)	Prec@1 74.219 (77.529)	
Epoch: [2][233/391]	LR: 0.005	Loss 0.9941 (0.7835)	Prec@1 75.000 (77.407)	
Epoch: [2][311/391]	LR: 0.005	Loss 1.0742 (0.7992)	Prec@1 68.750 (76.858)	
Epoch: [2][389/391]	LR: 0.005	Loss 0.9478 (0.8060)	Prec@1 71.094 (76.581)	
Total train loss: 0.8059

 * Prec@1 65.230 Prec@5 88.170 Loss 1.3379
Best acc: 65.230
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 0.6055 (0.7066)	Prec@1 82.812 (80.369)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.6504 (0.7319)	Prec@1 78.906 (79.277)	
Epoch: [3][233/391]	LR: 0.005	Loss 0.7710 (0.7390)	Prec@1 79.688 (79.053)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.6060 (0.7459)	Prec@1 85.938 (78.859)	
Epoch: [3][389/391]	LR: 0.005	Loss 0.6851 (0.7520)	Prec@1 80.469 (78.592)	
Total train loss: 0.7520

 * Prec@1 65.130 Prec@5 88.110 Loss 1.3369
Best acc: 65.230
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 0.6460 (0.6636)	Prec@1 81.250 (81.721)	
Epoch: [4][155/391]	LR: 0.005	Loss 0.8936 (0.6780)	Prec@1 74.219 (81.205)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.7148 (0.6899)	Prec@1 76.562 (80.672)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.6826 (0.7017)	Prec@1 78.906 (80.256)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.9067 (0.7107)	Prec@1 78.125 (79.978)	
Total train loss: 0.7108

 * Prec@1 64.830 Prec@5 88.000 Loss 1.3623
Best acc: 65.230
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.6543 (0.6071)	Prec@1 81.250 (83.714)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.6069 (0.6107)	Prec@1 86.719 (83.734)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.5894 (0.6056)	Prec@1 82.812 (83.864)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.5737 (0.5987)	Prec@1 86.719 (84.049)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.5557 (0.5974)	Prec@1 87.500 (83.980)	
Total train loss: 0.5974

 * Prec@1 65.570 Prec@5 88.330 Loss 1.3369
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.6948 (0.5537)	Prec@1 81.250 (85.537)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.5522 (0.5586)	Prec@1 85.938 (85.276)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.6294 (0.5650)	Prec@1 82.031 (85.086)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.5288 (0.5669)	Prec@1 87.500 (85.049)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.6978 (0.5725)	Prec@1 78.906 (84.878)	
Total train loss: 0.5726

 * Prec@1 65.310 Prec@5 88.240 Loss 1.3486
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.5078 (0.5389)	Prec@1 89.062 (86.649)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.5537 (0.5494)	Prec@1 85.938 (85.887)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.5498 (0.5556)	Prec@1 81.250 (85.610)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.5161 (0.5594)	Prec@1 86.719 (85.462)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.3979 (0.5623)	Prec@1 92.969 (85.272)	
Total train loss: 0.5626

 * Prec@1 65.360 Prec@5 88.230 Loss 1.3398
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.6064 (0.5484)	Prec@1 80.469 (85.877)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.6641 (0.5441)	Prec@1 80.469 (85.973)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.5527 (0.5455)	Prec@1 85.938 (85.941)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.4939 (0.5500)	Prec@1 86.719 (85.785)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.6099 (0.5508)	Prec@1 82.812 (85.797)	
Total train loss: 0.5508

 * Prec@1 65.020 Prec@5 87.890 Loss 1.3672
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.6016 (0.5468)	Prec@1 85.156 (86.218)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.5806 (0.5363)	Prec@1 83.594 (86.418)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.6431 (0.5402)	Prec@1 82.031 (86.221)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.5449 (0.5411)	Prec@1 88.281 (86.175)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.7002 (0.5441)	Prec@1 77.344 (86.066)	
Total train loss: 0.5439

 * Prec@1 64.960 Prec@5 87.950 Loss 1.3662
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.5029 (0.5147)	Prec@1 92.188 (87.119)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.6377 (0.5201)	Prec@1 82.031 (87.114)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.5215 (0.5225)	Prec@1 88.281 (86.879)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.5811 (0.5222)	Prec@1 88.281 (86.897)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.4751 (0.5219)	Prec@1 90.625 (86.907)	
Total train loss: 0.5219

 * Prec@1 65.070 Prec@5 87.860 Loss 1.3730
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.5977 (0.5232)	Prec@1 83.594 (87.099)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.4502 (0.5291)	Prec@1 92.188 (86.869)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.4680 (0.5293)	Prec@1 88.281 (86.772)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.5493 (0.5272)	Prec@1 82.031 (86.864)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.5464 (0.5239)	Prec@1 86.719 (86.893)	
Total train loss: 0.5240

 * Prec@1 65.010 Prec@5 87.820 Loss 1.3750
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.5244 (0.5160)	Prec@1 85.156 (87.190)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.4844 (0.5177)	Prec@1 86.719 (87.029)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.5967 (0.5206)	Prec@1 85.156 (86.812)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.4978 (0.5194)	Prec@1 90.625 (86.929)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.4204 (0.5175)	Prec@1 92.188 (87.049)	
Total train loss: 0.5175

 * Prec@1 64.990 Prec@5 87.980 Loss 1.3691
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.4280 (0.5168)	Prec@1 91.406 (87.220)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.4722 (0.5173)	Prec@1 89.844 (87.099)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.4580 (0.5198)	Prec@1 90.625 (87.103)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.4321 (0.5180)	Prec@1 92.188 (87.167)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.5127 (0.5160)	Prec@1 88.281 (87.234)	
Total train loss: 0.5163

 * Prec@1 65.170 Prec@5 87.780 Loss 1.3760
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.4958 (0.5280)	Prec@1 86.719 (86.889)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.4841 (0.5214)	Prec@1 87.500 (86.949)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.5601 (0.5237)	Prec@1 86.719 (86.879)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.5327 (0.5233)	Prec@1 86.719 (86.884)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.3833 (0.5205)	Prec@1 92.188 (86.973)	
Total train loss: 0.5204

 * Prec@1 64.820 Prec@5 87.750 Loss 1.3750
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.5464 (0.5036)	Prec@1 86.719 (88.061)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.4529 (0.5123)	Prec@1 89.844 (87.465)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.5425 (0.5139)	Prec@1 89.062 (87.306)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.3838 (0.5132)	Prec@1 94.531 (87.330)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.6001 (0.5139)	Prec@1 87.500 (87.306)	
Total train loss: 0.5146

 * Prec@1 65.110 Prec@5 87.710 Loss 1.3682
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.4985 (0.5026)	Prec@1 89.062 (87.800)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.4365 (0.5143)	Prec@1 87.500 (87.270)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.4766 (0.5125)	Prec@1 85.938 (87.223)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.4866 (0.5148)	Prec@1 88.281 (87.157)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.5112 (0.5167)	Prec@1 91.406 (87.103)	
Total train loss: 0.5170

 * Prec@1 65.110 Prec@5 87.790 Loss 1.3682
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.4817 (0.5165)	Prec@1 85.156 (87.129)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.4595 (0.5165)	Prec@1 88.281 (87.215)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.5127 (0.5162)	Prec@1 88.281 (87.250)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.5308 (0.5132)	Prec@1 89.062 (87.337)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.4497 (0.5137)	Prec@1 90.625 (87.258)	
Total train loss: 0.5138

 * Prec@1 64.920 Prec@5 87.680 Loss 1.3799
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.4785 (0.5124)	Prec@1 89.062 (87.390)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.5049 (0.5143)	Prec@1 85.156 (87.550)	
Epoch: [18][233/391]	LR: 4e-05	Loss 0.4851 (0.5158)	Prec@1 88.281 (87.447)	
Epoch: [18][311/391]	LR: 4e-05	Loss 0.4495 (0.5162)	Prec@1 91.406 (87.335)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.4263 (0.5157)	Prec@1 93.750 (87.332)	
Total train loss: 0.5157

 * Prec@1 64.840 Prec@5 87.990 Loss 1.3711
Best acc: 65.570
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.5029 (0.5104)	Prec@1 88.281 (87.059)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.4124 (0.5089)	Prec@1 92.969 (87.365)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.4658 (0.5141)	Prec@1 89.844 (87.139)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.5752 (0.5143)	Prec@1 83.594 (87.117)	
Epoch: [19][389/391]	LR: 4e-05	Loss 0.5483 (0.5168)	Prec@1 88.281 (87.135)	
Total train loss: 0.5169

 * Prec@1 65.050 Prec@5 87.850 Loss 1.3730
Best acc: 65.570
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 17
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 27.520 Prec@5 57.080 Loss 3.8301
Pre-trained Prec@1 with 17 layers frozen: 27.51999855041504 	 Loss: 3.830078125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 1.3223 (1.5154)	Prec@1 62.500 (58.924)	
Epoch: [0][155/391]	LR: 0.005	Loss 1.1904 (1.4098)	Prec@1 65.625 (61.048)	
Epoch: [0][233/391]	LR: 0.005	Loss 1.2764 (1.3510)	Prec@1 62.500 (62.216)	
Epoch: [0][311/391]	LR: 0.005	Loss 1.2500 (1.3168)	Prec@1 65.625 (63.103)	
Epoch: [0][389/391]	LR: 0.005	Loss 1.1064 (1.2937)	Prec@1 73.438 (63.636)	
Total train loss: 1.2946

 * Prec@1 62.620 Prec@5 87.010 Loss 1.4062
Best acc: 62.620
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 1.1846 (1.0791)	Prec@1 64.062 (69.511)	
Epoch: [1][155/391]	LR: 0.005	Loss 1.0205 (1.0898)	Prec@1 68.750 (68.920)	
Epoch: [1][233/391]	LR: 0.005	Loss 1.2129 (1.0988)	Prec@1 61.719 (68.710)	
Epoch: [1][311/391]	LR: 0.005	Loss 1.0684 (1.1056)	Prec@1 67.188 (68.597)	
Epoch: [1][389/391]	LR: 0.005	Loss 1.0908 (1.1037)	Prec@1 72.656 (68.598)	
Total train loss: 1.1039

 * Prec@1 63.250 Prec@5 87.490 Loss 1.3613
Best acc: 63.250
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 1.1533 (1.0244)	Prec@1 64.844 (71.134)	
Epoch: [2][155/391]	LR: 0.005	Loss 0.9438 (1.0275)	Prec@1 73.438 (70.528)	
Epoch: [2][233/391]	LR: 0.005	Loss 1.0469 (1.0369)	Prec@1 73.438 (70.239)	
Epoch: [2][311/391]	LR: 0.005	Loss 1.0732 (1.0414)	Prec@1 67.969 (70.080)	
Epoch: [2][389/391]	LR: 0.005	Loss 1.0293 (1.0428)	Prec@1 68.750 (70.020)	
Total train loss: 1.0425

 * Prec@1 63.280 Prec@5 87.590 Loss 1.3652
Best acc: 63.280
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 1.0977 (0.9907)	Prec@1 69.531 (71.534)	
Epoch: [3][155/391]	LR: 0.005	Loss 0.8203 (0.9876)	Prec@1 75.000 (71.780)	
Epoch: [3][233/391]	LR: 0.005	Loss 1.1543 (0.9995)	Prec@1 69.531 (71.504)	
Epoch: [3][311/391]	LR: 0.005	Loss 0.9688 (0.9970)	Prec@1 72.656 (71.549)	
Epoch: [3][389/391]	LR: 0.005	Loss 1.2217 (0.9999)	Prec@1 65.625 (71.560)	
Total train loss: 1.0001

 * Prec@1 63.580 Prec@5 87.760 Loss 1.3691
Best acc: 63.580
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 1.0049 (0.9440)	Prec@1 70.312 (73.568)	
Epoch: [4][155/391]	LR: 0.005	Loss 1.0469 (0.9548)	Prec@1 67.188 (73.137)	
Epoch: [4][233/391]	LR: 0.005	Loss 0.8550 (0.9619)	Prec@1 76.562 (72.860)	
Epoch: [4][311/391]	LR: 0.005	Loss 0.9966 (0.9666)	Prec@1 70.312 (72.604)	
Epoch: [4][389/391]	LR: 0.005	Loss 0.9961 (0.9702)	Prec@1 71.875 (72.460)	
Total train loss: 0.9702

 * Prec@1 63.430 Prec@5 87.790 Loss 1.3711
Best acc: 63.580
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 0.7534 (0.9031)	Prec@1 80.469 (74.339)	
Epoch: [5][155/391]	LR: 0.001	Loss 0.8413 (0.8904)	Prec@1 76.562 (74.679)	
Epoch: [5][233/391]	LR: 0.001	Loss 0.8721 (0.8900)	Prec@1 78.125 (74.793)	
Epoch: [5][311/391]	LR: 0.001	Loss 0.8911 (0.8853)	Prec@1 76.562 (74.860)	
Epoch: [5][389/391]	LR: 0.001	Loss 0.7954 (0.8854)	Prec@1 79.688 (74.908)	
Total train loss: 0.8852

 * Prec@1 64.160 Prec@5 87.630 Loss 1.3486
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 0.8037 (0.8708)	Prec@1 81.250 (75.511)	
Epoch: [6][155/391]	LR: 0.001	Loss 0.8511 (0.8615)	Prec@1 75.781 (75.921)	
Epoch: [6][233/391]	LR: 0.001	Loss 0.8984 (0.8669)	Prec@1 71.094 (75.614)	
Epoch: [6][311/391]	LR: 0.001	Loss 0.8052 (0.8637)	Prec@1 79.688 (75.679)	
Epoch: [6][389/391]	LR: 0.001	Loss 0.8750 (0.8678)	Prec@1 73.438 (75.547)	
Total train loss: 0.8681

 * Prec@1 64.110 Prec@5 87.730 Loss 1.3574
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 0.8516 (0.8486)	Prec@1 78.125 (76.242)	
Epoch: [7][155/391]	LR: 0.001	Loss 0.8237 (0.8606)	Prec@1 81.250 (75.916)	
Epoch: [7][233/391]	LR: 0.001	Loss 0.9287 (0.8566)	Prec@1 76.562 (75.845)	
Epoch: [7][311/391]	LR: 0.001	Loss 0.8501 (0.8617)	Prec@1 79.688 (75.891)	
Epoch: [7][389/391]	LR: 0.001	Loss 0.7515 (0.8607)	Prec@1 80.469 (75.859)	
Total train loss: 0.8611

 * Prec@1 64.070 Prec@5 87.690 Loss 1.3525
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 0.8677 (0.8579)	Prec@1 75.781 (76.102)	
Epoch: [8][155/391]	LR: 0.001	Loss 0.8491 (0.8503)	Prec@1 77.344 (76.322)	
Epoch: [8][233/391]	LR: 0.001	Loss 0.9199 (0.8514)	Prec@1 75.781 (76.249)	
Epoch: [8][311/391]	LR: 0.001	Loss 0.8584 (0.8519)	Prec@1 75.781 (76.137)	
Epoch: [8][389/391]	LR: 0.001	Loss 0.8335 (0.8515)	Prec@1 74.219 (76.162)	
Total train loss: 0.8520

 * Prec@1 64.010 Prec@5 87.690 Loss 1.3555
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 0.7979 (0.8212)	Prec@1 79.688 (77.153)	
Epoch: [9][155/391]	LR: 0.001	Loss 0.8022 (0.8317)	Prec@1 75.000 (76.838)	
Epoch: [9][233/391]	LR: 0.001	Loss 0.9341 (0.8370)	Prec@1 72.656 (76.666)	
Epoch: [9][311/391]	LR: 0.001	Loss 0.9346 (0.8427)	Prec@1 70.312 (76.402)	
Epoch: [9][389/391]	LR: 0.001	Loss 0.8887 (0.8464)	Prec@1 75.000 (76.252)	
Total train loss: 0.8467

 * Prec@1 63.890 Prec@5 87.710 Loss 1.3633
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 0.9263 (0.8414)	Prec@1 73.438 (76.502)	
Epoch: [10][155/391]	LR: 0.0002	Loss 0.6431 (0.8383)	Prec@1 82.812 (76.487)	
Epoch: [10][233/391]	LR: 0.0002	Loss 0.7925 (0.8359)	Prec@1 75.781 (76.626)	
Epoch: [10][311/391]	LR: 0.0002	Loss 0.9082 (0.8313)	Prec@1 68.750 (76.718)	
Epoch: [10][389/391]	LR: 0.0002	Loss 0.6211 (0.8323)	Prec@1 83.594 (76.785)	
Total train loss: 0.8322

 * Prec@1 63.790 Prec@5 87.540 Loss 1.3623
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 0.7510 (0.8317)	Prec@1 83.594 (77.023)	
Epoch: [11][155/391]	LR: 0.0002	Loss 0.9531 (0.8298)	Prec@1 74.219 (76.783)	
Epoch: [11][233/391]	LR: 0.0002	Loss 0.9312 (0.8274)	Prec@1 75.781 (76.766)	
Epoch: [11][311/391]	LR: 0.0002	Loss 0.8022 (0.8261)	Prec@1 78.906 (76.876)	
Epoch: [11][389/391]	LR: 0.0002	Loss 0.8364 (0.8286)	Prec@1 78.125 (76.819)	
Total train loss: 0.8286

 * Prec@1 63.860 Prec@5 87.760 Loss 1.3604
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 0.7627 (0.8314)	Prec@1 78.125 (76.022)	
Epoch: [12][155/391]	LR: 0.0002	Loss 0.8657 (0.8222)	Prec@1 76.562 (76.698)	
Epoch: [12][233/391]	LR: 0.0002	Loss 0.7520 (0.8279)	Prec@1 82.031 (76.679)	
Epoch: [12][311/391]	LR: 0.0002	Loss 0.9590 (0.8273)	Prec@1 74.219 (76.678)	
Epoch: [12][389/391]	LR: 0.0002	Loss 0.6245 (0.8292)	Prec@1 85.938 (76.723)	
Total train loss: 0.8296

 * Prec@1 64.000 Prec@5 87.460 Loss 1.3652
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 0.7314 (0.8203)	Prec@1 76.562 (76.993)	
Epoch: [13][155/391]	LR: 0.0002	Loss 0.9766 (0.8278)	Prec@1 75.000 (76.718)	
Epoch: [13][233/391]	LR: 0.0002	Loss 0.7637 (0.8289)	Prec@1 79.688 (76.826)	
Epoch: [13][311/391]	LR: 0.0002	Loss 0.8867 (0.8241)	Prec@1 78.125 (76.906)	
Epoch: [13][389/391]	LR: 0.0002	Loss 0.8696 (0.8262)	Prec@1 81.250 (76.841)	
Total train loss: 0.8265

 * Prec@1 63.880 Prec@5 87.460 Loss 1.3711
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 0.7661 (0.8291)	Prec@1 76.562 (76.813)	
Epoch: [14][155/391]	LR: 0.0002	Loss 0.9556 (0.8291)	Prec@1 69.531 (76.648)	
Epoch: [14][233/391]	LR: 0.0002	Loss 0.9736 (0.8229)	Prec@1 75.000 (76.903)	
Epoch: [14][311/391]	LR: 0.0002	Loss 0.9053 (0.8261)	Prec@1 75.000 (76.865)	
Epoch: [14][389/391]	LR: 0.0002	Loss 0.7729 (0.8245)	Prec@1 78.906 (76.901)	
Total train loss: 0.8245

 * Prec@1 64.010 Prec@5 87.620 Loss 1.3633
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 0.8428 (0.8129)	Prec@1 77.344 (77.153)	
Epoch: [15][155/391]	LR: 4e-05	Loss 0.7192 (0.8159)	Prec@1 80.469 (77.249)	
Epoch: [15][233/391]	LR: 4e-05	Loss 0.8906 (0.8197)	Prec@1 72.656 (77.113)	
Epoch: [15][311/391]	LR: 4e-05	Loss 0.7783 (0.8175)	Prec@1 78.125 (77.229)	
Epoch: [15][389/391]	LR: 4e-05	Loss 0.9487 (0.8229)	Prec@1 75.781 (77.017)	
Total train loss: 0.8231

 * Prec@1 64.040 Prec@5 87.570 Loss 1.3682
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 0.9185 (0.8362)	Prec@1 76.562 (76.182)	
Epoch: [16][155/391]	LR: 4e-05	Loss 0.8633 (0.8330)	Prec@1 75.781 (76.422)	
Epoch: [16][233/391]	LR: 4e-05	Loss 0.7949 (0.8240)	Prec@1 76.562 (76.663)	
Epoch: [16][311/391]	LR: 4e-05	Loss 0.7671 (0.8206)	Prec@1 78.125 (76.888)	
Epoch: [16][389/391]	LR: 4e-05	Loss 0.9336 (0.8228)	Prec@1 71.875 (76.913)	
Total train loss: 0.8228

 * Prec@1 63.830 Prec@5 87.770 Loss 1.3594
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 0.8311 (0.8170)	Prec@1 78.906 (77.304)	
Epoch: [17][155/391]	LR: 4e-05	Loss 0.8262 (0.8293)	Prec@1 75.000 (76.923)	
Epoch: [17][233/391]	LR: 4e-05	Loss 0.7949 (0.8285)	Prec@1 80.469 (76.960)	
Epoch: [17][311/391]	LR: 4e-05	Loss 0.8291 (0.8243)	Prec@1 78.125 (76.958)	
Epoch: [17][389/391]	LR: 4e-05	Loss 0.7886 (0.8248)	Prec@1 78.125 (76.989)	
Total train loss: 0.8249

 * Prec@1 64.060 Prec@5 87.610 Loss 1.3623
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 0.7778 (0.8218)	Prec@1 81.250 (76.983)	
Epoch: [18][155/391]	LR: 4e-05	Loss 0.7637 (0.8272)	Prec@1 78.125 (76.848)	
Epoch: [18][233/391]	LR: 4e-05	Loss 1.0371 (0.8257)	Prec@1 69.531 (76.903)	
Epoch: [18][311/391]	LR: 4e-05	Loss 1.0107 (0.8253)	Prec@1 67.188 (76.926)	
Epoch: [18][389/391]	LR: 4e-05	Loss 0.8501 (0.8269)	Prec@1 78.906 (76.909)	
Total train loss: 0.8269

 * Prec@1 64.050 Prec@5 87.460 Loss 1.3613
Best acc: 64.160
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 0.9097 (0.8245)	Prec@1 77.344 (77.163)	
Epoch: [19][155/391]	LR: 4e-05	Loss 0.7036 (0.8257)	Prec@1 81.250 (76.818)	
Epoch: [19][233/391]	LR: 4e-05	Loss 0.7773 (0.8274)	Prec@1 77.344 (76.773)	
Epoch: [19][311/391]	LR: 4e-05	Loss 0.8789 (0.8244)	Prec@1 75.000 (76.775)	
Epoch: [19][389/391]	LR: 4e-05	Loss 1.0645 (0.8263)	Prec@1 69.531 (76.809)	
Total train loss: 0.8263

 * Prec@1 64.160 Prec@5 87.480 Loss 1.3633
Best acc: 64.160
--------------------------------------------------------------------------------

      ==> Arguments:
          dataset: cifar100
          model: resnet20
          load_dir: /home/nano01/a/esoufler/activations/one_batch/
          savedir: ../pretrained_models/frozen/
          pretrained: ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar
          workers: 8
          epochs: 20
          start_epoch: 0
          batch_size: 128
          lr: 0.005
          momentum: 0.9
          weight_decay: 0.0001
          gamma: 0.2
          milestones: [5, 10, 15]
          loss: crossentropy
          optim: sgd
          dropout: 0.5
          print_freq: 5
          resume: 
          evaluate: False
          half: True
          save_every: 10
          gpus: 0
          frozen_layers: 19
DEVICE: cuda
GPU Id(s) being used: 0
==> Building model for resnet20 ...
==> Initializing model with pre-trained parameters ...
==> Load pretrained model form ../pretrained_models/ideal/resnet20fp_cifar100.pth.tar ...
Original model accuracy: 69.5999984741211
 * Prec@1 23.800 Prec@5 53.360 Loss 4.3320
Pre-trained Prec@1 with 19 layers frozen: 23.799999237060547 	 Loss: 4.33203125

Starting training on SRAM layers...
Epoch: [0][77/391]	LR: 0.005	Loss 1.9297 (1.8976)	Prec@1 46.875 (50.270)	
Epoch: [0][155/391]	LR: 0.005	Loss 1.7275 (1.8123)	Prec@1 53.906 (52.063)	
Epoch: [0][233/391]	LR: 0.005	Loss 1.7695 (1.7545)	Prec@1 55.469 (53.522)	
Epoch: [0][311/391]	LR: 0.005	Loss 1.6299 (1.7193)	Prec@1 57.812 (54.239)	
Epoch: [0][389/391]	LR: 0.005	Loss 1.4492 (1.6937)	Prec@1 65.625 (54.808)	
Total train loss: 1.6937

 * Prec@1 56.830 Prec@5 83.960 Loss 1.6377
Best acc: 56.830
--------------------------------------------------------------------------------
Epoch: [1][77/391]	LR: 0.005	Loss 1.6475 (1.5303)	Prec@1 56.250 (58.313)	
Epoch: [1][155/391]	LR: 0.005	Loss 1.4902 (1.5287)	Prec@1 59.375 (58.433)	
Epoch: [1][233/391]	LR: 0.005	Loss 1.4570 (1.5286)	Prec@1 61.719 (58.651)	
Epoch: [1][311/391]	LR: 0.005	Loss 1.4473 (1.5303)	Prec@1 63.281 (58.534)	
Epoch: [1][389/391]	LR: 0.005	Loss 1.6250 (1.5295)	Prec@1 57.812 (58.546)	
Total train loss: 1.5299

 * Prec@1 57.990 Prec@5 84.790 Loss 1.5859
Best acc: 57.990
--------------------------------------------------------------------------------
Epoch: [2][77/391]	LR: 0.005	Loss 1.7188 (1.4935)	Prec@1 50.781 (59.675)	
Epoch: [2][155/391]	LR: 0.005	Loss 1.4453 (1.4924)	Prec@1 60.938 (59.385)	
Epoch: [2][233/391]	LR: 0.005	Loss 1.5684 (1.4866)	Prec@1 57.031 (59.425)	
Epoch: [2][311/391]	LR: 0.005	Loss 1.3975 (1.4920)	Prec@1 64.844 (59.257)	
Epoch: [2][389/391]	LR: 0.005	Loss 1.7695 (1.4948)	Prec@1 53.906 (59.311)	
Total train loss: 1.4945

 * Prec@1 58.320 Prec@5 84.820 Loss 1.5674
Best acc: 58.320
--------------------------------------------------------------------------------
Epoch: [3][77/391]	LR: 0.005	Loss 1.4082 (1.4523)	Prec@1 61.719 (60.577)	
Epoch: [3][155/391]	LR: 0.005	Loss 1.3516 (1.4587)	Prec@1 61.719 (60.542)	
Epoch: [3][233/391]	LR: 0.005	Loss 1.3975 (1.4620)	Prec@1 66.406 (60.400)	
Epoch: [3][311/391]	LR: 0.005	Loss 1.2725 (1.4691)	Prec@1 64.062 (60.059)	
Epoch: [3][389/391]	LR: 0.005	Loss 1.6104 (1.4723)	Prec@1 60.156 (59.962)	
Total train loss: 1.4724

 * Prec@1 58.810 Prec@5 85.090 Loss 1.5479
Best acc: 58.810
--------------------------------------------------------------------------------
Epoch: [4][77/391]	LR: 0.005	Loss 1.5801 (1.4344)	Prec@1 57.031 (61.048)	
Epoch: [4][155/391]	LR: 0.005	Loss 1.5488 (1.4549)	Prec@1 62.500 (60.452)	
Epoch: [4][233/391]	LR: 0.005	Loss 1.4834 (1.4550)	Prec@1 60.938 (60.460)	
Epoch: [4][311/391]	LR: 0.005	Loss 1.3652 (1.4629)	Prec@1 62.500 (60.231)	
Epoch: [4][389/391]	LR: 0.005	Loss 1.4561 (1.4628)	Prec@1 57.031 (60.351)	
Total train loss: 1.4628

 * Prec@1 58.750 Prec@5 85.080 Loss 1.5352
Best acc: 58.810
--------------------------------------------------------------------------------
Epoch: [5][77/391]	LR: 0.001	Loss 1.4580 (1.4384)	Prec@1 60.938 (61.098)	
Epoch: [5][155/391]	LR: 0.001	Loss 1.4717 (1.4336)	Prec@1 62.500 (61.333)	
Epoch: [5][233/391]	LR: 0.001	Loss 1.2002 (1.4434)	Prec@1 71.875 (60.968)	
Epoch: [5][311/391]	LR: 0.001	Loss 1.4189 (1.4399)	Prec@1 67.188 (61.078)	
Epoch: [5][389/391]	LR: 0.001	Loss 1.3818 (1.4386)	Prec@1 60.156 (60.940)	
Total train loss: 1.4387

 * Prec@1 58.840 Prec@5 85.050 Loss 1.5391
Best acc: 58.840
--------------------------------------------------------------------------------
Epoch: [6][77/391]	LR: 0.001	Loss 1.5010 (1.4324)	Prec@1 57.812 (61.068)	
Epoch: [6][155/391]	LR: 0.001	Loss 1.1992 (1.4370)	Prec@1 68.750 (60.922)	
Epoch: [6][233/391]	LR: 0.001	Loss 1.4707 (1.4360)	Prec@1 59.375 (61.081)	
Epoch: [6][311/391]	LR: 0.001	Loss 1.3350 (1.4395)	Prec@1 60.938 (60.880)	
Epoch: [6][389/391]	LR: 0.001	Loss 1.6162 (1.4363)	Prec@1 58.594 (60.851)	
Total train loss: 1.4364

 * Prec@1 58.830 Prec@5 85.170 Loss 1.5400
Best acc: 58.840
--------------------------------------------------------------------------------
Epoch: [7][77/391]	LR: 0.001	Loss 1.5010 (1.4478)	Prec@1 59.375 (60.727)	
Epoch: [7][155/391]	LR: 0.001	Loss 1.3789 (1.4359)	Prec@1 61.719 (60.892)	
Epoch: [7][233/391]	LR: 0.001	Loss 1.2979 (1.4418)	Prec@1 67.188 (60.811)	
Epoch: [7][311/391]	LR: 0.001	Loss 1.3516 (1.4355)	Prec@1 66.406 (60.988)	
Epoch: [7][389/391]	LR: 0.001	Loss 1.4883 (1.4325)	Prec@1 64.844 (61.086)	
Total train loss: 1.4326

 * Prec@1 58.960 Prec@5 85.210 Loss 1.5410
Best acc: 58.960
--------------------------------------------------------------------------------
Epoch: [8][77/391]	LR: 0.001	Loss 1.5508 (1.4524)	Prec@1 59.375 (60.497)	
Epoch: [8][155/391]	LR: 0.001	Loss 1.6318 (1.4313)	Prec@1 60.156 (61.308)	
Epoch: [8][233/391]	LR: 0.001	Loss 1.3730 (1.4330)	Prec@1 62.500 (60.911)	
Epoch: [8][311/391]	LR: 0.001	Loss 1.3174 (1.4362)	Prec@1 60.156 (60.827)	
Epoch: [8][389/391]	LR: 0.001	Loss 1.2871 (1.4342)	Prec@1 64.062 (60.851)	
Total train loss: 1.4346

 * Prec@1 59.080 Prec@5 85.270 Loss 1.5352
Best acc: 59.080
--------------------------------------------------------------------------------
Epoch: [9][77/391]	LR: 0.001	Loss 1.4375 (1.4275)	Prec@1 61.719 (61.218)	
Epoch: [9][155/391]	LR: 0.001	Loss 1.2432 (1.4412)	Prec@1 70.312 (60.943)	
Epoch: [9][233/391]	LR: 0.001	Loss 1.4141 (1.4377)	Prec@1 63.281 (60.964)	
Epoch: [9][311/391]	LR: 0.001	Loss 1.4102 (1.4337)	Prec@1 56.250 (60.955)	
Epoch: [9][389/391]	LR: 0.001	Loss 1.4570 (1.4319)	Prec@1 62.500 (61.122)	
Total train loss: 1.4321

 * Prec@1 59.250 Prec@5 85.200 Loss 1.5312
Best acc: 59.250
--------------------------------------------------------------------------------
Epoch: [10][77/391]	LR: 0.0002	Loss 1.2910 (1.4038)	Prec@1 64.844 (61.899)	
Epoch: [10][155/391]	LR: 0.0002	Loss 1.3984 (1.4101)	Prec@1 60.156 (61.719)	
Epoch: [10][233/391]	LR: 0.0002	Loss 1.3896 (1.4230)	Prec@1 63.281 (61.335)	
Epoch: [10][311/391]	LR: 0.0002	Loss 1.4453 (1.4259)	Prec@1 61.719 (61.328)	
Epoch: [10][389/391]	LR: 0.0002	Loss 1.4590 (1.4287)	Prec@1 60.938 (61.342)	
Total train loss: 1.4290

 * Prec@1 58.840 Prec@5 85.200 Loss 1.5371
Best acc: 59.250
--------------------------------------------------------------------------------
Epoch: [11][77/391]	LR: 0.0002	Loss 1.3008 (1.4418)	Prec@1 60.938 (60.978)	
Epoch: [11][155/391]	LR: 0.0002	Loss 1.4775 (1.4365)	Prec@1 59.375 (60.943)	
Epoch: [11][233/391]	LR: 0.0002	Loss 1.3750 (1.4369)	Prec@1 68.750 (60.847)	
Epoch: [11][311/391]	LR: 0.0002	Loss 1.3408 (1.4303)	Prec@1 60.938 (61.035)	
Epoch: [11][389/391]	LR: 0.0002	Loss 1.5508 (1.4265)	Prec@1 59.375 (61.200)	
Total train loss: 1.4265

 * Prec@1 58.780 Prec@5 85.190 Loss 1.5342
Best acc: 59.250
--------------------------------------------------------------------------------
Epoch: [12][77/391]	LR: 0.0002	Loss 1.3730 (1.4264)	Prec@1 58.594 (60.867)	
Epoch: [12][155/391]	LR: 0.0002	Loss 1.2959 (1.4138)	Prec@1 64.844 (61.433)	
Epoch: [12][233/391]	LR: 0.0002	Loss 1.3174 (1.4160)	Prec@1 69.531 (61.632)	
Epoch: [12][311/391]	LR: 0.0002	Loss 1.4219 (1.4212)	Prec@1 60.156 (61.496)	
Epoch: [12][389/391]	LR: 0.0002	Loss 1.3438 (1.4277)	Prec@1 61.719 (61.234)	
Total train loss: 1.4275

 * Prec@1 59.280 Prec@5 85.220 Loss 1.5342
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [13][77/391]	LR: 0.0002	Loss 1.4404 (1.4461)	Prec@1 57.812 (60.757)	
Epoch: [13][155/391]	LR: 0.0002	Loss 1.3643 (1.4339)	Prec@1 62.500 (61.043)	
Epoch: [13][233/391]	LR: 0.0002	Loss 1.5400 (1.4317)	Prec@1 59.375 (61.054)	
Epoch: [13][311/391]	LR: 0.0002	Loss 1.5156 (1.4305)	Prec@1 58.594 (61.063)	
Epoch: [13][389/391]	LR: 0.0002	Loss 1.5146 (1.4290)	Prec@1 58.594 (61.060)	
Total train loss: 1.4288

 * Prec@1 59.060 Prec@5 85.150 Loss 1.5361
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [14][77/391]	LR: 0.0002	Loss 1.4375 (1.4273)	Prec@1 60.938 (60.958)	
Epoch: [14][155/391]	LR: 0.0002	Loss 1.3887 (1.4324)	Prec@1 57.812 (60.577)	
Epoch: [14][233/391]	LR: 0.0002	Loss 1.3848 (1.4281)	Prec@1 60.938 (60.911)	
Epoch: [14][311/391]	LR: 0.0002	Loss 1.2607 (1.4299)	Prec@1 65.625 (60.940)	
Epoch: [14][389/391]	LR: 0.0002	Loss 1.3291 (1.4266)	Prec@1 67.969 (61.048)	
Total train loss: 1.4272

 * Prec@1 58.910 Prec@5 85.310 Loss 1.5352
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [15][77/391]	LR: 4e-05	Loss 1.3721 (1.4181)	Prec@1 64.062 (61.198)	
Epoch: [15][155/391]	LR: 4e-05	Loss 1.3916 (1.4233)	Prec@1 60.938 (61.263)	
Epoch: [15][233/391]	LR: 4e-05	Loss 1.3184 (1.4246)	Prec@1 65.625 (61.308)	
Epoch: [15][311/391]	LR: 4e-05	Loss 1.4385 (1.4240)	Prec@1 59.375 (61.293)	
Epoch: [15][389/391]	LR: 4e-05	Loss 1.5439 (1.4297)	Prec@1 54.688 (61.092)	
Total train loss: 1.4297

 * Prec@1 59.110 Prec@5 85.240 Loss 1.5391
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [16][77/391]	LR: 4e-05	Loss 1.4238 (1.4549)	Prec@1 60.938 (60.317)	
Epoch: [16][155/391]	LR: 4e-05	Loss 1.4893 (1.4371)	Prec@1 58.594 (60.842)	
Epoch: [16][233/391]	LR: 4e-05	Loss 1.4082 (1.4340)	Prec@1 65.625 (60.831)	
Epoch: [16][311/391]	LR: 4e-05	Loss 1.6064 (1.4326)	Prec@1 60.156 (60.900)	
Epoch: [16][389/391]	LR: 4e-05	Loss 1.5049 (1.4290)	Prec@1 58.594 (61.120)	
Total train loss: 1.4291

 * Prec@1 58.960 Prec@5 85.220 Loss 1.5352
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [17][77/391]	LR: 4e-05	Loss 1.5332 (1.4071)	Prec@1 55.469 (62.029)	
Epoch: [17][155/391]	LR: 4e-05	Loss 1.3799 (1.4202)	Prec@1 64.062 (61.108)	
Epoch: [17][233/391]	LR: 4e-05	Loss 1.4971 (1.4275)	Prec@1 56.250 (60.984)	
Epoch: [17][311/391]	LR: 4e-05	Loss 1.3398 (1.4255)	Prec@1 65.625 (61.120)	
Epoch: [17][389/391]	LR: 4e-05	Loss 1.0352 (1.4269)	Prec@1 71.094 (61.054)	
Total train loss: 1.4269

 * Prec@1 58.870 Prec@5 85.360 Loss 1.5322
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [18][77/391]	LR: 4e-05	Loss 1.5166 (1.4239)	Prec@1 55.469 (61.338)	
Epoch: [18][155/391]	LR: 4e-05	Loss 1.6182 (1.4221)	Prec@1 56.250 (61.258)	
Epoch: [18][233/391]	LR: 4e-05	Loss 1.5674 (1.4278)	Prec@1 60.156 (61.118)	
Epoch: [18][311/391]	LR: 4e-05	Loss 1.4697 (1.4302)	Prec@1 62.500 (61.058)	
Epoch: [18][389/391]	LR: 4e-05	Loss 1.1758 (1.4291)	Prec@1 69.531 (61.086)	
Total train loss: 1.4291

 * Prec@1 59.210 Prec@5 85.270 Loss 1.5342
Best acc: 59.280
--------------------------------------------------------------------------------
Epoch: [19][77/391]	LR: 4e-05	Loss 1.1836 (1.4266)	Prec@1 64.844 (61.258)	
Epoch: [19][155/391]	LR: 4e-05	Loss 1.1338 (1.4307)	Prec@1 71.875 (60.968)	
Epoch: [19][233/391]	LR: 4e-05	Loss 1.3672 (1.4403)	Prec@1 66.406 (60.894)	
Epoch: [19][311/391]	LR: 4e-05	Loss 1.3848 (1.4332)	Prec@1 61.719 (60.988)	
Epoch: [19][389/391]	LR: 4e-05	Loss 1.7480 (1.4299)	Prec@1 53.906 (61.094)	
Total train loss: 1.4295

 * Prec@1 59.050 Prec@5 85.160 Loss 1.5303
Best acc: 59.280
--------------------------------------------------------------------------------
