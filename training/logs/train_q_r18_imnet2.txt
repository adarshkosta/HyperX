
      ==> Arguments:
          dataset: imagenet
          model: resnet18
          workers: 8
          epochs: 150
          start_epoch: 0
          batch_size: 1024
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          tag: qfp_i8b7f_w8b7f
          milestones: [20, 40, 60, 80, 120]
          gamma: 0.1
          input_size: None
          print_freq: 1000
          resume: 
          evaluate: False
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          half: False
          savedir: ../pretrained_models/ideal/
          save_every: 10
          gpus: 0,1,2,3
DEVICE: cuda
GPU Id(s) being used: 0,1,2,3
==> Building model for resnet18 ...
ResNet18(
  (fq0): activation_quantize_fn()
  (conv1): Conv2d_Q(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU(inplace=True)
  (fq1): activation_quantize_fn()
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (fq2): activation_quantize_fn()
  (conv3): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (fq3): activation_quantize_fn()
  (conv4): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (fq4): activation_quantize_fn()
  (conv5): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (fq5): activation_quantize_fn()
  (conv6): Conv2d_Q(
    64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr1): activation_quantize_fn()
  (resconv1): Sequential(
    (0): Conv2d_Q(
      64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (fq6): activation_quantize_fn()
  (conv7): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (fq7): activation_quantize_fn()
  (conv8): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (fq8): activation_quantize_fn()
  (conv9): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (fq9): activation_quantize_fn()
  (conv10): Conv2d_Q(
    128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr2): activation_quantize_fn()
  (resconv2): Sequential(
    (0): Conv2d_Q(
      128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (fq10): activation_quantize_fn()
  (conv11): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (fq11): activation_quantize_fn()
  (conv12): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (fq12): activation_quantize_fn()
  (conv13): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (fq13): activation_quantize_fn()
  (conv14): Conv2d_Q(
    256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr3): activation_quantize_fn()
  (resconv3): Sequential(
    (0): Conv2d_Q(
      256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (fq14): activation_quantize_fn()
  (conv15): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (fq15): activation_quantize_fn()
  (conv16): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (fq16): activation_quantize_fn()
  (conv17): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (fq17): activation_quantize_fn()
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fq18): activation_quantize_fn()
  (fc): Linear_Q(
    in_features=512, out_features=1000, bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn19): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
=> loading pretrained model '../pretrained_models/ideal/resnet18fp_imnet.pth.tar'
Pretrained model accuracy: 69.93189239501953
=> loaded pretrained model from ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
Test: [0/49]	Loss 10.5517 (10.5517)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
 * Prec@1 0.100 Prec@5 0.488
Pretrained model accuracy: 0.09999999403953552
Epoch: [0][0/1252]	Loss 8.1142 (8.1142)	Prec@1 0.195 (0.195)	Prec@5 1.172 (1.172)	LR: 0.1
Epoch: [0][1000/1252]	Loss 3.7843 (4.2324)	Prec@1 25.781 (21.142)	Prec@5 48.145 (40.760)	LR: 0.1
 * Prec@1 22.113 Prec@5 42.219
Best Train Accuracy: 22.11%

Test: [0/49]	Loss 3.8583 (3.8583)	Prec@1 26.855 (26.855)	Prec@5 50.000 (50.000)
 * Prec@1 18.664 Prec@5 37.432
Best accuracy: 18.66%

Epoch: [1][0/1252]	Loss 3.8731 (3.8731)	Prec@1 24.805 (24.805)	Prec@5 47.656 (47.656)	LR: 0.1
Epoch: [1][1000/1252]	Loss 4.0921 (3.9990)	Prec@1 21.680 (23.614)	Prec@5 43.457 (44.485)	LR: 0.1
 * Prec@1 23.633 Prec@5 44.524
Best Train Accuracy: 23.63%

Test: [0/49]	Loss 3.1652 (3.1652)	Prec@1 34.473 (34.473)	Prec@5 59.668 (59.668)
 * Prec@1 24.400 Prec@5 46.188
Best accuracy: 24.40%

Epoch: [2][0/1252]	Loss 3.9249 (3.9249)	Prec@1 25.195 (25.195)	Prec@5 45.605 (45.605)	LR: 0.1
Epoch: [2][1000/1252]	Loss 3.7277 (3.8953)	Prec@1 25.977 (24.889)	Prec@5 49.902 (46.291)	LR: 0.1
 * Prec@1 25.012 Prec@5 46.472
Best Train Accuracy: 25.01%

Test: [0/49]	Loss 2.9247 (2.9247)	Prec@1 40.332 (40.332)	Prec@5 64.160 (64.160)
 * Prec@1 26.052 Prec@5 48.036
Best accuracy: 26.05%

Epoch: [3][0/1252]	Loss 3.8571 (3.8571)	Prec@1 26.758 (26.758)	Prec@5 46.289 (46.289)	LR: 0.1
Epoch: [3][1000/1252]	Loss 3.8265 (3.7952)	Prec@1 26.270 (26.251)	Prec@5 45.703 (48.082)	LR: 0.1
 * Prec@1 26.310 Prec@5 48.141
Best Train Accuracy: 26.31%

Test: [0/49]	Loss 3.0718 (3.0718)	Prec@1 35.840 (35.840)	Prec@5 61.035 (61.035)
 * Prec@1 27.552 Prec@5 50.472
Best accuracy: 27.55%

Epoch: [4][0/1252]	Loss 3.7923 (3.7923)	Prec@1 28.125 (28.125)	Prec@5 48.438 (48.438)	LR: 0.1
Epoch: [4][1000/1252]	Loss 3.9058 (3.7390)	Prec@1 25.098 (27.065)	Prec@5 45.312 (49.092)	LR: 0.1
 * Prec@1 27.088 Prec@5 49.108
Best Train Accuracy: 27.09%

Test: [0/49]	Loss 3.3194 (3.3194)	Prec@1 34.570 (34.570)	Prec@5 58.594 (58.594)
 * Prec@1 26.110 Prec@5 47.898
Best accuracy: 27.55%

Epoch: [5][0/1252]	Loss 3.7770 (3.7770)	Prec@1 26.855 (26.855)	Prec@5 48.828 (48.828)	LR: 0.1
Epoch: [5][1000/1252]	Loss 3.6531 (3.7050)	Prec@1 27.148 (27.572)	Prec@5 48.730 (49.675)	LR: 0.1
 * Prec@1 27.554 Prec@5 49.656
Best Train Accuracy: 27.55%

Test: [0/49]	Loss 3.0199 (3.0199)	Prec@1 37.500 (37.500)	Prec@5 61.914 (61.914)
 * Prec@1 27.264 Prec@5 49.164
Best accuracy: 27.55%

Epoch: [6][0/1252]	Loss 3.6878 (3.6878)	Prec@1 27.832 (27.832)	Prec@5 49.707 (49.707)	LR: 0.1
Epoch: [6][1000/1252]	Loss 3.6237 (3.6823)	Prec@1 30.078 (27.869)	Prec@5 50.488 (50.026)	LR: 0.1
 * Prec@1 27.864 Prec@5 50.039
Best Train Accuracy: 27.86%

Test: [0/49]	Loss 2.7592 (2.7592)	Prec@1 42.188 (42.188)	Prec@5 66.406 (66.406)
 * Prec@1 27.620 Prec@5 49.906
Best accuracy: 27.62%

Epoch: [7][0/1252]	Loss 3.6345 (3.6345)	Prec@1 28.223 (28.223)	Prec@5 50.391 (50.391)	LR: 0.1
Epoch: [7][1000/1252]	Loss 3.6690 (3.6563)	Prec@1 27.832 (28.252)	Prec@5 50.000 (50.500)	LR: 0.1
 * Prec@1 28.212 Prec@5 50.462
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.8063 (2.8063)	Prec@1 40.527 (40.527)	Prec@5 65.332 (65.332)
 * Prec@1 28.896 Prec@5 52.190
Best accuracy: 28.90%

Epoch: [8][0/1252]	Loss 3.6392 (3.6392)	Prec@1 28.223 (28.223)	Prec@5 51.562 (51.562)	LR: 0.1
Epoch: [8][1000/1252]	Loss 3.7658 (3.6971)	Prec@1 25.488 (27.699)	Prec@5 46.973 (49.808)	LR: 0.1
 * Prec@1 27.427 Prec@5 49.458
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.0129 (3.0129)	Prec@1 39.453 (39.453)	Prec@5 63.281 (63.281)
 * Prec@1 26.880 Prec@5 48.918
Best accuracy: 28.90%

Epoch: [9][0/1252]	Loss 3.8896 (3.8896)	Prec@1 26.855 (26.855)	Prec@5 47.656 (47.656)	LR: 0.1
Epoch: [9][1000/1252]	Loss 3.7394 (3.7709)	Prec@1 27.051 (26.695)	Prec@5 48.242 (48.483)	LR: 0.1
 * Prec@1 26.680 Prec@5 48.485
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.3679 (3.3679)	Prec@1 34.668 (34.668)	Prec@5 58.008 (58.008)
 * Prec@1 24.206 Prec@5 45.778
Best accuracy: 28.90%

Epoch: [10][0/1252]	Loss 3.7641 (3.7641)	Prec@1 25.684 (25.684)	Prec@5 48.535 (48.535)	LR: 0.1
Epoch: [10][1000/1252]	Loss 3.7276 (3.7509)	Prec@1 27.246 (26.910)	Prec@5 50.000 (48.844)	LR: 0.1
 * Prec@1 27.017 Prec@5 48.925
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.1498 (3.1498)	Prec@1 36.523 (36.523)	Prec@5 59.473 (59.473)
 * Prec@1 28.268 Prec@5 50.568
Best accuracy: 28.90%

Epoch: [11][0/1252]	Loss 3.7872 (3.7872)	Prec@1 26.660 (26.660)	Prec@5 47.461 (47.461)	LR: 0.1
Epoch: [11][1000/1252]	Loss 3.7269 (3.6921)	Prec@1 27.734 (27.773)	Prec@5 48.633 (49.918)	LR: 0.1
 * Prec@1 27.768 Prec@5 49.874
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.7896 (2.7896)	Prec@1 38.574 (38.574)	Prec@5 66.504 (66.504)
 * Prec@1 28.064 Prec@5 50.888
Best accuracy: 28.90%

Epoch: [12][0/1252]	Loss 3.7091 (3.7091)	Prec@1 26.270 (26.270)	Prec@5 48.633 (48.633)	LR: 0.1
Epoch: [12][1000/1252]	Loss 3.7071 (3.6743)	Prec@1 26.660 (27.971)	Prec@5 50.586 (50.193)	LR: 0.1
 * Prec@1 27.946 Prec@5 50.147
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.5351 (3.5351)	Prec@1 32.812 (32.812)	Prec@5 55.957 (55.957)
 * Prec@1 27.120 Prec@5 48.834
Best accuracy: 28.90%

Epoch: [13][0/1252]	Loss 3.6197 (3.6197)	Prec@1 29.395 (29.395)	Prec@5 51.172 (51.172)	LR: 0.1
Epoch: [13][1000/1252]	Loss 3.6462 (3.6631)	Prec@1 26.855 (28.232)	Prec@5 50.293 (50.348)	LR: 0.1
 * Prec@1 28.177 Prec@5 50.329
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.5411 (2.5411)	Prec@1 42.773 (42.773)	Prec@5 70.996 (70.996)
 * Prec@1 29.034 Prec@5 51.958
Best accuracy: 29.03%

Epoch: [14][0/1252]	Loss 3.7556 (3.7556)	Prec@1 28.223 (28.223)	Prec@5 48.340 (48.340)	LR: 0.1
Epoch: [14][1000/1252]	Loss 3.6521 (3.6631)	Prec@1 26.172 (28.208)	Prec@5 49.512 (50.371)	LR: 0.1
 * Prec@1 28.170 Prec@5 50.335
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.0270 (3.0270)	Prec@1 40.430 (40.430)	Prec@5 63.965 (63.965)
 * Prec@1 28.188 Prec@5 50.952
Best accuracy: 29.03%

Epoch: [15][0/1252]	Loss 3.5860 (3.5860)	Prec@1 29.199 (29.199)	Prec@5 51.270 (51.270)	LR: 0.1
Epoch: [15][1000/1252]	Loss 3.6333 (3.6503)	Prec@1 26.660 (28.297)	Prec@5 51.953 (50.616)	LR: 0.1
 * Prec@1 28.301 Prec@5 50.583
Best Train Accuracy: 28.30%

Test: [0/49]	Loss 2.8485 (2.8485)	Prec@1 39.844 (39.844)	Prec@5 66.113 (66.113)
 * Prec@1 28.592 Prec@5 51.834
Best accuracy: 29.03%

Epoch: [16][0/1252]	Loss 3.6409 (3.6409)	Prec@1 28.125 (28.125)	Prec@5 50.977 (50.977)	LR: 0.1
Epoch: [16][1000/1252]	Loss 3.6537 (3.6395)	Prec@1 29.492 (28.523)	Prec@5 50.586 (50.749)	LR: 0.1
 * Prec@1 28.437 Prec@5 50.681
Best Train Accuracy: 28.44%

Test: [0/49]	Loss 2.6606 (2.6606)	Prec@1 42.578 (42.578)	Prec@5 68.164 (68.164)
 * Prec@1 29.522 Prec@5 52.872
Best accuracy: 29.52%

Epoch: [17][0/1252]	Loss 3.7290 (3.7290)	Prec@1 26.172 (26.172)	Prec@5 49.219 (49.219)	LR: 0.1
Epoch: [17][1000/1252]	Loss 3.6424 (3.6362)	Prec@1 27.734 (28.534)	Prec@5 50.488 (50.808)	LR: 0.1
 * Prec@1 28.492 Prec@5 50.745
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 2.7378 (2.7378)	Prec@1 40.430 (40.430)	Prec@5 65.137 (65.137)
 * Prec@1 29.014 Prec@5 52.496
Best accuracy: 29.52%

Epoch: [18][0/1252]	Loss 3.6836 (3.6836)	Prec@1 29.004 (29.004)	Prec@5 50.684 (50.684)	LR: 0.1
Epoch: [18][1000/1252]	Loss 3.7497 (3.6780)	Prec@1 25.488 (28.005)	Prec@5 49.121 (50.105)	LR: 0.1
 * Prec@1 27.857 Prec@5 49.927
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 3.0966 (3.0966)	Prec@1 39.355 (39.355)	Prec@5 61.133 (61.133)
 * Prec@1 27.294 Prec@5 49.606
Best accuracy: 29.52%

Epoch: [19][0/1252]	Loss 3.7744 (3.7744)	Prec@1 28.125 (28.125)	Prec@5 47.461 (47.461)	LR: 0.1
Epoch: [19][1000/1252]	Loss 3.7286 (3.7365)	Prec@1 25.879 (27.191)	Prec@5 49.316 (49.084)	LR: 0.1
 * Prec@1 27.287 Prec@5 49.201
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 3.1838 (3.1838)	Prec@1 35.840 (35.840)	Prec@5 60.840 (60.840)
 * Prec@1 26.962 Prec@5 49.302
Best accuracy: 29.52%

Epoch: [20][0/1252]	Loss 3.7045 (3.7045)	Prec@1 27.539 (27.539)	Prec@5 50.879 (50.879)	LR: 0.010000000000000002
Epoch: [20][1000/1252]	Loss 3.4892 (3.4490)	Prec@1 30.078 (31.374)	Prec@5 52.246 (54.108)	LR: 0.010000000000000002
 * Prec@1 31.595 Prec@5 54.311
Best Train Accuracy: 31.59%

Test: [0/49]	Loss 2.3413 (2.3413)	Prec@1 50.488 (50.488)	Prec@5 73.145 (73.145)
 * Prec@1 36.512 Prec@5 60.656
Best accuracy: 36.51%

Epoch: [21][0/1252]	Loss 3.4370 (3.4370)	Prec@1 32.227 (32.227)	Prec@5 52.832 (52.832)	LR: 0.010000000000000002
Epoch: [21][1000/1252]	Loss 3.2308 (3.3688)	Prec@1 35.352 (32.707)	Prec@5 57.324 (55.453)	LR: 0.010000000000000002
 * Prec@1 32.720 Prec@5 55.496
Best Train Accuracy: 32.72%

Test: [0/49]	Loss 2.2582 (2.2582)	Prec@1 51.367 (51.367)	Prec@5 73.828 (73.828)
 * Prec@1 36.894 Prec@5 60.780
Best accuracy: 36.89%

Epoch: [22][0/1252]	Loss 3.3194 (3.3194)	Prec@1 33.008 (33.008)	Prec@5 56.641 (56.641)	LR: 0.010000000000000002
Epoch: [22][1000/1252]	Loss 3.4003 (3.3466)	Prec@1 33.008 (32.985)	Prec@5 55.078 (55.782)	LR: 0.010000000000000002
 * Prec@1 32.940 Prec@5 55.709
Best Train Accuracy: 32.94%

Test: [0/49]	Loss 2.2969 (2.2969)	Prec@1 50.195 (50.195)	Prec@5 74.121 (74.121)
 * Prec@1 37.014 Prec@5 60.962
Best accuracy: 37.01%

Epoch: [23][0/1252]	Loss 3.3560 (3.3560)	Prec@1 34.863 (34.863)	Prec@5 55.664 (55.664)	LR: 0.010000000000000002
Epoch: [23][1000/1252]	Loss 3.2789 (3.3275)	Prec@1 34.570 (33.324)	Prec@5 57.324 (56.106)	LR: 0.010000000000000002
 * Prec@1 33.365 Prec@5 56.149
Best Train Accuracy: 33.37%

Test: [0/49]	Loss 2.2308 (2.2308)	Prec@1 52.246 (52.246)	Prec@5 74.023 (74.023)
 * Prec@1 37.508 Prec@5 61.622
Best accuracy: 37.51%

Epoch: [24][0/1252]	Loss 3.4831 (3.4831)	Prec@1 30.078 (30.078)	Prec@5 52.734 (52.734)	LR: 0.010000000000000002
Epoch: [24][1000/1252]	Loss 3.3609 (3.2959)	Prec@1 32.812 (33.813)	Prec@5 56.543 (56.734)	LR: 0.010000000000000002
 * Prec@1 33.812 Prec@5 56.718
Best Train Accuracy: 33.81%

Test: [0/49]	Loss 2.2603 (2.2603)	Prec@1 52.832 (52.832)	Prec@5 75.000 (75.000)
 * Prec@1 37.580 Prec@5 61.640
Best accuracy: 37.58%

Epoch: [25][0/1252]	Loss 3.3818 (3.3818)	Prec@1 32.227 (32.227)	Prec@5 55.957 (55.957)	LR: 0.010000000000000002
Epoch: [25][1000/1252]	Loss 3.2528 (3.2781)	Prec@1 33.984 (34.096)	Prec@5 56.152 (56.940)	LR: 0.010000000000000002
 * Prec@1 34.126 Prec@5 57.000
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3152 (2.3152)	Prec@1 50.195 (50.195)	Prec@5 72.949 (72.949)
 * Prec@1 37.774 Prec@5 62.066
Best accuracy: 37.77%

Epoch: [26][0/1252]	Loss 3.2443 (3.2443)	Prec@1 34.180 (34.180)	Prec@5 58.301 (58.301)	LR: 0.010000000000000002
Epoch: [26][1000/1252]	Loss 3.5192 (3.4407)	Prec@1 33.691 (31.782)	Prec@5 54.395 (54.157)	LR: 0.010000000000000002
 * Prec@1 31.552 Prec@5 53.852
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5582 (2.5582)	Prec@1 45.605 (45.605)	Prec@5 68.262 (68.262)
 * Prec@1 34.352 Prec@5 58.094
Best accuracy: 37.77%

Epoch: [27][0/1252]	Loss 3.5630 (3.5630)	Prec@1 29.395 (29.395)	Prec@5 52.637 (52.637)	LR: 0.010000000000000002
Epoch: [27][1000/1252]	Loss 3.3910 (3.4704)	Prec@1 31.543 (31.287)	Prec@5 55.957 (53.638)	LR: 0.010000000000000002
 * Prec@1 31.361 Prec@5 53.716
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5757 (2.5757)	Prec@1 46.875 (46.875)	Prec@5 69.531 (69.531)
 * Prec@1 35.458 Prec@5 58.882
Best accuracy: 37.77%

Epoch: [28][0/1252]	Loss 3.3894 (3.3894)	Prec@1 33.691 (33.691)	Prec@5 55.469 (55.469)	LR: 0.010000000000000002
Epoch: [28][1000/1252]	Loss 3.2948 (3.4242)	Prec@1 34.277 (31.926)	Prec@5 56.738 (54.425)	LR: 0.010000000000000002
 * Prec@1 31.955 Prec@5 54.452
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5266 (2.5266)	Prec@1 46.289 (46.289)	Prec@5 69.922 (69.922)
 * Prec@1 35.058 Prec@5 58.678
Best accuracy: 37.77%

Epoch: [29][0/1252]	Loss 3.4326 (3.4326)	Prec@1 31.543 (31.543)	Prec@5 52.930 (52.930)	LR: 0.010000000000000002
Epoch: [29][1000/1252]	Loss 3.5918 (3.3993)	Prec@1 32.227 (32.316)	Prec@5 50.977 (54.850)	LR: 0.010000000000000002
 * Prec@1 32.315 Prec@5 54.850
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.4460 (2.4460)	Prec@1 49.805 (49.805)	Prec@5 71.094 (71.094)
 * Prec@1 36.176 Prec@5 59.836
Best accuracy: 37.77%

Epoch: [30][0/1252]	Loss 3.4289 (3.4289)	Prec@1 34.082 (34.082)	Prec@5 54.785 (54.785)	LR: 0.010000000000000002
Epoch: [30][1000/1252]	Loss 3.3226 (3.3894)	Prec@1 33.105 (32.519)	Prec@5 55.859 (55.047)	LR: 0.010000000000000002
 * Prec@1 32.521 Prec@5 55.011
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3497 (2.3497)	Prec@1 49.121 (49.121)	Prec@5 71.973 (71.973)
 * Prec@1 35.940 Prec@5 59.886
Best accuracy: 37.77%

Epoch: [31][0/1252]	Loss 3.3892 (3.3892)	Prec@1 33.398 (33.398)	Prec@5 55.078 (55.078)	LR: 0.010000000000000002
Epoch: [31][1000/1252]	Loss 3.3124 (3.3728)	Prec@1 32.617 (32.708)	Prec@5 56.738 (55.319)	LR: 0.010000000000000002
 * Prec@1 32.732 Prec@5 55.320
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3447 (2.3447)	Prec@1 49.707 (49.707)	Prec@5 72.559 (72.559)
 * Prec@1 35.514 Prec@5 59.064
Best accuracy: 37.77%

Epoch: [32][0/1252]	Loss 3.3469 (3.3469)	Prec@1 34.277 (34.277)	Prec@5 56.348 (56.348)	LR: 0.010000000000000002
Epoch: [32][1000/1252]	Loss 3.3561 (3.3584)	Prec@1 30.273 (33.007)	Prec@5 56.445 (55.588)	LR: 0.010000000000000002
 * Prec@1 32.971 Prec@5 55.568
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.4496 (2.4496)	Prec@1 49.023 (49.023)	Prec@5 70.215 (70.215)
 * Prec@1 36.076 Prec@5 59.830
Best accuracy: 37.77%

Epoch: [33][0/1252]	Loss 3.3499 (3.3499)	Prec@1 33.105 (33.105)	Prec@5 54.492 (54.492)	LR: 0.010000000000000002
Epoch: [33][1000/1252]	Loss 3.2885 (3.3410)	Prec@1 35.254 (33.239)	Prec@5 55.762 (55.899)	LR: 0.010000000000000002
 * Prec@1 33.264 Prec@5 55.910
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3764 (2.3764)	Prec@1 48.926 (48.926)	Prec@5 72.363 (72.363)
 * Prec@1 36.028 Prec@5 59.358
Best accuracy: 37.77%

Epoch: [34][0/1252]	Loss 3.2980 (3.2980)	Prec@1 33.691 (33.691)	Prec@5 55.957 (55.957)	LR: 0.010000000000000002
Epoch: [34][1000/1252]	Loss 3.4333 (3.3253)	Prec@1 31.641 (33.506)	Prec@5 53.125 (56.143)	LR: 0.010000000000000002
 * Prec@1 33.510 Prec@5 56.115
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3517 (2.3517)	Prec@1 51.270 (51.270)	Prec@5 72.266 (72.266)
 * Prec@1 36.490 Prec@5 60.004
Best accuracy: 37.77%

Epoch: [35][0/1252]	Loss 3.3305 (3.3305)	Prec@1 32.617 (32.617)	Prec@5 56.348 (56.348)	LR: 0.010000000000000002
Epoch: [35][1000/1252]	Loss 3.3357 (3.3090)	Prec@1 33.301 (33.713)	Prec@5 55.664 (56.458)	LR: 0.010000000000000002
 * Prec@1 33.690 Prec@5 56.423
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.2447 (2.2447)	Prec@1 52.832 (52.832)	Prec@5 74.512 (74.512)
 * Prec@1 36.894 Prec@5 60.502
Best accuracy: 37.77%

