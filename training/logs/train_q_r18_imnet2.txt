
      ==> Arguments:
          dataset: imagenet
          model: resnet18
          workers: 8
          epochs: 150
          start_epoch: 0
          batch_size: 1024
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0001
          tag: qfp_i8b7f_w8b7f
          milestones: [20, 40, 60, 80, 120]
          gamma: 0.1
          input_size: None
          print_freq: 1000
          resume: 
          evaluate: False
          pretrained: ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
          half: False
          savedir: ../pretrained_models/ideal/
          save_every: 10
          gpus: 0,1,2,3
DEVICE: cuda
GPU Id(s) being used: 0,1,2,3
==> Building model for resnet18 ...
ResNet18(
  (fq0): activation_quantize_fn()
  (conv1): Conv2d_Q(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU(inplace=True)
  (fq1): activation_quantize_fn()
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (fq2): activation_quantize_fn()
  (conv3): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (fq3): activation_quantize_fn()
  (conv4): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (fq4): activation_quantize_fn()
  (conv5): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (fq5): activation_quantize_fn()
  (conv6): Conv2d_Q(
    64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr1): activation_quantize_fn()
  (resconv1): Sequential(
    (0): Conv2d_Q(
      64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (fq6): activation_quantize_fn()
  (conv7): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (fq7): activation_quantize_fn()
  (conv8): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (fq8): activation_quantize_fn()
  (conv9): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (fq9): activation_quantize_fn()
  (conv10): Conv2d_Q(
    128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr2): activation_quantize_fn()
  (resconv2): Sequential(
    (0): Conv2d_Q(
      128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (fq10): activation_quantize_fn()
  (conv11): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (fq11): activation_quantize_fn()
  (conv12): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (fq12): activation_quantize_fn()
  (conv13): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (fq13): activation_quantize_fn()
  (conv14): Conv2d_Q(
    256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr3): activation_quantize_fn()
  (resconv3): Sequential(
    (0): Conv2d_Q(
      256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (fq14): activation_quantize_fn()
  (conv15): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (fq15): activation_quantize_fn()
  (conv16): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (fq16): activation_quantize_fn()
  (conv17): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (fq17): activation_quantize_fn()
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fq18): activation_quantize_fn()
  (fc): Linear_Q(
    in_features=512, out_features=1000, bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn19): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
=> loading pretrained model '../pretrained_models/ideal/resnet18fp_imnet.pth.tar'
Pretrained model accuracy: 69.93189239501953
=> loaded pretrained model from ../pretrained_models/ideal/resnet18fp_imnet.pth.tar
Test: [0/49]	Loss 10.5517 (10.5517)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
 * Prec@1 0.100 Prec@5 0.488
Pretrained model accuracy: 0.09999999403953552
Epoch: [0][0/1252]	Loss 8.1142 (8.1142)	Prec@1 0.195 (0.195)	Prec@5 1.172 (1.172)	LR: 0.1
Epoch: [0][1000/1252]	Loss 3.7843 (4.2324)	Prec@1 25.781 (21.142)	Prec@5 48.145 (40.760)	LR: 0.1
 * Prec@1 22.113 Prec@5 42.219
Best Train Accuracy: 22.11%

Test: [0/49]	Loss 3.8583 (3.8583)	Prec@1 26.855 (26.855)	Prec@5 50.000 (50.000)
 * Prec@1 18.664 Prec@5 37.432
Best accuracy: 18.66%

Epoch: [1][0/1252]	Loss 3.8731 (3.8731)	Prec@1 24.805 (24.805)	Prec@5 47.656 (47.656)	LR: 0.1
Epoch: [1][1000/1252]	Loss 4.0921 (3.9990)	Prec@1 21.680 (23.614)	Prec@5 43.457 (44.485)	LR: 0.1
 * Prec@1 23.633 Prec@5 44.524
Best Train Accuracy: 23.63%

Test: [0/49]	Loss 3.1652 (3.1652)	Prec@1 34.473 (34.473)	Prec@5 59.668 (59.668)
 * Prec@1 24.400 Prec@5 46.188
Best accuracy: 24.40%

Epoch: [2][0/1252]	Loss 3.9249 (3.9249)	Prec@1 25.195 (25.195)	Prec@5 45.605 (45.605)	LR: 0.1
Epoch: [2][1000/1252]	Loss 3.7277 (3.8953)	Prec@1 25.977 (24.889)	Prec@5 49.902 (46.291)	LR: 0.1
 * Prec@1 25.012 Prec@5 46.472
Best Train Accuracy: 25.01%

Test: [0/49]	Loss 2.9247 (2.9247)	Prec@1 40.332 (40.332)	Prec@5 64.160 (64.160)
 * Prec@1 26.052 Prec@5 48.036
Best accuracy: 26.05%

Epoch: [3][0/1252]	Loss 3.8571 (3.8571)	Prec@1 26.758 (26.758)	Prec@5 46.289 (46.289)	LR: 0.1
Epoch: [3][1000/1252]	Loss 3.8265 (3.7952)	Prec@1 26.270 (26.251)	Prec@5 45.703 (48.082)	LR: 0.1
 * Prec@1 26.310 Prec@5 48.141
Best Train Accuracy: 26.31%

Test: [0/49]	Loss 3.0718 (3.0718)	Prec@1 35.840 (35.840)	Prec@5 61.035 (61.035)
 * Prec@1 27.552 Prec@5 50.472
Best accuracy: 27.55%

Epoch: [4][0/1252]	Loss 3.7923 (3.7923)	Prec@1 28.125 (28.125)	Prec@5 48.438 (48.438)	LR: 0.1
Epoch: [4][1000/1252]	Loss 3.9058 (3.7390)	Prec@1 25.098 (27.065)	Prec@5 45.312 (49.092)	LR: 0.1
 * Prec@1 27.088 Prec@5 49.108
Best Train Accuracy: 27.09%

Test: [0/49]	Loss 3.3194 (3.3194)	Prec@1 34.570 (34.570)	Prec@5 58.594 (58.594)
 * Prec@1 26.110 Prec@5 47.898
Best accuracy: 27.55%

Epoch: [5][0/1252]	Loss 3.7770 (3.7770)	Prec@1 26.855 (26.855)	Prec@5 48.828 (48.828)	LR: 0.1
Epoch: [5][1000/1252]	Loss 3.6531 (3.7050)	Prec@1 27.148 (27.572)	Prec@5 48.730 (49.675)	LR: 0.1
 * Prec@1 27.554 Prec@5 49.656
Best Train Accuracy: 27.55%

Test: [0/49]	Loss 3.0199 (3.0199)	Prec@1 37.500 (37.500)	Prec@5 61.914 (61.914)
 * Prec@1 27.264 Prec@5 49.164
Best accuracy: 27.55%

Epoch: [6][0/1252]	Loss 3.6878 (3.6878)	Prec@1 27.832 (27.832)	Prec@5 49.707 (49.707)	LR: 0.1
Epoch: [6][1000/1252]	Loss 3.6237 (3.6823)	Prec@1 30.078 (27.869)	Prec@5 50.488 (50.026)	LR: 0.1
 * Prec@1 27.864 Prec@5 50.039
Best Train Accuracy: 27.86%

Test: [0/49]	Loss 2.7592 (2.7592)	Prec@1 42.188 (42.188)	Prec@5 66.406 (66.406)
 * Prec@1 27.620 Prec@5 49.906
Best accuracy: 27.62%

Epoch: [7][0/1252]	Loss 3.6345 (3.6345)	Prec@1 28.223 (28.223)	Prec@5 50.391 (50.391)	LR: 0.1
Epoch: [7][1000/1252]	Loss 3.6690 (3.6563)	Prec@1 27.832 (28.252)	Prec@5 50.000 (50.500)	LR: 0.1
 * Prec@1 28.212 Prec@5 50.462
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.8063 (2.8063)	Prec@1 40.527 (40.527)	Prec@5 65.332 (65.332)
 * Prec@1 28.896 Prec@5 52.190
Best accuracy: 28.90%

Epoch: [8][0/1252]	Loss 3.6392 (3.6392)	Prec@1 28.223 (28.223)	Prec@5 51.562 (51.562)	LR: 0.1
Epoch: [8][1000/1252]	Loss 3.7658 (3.6971)	Prec@1 25.488 (27.699)	Prec@5 46.973 (49.808)	LR: 0.1
 * Prec@1 27.427 Prec@5 49.458
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.0129 (3.0129)	Prec@1 39.453 (39.453)	Prec@5 63.281 (63.281)
 * Prec@1 26.880 Prec@5 48.918
Best accuracy: 28.90%

Epoch: [9][0/1252]	Loss 3.8896 (3.8896)	Prec@1 26.855 (26.855)	Prec@5 47.656 (47.656)	LR: 0.1
Epoch: [9][1000/1252]	Loss 3.7394 (3.7709)	Prec@1 27.051 (26.695)	Prec@5 48.242 (48.483)	LR: 0.1
 * Prec@1 26.680 Prec@5 48.485
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.3679 (3.3679)	Prec@1 34.668 (34.668)	Prec@5 58.008 (58.008)
 * Prec@1 24.206 Prec@5 45.778
Best accuracy: 28.90%

Epoch: [10][0/1252]	Loss 3.7641 (3.7641)	Prec@1 25.684 (25.684)	Prec@5 48.535 (48.535)	LR: 0.1
Epoch: [10][1000/1252]	Loss 3.7276 (3.7509)	Prec@1 27.246 (26.910)	Prec@5 50.000 (48.844)	LR: 0.1
 * Prec@1 27.017 Prec@5 48.925
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.1498 (3.1498)	Prec@1 36.523 (36.523)	Prec@5 59.473 (59.473)
 * Prec@1 28.268 Prec@5 50.568
Best accuracy: 28.90%

Epoch: [11][0/1252]	Loss 3.7872 (3.7872)	Prec@1 26.660 (26.660)	Prec@5 47.461 (47.461)	LR: 0.1
Epoch: [11][1000/1252]	Loss 3.7269 (3.6921)	Prec@1 27.734 (27.773)	Prec@5 48.633 (49.918)	LR: 0.1
 * Prec@1 27.768 Prec@5 49.874
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.7896 (2.7896)	Prec@1 38.574 (38.574)	Prec@5 66.504 (66.504)
 * Prec@1 28.064 Prec@5 50.888
Best accuracy: 28.90%

Epoch: [12][0/1252]	Loss 3.7091 (3.7091)	Prec@1 26.270 (26.270)	Prec@5 48.633 (48.633)	LR: 0.1
Epoch: [12][1000/1252]	Loss 3.7071 (3.6743)	Prec@1 26.660 (27.971)	Prec@5 50.586 (50.193)	LR: 0.1
 * Prec@1 27.946 Prec@5 50.147
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.5351 (3.5351)	Prec@1 32.812 (32.812)	Prec@5 55.957 (55.957)
 * Prec@1 27.120 Prec@5 48.834
Best accuracy: 28.90%

Epoch: [13][0/1252]	Loss 3.6197 (3.6197)	Prec@1 29.395 (29.395)	Prec@5 51.172 (51.172)	LR: 0.1
Epoch: [13][1000/1252]	Loss 3.6462 (3.6631)	Prec@1 26.855 (28.232)	Prec@5 50.293 (50.348)	LR: 0.1
 * Prec@1 28.177 Prec@5 50.329
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 2.5411 (2.5411)	Prec@1 42.773 (42.773)	Prec@5 70.996 (70.996)
 * Prec@1 29.034 Prec@5 51.958
Best accuracy: 29.03%

Epoch: [14][0/1252]	Loss 3.7556 (3.7556)	Prec@1 28.223 (28.223)	Prec@5 48.340 (48.340)	LR: 0.1
Epoch: [14][1000/1252]	Loss 3.6521 (3.6631)	Prec@1 26.172 (28.208)	Prec@5 49.512 (50.371)	LR: 0.1
 * Prec@1 28.170 Prec@5 50.335
Best Train Accuracy: 28.21%

Test: [0/49]	Loss 3.0270 (3.0270)	Prec@1 40.430 (40.430)	Prec@5 63.965 (63.965)
 * Prec@1 28.188 Prec@5 50.952
Best accuracy: 29.03%

Epoch: [15][0/1252]	Loss 3.5860 (3.5860)	Prec@1 29.199 (29.199)	Prec@5 51.270 (51.270)	LR: 0.1
Epoch: [15][1000/1252]	Loss 3.6333 (3.6503)	Prec@1 26.660 (28.297)	Prec@5 51.953 (50.616)	LR: 0.1
 * Prec@1 28.301 Prec@5 50.583
Best Train Accuracy: 28.30%

Test: [0/49]	Loss 2.8485 (2.8485)	Prec@1 39.844 (39.844)	Prec@5 66.113 (66.113)
 * Prec@1 28.592 Prec@5 51.834
Best accuracy: 29.03%

Epoch: [16][0/1252]	Loss 3.6409 (3.6409)	Prec@1 28.125 (28.125)	Prec@5 50.977 (50.977)	LR: 0.1
Epoch: [16][1000/1252]	Loss 3.6537 (3.6395)	Prec@1 29.492 (28.523)	Prec@5 50.586 (50.749)	LR: 0.1
 * Prec@1 28.437 Prec@5 50.681
Best Train Accuracy: 28.44%

Test: [0/49]	Loss 2.6606 (2.6606)	Prec@1 42.578 (42.578)	Prec@5 68.164 (68.164)
 * Prec@1 29.522 Prec@5 52.872
Best accuracy: 29.52%

Epoch: [17][0/1252]	Loss 3.7290 (3.7290)	Prec@1 26.172 (26.172)	Prec@5 49.219 (49.219)	LR: 0.1
Epoch: [17][1000/1252]	Loss 3.6424 (3.6362)	Prec@1 27.734 (28.534)	Prec@5 50.488 (50.808)	LR: 0.1
 * Prec@1 28.492 Prec@5 50.745
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 2.7378 (2.7378)	Prec@1 40.430 (40.430)	Prec@5 65.137 (65.137)
 * Prec@1 29.014 Prec@5 52.496
Best accuracy: 29.52%

Epoch: [18][0/1252]	Loss 3.6836 (3.6836)	Prec@1 29.004 (29.004)	Prec@5 50.684 (50.684)	LR: 0.1
Epoch: [18][1000/1252]	Loss 3.7497 (3.6780)	Prec@1 25.488 (28.005)	Prec@5 49.121 (50.105)	LR: 0.1
 * Prec@1 27.857 Prec@5 49.927
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 3.0966 (3.0966)	Prec@1 39.355 (39.355)	Prec@5 61.133 (61.133)
 * Prec@1 27.294 Prec@5 49.606
Best accuracy: 29.52%

Epoch: [19][0/1252]	Loss 3.7744 (3.7744)	Prec@1 28.125 (28.125)	Prec@5 47.461 (47.461)	LR: 0.1
Epoch: [19][1000/1252]	Loss 3.7286 (3.7365)	Prec@1 25.879 (27.191)	Prec@5 49.316 (49.084)	LR: 0.1
 * Prec@1 27.287 Prec@5 49.201
Best Train Accuracy: 28.49%

Test: [0/49]	Loss 3.1838 (3.1838)	Prec@1 35.840 (35.840)	Prec@5 60.840 (60.840)
 * Prec@1 26.962 Prec@5 49.302
Best accuracy: 29.52%

Epoch: [20][0/1252]	Loss 3.7045 (3.7045)	Prec@1 27.539 (27.539)	Prec@5 50.879 (50.879)	LR: 0.010000000000000002
Epoch: [20][1000/1252]	Loss 3.4892 (3.4490)	Prec@1 30.078 (31.374)	Prec@5 52.246 (54.108)	LR: 0.010000000000000002
 * Prec@1 31.595 Prec@5 54.311
Best Train Accuracy: 31.59%

Test: [0/49]	Loss 2.3413 (2.3413)	Prec@1 50.488 (50.488)	Prec@5 73.145 (73.145)
 * Prec@1 36.512 Prec@5 60.656
Best accuracy: 36.51%

Epoch: [21][0/1252]	Loss 3.4370 (3.4370)	Prec@1 32.227 (32.227)	Prec@5 52.832 (52.832)	LR: 0.010000000000000002
Epoch: [21][1000/1252]	Loss 3.2308 (3.3688)	Prec@1 35.352 (32.707)	Prec@5 57.324 (55.453)	LR: 0.010000000000000002
 * Prec@1 32.720 Prec@5 55.496
Best Train Accuracy: 32.72%

Test: [0/49]	Loss 2.2582 (2.2582)	Prec@1 51.367 (51.367)	Prec@5 73.828 (73.828)
 * Prec@1 36.894 Prec@5 60.780
Best accuracy: 36.89%

Epoch: [22][0/1252]	Loss 3.3194 (3.3194)	Prec@1 33.008 (33.008)	Prec@5 56.641 (56.641)	LR: 0.010000000000000002
Epoch: [22][1000/1252]	Loss 3.4003 (3.3466)	Prec@1 33.008 (32.985)	Prec@5 55.078 (55.782)	LR: 0.010000000000000002
 * Prec@1 32.940 Prec@5 55.709
Best Train Accuracy: 32.94%

Test: [0/49]	Loss 2.2969 (2.2969)	Prec@1 50.195 (50.195)	Prec@5 74.121 (74.121)
 * Prec@1 37.014 Prec@5 60.962
Best accuracy: 37.01%

Epoch: [23][0/1252]	Loss 3.3560 (3.3560)	Prec@1 34.863 (34.863)	Prec@5 55.664 (55.664)	LR: 0.010000000000000002
Epoch: [23][1000/1252]	Loss 3.2789 (3.3275)	Prec@1 34.570 (33.324)	Prec@5 57.324 (56.106)	LR: 0.010000000000000002
 * Prec@1 33.365 Prec@5 56.149
Best Train Accuracy: 33.37%

Test: [0/49]	Loss 2.2308 (2.2308)	Prec@1 52.246 (52.246)	Prec@5 74.023 (74.023)
 * Prec@1 37.508 Prec@5 61.622
Best accuracy: 37.51%

Epoch: [24][0/1252]	Loss 3.4831 (3.4831)	Prec@1 30.078 (30.078)	Prec@5 52.734 (52.734)	LR: 0.010000000000000002
Epoch: [24][1000/1252]	Loss 3.3609 (3.2959)	Prec@1 32.812 (33.813)	Prec@5 56.543 (56.734)	LR: 0.010000000000000002
 * Prec@1 33.812 Prec@5 56.718
Best Train Accuracy: 33.81%

Test: [0/49]	Loss 2.2603 (2.2603)	Prec@1 52.832 (52.832)	Prec@5 75.000 (75.000)
 * Prec@1 37.580 Prec@5 61.640
Best accuracy: 37.58%

Epoch: [25][0/1252]	Loss 3.3818 (3.3818)	Prec@1 32.227 (32.227)	Prec@5 55.957 (55.957)	LR: 0.010000000000000002
Epoch: [25][1000/1252]	Loss 3.2528 (3.2781)	Prec@1 33.984 (34.096)	Prec@5 56.152 (56.940)	LR: 0.010000000000000002
 * Prec@1 34.126 Prec@5 57.000
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3152 (2.3152)	Prec@1 50.195 (50.195)	Prec@5 72.949 (72.949)
 * Prec@1 37.774 Prec@5 62.066
Best accuracy: 37.77%

Epoch: [26][0/1252]	Loss 3.2443 (3.2443)	Prec@1 34.180 (34.180)	Prec@5 58.301 (58.301)	LR: 0.010000000000000002
Epoch: [26][1000/1252]	Loss 3.5192 (3.4407)	Prec@1 33.691 (31.782)	Prec@5 54.395 (54.157)	LR: 0.010000000000000002
 * Prec@1 31.552 Prec@5 53.852
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5582 (2.5582)	Prec@1 45.605 (45.605)	Prec@5 68.262 (68.262)
 * Prec@1 34.352 Prec@5 58.094
Best accuracy: 37.77%

Epoch: [27][0/1252]	Loss 3.5630 (3.5630)	Prec@1 29.395 (29.395)	Prec@5 52.637 (52.637)	LR: 0.010000000000000002
Epoch: [27][1000/1252]	Loss 3.3910 (3.4704)	Prec@1 31.543 (31.287)	Prec@5 55.957 (53.638)	LR: 0.010000000000000002
 * Prec@1 31.361 Prec@5 53.716
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5757 (2.5757)	Prec@1 46.875 (46.875)	Prec@5 69.531 (69.531)
 * Prec@1 35.458 Prec@5 58.882
Best accuracy: 37.77%

Epoch: [28][0/1252]	Loss 3.3894 (3.3894)	Prec@1 33.691 (33.691)	Prec@5 55.469 (55.469)	LR: 0.010000000000000002
Epoch: [28][1000/1252]	Loss 3.2948 (3.4242)	Prec@1 34.277 (31.926)	Prec@5 56.738 (54.425)	LR: 0.010000000000000002
 * Prec@1 31.955 Prec@5 54.452
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.5266 (2.5266)	Prec@1 46.289 (46.289)	Prec@5 69.922 (69.922)
 * Prec@1 35.058 Prec@5 58.678
Best accuracy: 37.77%

Epoch: [29][0/1252]	Loss 3.4326 (3.4326)	Prec@1 31.543 (31.543)	Prec@5 52.930 (52.930)	LR: 0.010000000000000002
Epoch: [29][1000/1252]	Loss 3.5918 (3.3993)	Prec@1 32.227 (32.316)	Prec@5 50.977 (54.850)	LR: 0.010000000000000002
 * Prec@1 32.315 Prec@5 54.850
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.4460 (2.4460)	Prec@1 49.805 (49.805)	Prec@5 71.094 (71.094)
 * Prec@1 36.176 Prec@5 59.836
Best accuracy: 37.77%

Epoch: [30][0/1252]	Loss 3.4289 (3.4289)	Prec@1 34.082 (34.082)	Prec@5 54.785 (54.785)	LR: 0.010000000000000002
Epoch: [30][1000/1252]	Loss 3.3226 (3.3894)	Prec@1 33.105 (32.519)	Prec@5 55.859 (55.047)	LR: 0.010000000000000002
 * Prec@1 32.521 Prec@5 55.011
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3497 (2.3497)	Prec@1 49.121 (49.121)	Prec@5 71.973 (71.973)
 * Prec@1 35.940 Prec@5 59.886
Best accuracy: 37.77%

Epoch: [31][0/1252]	Loss 3.3892 (3.3892)	Prec@1 33.398 (33.398)	Prec@5 55.078 (55.078)	LR: 0.010000000000000002
Epoch: [31][1000/1252]	Loss 3.3124 (3.3728)	Prec@1 32.617 (32.708)	Prec@5 56.738 (55.319)	LR: 0.010000000000000002
 * Prec@1 32.732 Prec@5 55.320
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3447 (2.3447)	Prec@1 49.707 (49.707)	Prec@5 72.559 (72.559)
 * Prec@1 35.514 Prec@5 59.064
Best accuracy: 37.77%

Epoch: [32][0/1252]	Loss 3.3469 (3.3469)	Prec@1 34.277 (34.277)	Prec@5 56.348 (56.348)	LR: 0.010000000000000002
Epoch: [32][1000/1252]	Loss 3.3561 (3.3584)	Prec@1 30.273 (33.007)	Prec@5 56.445 (55.588)	LR: 0.010000000000000002
 * Prec@1 32.971 Prec@5 55.568
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.4496 (2.4496)	Prec@1 49.023 (49.023)	Prec@5 70.215 (70.215)
 * Prec@1 36.076 Prec@5 59.830
Best accuracy: 37.77%

Epoch: [33][0/1252]	Loss 3.3499 (3.3499)	Prec@1 33.105 (33.105)	Prec@5 54.492 (54.492)	LR: 0.010000000000000002
Epoch: [33][1000/1252]	Loss 3.2885 (3.3410)	Prec@1 35.254 (33.239)	Prec@5 55.762 (55.899)	LR: 0.010000000000000002
 * Prec@1 33.264 Prec@5 55.910
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3764 (2.3764)	Prec@1 48.926 (48.926)	Prec@5 72.363 (72.363)
 * Prec@1 36.028 Prec@5 59.358
Best accuracy: 37.77%

Epoch: [34][0/1252]	Loss 3.2980 (3.2980)	Prec@1 33.691 (33.691)	Prec@5 55.957 (55.957)	LR: 0.010000000000000002
Epoch: [34][1000/1252]	Loss 3.4333 (3.3253)	Prec@1 31.641 (33.506)	Prec@5 53.125 (56.143)	LR: 0.010000000000000002
 * Prec@1 33.510 Prec@5 56.115
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3517 (2.3517)	Prec@1 51.270 (51.270)	Prec@5 72.266 (72.266)
 * Prec@1 36.490 Prec@5 60.004
Best accuracy: 37.77%

Epoch: [35][0/1252]	Loss 3.3305 (3.3305)	Prec@1 32.617 (32.617)	Prec@5 56.348 (56.348)	LR: 0.010000000000000002
Epoch: [35][1000/1252]	Loss 3.3357 (3.3090)	Prec@1 33.301 (33.713)	Prec@5 55.664 (56.458)	LR: 0.010000000000000002
 * Prec@1 33.690 Prec@5 56.423
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.2447 (2.2447)	Prec@1 52.832 (52.832)	Prec@5 74.512 (74.512)
 * Prec@1 36.894 Prec@5 60.502
Best accuracy: 37.77%

Epoch: [36][0/1252]	Loss 3.2500 (3.2500)	Prec@1 36.426 (36.426)	Prec@5 58.789 (58.789)	LR: 0.010000000000000002
Epoch: [36][1000/1252]	Loss 3.3802 (3.2979)	Prec@1 33.691 (33.866)	Prec@5 55.957 (56.621)	LR: 0.010000000000000002
 * Prec@1 33.821 Prec@5 56.580
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3073 (2.3073)	Prec@1 51.074 (51.074)	Prec@5 72.754 (72.754)
 * Prec@1 37.218 Prec@5 60.812
Best accuracy: 37.77%

Epoch: [37][0/1252]	Loss 3.2815 (3.2815)	Prec@1 33.789 (33.789)	Prec@5 57.324 (57.324)	LR: 0.010000000000000002
Epoch: [37][1000/1252]	Loss 3.3733 (3.2918)	Prec@1 33.008 (33.981)	Prec@5 54.980 (56.723)	LR: 0.010000000000000002
 * Prec@1 33.992 Prec@5 56.717
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.2831 (2.2831)	Prec@1 49.707 (49.707)	Prec@5 73.145 (73.145)
 * Prec@1 37.086 Prec@5 61.010
Best accuracy: 37.77%

Epoch: [38][0/1252]	Loss 3.3391 (3.3391)	Prec@1 34.570 (34.570)	Prec@5 55.566 (55.566)	LR: 0.010000000000000002
Epoch: [38][1000/1252]	Loss 3.3355 (3.2823)	Prec@1 33.496 (34.140)	Prec@5 55.469 (56.910)	LR: 0.010000000000000002
 * Prec@1 34.101 Prec@5 56.868
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.2374 (2.2374)	Prec@1 50.977 (50.977)	Prec@5 73.535 (73.535)
 * Prec@1 37.026 Prec@5 60.562
Best accuracy: 37.77%

Epoch: [39][0/1252]	Loss 3.2452 (3.2452)	Prec@1 36.035 (36.035)	Prec@5 58.008 (58.008)	LR: 0.010000000000000002
Epoch: [39][1000/1252]	Loss 3.2198 (3.2985)	Prec@1 35.840 (33.896)	Prec@5 57.422 (56.637)	LR: 0.010000000000000002
 * Prec@1 33.872 Prec@5 56.610
Best Train Accuracy: 34.13%

Test: [0/49]	Loss 2.3750 (2.3750)	Prec@1 50.000 (50.000)	Prec@5 71.680 (71.680)
 * Prec@1 36.624 Prec@5 60.292
Best accuracy: 37.77%

Epoch: [40][0/1252]	Loss 3.3566 (3.3566)	Prec@1 33.691 (33.691)	Prec@5 55.664 (55.664)	LR: 0.0010000000000000002
Epoch: [40][1000/1252]	Loss 3.3823 (3.2332)	Prec@1 32.520 (34.880)	Prec@5 55.957 (57.721)	LR: 0.0010000000000000002
 * Prec@1 34.941 Prec@5 57.759
Best Train Accuracy: 34.94%

Test: [0/49]	Loss 2.2249 (2.2249)	Prec@1 52.441 (52.441)	Prec@5 74.512 (74.512)
 * Prec@1 38.578 Prec@5 62.362
Best accuracy: 38.58%

Epoch: [41][0/1252]	Loss 3.1755 (3.1755)	Prec@1 37.207 (37.207)	Prec@5 58.203 (58.203)	LR: 0.0010000000000000002
Epoch: [41][1000/1252]	Loss 3.2054 (3.2119)	Prec@1 35.156 (35.241)	Prec@5 58.008 (58.093)	LR: 0.0010000000000000002
 * Prec@1 35.215 Prec@5 58.090
Best Train Accuracy: 35.22%

Test: [0/49]	Loss 2.1817 (2.1817)	Prec@1 54.004 (54.004)	Prec@5 75.098 (75.098)
 * Prec@1 38.798 Prec@5 62.752
Best accuracy: 38.80%

Epoch: [42][0/1252]	Loss 3.2659 (3.2659)	Prec@1 32.520 (32.520)	Prec@5 55.664 (55.664)	LR: 0.0010000000000000002
Epoch: [42][1000/1252]	Loss 3.1410 (3.2000)	Prec@1 36.035 (35.463)	Prec@5 59.473 (58.345)	LR: 0.0010000000000000002
 * Prec@1 35.430 Prec@5 58.330
Best Train Accuracy: 35.43%

Test: [0/49]	Loss 2.2179 (2.2179)	Prec@1 52.637 (52.637)	Prec@5 75.586 (75.586)
 * Prec@1 38.766 Prec@5 62.584
Best accuracy: 38.80%

Epoch: [43][0/1252]	Loss 3.1421 (3.1421)	Prec@1 36.426 (36.426)	Prec@5 59.961 (59.961)	LR: 0.0010000000000000002
Epoch: [43][1000/1252]	Loss 3.2237 (3.1979)	Prec@1 34.961 (35.532)	Prec@5 57.422 (58.348)	LR: 0.0010000000000000002
 * Prec@1 35.516 Prec@5 58.358
Best Train Accuracy: 35.52%

Test: [0/49]	Loss 2.1730 (2.1730)	Prec@1 53.906 (53.906)	Prec@5 75.000 (75.000)
 * Prec@1 38.942 Prec@5 62.746
Best accuracy: 38.94%

Epoch: [44][0/1252]	Loss 3.3379 (3.3379)	Prec@1 33.301 (33.301)	Prec@5 57.422 (57.422)	LR: 0.0010000000000000002
Epoch: [44][1000/1252]	Loss 3.0795 (3.1888)	Prec@1 36.523 (35.604)	Prec@5 60.645 (58.477)	LR: 0.0010000000000000002
 * Prec@1 35.609 Prec@5 58.461
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 2.2108 (2.2108)	Prec@1 52.734 (52.734)	Prec@5 74.902 (74.902)
 * Prec@1 38.998 Prec@5 62.802
Best accuracy: 39.00%

Epoch: [45][0/1252]	Loss 3.2330 (3.2330)	Prec@1 33.984 (33.984)	Prec@5 57.812 (57.812)	LR: 0.0010000000000000002
Epoch: [45][1000/1252]	Loss 6.1290 (4.1528)	Prec@1 3.809 (26.476)	Prec@5 11.328 (44.405)	LR: 0.0010000000000000002
 * Prec@1 22.249 Prec@5 38.429
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 5.2907 (5.2907)	Prec@1 10.742 (10.742)	Prec@5 25.098 (25.098)
 * Prec@1 8.160 Prec@5 20.436
Best accuracy: 39.00%

Epoch: [46][0/1252]	Loss 5.6490 (5.6490)	Prec@1 7.031 (7.031)	Prec@5 17.383 (17.383)	LR: 0.0010000000000000002
Epoch: [46][1000/1252]	Loss 5.1045 (5.3174)	Prec@1 10.742 (9.281)	Prec@5 23.242 (22.014)	LR: 0.0010000000000000002
 * Prec@1 9.782 Prec@5 22.880
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 4.3065 (4.3065)	Prec@1 20.703 (20.703)	Prec@5 40.820 (40.820)
 * Prec@1 14.496 Prec@5 31.324
Best accuracy: 39.00%

Epoch: [47][0/1252]	Loss 5.0260 (5.0260)	Prec@1 11.230 (11.230)	Prec@5 27.148 (27.148)	LR: 0.0010000000000000002
Epoch: [47][1000/1252]	Loss 4.7582 (4.8991)	Prec@1 13.672 (13.329)	Prec@5 30.566 (28.888)	LR: 0.0010000000000000002
 * Prec@1 13.552 Prec@5 29.273
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 4.0726 (4.0726)	Prec@1 23.828 (23.828)	Prec@5 44.629 (44.629)
 * Prec@1 17.666 Prec@5 35.764
Best accuracy: 39.00%

Epoch: [48][0/1252]	Loss 4.7844 (4.7844)	Prec@1 12.988 (12.988)	Prec@5 29.883 (29.883)	LR: 0.0010000000000000002
Epoch: [48][1000/1252]	Loss 4.5858 (4.6900)	Prec@1 16.504 (15.590)	Prec@5 32.812 (32.476)	LR: 0.0010000000000000002
 * Prec@1 15.799 Prec@5 32.782
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.7591 (3.7591)	Prec@1 28.125 (28.125)	Prec@5 50.098 (50.098)
 * Prec@1 19.734 Prec@5 38.972
Best accuracy: 39.00%

Epoch: [49][0/1252]	Loss 4.5692 (4.5692)	Prec@1 17.285 (17.285)	Prec@5 34.863 (34.863)	LR: 0.0010000000000000002
Epoch: [49][1000/1252]	Loss 4.5730 (4.5338)	Prec@1 16.895 (17.373)	Prec@5 32.910 (35.197)	LR: 0.0010000000000000002
 * Prec@1 17.495 Prec@5 35.370
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.7314 (3.7314)	Prec@1 28.516 (28.516)	Prec@5 49.414 (49.414)
 * Prec@1 21.020 Prec@5 40.790
Best accuracy: 39.00%

Epoch: [50][0/1252]	Loss 4.4848 (4.4848)	Prec@1 18.457 (18.457)	Prec@5 38.086 (38.086)	LR: 0.0010000000000000002
Epoch: [50][1000/1252]	Loss 4.3242 (4.4305)	Prec@1 19.141 (18.695)	Prec@5 38.867 (36.927)	LR: 0.0010000000000000002
 * Prec@1 18.779 Prec@5 37.059
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.6276 (3.6276)	Prec@1 28.906 (28.906)	Prec@5 51.172 (51.172)
 * Prec@1 21.546 Prec@5 41.408
Best accuracy: 39.00%

Epoch: [51][0/1252]	Loss 4.3567 (4.3567)	Prec@1 19.141 (19.141)	Prec@5 38.379 (38.379)	LR: 0.0010000000000000002
Epoch: [51][1000/1252]	Loss 4.1215 (4.3564)	Prec@1 22.461 (19.576)	Prec@5 41.309 (38.236)	LR: 0.0010000000000000002
 * Prec@1 19.660 Prec@5 38.386
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4981 (3.4981)	Prec@1 31.055 (31.055)	Prec@5 52.148 (52.148)
 * Prec@1 22.658 Prec@5 42.756
Best accuracy: 39.00%

Epoch: [52][0/1252]	Loss 4.3342 (4.3342)	Prec@1 20.410 (20.410)	Prec@5 38.965 (38.965)	LR: 0.0010000000000000002
Epoch: [52][1000/1252]	Loss 4.3320 (4.2921)	Prec@1 20.117 (20.334)	Prec@5 39.160 (39.405)	LR: 0.0010000000000000002
 * Prec@1 20.416 Prec@5 39.494
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5215 (3.5215)	Prec@1 29.297 (29.297)	Prec@5 54.395 (54.395)
 * Prec@1 23.406 Prec@5 44.304
Best accuracy: 39.00%

Epoch: [53][0/1252]	Loss 4.2486 (4.2486)	Prec@1 20.020 (20.020)	Prec@5 38.281 (38.281)	LR: 0.0010000000000000002
Epoch: [53][1000/1252]	Loss 4.1991 (4.2400)	Prec@1 20.898 (20.966)	Prec@5 40.039 (40.270)	LR: 0.0010000000000000002
 * Prec@1 21.036 Prec@5 40.338
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5420 (3.5420)	Prec@1 30.371 (30.371)	Prec@5 53.320 (53.320)
 * Prec@1 22.640 Prec@5 42.978
Best accuracy: 39.00%

Epoch: [54][0/1252]	Loss 4.1682 (4.1682)	Prec@1 21.094 (21.094)	Prec@5 40.137 (40.137)	LR: 0.0010000000000000002
Epoch: [54][1000/1252]	Loss 4.1901 (4.2024)	Prec@1 22.070 (21.518)	Prec@5 41.602 (40.933)	LR: 0.0010000000000000002
 * Prec@1 21.488 Prec@5 40.905
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5551 (3.5551)	Prec@1 31.543 (31.543)	Prec@5 51.367 (51.367)
 * Prec@1 23.088 Prec@5 43.500
Best accuracy: 39.00%

Epoch: [55][0/1252]	Loss 4.4165 (4.4165)	Prec@1 19.531 (19.531)	Prec@5 37.695 (37.695)	LR: 0.0010000000000000002
Epoch: [55][1000/1252]	Loss 4.2326 (4.1793)	Prec@1 20.605 (21.781)	Prec@5 40.137 (41.332)	LR: 0.0010000000000000002
 * Prec@1 21.817 Prec@5 41.390
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.3043 (3.3043)	Prec@1 33.789 (33.789)	Prec@5 56.836 (56.836)
 * Prec@1 24.318 Prec@5 45.178
Best accuracy: 39.00%

Epoch: [56][0/1252]	Loss 4.2010 (4.2010)	Prec@1 20.508 (20.508)	Prec@5 41.309 (41.309)	LR: 0.0010000000000000002
Epoch: [56][1000/1252]	Loss 4.1580 (4.1374)	Prec@1 19.824 (22.317)	Prec@5 40.137 (42.087)	LR: 0.0010000000000000002
 * Prec@1 22.357 Prec@5 42.150
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.2225 (3.2225)	Prec@1 35.059 (35.059)	Prec@5 57.422 (57.422)
 * Prec@1 24.346 Prec@5 45.602
Best accuracy: 39.00%

Epoch: [57][0/1252]	Loss 4.0891 (4.0891)	Prec@1 23.340 (23.340)	Prec@5 43.945 (43.945)	LR: 0.0010000000000000002
Epoch: [57][1000/1252]	Loss 4.0267 (4.1066)	Prec@1 24.121 (22.726)	Prec@5 44.434 (42.582)	LR: 0.0010000000000000002
 * Prec@1 22.753 Prec@5 42.642
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5153 (3.5153)	Prec@1 33.496 (33.496)	Prec@5 53.320 (53.320)
 * Prec@1 23.226 Prec@5 43.716
Best accuracy: 39.00%

Epoch: [58][0/1252]	Loss 4.2134 (4.2134)	Prec@1 21.777 (21.777)	Prec@5 41.211 (41.211)	LR: 0.0010000000000000002
Epoch: [58][1000/1252]	Loss 4.1571 (4.0819)	Prec@1 24.121 (23.064)	Prec@5 42.188 (43.070)	LR: 0.0010000000000000002
 * Prec@1 23.099 Prec@5 43.118
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0787 (3.0787)	Prec@1 37.012 (37.012)	Prec@5 60.352 (60.352)
 * Prec@1 26.088 Prec@5 47.348
Best accuracy: 39.00%

Epoch: [59][0/1252]	Loss 3.9972 (3.9972)	Prec@1 23.242 (23.242)	Prec@5 43.457 (43.457)	LR: 0.0010000000000000002
Epoch: [59][1000/1252]	Loss 4.1272 (4.0771)	Prec@1 22.070 (23.089)	Prec@5 42.090 (43.153)	LR: 0.0010000000000000002
 * Prec@1 22.947 Prec@5 42.915
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.3819 (3.3819)	Prec@1 33.594 (33.594)	Prec@5 55.273 (55.273)
 * Prec@1 24.894 Prec@5 45.832
Best accuracy: 39.00%

Epoch: [60][0/1252]	Loss 4.1558 (4.1558)	Prec@1 21.387 (21.387)	Prec@5 41.406 (41.406)	LR: 0.00010000000000000003
Epoch: [60][1000/1252]	Loss 4.1631 (4.1085)	Prec@1 23.926 (22.740)	Prec@5 40.625 (42.601)	LR: 0.00010000000000000003
 * Prec@1 22.752 Prec@5 42.600
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1093 (3.1093)	Prec@1 36.328 (36.328)	Prec@5 61.426 (61.426)
 * Prec@1 26.124 Prec@5 47.572
Best accuracy: 39.00%

Epoch: [61][0/1252]	Loss 4.0382 (4.0382)	Prec@1 23.047 (23.047)	Prec@5 43.359 (43.359)	LR: 0.00010000000000000003
Epoch: [61][1000/1252]	Loss 4.1599 (4.1055)	Prec@1 22.266 (22.763)	Prec@5 41.406 (42.605)	LR: 0.00010000000000000003
 * Prec@1 22.757 Prec@5 42.587
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1134 (3.1134)	Prec@1 37.305 (37.305)	Prec@5 61.230 (61.230)
 * Prec@1 26.162 Prec@5 47.550
Best accuracy: 39.00%

Epoch: [62][0/1252]	Loss 4.0515 (4.0515)	Prec@1 23.047 (23.047)	Prec@5 43.652 (43.652)	LR: 0.00010000000000000003
Epoch: [62][1000/1252]	Loss 4.1320 (4.1010)	Prec@1 22.266 (22.829)	Prec@5 42.773 (42.709)	LR: 0.00010000000000000003
 * Prec@1 22.850 Prec@5 42.726
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.2174 (3.2174)	Prec@1 35.254 (35.254)	Prec@5 58.984 (58.984)
 * Prec@1 26.180 Prec@5 47.542
Best accuracy: 39.00%

Epoch: [63][0/1252]	Loss 4.1021 (4.1021)	Prec@1 22.461 (22.461)	Prec@5 43.359 (43.359)	LR: 0.00010000000000000003
Epoch: [63][1000/1252]	Loss 4.2256 (4.0955)	Prec@1 21.094 (22.925)	Prec@5 40.918 (42.790)	LR: 0.00010000000000000003
 * Prec@1 22.939 Prec@5 42.817
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1092 (3.1092)	Prec@1 36.523 (36.523)	Prec@5 60.254 (60.254)
 * Prec@1 25.922 Prec@5 47.274
Best accuracy: 39.00%

Epoch: [64][0/1252]	Loss 4.0877 (4.0877)	Prec@1 25.586 (25.586)	Prec@5 45.605 (45.605)	LR: 0.00010000000000000003
Epoch: [64][1000/1252]	Loss 4.1255 (4.0895)	Prec@1 20.801 (22.987)	Prec@5 42.773 (42.913)	LR: 0.00010000000000000003
 * Prec@1 22.993 Prec@5 42.900
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1076 (3.1076)	Prec@1 36.719 (36.719)	Prec@5 62.109 (62.109)
 * Prec@1 26.346 Prec@5 47.904
Best accuracy: 39.00%

Epoch: [65][0/1252]	Loss 4.1366 (4.1366)	Prec@1 21.973 (21.973)	Prec@5 41.309 (41.309)	LR: 0.00010000000000000003
Epoch: [65][1000/1252]	Loss 4.0600 (4.0892)	Prec@1 21.973 (23.003)	Prec@5 43.457 (42.917)	LR: 0.00010000000000000003
 * Prec@1 23.002 Prec@5 42.931
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1158 (3.1158)	Prec@1 35.938 (35.938)	Prec@5 60.840 (60.840)
 * Prec@1 26.294 Prec@5 47.768
Best accuracy: 39.00%

Epoch: [66][0/1252]	Loss 4.1053 (4.1053)	Prec@1 22.656 (22.656)	Prec@5 43.164 (43.164)	LR: 0.00010000000000000003
Epoch: [66][1000/1252]	Loss 4.0746 (4.0810)	Prec@1 23.633 (23.131)	Prec@5 43.945 (43.067)	LR: 0.00010000000000000003
 * Prec@1 23.127 Prec@5 43.089
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1464 (3.1464)	Prec@1 35.645 (35.645)	Prec@5 59.961 (59.961)
 * Prec@1 26.490 Prec@5 47.988
Best accuracy: 39.00%

Epoch: [67][0/1252]	Loss 3.9362 (3.9362)	Prec@1 23.926 (23.926)	Prec@5 44.824 (44.824)	LR: 0.00010000000000000003
Epoch: [67][1000/1252]	Loss 4.0728 (4.0785)	Prec@1 20.898 (23.114)	Prec@5 42.383 (43.127)	LR: 0.00010000000000000003
 * Prec@1 23.116 Prec@5 43.105
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1878 (3.1878)	Prec@1 34.863 (34.863)	Prec@5 59.570 (59.570)
 * Prec@1 26.368 Prec@5 47.874
Best accuracy: 39.00%

Epoch: [68][0/1252]	Loss 4.0277 (4.0277)	Prec@1 22.656 (22.656)	Prec@5 43.555 (43.555)	LR: 0.00010000000000000003
Epoch: [68][1000/1252]	Loss 4.1297 (4.0744)	Prec@1 22.754 (23.156)	Prec@5 43.066 (43.170)	LR: 0.00010000000000000003
 * Prec@1 23.177 Prec@5 43.192
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1552 (3.1552)	Prec@1 35.938 (35.938)	Prec@5 60.059 (60.059)
 * Prec@1 26.340 Prec@5 47.938
Best accuracy: 39.00%

Epoch: [69][0/1252]	Loss 4.0431 (4.0431)	Prec@1 22.852 (22.852)	Prec@5 43.457 (43.457)	LR: 0.00010000000000000003
Epoch: [69][1000/1252]	Loss 4.1909 (4.0766)	Prec@1 22.754 (23.173)	Prec@5 41.504 (43.096)	LR: 0.00010000000000000003
 * Prec@1 23.179 Prec@5 43.113
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1370 (3.1370)	Prec@1 36.914 (36.914)	Prec@5 59.863 (59.863)
 * Prec@1 26.170 Prec@5 47.758
Best accuracy: 39.00%

Epoch: [70][0/1252]	Loss 4.1525 (4.1525)	Prec@1 21.387 (21.387)	Prec@5 41.504 (41.504)	LR: 0.00010000000000000003
Epoch: [70][1000/1252]	Loss 4.0764 (4.0723)	Prec@1 25.195 (23.185)	Prec@5 43.359 (43.249)	LR: 0.00010000000000000003
 * Prec@1 23.200 Prec@5 43.262
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1388 (3.1388)	Prec@1 35.059 (35.059)	Prec@5 59.766 (59.766)
 * Prec@1 26.512 Prec@5 48.006
Best accuracy: 39.00%

Epoch: [71][0/1252]	Loss 3.9841 (3.9841)	Prec@1 23.047 (23.047)	Prec@5 45.117 (45.117)	LR: 0.00010000000000000003
Epoch: [71][1000/1252]	Loss 4.1745 (4.0660)	Prec@1 21.875 (23.258)	Prec@5 41.113 (43.289)	LR: 0.00010000000000000003
 * Prec@1 23.247 Prec@5 43.261
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1204 (3.1204)	Prec@1 36.328 (36.328)	Prec@5 60.742 (60.742)
 * Prec@1 26.658 Prec@5 48.346
Best accuracy: 39.00%

Epoch: [72][0/1252]	Loss 4.0935 (4.0935)	Prec@1 20.898 (20.898)	Prec@5 44.043 (44.043)	LR: 0.00010000000000000003
Epoch: [72][1000/1252]	Loss 4.0675 (4.0651)	Prec@1 23.730 (23.300)	Prec@5 43.848 (43.327)	LR: 0.00010000000000000003
 * Prec@1 23.284 Prec@5 43.325
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1024 (3.1024)	Prec@1 37.500 (37.500)	Prec@5 59.961 (59.961)
 * Prec@1 26.464 Prec@5 48.030
Best accuracy: 39.00%

Epoch: [73][0/1252]	Loss 4.0621 (4.0621)	Prec@1 24.414 (24.414)	Prec@5 42.090 (42.090)	LR: 0.00010000000000000003
Epoch: [73][1000/1252]	Loss 4.1954 (4.0609)	Prec@1 21.387 (23.331)	Prec@5 42.285 (43.402)	LR: 0.00010000000000000003
 * Prec@1 23.324 Prec@5 43.402
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1701 (3.1701)	Prec@1 36.035 (36.035)	Prec@5 59.277 (59.277)
 * Prec@1 26.496 Prec@5 48.074
Best accuracy: 39.00%

Epoch: [74][0/1252]	Loss 4.0033 (4.0033)	Prec@1 23.535 (23.535)	Prec@5 42.871 (42.871)	LR: 0.00010000000000000003
Epoch: [74][1000/1252]	Loss 4.0482 (4.0583)	Prec@1 23.828 (23.383)	Prec@5 45.020 (43.435)	LR: 0.00010000000000000003
 * Prec@1 23.357 Prec@5 43.429
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0909 (3.0909)	Prec@1 37.305 (37.305)	Prec@5 59.473 (59.473)
 * Prec@1 26.676 Prec@5 48.286
Best accuracy: 39.00%

Epoch: [75][0/1252]	Loss 3.9953 (3.9953)	Prec@1 22.363 (22.363)	Prec@5 42.383 (42.383)	LR: 0.00010000000000000003
Epoch: [75][1000/1252]	Loss 3.9780 (4.0580)	Prec@1 22.656 (23.318)	Prec@5 45.898 (43.473)	LR: 0.00010000000000000003
 * Prec@1 23.377 Prec@5 43.482
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1085 (3.1085)	Prec@1 36.035 (36.035)	Prec@5 59.863 (59.863)
 * Prec@1 26.548 Prec@5 47.986
Best accuracy: 39.00%

Epoch: [76][0/1252]	Loss 4.1658 (4.1658)	Prec@1 23.242 (23.242)	Prec@5 41.309 (41.309)	LR: 0.00010000000000000003
Epoch: [76][1000/1252]	Loss 4.1563 (4.0539)	Prec@1 21.777 (23.451)	Prec@5 41.992 (43.544)	LR: 0.00010000000000000003
 * Prec@1 23.407 Prec@5 43.491
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0559 (3.0559)	Prec@1 37.305 (37.305)	Prec@5 61.328 (61.328)
 * Prec@1 26.718 Prec@5 48.276
Best accuracy: 39.00%

Epoch: [77][0/1252]	Loss 4.0387 (4.0387)	Prec@1 23.633 (23.633)	Prec@5 44.141 (44.141)	LR: 0.00010000000000000003
Epoch: [77][1000/1252]	Loss 4.0613 (4.0529)	Prec@1 21.875 (23.458)	Prec@5 43.848 (43.542)	LR: 0.00010000000000000003
 * Prec@1 23.449 Prec@5 43.536
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1901 (3.1901)	Prec@1 35.059 (35.059)	Prec@5 58.789 (58.789)
 * Prec@1 26.420 Prec@5 48.338
Best accuracy: 39.00%

Epoch: [78][0/1252]	Loss 4.0866 (4.0866)	Prec@1 22.070 (22.070)	Prec@5 42.285 (42.285)	LR: 0.00010000000000000003
Epoch: [78][1000/1252]	Loss 4.1628 (4.0515)	Prec@1 22.168 (23.491)	Prec@5 41.602 (43.559)	LR: 0.00010000000000000003
 * Prec@1 23.512 Prec@5 43.589
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1641 (3.1641)	Prec@1 35.156 (35.156)	Prec@5 59.473 (59.473)
 * Prec@1 26.784 Prec@5 48.288
Best accuracy: 39.00%

Epoch: [79][0/1252]	Loss 3.9888 (3.9888)	Prec@1 24.219 (24.219)	Prec@5 44.531 (44.531)	LR: 0.00010000000000000003
Epoch: [79][1000/1252]	Loss 3.9649 (4.0510)	Prec@1 26.270 (23.513)	Prec@5 45.020 (43.620)	LR: 0.00010000000000000003
 * Prec@1 23.525 Prec@5 43.608
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0706 (3.0706)	Prec@1 36.719 (36.719)	Prec@5 61.328 (61.328)
 * Prec@1 26.902 Prec@5 48.436
Best accuracy: 39.00%

Epoch: [80][0/1252]	Loss 4.0536 (4.0536)	Prec@1 24.316 (24.316)	Prec@5 41.211 (41.211)	LR: 1.0000000000000004e-05
Epoch: [80][1000/1252]	Loss 3.9833 (4.0425)	Prec@1 24.219 (23.581)	Prec@5 43.848 (43.729)	LR: 1.0000000000000004e-05
 * Prec@1 23.591 Prec@5 43.725
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0323 (3.0323)	Prec@1 37.988 (37.988)	Prec@5 63.281 (63.281)
 * Prec@1 26.978 Prec@5 48.638
Best accuracy: 39.00%

Epoch: [81][0/1252]	Loss 4.0198 (4.0198)	Prec@1 22.559 (22.559)	Prec@5 43.848 (43.848)	LR: 1.0000000000000004e-05
Epoch: [81][1000/1252]	Loss 3.9639 (4.0454)	Prec@1 25.684 (23.590)	Prec@5 44.141 (43.663)	LR: 1.0000000000000004e-05
 * Prec@1 23.594 Prec@5 43.673
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1133 (3.1133)	Prec@1 35.254 (35.254)	Prec@5 60.059 (60.059)
 * Prec@1 26.782 Prec@5 48.482
Best accuracy: 39.00%

Epoch: [82][0/1252]	Loss 4.0604 (4.0604)	Prec@1 22.168 (22.168)	Prec@5 43.848 (43.848)	LR: 1.0000000000000004e-05
Epoch: [82][1000/1252]	Loss 4.1317 (4.0452)	Prec@1 21.582 (23.550)	Prec@5 42.871 (43.649)	LR: 1.0000000000000004e-05
 * Prec@1 23.555 Prec@5 43.689
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1083 (3.1083)	Prec@1 36.523 (36.523)	Prec@5 61.816 (61.816)
 * Prec@1 26.542 Prec@5 48.328
Best accuracy: 39.00%

Epoch: [83][0/1252]	Loss 3.9994 (3.9994)	Prec@1 25.000 (25.000)	Prec@5 44.531 (44.531)	LR: 1.0000000000000004e-05
Epoch: [83][1000/1252]	Loss 4.0975 (4.0426)	Prec@1 25.586 (23.575)	Prec@5 41.895 (43.727)	LR: 1.0000000000000004e-05
 * Prec@1 23.589 Prec@5 43.732
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0675 (3.0675)	Prec@1 36.816 (36.816)	Prec@5 60.156 (60.156)
 * Prec@1 26.816 Prec@5 48.418
Best accuracy: 39.00%

Epoch: [84][0/1252]	Loss 4.0789 (4.0789)	Prec@1 21.582 (21.582)	Prec@5 42.773 (42.773)	LR: 1.0000000000000004e-05
Epoch: [84][1000/1252]	Loss 3.9069 (4.0478)	Prec@1 24.902 (23.524)	Prec@5 46.484 (43.642)	LR: 1.0000000000000004e-05
 * Prec@1 23.564 Prec@5 43.676
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1338 (3.1338)	Prec@1 35.938 (35.938)	Prec@5 59.277 (59.277)
 * Prec@1 27.014 Prec@5 48.384
Best accuracy: 39.00%

Epoch: [85][0/1252]	Loss 4.0298 (4.0298)	Prec@1 22.852 (22.852)	Prec@5 44.922 (44.922)	LR: 1.0000000000000004e-05
Epoch: [85][1000/1252]	Loss 4.1192 (4.0452)	Prec@1 23.242 (23.557)	Prec@5 43.457 (43.725)	LR: 1.0000000000000004e-05
 * Prec@1 23.558 Prec@5 43.706
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0294 (3.0294)	Prec@1 36.621 (36.621)	Prec@5 62.207 (62.207)
 * Prec@1 26.782 Prec@5 48.332
Best accuracy: 39.00%

Epoch: [86][0/1252]	Loss 3.9575 (3.9575)	Prec@1 24.609 (24.609)	Prec@5 45.801 (45.801)	LR: 1.0000000000000004e-05
Epoch: [86][1000/1252]	Loss 3.9870 (4.0408)	Prec@1 24.316 (23.598)	Prec@5 46.094 (43.755)	LR: 1.0000000000000004e-05
 * Prec@1 23.595 Prec@5 43.717
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0723 (3.0723)	Prec@1 37.891 (37.891)	Prec@5 61.621 (61.621)
 * Prec@1 26.964 Prec@5 48.612
Best accuracy: 39.00%

Epoch: [87][0/1252]	Loss 4.0317 (4.0317)	Prec@1 23.145 (23.145)	Prec@5 41.895 (41.895)	LR: 1.0000000000000004e-05
Epoch: [87][1000/1252]	Loss 4.0290 (4.0422)	Prec@1 22.461 (23.578)	Prec@5 44.141 (43.734)	LR: 1.0000000000000004e-05
 * Prec@1 23.542 Prec@5 43.710
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1341 (3.1341)	Prec@1 36.133 (36.133)	Prec@5 59.863 (59.863)
 * Prec@1 26.892 Prec@5 48.442
Best accuracy: 39.00%

Epoch: [88][0/1252]	Loss 4.0414 (4.0414)	Prec@1 21.973 (21.973)	Prec@5 43.555 (43.555)	LR: 1.0000000000000004e-05
Epoch: [88][1000/1252]	Loss 4.0008 (4.0433)	Prec@1 24.609 (23.529)	Prec@5 43.066 (43.738)	LR: 1.0000000000000004e-05
 * Prec@1 23.559 Prec@5 43.731
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0552 (3.0552)	Prec@1 37.695 (37.695)	Prec@5 60.645 (60.645)
 * Prec@1 26.768 Prec@5 48.462
Best accuracy: 39.00%

Epoch: [89][0/1252]	Loss 3.9087 (3.9087)	Prec@1 26.562 (26.562)	Prec@5 46.191 (46.191)	LR: 1.0000000000000004e-05
Epoch: [89][1000/1252]	Loss 4.0515 (4.0436)	Prec@1 24.023 (23.587)	Prec@5 43.359 (43.729)	LR: 1.0000000000000004e-05
 * Prec@1 23.569 Prec@5 43.711
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1227 (3.1227)	Prec@1 35.645 (35.645)	Prec@5 61.035 (61.035)
 * Prec@1 27.226 Prec@5 48.730
Best accuracy: 39.00%

Epoch: [90][0/1252]	Loss 3.8927 (3.8927)	Prec@1 27.441 (27.441)	Prec@5 47.266 (47.266)	LR: 1.0000000000000004e-05
Epoch: [90][1000/1252]	Loss 4.1942 (4.0434)	Prec@1 21.875 (23.574)	Prec@5 41.211 (43.693)	LR: 1.0000000000000004e-05
 * Prec@1 23.590 Prec@5 43.702
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0762 (3.0762)	Prec@1 37.305 (37.305)	Prec@5 59.961 (59.961)
 * Prec@1 26.680 Prec@5 48.262
Best accuracy: 39.00%

Epoch: [91][0/1252]	Loss 4.0728 (4.0728)	Prec@1 25.098 (25.098)	Prec@5 43.555 (43.555)	LR: 1.0000000000000004e-05
Epoch: [91][1000/1252]	Loss 4.0963 (4.0426)	Prec@1 23.340 (23.622)	Prec@5 43.066 (43.801)	LR: 1.0000000000000004e-05
 * Prec@1 23.615 Prec@5 43.764
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1870 (3.1870)	Prec@1 35.449 (35.449)	Prec@5 57.715 (57.715)
 * Prec@1 26.634 Prec@5 48.224
Best accuracy: 39.00%

Epoch: [92][0/1252]	Loss 4.0380 (4.0380)	Prec@1 24.609 (24.609)	Prec@5 42.773 (42.773)	LR: 1.0000000000000004e-05
Epoch: [92][1000/1252]	Loss 4.0366 (4.0419)	Prec@1 23.828 (23.588)	Prec@5 44.043 (43.758)	LR: 1.0000000000000004e-05
 * Prec@1 23.558 Prec@5 43.726
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1334 (3.1334)	Prec@1 36.133 (36.133)	Prec@5 59.473 (59.473)
 * Prec@1 26.790 Prec@5 48.048
Best accuracy: 39.00%

Epoch: [93][0/1252]	Loss 4.0035 (4.0035)	Prec@1 24.121 (24.121)	Prec@5 45.215 (45.215)	LR: 1.0000000000000004e-05
Epoch: [93][1000/1252]	Loss 4.0368 (4.0390)	Prec@1 24.316 (23.674)	Prec@5 43.457 (43.845)	LR: 1.0000000000000004e-05
 * Prec@1 23.661 Prec@5 43.809
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.1080 (3.1080)	Prec@1 36.914 (36.914)	Prec@5 60.059 (60.059)
 * Prec@1 26.904 Prec@5 48.410
Best accuracy: 39.00%

Epoch: [94][0/1252]	Loss 4.0167 (4.0167)	Prec@1 24.609 (24.609)	Prec@5 45.312 (45.312)	LR: 1.0000000000000004e-05
Epoch: [94][1000/1252]	Loss 4.0445 (4.0429)	Prec@1 23.828 (23.530)	Prec@5 44.141 (43.796)	LR: 1.0000000000000004e-05
 * Prec@1 23.542 Prec@5 43.773
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.0799 (3.0799)	Prec@1 36.230 (36.230)	Prec@5 62.598 (62.598)
 * Prec@1 26.734 Prec@5 48.384
Best accuracy: 39.00%

Epoch: [95][0/1252]	Loss 4.1855 (4.1855)	Prec@1 22.266 (22.266)	Prec@5 41.504 (41.504)	LR: 1.0000000000000004e-05
Epoch: [95][1000/1252]	Loss 4.0476 (4.0451)	Prec@1 23.828 (23.574)	Prec@5 42.871 (43.681)	LR: 1.0000000000000004e-05
 * Prec@1 22.524 Prec@5 42.009
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 4.9530 (4.9530)	Prec@1 14.258 (14.258)	Prec@5 32.520 (32.520)
 * Prec@1 11.332 Prec@5 25.454
Best accuracy: 39.00%

Epoch: [96][0/1252]	Loss 5.6676 (5.6676)	Prec@1 6.348 (6.348)	Prec@5 17.676 (17.676)	LR: 1.0000000000000004e-05
Epoch: [96][1000/1252]	Loss 4.6916 (4.9782)	Prec@1 15.723 (13.159)	Prec@5 33.203 (28.398)	LR: 1.0000000000000004e-05
 * Prec@1 13.597 Prec@5 29.101
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.9206 (3.9206)	Prec@1 24.414 (24.414)	Prec@5 47.754 (47.754)
 * Prec@1 18.286 Prec@5 37.086
Best accuracy: 39.00%

Epoch: [97][0/1252]	Loss 4.7869 (4.7869)	Prec@1 14.941 (14.941)	Prec@5 29.004 (29.004)	LR: 1.0000000000000004e-05
Epoch: [97][1000/1252]	Loss 4.6505 (4.6592)	Prec@1 16.699 (16.207)	Prec@5 33.594 (33.211)	LR: 1.0000000000000004e-05
 * Prec@1 16.327 Prec@5 33.399
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.8595 (3.8595)	Prec@1 26.172 (26.172)	Prec@5 48.242 (48.242)
 * Prec@1 19.584 Prec@5 38.888
Best accuracy: 39.00%

Epoch: [98][0/1252]	Loss 4.5950 (4.5950)	Prec@1 17.480 (17.480)	Prec@5 35.254 (35.254)	LR: 1.0000000000000004e-05
Epoch: [98][1000/1252]	Loss 4.5180 (4.5690)	Prec@1 17.480 (17.142)	Prec@5 35.547 (34.664)	LR: 1.0000000000000004e-05
 * Prec@1 17.183 Prec@5 34.741
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.7165 (3.7165)	Prec@1 27.832 (27.832)	Prec@5 50.586 (50.586)
 * Prec@1 20.294 Prec@5 39.738
Best accuracy: 39.00%

Epoch: [99][0/1252]	Loss 4.6583 (4.6583)	Prec@1 15.625 (15.625)	Prec@5 33.594 (33.594)	LR: 1.0000000000000004e-05
Epoch: [99][1000/1252]	Loss 4.4685 (4.5241)	Prec@1 16.504 (17.638)	Prec@5 34.668 (35.465)	LR: 1.0000000000000004e-05
 * Prec@1 17.680 Prec@5 35.524
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.7010 (3.7010)	Prec@1 28.223 (28.223)	Prec@5 50.684 (50.684)
 * Prec@1 20.844 Prec@5 40.672
Best accuracy: 39.00%

Epoch: [100][0/1252]	Loss 4.4961 (4.4961)	Prec@1 17.480 (17.480)	Prec@5 35.254 (35.254)	LR: 1.0000000000000004e-05
Epoch: [100][1000/1252]	Loss 4.5492 (4.4900)	Prec@1 16.895 (18.074)	Prec@5 34.766 (36.021)	LR: 1.0000000000000004e-05
 * Prec@1 18.100 Prec@5 36.060
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.6167 (3.6167)	Prec@1 29.980 (29.980)	Prec@5 52.539 (52.539)
 * Prec@1 21.122 Prec@5 40.986
Best accuracy: 39.00%

Epoch: [101][0/1252]	Loss 4.4639 (4.4639)	Prec@1 19.141 (19.141)	Prec@5 37.891 (37.891)	LR: 1.0000000000000004e-05
Epoch: [101][1000/1252]	Loss 4.4611 (4.4728)	Prec@1 18.555 (18.184)	Prec@5 36.133 (36.321)	LR: 1.0000000000000004e-05
 * Prec@1 18.212 Prec@5 36.346
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.7539 (3.7539)	Prec@1 28.125 (28.125)	Prec@5 50.977 (50.977)
 * Prec@1 21.332 Prec@5 41.462
Best accuracy: 39.00%

Epoch: [102][0/1252]	Loss 4.5132 (4.5132)	Prec@1 18.066 (18.066)	Prec@5 37.109 (37.109)	LR: 1.0000000000000004e-05
Epoch: [102][1000/1252]	Loss 4.4267 (4.4536)	Prec@1 19.531 (18.394)	Prec@5 37.598 (36.662)	LR: 1.0000000000000004e-05
 * Prec@1 18.425 Prec@5 36.665
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.6476 (3.6476)	Prec@1 29.785 (29.785)	Prec@5 53.125 (53.125)
 * Prec@1 21.480 Prec@5 41.472
Best accuracy: 39.00%

Epoch: [103][0/1252]	Loss 4.5092 (4.5092)	Prec@1 18.652 (18.652)	Prec@5 35.352 (35.352)	LR: 1.0000000000000004e-05
Epoch: [103][1000/1252]	Loss 4.4555 (4.4418)	Prec@1 17.188 (18.581)	Prec@5 37.988 (36.851)	LR: 1.0000000000000004e-05
 * Prec@1 18.588 Prec@5 36.856
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.6147 (3.6147)	Prec@1 30.566 (30.566)	Prec@5 52.051 (52.051)
 * Prec@1 21.676 Prec@5 41.682
Best accuracy: 39.00%

Epoch: [104][0/1252]	Loss 4.5018 (4.5018)	Prec@1 18.848 (18.848)	Prec@5 33.105 (33.105)	LR: 1.0000000000000004e-05
Epoch: [104][1000/1252]	Loss 4.3504 (4.4276)	Prec@1 18.164 (18.714)	Prec@5 40.234 (37.065)	LR: 1.0000000000000004e-05
 * Prec@1 18.735 Prec@5 37.080
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5731 (3.5731)	Prec@1 30.762 (30.762)	Prec@5 53.711 (53.711)
 * Prec@1 21.746 Prec@5 41.516
Best accuracy: 39.00%

Epoch: [105][0/1252]	Loss 4.2279 (4.2279)	Prec@1 21.094 (21.094)	Prec@5 43.359 (43.359)	LR: 1.0000000000000004e-05
Epoch: [105][1000/1252]	Loss 4.3584 (4.4167)	Prec@1 19.043 (18.853)	Prec@5 37.402 (37.198)	LR: 1.0000000000000004e-05
 * Prec@1 18.848 Prec@5 37.218
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5590 (3.5590)	Prec@1 29.785 (29.785)	Prec@5 53.418 (53.418)
 * Prec@1 21.848 Prec@5 41.988
Best accuracy: 39.00%

Epoch: [106][0/1252]	Loss 4.3735 (4.3735)	Prec@1 19.336 (19.336)	Prec@5 38.770 (38.770)	LR: 1.0000000000000004e-05
Epoch: [106][1000/1252]	Loss 4.4279 (4.4091)	Prec@1 17.285 (18.999)	Prec@5 35.156 (37.395)	LR: 1.0000000000000004e-05
 * Prec@1 19.027 Prec@5 37.424
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5534 (3.5534)	Prec@1 29.492 (29.492)	Prec@5 53.027 (53.027)
 * Prec@1 22.072 Prec@5 42.216
Best accuracy: 39.00%

Epoch: [107][0/1252]	Loss 4.4392 (4.4392)	Prec@1 20.117 (20.117)	Prec@5 36.621 (36.621)	LR: 1.0000000000000004e-05
Epoch: [107][1000/1252]	Loss 4.4216 (4.4030)	Prec@1 19.922 (18.985)	Prec@5 38.184 (37.442)	LR: 1.0000000000000004e-05
 * Prec@1 19.007 Prec@5 37.472
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4959 (3.4959)	Prec@1 30.273 (30.273)	Prec@5 54.492 (54.492)
 * Prec@1 22.016 Prec@5 42.340
Best accuracy: 39.00%

Epoch: [108][0/1252]	Loss 4.3814 (4.3814)	Prec@1 19.336 (19.336)	Prec@5 39.551 (39.551)	LR: 1.0000000000000004e-05
Epoch: [108][1000/1252]	Loss 4.4421 (4.3949)	Prec@1 17.871 (19.115)	Prec@5 35.449 (37.588)	LR: 1.0000000000000004e-05
 * Prec@1 19.109 Prec@5 37.589
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5567 (3.5567)	Prec@1 30.859 (30.859)	Prec@5 54.297 (54.297)
 * Prec@1 22.288 Prec@5 42.408
Best accuracy: 39.00%

Epoch: [109][0/1252]	Loss 4.4562 (4.4562)	Prec@1 19.531 (19.531)	Prec@5 37.207 (37.207)	LR: 1.0000000000000004e-05
Epoch: [109][1000/1252]	Loss 4.4460 (4.3929)	Prec@1 18.457 (19.126)	Prec@5 36.914 (37.640)	LR: 1.0000000000000004e-05
 * Prec@1 19.164 Prec@5 37.687
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.6213 (3.6213)	Prec@1 30.176 (30.176)	Prec@5 50.488 (50.488)
 * Prec@1 22.388 Prec@5 42.408
Best accuracy: 39.00%

Epoch: [110][0/1252]	Loss 4.5200 (4.5200)	Prec@1 17.090 (17.090)	Prec@5 34.473 (34.473)	LR: 1.0000000000000004e-05
Epoch: [110][1000/1252]	Loss 4.3931 (4.3815)	Prec@1 18.457 (19.259)	Prec@5 39.160 (37.834)	LR: 1.0000000000000004e-05
 * Prec@1 19.276 Prec@5 37.813
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5533 (3.5533)	Prec@1 31.152 (31.152)	Prec@5 53.418 (53.418)
 * Prec@1 22.470 Prec@5 42.646
Best accuracy: 39.00%

Epoch: [111][0/1252]	Loss 4.4026 (4.4026)	Prec@1 19.043 (19.043)	Prec@5 37.695 (37.695)	LR: 1.0000000000000004e-05
Epoch: [111][1000/1252]	Loss 4.3820 (4.3795)	Prec@1 20.801 (19.311)	Prec@5 37.012 (37.890)	LR: 1.0000000000000004e-05
 * Prec@1 19.307 Prec@5 37.840
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5692 (3.5692)	Prec@1 30.957 (30.957)	Prec@5 52.832 (52.832)
 * Prec@1 22.308 Prec@5 42.670
Best accuracy: 39.00%

Epoch: [112][0/1252]	Loss 4.3124 (4.3124)	Prec@1 20.801 (20.801)	Prec@5 38.379 (38.379)	LR: 1.0000000000000004e-05
Epoch: [112][1000/1252]	Loss 4.2484 (4.3734)	Prec@1 22.070 (19.340)	Prec@5 40.137 (37.997)	LR: 1.0000000000000004e-05
 * Prec@1 19.328 Prec@5 37.975
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4883 (3.4883)	Prec@1 31.152 (31.152)	Prec@5 54.980 (54.980)
 * Prec@1 22.608 Prec@5 42.908
Best accuracy: 39.00%

Epoch: [113][0/1252]	Loss 4.4056 (4.4056)	Prec@1 19.043 (19.043)	Prec@5 37.207 (37.207)	LR: 1.0000000000000004e-05
Epoch: [113][1000/1252]	Loss 4.4770 (4.3699)	Prec@1 18.066 (19.394)	Prec@5 37.891 (38.040)	LR: 1.0000000000000004e-05
 * Prec@1 19.395 Prec@5 38.027
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4789 (3.4789)	Prec@1 30.273 (30.273)	Prec@5 53.223 (53.223)
 * Prec@1 22.510 Prec@5 42.730
Best accuracy: 39.00%

Epoch: [114][0/1252]	Loss 4.2382 (4.2382)	Prec@1 20.215 (20.215)	Prec@5 41.602 (41.602)	LR: 1.0000000000000004e-05
Epoch: [114][1000/1252]	Loss 4.3233 (4.3673)	Prec@1 21.680 (19.479)	Prec@5 39.355 (38.052)	LR: 1.0000000000000004e-05
 * Prec@1 19.474 Prec@5 38.052
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5384 (3.5384)	Prec@1 28.711 (28.711)	Prec@5 54.492 (54.492)
 * Prec@1 22.716 Prec@5 42.894
Best accuracy: 39.00%

Epoch: [115][0/1252]	Loss 4.4027 (4.4027)	Prec@1 19.434 (19.434)	Prec@5 37.598 (37.598)	LR: 1.0000000000000004e-05
Epoch: [115][1000/1252]	Loss 4.4091 (4.3614)	Prec@1 18.848 (19.488)	Prec@5 38.574 (38.167)	LR: 1.0000000000000004e-05
 * Prec@1 19.471 Prec@5 38.136
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4716 (3.4716)	Prec@1 30.664 (30.664)	Prec@5 55.176 (55.176)
 * Prec@1 22.600 Prec@5 42.924
Best accuracy: 39.00%

Epoch: [116][0/1252]	Loss 4.4204 (4.4204)	Prec@1 18.555 (18.555)	Prec@5 38.184 (38.184)	LR: 1.0000000000000004e-05
Epoch: [116][1000/1252]	Loss 4.3430 (4.3575)	Prec@1 20.312 (19.574)	Prec@5 39.258 (38.217)	LR: 1.0000000000000004e-05
 * Prec@1 19.551 Prec@5 38.191
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4738 (3.4738)	Prec@1 31.055 (31.055)	Prec@5 54.395 (54.395)
 * Prec@1 22.596 Prec@5 42.612
Best accuracy: 39.00%

Epoch: [117][0/1252]	Loss 4.3443 (4.3443)	Prec@1 19.434 (19.434)	Prec@5 38.477 (38.477)	LR: 1.0000000000000004e-05
Epoch: [117][1000/1252]	Loss 4.2309 (4.3576)	Prec@1 21.875 (19.596)	Prec@5 40.234 (38.279)	LR: 1.0000000000000004e-05
 * Prec@1 19.578 Prec@5 38.241
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4764 (3.4764)	Prec@1 32.422 (32.422)	Prec@5 54.102 (54.102)
 * Prec@1 22.568 Prec@5 42.902
Best accuracy: 39.00%

Epoch: [118][0/1252]	Loss 4.2709 (4.2709)	Prec@1 19.043 (19.043)	Prec@5 39.941 (39.941)	LR: 1.0000000000000004e-05
Epoch: [118][1000/1252]	Loss 4.3960 (4.3556)	Prec@1 19.824 (19.592)	Prec@5 37.402 (38.260)	LR: 1.0000000000000004e-05
 * Prec@1 19.584 Prec@5 38.232
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4221 (3.4221)	Prec@1 31.836 (31.836)	Prec@5 55.469 (55.469)
 * Prec@1 22.680 Prec@5 43.052
Best accuracy: 39.00%

Epoch: [119][0/1252]	Loss 4.2411 (4.2411)	Prec@1 22.070 (22.070)	Prec@5 38.770 (38.770)	LR: 1.0000000000000004e-05
Epoch: [119][1000/1252]	Loss 4.3879 (4.3564)	Prec@1 19.043 (19.584)	Prec@5 36.914 (38.302)	LR: 1.0000000000000004e-05
 * Prec@1 19.621 Prec@5 38.306
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4638 (3.4638)	Prec@1 31.445 (31.445)	Prec@5 54.980 (54.980)
 * Prec@1 22.684 Prec@5 43.062
Best accuracy: 39.00%

Epoch: [120][0/1252]	Loss 4.4332 (4.4332)	Prec@1 17.480 (17.480)	Prec@5 35.059 (35.059)	LR: 1.0000000000000004e-06
Epoch: [120][1000/1252]	Loss 4.4075 (4.3537)	Prec@1 19.727 (19.594)	Prec@5 36.816 (38.366)	LR: 1.0000000000000004e-06
 * Prec@1 19.587 Prec@5 38.360
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4951 (3.4951)	Prec@1 31.055 (31.055)	Prec@5 54.297 (54.297)
 * Prec@1 22.806 Prec@5 42.976
Best accuracy: 39.00%

Epoch: [121][0/1252]	Loss 4.4268 (4.4268)	Prec@1 20.117 (20.117)	Prec@5 35.547 (35.547)	LR: 1.0000000000000004e-06
Epoch: [121][1000/1252]	Loss 4.2703 (4.3526)	Prec@1 20.410 (19.612)	Prec@5 39.453 (38.354)	LR: 1.0000000000000004e-06
 * Prec@1 19.601 Prec@5 38.343
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4830 (3.4830)	Prec@1 31.543 (31.543)	Prec@5 53.027 (53.027)
 * Prec@1 22.850 Prec@5 42.988
Best accuracy: 39.00%

Epoch: [122][0/1252]	Loss 4.3081 (4.3081)	Prec@1 20.996 (20.996)	Prec@5 39.941 (39.941)	LR: 1.0000000000000004e-06
Epoch: [122][1000/1252]	Loss 4.3081 (4.3545)	Prec@1 20.703 (19.614)	Prec@5 39.160 (38.302)	LR: 1.0000000000000004e-06
 * Prec@1 19.606 Prec@5 38.305
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4835 (3.4835)	Prec@1 30.371 (30.371)	Prec@5 54.980 (54.980)
 * Prec@1 22.570 Prec@5 42.990
Best accuracy: 39.00%

Epoch: [123][0/1252]	Loss 4.3461 (4.3461)	Prec@1 20.996 (20.996)	Prec@5 37.598 (37.598)	LR: 1.0000000000000004e-06
Epoch: [123][1000/1252]	Loss 4.4896 (4.3533)	Prec@1 17.676 (19.643)	Prec@5 35.059 (38.321)	LR: 1.0000000000000004e-06
 * Prec@1 19.628 Prec@5 38.325
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4676 (3.4676)	Prec@1 31.641 (31.641)	Prec@5 54.297 (54.297)
 * Prec@1 22.894 Prec@5 43.056
Best accuracy: 39.00%

Epoch: [124][0/1252]	Loss 4.4462 (4.4462)	Prec@1 17.773 (17.773)	Prec@5 37.402 (37.402)	LR: 1.0000000000000004e-06
Epoch: [124][1000/1252]	Loss 4.3262 (4.3505)	Prec@1 19.824 (19.635)	Prec@5 37.988 (38.363)	LR: 1.0000000000000004e-06
 * Prec@1 19.629 Prec@5 38.380
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4822 (3.4822)	Prec@1 31.445 (31.445)	Prec@5 55.371 (55.371)
 * Prec@1 22.524 Prec@5 43.138
Best accuracy: 39.00%

Epoch: [125][0/1252]	Loss 4.3703 (4.3703)	Prec@1 18.750 (18.750)	Prec@5 39.355 (39.355)	LR: 1.0000000000000004e-06
Epoch: [125][1000/1252]	Loss 4.4435 (4.3521)	Prec@1 17.773 (19.608)	Prec@5 36.328 (38.320)	LR: 1.0000000000000004e-06
 * Prec@1 19.615 Prec@5 38.321
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4840 (3.4840)	Prec@1 32.129 (32.129)	Prec@5 55.078 (55.078)
 * Prec@1 22.750 Prec@5 43.318
Best accuracy: 39.00%

Epoch: [126][0/1252]	Loss 4.4327 (4.4327)	Prec@1 20.312 (20.312)	Prec@5 37.500 (37.500)	LR: 1.0000000000000004e-06
Epoch: [126][1000/1252]	Loss 4.4596 (4.3509)	Prec@1 18.750 (19.606)	Prec@5 36.426 (38.353)	LR: 1.0000000000000004e-06
 * Prec@1 19.593 Prec@5 38.332
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4651 (3.4651)	Prec@1 31.250 (31.250)	Prec@5 53.223 (53.223)
 * Prec@1 22.534 Prec@5 42.806
Best accuracy: 39.00%

Epoch: [127][0/1252]	Loss 4.4362 (4.4362)	Prec@1 20.020 (20.020)	Prec@5 36.621 (36.621)	LR: 1.0000000000000004e-06
Epoch: [127][1000/1252]	Loss 4.2435 (4.3538)	Prec@1 20.117 (19.635)	Prec@5 38.574 (38.288)	LR: 1.0000000000000004e-06
 * Prec@1 19.634 Prec@5 38.275
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4118 (3.4118)	Prec@1 30.859 (30.859)	Prec@5 55.469 (55.469)
 * Prec@1 22.484 Prec@5 43.146
Best accuracy: 39.00%

Epoch: [128][0/1252]	Loss 4.4118 (4.4118)	Prec@1 18.262 (18.262)	Prec@5 36.523 (36.523)	LR: 1.0000000000000004e-06
Epoch: [128][1000/1252]	Loss 4.3767 (4.3551)	Prec@1 19.629 (19.564)	Prec@5 39.062 (38.265)	LR: 1.0000000000000004e-06
 * Prec@1 19.553 Prec@5 38.271
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4885 (3.4885)	Prec@1 31.543 (31.543)	Prec@5 53.223 (53.223)
 * Prec@1 22.774 Prec@5 43.228
Best accuracy: 39.00%

Epoch: [129][0/1252]	Loss 4.3493 (4.3493)	Prec@1 19.238 (19.238)	Prec@5 39.160 (39.160)	LR: 1.0000000000000004e-06
Epoch: [129][1000/1252]	Loss 4.2663 (4.3559)	Prec@1 19.922 (19.583)	Prec@5 39.160 (38.292)	LR: 1.0000000000000004e-06
 * Prec@1 19.607 Prec@5 38.292
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5092 (3.5092)	Prec@1 30.566 (30.566)	Prec@5 54.492 (54.492)
 * Prec@1 22.476 Prec@5 42.914
Best accuracy: 39.00%

Epoch: [130][0/1252]	Loss 4.4691 (4.4691)	Prec@1 17.090 (17.090)	Prec@5 36.719 (36.719)	LR: 1.0000000000000004e-06
Epoch: [130][1000/1252]	Loss 4.3480 (4.3554)	Prec@1 20.508 (19.595)	Prec@5 38.672 (38.305)	LR: 1.0000000000000004e-06
 * Prec@1 19.605 Prec@5 38.339
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4388 (3.4388)	Prec@1 32.227 (32.227)	Prec@5 55.957 (55.957)
 * Prec@1 22.506 Prec@5 42.922
Best accuracy: 39.00%

Epoch: [131][0/1252]	Loss 4.3606 (4.3606)	Prec@1 19.922 (19.922)	Prec@5 39.648 (39.648)	LR: 1.0000000000000004e-06
Epoch: [131][1000/1252]	Loss 4.3393 (4.3586)	Prec@1 19.336 (19.515)	Prec@5 38.867 (38.221)	LR: 1.0000000000000004e-06
 * Prec@1 19.533 Prec@5 38.246
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5309 (3.5309)	Prec@1 30.664 (30.664)	Prec@5 53.125 (53.125)
 * Prec@1 22.706 Prec@5 43.122
Best accuracy: 39.00%

Epoch: [132][0/1252]	Loss 4.3499 (4.3499)	Prec@1 20.312 (20.312)	Prec@5 39.160 (39.160)	LR: 1.0000000000000004e-06
Epoch: [132][1000/1252]	Loss 4.2908 (4.3535)	Prec@1 19.629 (19.605)	Prec@5 40.234 (38.342)	LR: 1.0000000000000004e-06
 * Prec@1 19.618 Prec@5 38.341
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4625 (3.4625)	Prec@1 32.031 (32.031)	Prec@5 55.078 (55.078)
 * Prec@1 22.752 Prec@5 42.976
Best accuracy: 39.00%

Epoch: [133][0/1252]	Loss 4.4061 (4.4061)	Prec@1 17.578 (17.578)	Prec@5 36.719 (36.719)	LR: 1.0000000000000004e-06
Epoch: [133][1000/1252]	Loss 4.2471 (4.3566)	Prec@1 20.117 (19.548)	Prec@5 39.941 (38.253)	LR: 1.0000000000000004e-06
 * Prec@1 19.559 Prec@5 38.271
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4707 (3.4707)	Prec@1 31.445 (31.445)	Prec@5 54.297 (54.297)
 * Prec@1 22.578 Prec@5 42.822
Best accuracy: 39.00%

Epoch: [134][0/1252]	Loss 4.3958 (4.3958)	Prec@1 19.531 (19.531)	Prec@5 37.305 (37.305)	LR: 1.0000000000000004e-06
Epoch: [134][1000/1252]	Loss 4.3394 (4.3569)	Prec@1 20.605 (19.510)	Prec@5 39.062 (38.217)	LR: 1.0000000000000004e-06
 * Prec@1 19.551 Prec@5 38.257
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4323 (3.4323)	Prec@1 31.152 (31.152)	Prec@5 54.102 (54.102)
 * Prec@1 22.570 Prec@5 43.026
Best accuracy: 39.00%

Epoch: [135][0/1252]	Loss 4.3457 (4.3457)	Prec@1 20.996 (20.996)	Prec@5 37.793 (37.793)	LR: 1.0000000000000004e-06
Epoch: [135][1000/1252]	Loss 4.3226 (4.3549)	Prec@1 19.043 (19.575)	Prec@5 37.793 (38.243)	LR: 1.0000000000000004e-06
 * Prec@1 19.591 Prec@5 38.275
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4501 (3.4501)	Prec@1 31.738 (31.738)	Prec@5 53.418 (53.418)
 * Prec@1 22.642 Prec@5 42.900
Best accuracy: 39.00%

Epoch: [136][0/1252]	Loss 4.2268 (4.2268)	Prec@1 20.703 (20.703)	Prec@5 40.430 (40.430)	LR: 1.0000000000000004e-06
Epoch: [136][1000/1252]	Loss 4.5258 (4.3517)	Prec@1 17.578 (19.615)	Prec@5 35.352 (38.353)	LR: 1.0000000000000004e-06
 * Prec@1 19.625 Prec@5 38.351
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4271 (3.4271)	Prec@1 31.445 (31.445)	Prec@5 53.516 (53.516)
 * Prec@1 22.674 Prec@5 43.056
Best accuracy: 39.00%

Epoch: [137][0/1252]	Loss 4.4225 (4.4225)	Prec@1 18.555 (18.555)	Prec@5 36.426 (36.426)	LR: 1.0000000000000004e-06
Epoch: [137][1000/1252]	Loss 4.3225 (4.3525)	Prec@1 20.020 (19.600)	Prec@5 39.453 (38.334)	LR: 1.0000000000000004e-06
 * Prec@1 19.570 Prec@5 38.304
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4223 (3.4223)	Prec@1 32.617 (32.617)	Prec@5 54.980 (54.980)
 * Prec@1 22.554 Prec@5 42.810
Best accuracy: 39.00%

Epoch: [138][0/1252]	Loss 4.3535 (4.3535)	Prec@1 18.262 (18.262)	Prec@5 39.746 (39.746)	LR: 1.0000000000000004e-06
Epoch: [138][1000/1252]	Loss 4.3297 (4.3543)	Prec@1 19.141 (19.586)	Prec@5 38.574 (38.299)	LR: 1.0000000000000004e-06
 * Prec@1 19.583 Prec@5 38.262
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4981 (3.4981)	Prec@1 31.445 (31.445)	Prec@5 54.688 (54.688)
 * Prec@1 22.672 Prec@5 43.116
Best accuracy: 39.00%

Epoch: [139][0/1252]	Loss 4.2112 (4.2112)	Prec@1 22.852 (22.852)	Prec@5 40.918 (40.918)	LR: 1.0000000000000004e-06
Epoch: [139][1000/1252]	Loss 4.5218 (4.3676)	Prec@1 17.969 (19.440)	Prec@5 36.035 (38.070)	LR: 1.0000000000000004e-06
 * Prec@1 19.368 Prec@5 37.988
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4805 (3.4805)	Prec@1 31.152 (31.152)	Prec@5 53.516 (53.516)
 * Prec@1 22.238 Prec@5 42.530
Best accuracy: 39.00%

Epoch: [140][0/1252]	Loss 4.3500 (4.3500)	Prec@1 18.848 (18.848)	Prec@5 37.891 (37.891)	LR: 1.0000000000000004e-06
Epoch: [140][1000/1252]	Loss 4.2835 (4.3814)	Prec@1 19.238 (19.274)	Prec@5 39.355 (37.850)	LR: 1.0000000000000004e-06
 * Prec@1 19.256 Prec@5 37.834
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4794 (3.4794)	Prec@1 31.250 (31.250)	Prec@5 53.906 (53.906)
 * Prec@1 22.374 Prec@5 42.692
Best accuracy: 39.00%

Epoch: [141][0/1252]	Loss 4.4064 (4.4064)	Prec@1 17.578 (17.578)	Prec@5 37.012 (37.012)	LR: 1.0000000000000004e-06
Epoch: [141][1000/1252]	Loss 4.4323 (4.3774)	Prec@1 17.090 (19.316)	Prec@5 37.891 (37.899)	LR: 1.0000000000000004e-06
 * Prec@1 19.324 Prec@5 37.916
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5313 (3.5313)	Prec@1 31.348 (31.348)	Prec@5 52.246 (52.246)
 * Prec@1 22.248 Prec@5 42.624
Best accuracy: 39.00%

Epoch: [142][0/1252]	Loss 4.4015 (4.4015)	Prec@1 19.141 (19.141)	Prec@5 38.379 (38.379)	LR: 1.0000000000000004e-06
Epoch: [142][1000/1252]	Loss 4.2803 (4.3757)	Prec@1 21.777 (19.371)	Prec@5 39.258 (37.945)	LR: 1.0000000000000004e-06
 * Prec@1 19.360 Prec@5 37.956
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5278 (3.5278)	Prec@1 31.055 (31.055)	Prec@5 53.418 (53.418)
 * Prec@1 22.404 Prec@5 42.822
Best accuracy: 39.00%

Epoch: [143][0/1252]	Loss 4.3252 (4.3252)	Prec@1 20.898 (20.898)	Prec@5 38.574 (38.574)	LR: 1.0000000000000004e-06
Epoch: [143][1000/1252]	Loss 4.3770 (4.3741)	Prec@1 18.066 (19.327)	Prec@5 37.793 (37.903)	LR: 1.0000000000000004e-06
 * Prec@1 19.332 Prec@5 37.920
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5744 (3.5744)	Prec@1 31.445 (31.445)	Prec@5 53.418 (53.418)
 * Prec@1 22.422 Prec@5 42.592
Best accuracy: 39.00%

Epoch: [144][0/1252]	Loss 4.2728 (4.2728)	Prec@1 19.238 (19.238)	Prec@5 39.258 (39.258)	LR: 1.0000000000000004e-06
Epoch: [144][1000/1252]	Loss 4.3597 (4.3771)	Prec@1 20.215 (19.308)	Prec@5 38.281 (37.868)	LR: 1.0000000000000004e-06
 * Prec@1 19.345 Prec@5 37.920
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4840 (3.4840)	Prec@1 31.543 (31.543)	Prec@5 53.320 (53.320)
 * Prec@1 22.208 Prec@5 41.954
Best accuracy: 39.00%

Epoch: [145][0/1252]	Loss 4.3284 (4.3284)	Prec@1 17.773 (17.773)	Prec@5 38.965 (38.965)	LR: 1.0000000000000004e-06
Epoch: [145][1000/1252]	Loss 4.3598 (4.3734)	Prec@1 19.922 (19.428)	Prec@5 38.574 (37.975)	LR: 1.0000000000000004e-06
 * Prec@1 19.422 Prec@5 37.984
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5530 (3.5530)	Prec@1 30.273 (30.273)	Prec@5 53.809 (53.809)
 * Prec@1 22.098 Prec@5 42.034
Best accuracy: 39.00%

Epoch: [146][0/1252]	Loss 4.2769 (4.2769)	Prec@1 20.508 (20.508)	Prec@5 40.039 (40.039)	LR: 1.0000000000000004e-06
Epoch: [146][1000/1252]	Loss 4.4179 (4.3742)	Prec@1 19.336 (19.390)	Prec@5 37.988 (37.921)	LR: 1.0000000000000004e-06
 * Prec@1 19.407 Prec@5 37.962
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4157 (3.4157)	Prec@1 31.738 (31.738)	Prec@5 53.906 (53.906)
 * Prec@1 22.376 Prec@5 42.362
Best accuracy: 39.00%

Epoch: [147][0/1252]	Loss 4.4216 (4.4216)	Prec@1 20.215 (20.215)	Prec@5 37.695 (37.695)	LR: 1.0000000000000004e-06
Epoch: [147][1000/1252]	Loss 4.2962 (4.3687)	Prec@1 19.238 (19.415)	Prec@5 39.453 (38.044)	LR: 1.0000000000000004e-06
 * Prec@1 19.399 Prec@5 38.011
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4967 (3.4967)	Prec@1 31.055 (31.055)	Prec@5 54.980 (54.980)
 * Prec@1 22.638 Prec@5 43.128
Best accuracy: 39.00%

Epoch: [148][0/1252]	Loss 4.3586 (4.3586)	Prec@1 19.629 (19.629)	Prec@5 37.402 (37.402)	LR: 1.0000000000000004e-06
Epoch: [148][1000/1252]	Loss 4.2766 (4.3702)	Prec@1 19.043 (19.353)	Prec@5 40.137 (38.010)	LR: 1.0000000000000004e-06
 * Prec@1 19.363 Prec@5 38.036
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.4961 (3.4961)	Prec@1 32.227 (32.227)	Prec@5 53.320 (53.320)
 * Prec@1 22.562 Prec@5 42.922
Best accuracy: 39.00%

Epoch: [149][0/1252]	Loss 4.3832 (4.3832)	Prec@1 18.945 (18.945)	Prec@5 37.793 (37.793)	LR: 1.0000000000000004e-06
Epoch: [149][1000/1252]	Loss 4.2851 (4.3725)	Prec@1 21.484 (19.414)	Prec@5 39.160 (38.037)	LR: 1.0000000000000004e-06
 * Prec@1 19.427 Prec@5 38.062
Best Train Accuracy: 35.61%

Test: [0/49]	Loss 3.5117 (3.5117)	Prec@1 30.176 (30.176)	Prec@5 53.320 (53.320)
 * Prec@1 22.356 Prec@5 42.692
Best accuracy: 39.00%

