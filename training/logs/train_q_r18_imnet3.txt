
      ==> Arguments:
          dataset: imagenet
          model: resnet18
          workers: 8
          epochs: 150
          start_epoch: 0
          batch_size: 512
          lr: 0.1
          momentum: 0.9
          weight_decay: 0.0005
          tag: qfp_i8b7f_w8b7f_3
          milestones: [40, 80, 120]
          gamma: 0.1
          input_size: None
          print_freq: 1000
          resume: 
          evaluate: False
          pretrained: None
          half: False
          savedir: ../pretrained_models/ideal/
          save_every: 10
          gpus: 0,1,2
DEVICE: cuda
GPU Id(s) being used: 0,1,2
==> Building model for resnet18 ...
ResNet18(
  (fq0): activation_quantize_fn()
  (conv1): Conv2d_Q(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU(inplace=True)
  (fq1): activation_quantize_fn()
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU(inplace=True)
  (fq2): activation_quantize_fn()
  (conv3): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu3): ReLU(inplace=True)
  (fq3): activation_quantize_fn()
  (conv4): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu4): ReLU(inplace=True)
  (fq4): activation_quantize_fn()
  (conv5): Conv2d_Q(
    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu5): ReLU(inplace=True)
  (fq5): activation_quantize_fn()
  (conv6): Conv2d_Q(
    64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr1): activation_quantize_fn()
  (resconv1): Sequential(
    (0): Conv2d_Q(
      64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu6): ReLU(inplace=True)
  (fq6): activation_quantize_fn()
  (conv7): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu7): ReLU(inplace=True)
  (fq7): activation_quantize_fn()
  (conv8): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu8): ReLU(inplace=True)
  (fq8): activation_quantize_fn()
  (conv9): Conv2d_Q(
    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu9): ReLU(inplace=True)
  (fq9): activation_quantize_fn()
  (conv10): Conv2d_Q(
    128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr2): activation_quantize_fn()
  (resconv2): Sequential(
    (0): Conv2d_Q(
      128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu10): ReLU(inplace=True)
  (fq10): activation_quantize_fn()
  (conv11): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu11): ReLU(inplace=True)
  (fq11): activation_quantize_fn()
  (conv12): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu12): ReLU(inplace=True)
  (fq12): activation_quantize_fn()
  (conv13): Conv2d_Q(
    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu13): ReLU(inplace=True)
  (fq13): activation_quantize_fn()
  (conv14): Conv2d_Q(
    256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fqr3): activation_quantize_fn()
  (resconv3): Sequential(
    (0): Conv2d_Q(
      256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
      (quantize_fn): weight_quantize_fn()
    )
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (relu14): ReLU(inplace=True)
  (fq14): activation_quantize_fn()
  (conv15): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu15): ReLU(inplace=True)
  (fq15): activation_quantize_fn()
  (conv16): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu16): ReLU(inplace=True)
  (fq16): activation_quantize_fn()
  (conv17): Conv2d_Q(
    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu17): ReLU(inplace=True)
  (fq17): activation_quantize_fn()
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (bn18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fq18): activation_quantize_fn()
  (fc): Linear_Q(
    in_features=512, out_features=1000, bias=False
    (quantize_fn): weight_quantize_fn()
  )
  (bn19): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (logsoftmax): LogSoftmax(dim=1)
)
Intializing model with normal distribution...
Test: [0/98]	Loss 6.9077 (6.9077)	Prec@1 0.195 (0.195)	Prec@5 0.977 (0.977)
 * Prec@1 0.118 Prec@5 0.516
Pretrained model accuracy: 0.11800000071525574
Epoch: [0][0/2503]	Loss 6.9086 (6.9086)	Prec@1 0.000 (0.000)	Prec@5 0.586 (0.586)	LR: 0.1
Epoch: [0][1000/2503]	Loss 6.8451 (6.8794)	Prec@1 0.586 (0.169)	Prec@5 0.781 (0.851)	LR: 0.1
Epoch: [0][2000/2503]	Loss 5.9069 (6.6533)	Prec@1 5.078 (0.900)	Prec@5 13.477 (3.286)	LR: 0.1
 * Prec@1 1.891 Prec@5 5.951
Best Train Accuracy: 1.89%

Test: [0/98]	Loss 5.0400 (5.0400)	Prec@1 11.328 (11.328)	Prec@5 34.570 (34.570)
 * Prec@1 6.396 Prec@5 17.848
Best accuracy: 6.40%

Epoch: [1][0/2503]	Loss 5.5316 (5.5316)	Prec@1 6.250 (6.250)	Prec@5 17.773 (17.773)	LR: 0.1
Epoch: [1][1000/2503]	Loss 4.7836 (5.1390)	Prec@1 12.891 (10.602)	Prec@5 31.250 (25.691)	LR: 0.1
Epoch: [1][2000/2503]	Loss 4.5875 (4.9063)	Prec@1 16.602 (12.946)	Prec@5 36.328 (29.720)	LR: 0.1
 * Prec@1 13.917 Prec@5 31.321
Best Train Accuracy: 13.92%

Test: [0/98]	Loss 4.0077 (4.0077)	Prec@1 23.242 (23.242)	Prec@5 51.562 (51.562)
 * Prec@1 15.428 Prec@5 33.802
Best accuracy: 15.43%

Epoch: [2][0/2503]	Loss 4.5325 (4.5325)	Prec@1 17.188 (17.188)	Prec@5 37.500 (37.500)	LR: 0.1
Epoch: [2][1000/2503]	Loss 4.1301 (4.2889)	Prec@1 20.117 (19.811)	Prec@5 42.188 (40.749)	LR: 0.1
Epoch: [2][2000/2503]	Loss 4.1549 (4.2175)	Prec@1 18.945 (20.736)	Prec@5 43.750 (42.032)	LR: 0.1
 * Prec@1 21.141 Prec@5 42.620
Best Train Accuracy: 21.14%

Test: [0/98]	Loss 4.0825 (4.0825)	Prec@1 21.875 (21.875)	Prec@5 45.898 (45.898)
 * Prec@1 13.634 Prec@5 29.622
Best accuracy: 15.43%

Epoch: [3][0/2503]	Loss 4.1696 (4.1696)	Prec@1 22.852 (22.852)	Prec@5 44.531 (44.531)	LR: 0.1
Epoch: [3][1000/2503]	Loss 3.9519 (3.9867)	Prec@1 25.195 (23.799)	Prec@5 44.336 (46.271)	LR: 0.1
Epoch: [3][2000/2503]	Loss 3.8268 (3.9595)	Prec@1 26.758 (24.090)	Prec@5 50.000 (46.727)	LR: 0.1
 * Prec@1 24.277 Prec@5 46.961
Best Train Accuracy: 24.28%

Test: [0/98]	Loss 3.7020 (3.7020)	Prec@1 23.438 (23.438)	Prec@5 54.688 (54.688)
 * Prec@1 21.624 Prec@5 44.044
Best accuracy: 21.62%

Epoch: [4][0/2503]	Loss 4.0414 (4.0414)	Prec@1 23.047 (23.047)	Prec@5 48.047 (48.047)	LR: 0.1
Epoch: [4][1000/2503]	Loss 3.9239 (3.8526)	Prec@1 25.195 (25.532)	Prec@5 45.898 (48.729)	LR: 0.1
Epoch: [4][2000/2503]	Loss 4.0251 (3.8363)	Prec@1 23.242 (25.745)	Prec@5 46.289 (48.984)	LR: 0.1
 * Prec@1 25.859 Prec@5 49.149
Best Train Accuracy: 25.86%

Test: [0/98]	Loss 3.2608 (3.2608)	Prec@1 35.938 (35.938)	Prec@5 62.305 (62.305)
 * Prec@1 21.632 Prec@5 43.288
Best accuracy: 21.63%

Epoch: [5][0/2503]	Loss 3.9379 (3.9379)	Prec@1 23.828 (23.828)	Prec@5 45.703 (45.703)	LR: 0.1
Epoch: [5][1000/2503]	Loss 3.5838 (3.7724)	Prec@1 26.953 (26.609)	Prec@5 52.344 (50.143)	LR: 0.1
Epoch: [5][2000/2503]	Loss 3.6200 (3.7672)	Prec@1 28.516 (26.686)	Prec@5 51.953 (50.241)	LR: 0.1
 * Prec@1 26.725 Prec@5 50.295
Best Train Accuracy: 26.73%

Test: [0/98]	Loss 3.5361 (3.5361)	Prec@1 30.664 (30.664)	Prec@5 53.516 (53.516)
 * Prec@1 23.632 Prec@5 46.128
Best accuracy: 23.63%

Epoch: [6][0/2503]	Loss 3.8251 (3.8251)	Prec@1 24.219 (24.219)	Prec@5 49.609 (49.609)	LR: 0.1
Epoch: [6][1000/2503]	Loss 3.7657 (3.7198)	Prec@1 27.930 (27.335)	Prec@5 49.414 (51.054)	LR: 0.1
Epoch: [6][2000/2503]	Loss 3.7661 (3.7196)	Prec@1 21.680 (27.356)	Prec@5 51.758 (51.069)	LR: 0.1
 * Prec@1 27.355 Prec@5 51.110
Best Train Accuracy: 27.35%

Test: [0/98]	Loss 3.6599 (3.6599)	Prec@1 25.977 (25.977)	Prec@5 54.102 (54.102)
 * Prec@1 19.188 Prec@5 40.138
Best accuracy: 23.63%

Epoch: [7][0/2503]	Loss 3.6505 (3.6505)	Prec@1 28.711 (28.711)	Prec@5 51.953 (51.953)	LR: 0.1
Epoch: [7][1000/2503]	Loss 3.5148 (3.6839)	Prec@1 29.297 (27.716)	Prec@5 55.664 (51.733)	LR: 0.1
Epoch: [7][2000/2503]	Loss 3.6809 (3.6817)	Prec@1 28.516 (27.767)	Prec@5 53.906 (51.761)	LR: 0.1
 * Prec@1 27.781 Prec@5 51.785
Best Train Accuracy: 27.78%

Test: [0/98]	Loss 3.1976 (3.1976)	Prec@1 34.375 (34.375)	Prec@5 62.305 (62.305)
 * Prec@1 23.006 Prec@5 45.834
Best accuracy: 23.63%

Epoch: [8][0/2503]	Loss 3.6383 (3.6383)	Prec@1 29.688 (29.688)	Prec@5 51.562 (51.562)	LR: 0.1
Epoch: [8][1000/2503]	Loss 3.6058 (3.6516)	Prec@1 29.492 (28.241)	Prec@5 51.953 (52.311)	LR: 0.1
Epoch: [8][2000/2503]	Loss 3.7321 (3.6495)	Prec@1 23.828 (28.282)	Prec@5 49.805 (52.367)	LR: 0.1
 * Prec@1 28.292 Prec@5 52.369
Best Train Accuracy: 28.29%

Test: [0/98]	Loss 2.8072 (2.8072)	Prec@1 41.602 (41.602)	Prec@5 68.945 (68.945)
 * Prec@1 24.904 Prec@5 48.482
Best accuracy: 24.90%

Epoch: [9][0/2503]	Loss 3.8165 (3.8165)	Prec@1 30.078 (30.078)	Prec@5 51.367 (51.367)	LR: 0.1
Epoch: [9][1000/2503]	Loss 3.4871 (3.6282)	Prec@1 28.125 (28.526)	Prec@5 56.641 (52.691)	LR: 0.1
Epoch: [9][2000/2503]	Loss 3.6706 (3.6255)	Prec@1 27.734 (28.598)	Prec@5 50.977 (52.745)	LR: 0.1
 * Prec@1 28.580 Prec@5 52.749
Best Train Accuracy: 28.58%

Test: [0/98]	Loss 3.0381 (3.0381)	Prec@1 35.352 (35.352)	Prec@5 66.406 (66.406)
 * Prec@1 23.986 Prec@5 47.580
Best accuracy: 24.90%

Epoch: [10][0/2503]	Loss 3.4343 (3.4343)	Prec@1 30.273 (30.273)	Prec@5 56.250 (56.250)	LR: 0.1
Epoch: [10][1000/2503]	Loss 3.6114 (3.6040)	Prec@1 28.906 (28.839)	Prec@5 54.102 (53.123)	LR: 0.1
Epoch: [10][2000/2503]	Loss 3.5210 (3.6048)	Prec@1 30.859 (28.815)	Prec@5 55.859 (53.135)	LR: 0.1
 * Prec@1 28.830 Prec@5 53.143
Best Train Accuracy: 28.83%

Test: [0/98]	Loss 3.1598 (3.1598)	Prec@1 35.547 (35.547)	Prec@5 63.086 (63.086)
 * Prec@1 24.306 Prec@5 47.328
Best accuracy: 24.90%

Epoch: [11][0/2503]	Loss 3.7829 (3.7829)	Prec@1 27.148 (27.148)	Prec@5 50.195 (50.195)	LR: 0.1
Epoch: [11][1000/2503]	Loss 3.6829 (3.5851)	Prec@1 25.781 (29.066)	Prec@5 51.172 (53.420)	LR: 0.1
Epoch: [11][2000/2503]	Loss 3.6300 (3.5832)	Prec@1 27.148 (29.123)	Prec@5 52.344 (53.462)	LR: 0.1
 * Prec@1 29.113 Prec@5 53.443
Best Train Accuracy: 29.11%

Test: [0/98]	Loss 3.0657 (3.0657)	Prec@1 38.477 (38.477)	Prec@5 65.234 (65.234)
 * Prec@1 27.084 Prec@5 51.132
Best accuracy: 27.08%

Epoch: [12][0/2503]	Loss 3.5527 (3.5527)	Prec@1 29.492 (29.492)	Prec@5 53.516 (53.516)	LR: 0.1
Epoch: [12][1000/2503]	Loss 3.6373 (3.5633)	Prec@1 27.344 (29.367)	Prec@5 52.148 (53.839)	LR: 0.1
Epoch: [12][2000/2503]	Loss 3.5754 (3.5648)	Prec@1 29.297 (29.347)	Prec@5 54.688 (53.768)	LR: 0.1
 * Prec@1 29.363 Prec@5 53.737
Best Train Accuracy: 29.36%

Test: [0/98]	Loss 3.0456 (3.0456)	Prec@1 36.133 (36.133)	Prec@5 64.648 (64.648)
 * Prec@1 21.060 Prec@5 41.774
Best accuracy: 27.08%

Epoch: [13][0/2503]	Loss 3.4752 (3.4752)	Prec@1 34.180 (34.180)	Prec@5 55.469 (55.469)	LR: 0.1
Epoch: [13][1000/2503]	Loss 3.4259 (3.5485)	Prec@1 28.320 (29.609)	Prec@5 57.227 (53.979)	LR: 0.1
Epoch: [13][2000/2503]	Loss 3.6391 (3.5518)	Prec@1 28.711 (29.560)	Prec@5 52.344 (53.965)	LR: 0.1
 * Prec@1 29.550 Prec@5 53.956
Best Train Accuracy: 29.55%

Test: [0/98]	Loss 3.0518 (3.0518)	Prec@1 36.133 (36.133)	Prec@5 63.867 (63.867)
 * Prec@1 26.546 Prec@5 50.494
Best accuracy: 27.08%

Epoch: [14][0/2503]	Loss 3.7481 (3.7481)	Prec@1 26.758 (26.758)	Prec@5 50.586 (50.586)	LR: 0.1
Epoch: [14][1000/2503]	Loss 3.4987 (3.5387)	Prec@1 31.445 (29.698)	Prec@5 57.031 (54.185)	LR: 0.1
Epoch: [14][2000/2503]	Loss 3.5163 (3.5434)	Prec@1 28.516 (29.646)	Prec@5 54.102 (54.119)	LR: 0.1
 * Prec@1 29.660 Prec@5 54.108
Best Train Accuracy: 29.66%

Test: [0/98]	Loss 3.2057 (3.2057)	Prec@1 30.078 (30.078)	Prec@5 62.305 (62.305)
 * Prec@1 20.840 Prec@5 41.960
Best accuracy: 27.08%

Epoch: [15][0/2503]	Loss 3.6851 (3.6851)	Prec@1 27.148 (27.148)	Prec@5 50.586 (50.586)	LR: 0.1
Epoch: [15][1000/2503]	Loss 3.6163 (3.5288)	Prec@1 30.469 (29.811)	Prec@5 53.125 (54.440)	LR: 0.1
Epoch: [15][2000/2503]	Loss 3.7007 (3.5316)	Prec@1 25.195 (29.788)	Prec@5 49.414 (54.367)	LR: 0.1
 * Prec@1 29.781 Prec@5 54.350
Best Train Accuracy: 29.78%

Test: [0/98]	Loss 2.4057 (2.4057)	Prec@1 44.141 (44.141)	Prec@5 73.438 (73.438)
 * Prec@1 29.168 Prec@5 54.326
Best accuracy: 29.17%

Epoch: [16][0/2503]	Loss 3.4652 (3.4652)	Prec@1 30.469 (30.469)	Prec@5 54.102 (54.102)	LR: 0.1
Epoch: [16][1000/2503]	Loss 3.5048 (3.5204)	Prec@1 31.250 (29.957)	Prec@5 57.031 (54.451)	LR: 0.1
Epoch: [16][2000/2503]	Loss 3.5607 (3.5263)	Prec@1 27.344 (29.847)	Prec@5 52.344 (54.377)	LR: 0.1
 * Prec@1 29.821 Prec@5 54.388
Best Train Accuracy: 29.82%

Test: [0/98]	Loss 3.0927 (3.0927)	Prec@1 33.789 (33.789)	Prec@5 66.406 (66.406)
 * Prec@1 23.540 Prec@5 46.766
Best accuracy: 29.17%

Epoch: [17][0/2503]	Loss 3.6182 (3.6182)	Prec@1 26.953 (26.953)	Prec@5 52.148 (52.148)	LR: 0.1
Epoch: [17][1000/2503]	Loss 3.5263 (3.5186)	Prec@1 28.125 (29.912)	Prec@5 53.711 (54.529)	LR: 0.1
Epoch: [17][2000/2503]	Loss 3.5225 (3.5222)	Prec@1 27.148 (29.865)	Prec@5 54.688 (54.453)	LR: 0.1
 * Prec@1 29.848 Prec@5 54.439
Best Train Accuracy: 29.85%

Test: [0/98]	Loss 3.7554 (3.7554)	Prec@1 28.516 (28.516)	Prec@5 54.688 (54.688)
 * Prec@1 21.444 Prec@5 42.660
Best accuracy: 29.17%

Epoch: [18][0/2503]	Loss 3.6889 (3.6889)	Prec@1 25.391 (25.391)	Prec@5 51.367 (51.367)	LR: 0.1
Epoch: [18][1000/2503]	Loss 3.5159 (3.5202)	Prec@1 31.055 (29.820)	Prec@5 54.883 (54.462)	LR: 0.1
Epoch: [18][2000/2503]	Loss 3.4694 (3.5169)	Prec@1 30.273 (29.936)	Prec@5 53.320 (54.528)	LR: 0.1
 * Prec@1 29.936 Prec@5 54.530
Best Train Accuracy: 29.94%

Test: [0/98]	Loss 2.9684 (2.9684)	Prec@1 35.352 (35.352)	Prec@5 63.672 (63.672)
 * Prec@1 23.214 Prec@5 46.404
Best accuracy: 29.17%

Epoch: [19][0/2503]	Loss 3.5939 (3.5939)	Prec@1 27.734 (27.734)	Prec@5 51.953 (51.953)	LR: 0.1
Epoch: [19][1000/2503]	Loss 3.5464 (3.5083)	Prec@1 33.984 (30.088)	Prec@5 55.078 (54.719)	LR: 0.1
Epoch: [19][2000/2503]	Loss 3.4479 (3.5090)	Prec@1 30.664 (30.067)	Prec@5 56.836 (54.696)	LR: 0.1
 * Prec@1 30.085 Prec@5 54.710
Best Train Accuracy: 30.09%

Test: [0/98]	Loss 2.5607 (2.5607)	Prec@1 38.867 (38.867)	Prec@5 72.656 (72.656)
 * Prec@1 24.056 Prec@5 47.540
Best accuracy: 29.17%

Epoch: [20][0/2503]	Loss 3.4631 (3.4631)	Prec@1 28.906 (28.906)	Prec@5 55.469 (55.469)	LR: 0.1
Epoch: [20][1000/2503]	Loss 3.4862 (3.4985)	Prec@1 29.883 (30.123)	Prec@5 54.883 (54.896)	LR: 0.1
Epoch: [20][2000/2503]	Loss 3.4350 (3.5033)	Prec@1 29.492 (30.106)	Prec@5 54.297 (54.803)	LR: 0.1
 * Prec@1 30.107 Prec@5 54.780
Best Train Accuracy: 30.11%

Test: [0/98]	Loss 3.6947 (3.6947)	Prec@1 28.906 (28.906)	Prec@5 52.148 (52.148)
 * Prec@1 19.228 Prec@5 38.686
Best accuracy: 29.17%

Epoch: [21][0/2503]	Loss 3.5830 (3.5830)	Prec@1 30.859 (30.859)	Prec@5 50.977 (50.977)	LR: 0.1
Epoch: [21][1000/2503]	Loss 3.4320 (3.4946)	Prec@1 30.469 (30.315)	Prec@5 56.445 (54.971)	LR: 0.1
Epoch: [21][2000/2503]	Loss 3.3399 (3.4985)	Prec@1 31.445 (30.244)	Prec@5 58.594 (54.911)	LR: 0.1
 * Prec@1 30.228 Prec@5 54.900
Best Train Accuracy: 30.23%

Test: [0/98]	Loss 2.3256 (2.3256)	Prec@1 46.289 (46.289)	Prec@5 77.734 (77.734)
 * Prec@1 26.486 Prec@5 50.762
Best accuracy: 29.17%

Epoch: [22][0/2503]	Loss 3.5138 (3.5138)	Prec@1 30.859 (30.859)	Prec@5 55.078 (55.078)	LR: 0.1
Epoch: [22][1000/2503]	Loss 3.5203 (3.4926)	Prec@1 29.688 (30.253)	Prec@5 54.688 (55.057)	LR: 0.1
Epoch: [22][2000/2503]	Loss 3.4720 (3.4948)	Prec@1 30.078 (30.248)	Prec@5 55.273 (54.995)	LR: 0.1
 * Prec@1 30.196 Prec@5 54.945
Best Train Accuracy: 30.23%

Test: [0/98]	Loss 2.9446 (2.9446)	Prec@1 36.719 (36.719)	Prec@5 64.648 (64.648)
 * Prec@1 24.432 Prec@5 48.248
Best accuracy: 29.17%

Epoch: [23][0/2503]	Loss 3.5916 (3.5916)	Prec@1 32.031 (32.031)	Prec@5 51.953 (51.953)	LR: 0.1
Epoch: [23][1000/2503]	Loss 3.3992 (3.4869)	Prec@1 31.055 (30.286)	Prec@5 55.859 (55.055)	LR: 0.1
Epoch: [23][2000/2503]	Loss 3.3699 (3.4902)	Prec@1 33.398 (30.293)	Prec@5 57.812 (55.002)	LR: 0.1
 * Prec@1 30.286 Prec@5 55.002
Best Train Accuracy: 30.29%

Test: [0/98]	Loss 3.3953 (3.3953)	Prec@1 32.031 (32.031)	Prec@5 57.031 (57.031)
 * Prec@1 21.746 Prec@5 43.988
Best accuracy: 29.17%

Epoch: [24][0/2503]	Loss 3.6787 (3.6787)	Prec@1 28.125 (28.125)	Prec@5 52.930 (52.930)	LR: 0.1
Epoch: [24][1000/2503]	Loss 3.5322 (3.4794)	Prec@1 30.273 (30.459)	Prec@5 55.078 (55.214)	LR: 0.1
Epoch: [24][2000/2503]	Loss 3.5447 (3.4863)	Prec@1 31.250 (30.357)	Prec@5 53.516 (55.108)	LR: 0.1
 * Prec@1 30.334 Prec@5 55.075
Best Train Accuracy: 30.33%

Test: [0/98]	Loss 3.9786 (3.9786)	Prec@1 26.758 (26.758)	Prec@5 48.633 (48.633)
 * Prec@1 23.418 Prec@5 46.454
Best accuracy: 29.17%

Epoch: [25][0/2503]	Loss 3.6902 (3.6902)	Prec@1 27.539 (27.539)	Prec@5 53.516 (53.516)	LR: 0.1
Epoch: [25][1000/2503]	Loss 3.3552 (3.4746)	Prec@1 31.641 (30.523)	Prec@5 58.203 (55.359)	LR: 0.1
Epoch: [25][2000/2503]	Loss 3.3499 (3.4837)	Prec@1 30.078 (30.424)	Prec@5 56.055 (55.188)	LR: 0.1
 * Prec@1 30.399 Prec@5 55.166
Best Train Accuracy: 30.40%

Test: [0/98]	Loss 3.1718 (3.1718)	Prec@1 37.305 (37.305)	Prec@5 65.430 (65.430)
 * Prec@1 29.112 Prec@5 53.724
Best accuracy: 29.17%

Epoch: [26][0/2503]	Loss 3.5891 (3.5891)	Prec@1 28.711 (28.711)	Prec@5 52.539 (52.539)	LR: 0.1
Epoch: [26][1000/2503]	Loss 3.5877 (3.4800)	Prec@1 29.883 (30.438)	Prec@5 52.344 (55.244)	LR: 0.1
Epoch: [26][2000/2503]	Loss 3.6080 (3.4824)	Prec@1 29.102 (30.442)	Prec@5 52.930 (55.200)	LR: 0.1
 * Prec@1 30.392 Prec@5 55.145
Best Train Accuracy: 30.40%

Test: [0/98]	Loss 3.1962 (3.1962)	Prec@1 35.352 (35.352)	Prec@5 61.523 (61.523)
 * Prec@1 18.436 Prec@5 37.586
Best accuracy: 29.17%

Epoch: [27][0/2503]	Loss 3.6997 (3.6997)	Prec@1 24.219 (24.219)	Prec@5 51.758 (51.758)	LR: 0.1
Epoch: [27][1000/2503]	Loss 3.4246 (3.4761)	Prec@1 32.031 (30.586)	Prec@5 56.836 (55.254)	LR: 0.1
Epoch: [27][2000/2503]	Loss 3.4969 (3.4786)	Prec@1 27.930 (30.528)	Prec@5 54.297 (55.220)	LR: 0.1
 * Prec@1 30.479 Prec@5 55.193
Best Train Accuracy: 30.48%

Test: [0/98]	Loss 3.2011 (3.2011)	Prec@1 32.227 (32.227)	Prec@5 63.086 (63.086)
 * Prec@1 23.522 Prec@5 46.458
Best accuracy: 29.17%

Epoch: [28][0/2503]	Loss 3.5691 (3.5691)	Prec@1 30.664 (30.664)	Prec@5 52.148 (52.148)	LR: 0.1
Epoch: [28][1000/2503]	Loss 3.3944 (3.4711)	Prec@1 32.617 (30.552)	Prec@5 58.203 (55.395)	LR: 0.1
Epoch: [28][2000/2503]	Loss 3.4483 (3.4742)	Prec@1 30.469 (30.558)	Prec@5 56.836 (55.311)	LR: 0.1
 * Prec@1 30.519 Prec@5 55.284
Best Train Accuracy: 30.52%

Test: [0/98]	Loss 3.3439 (3.3439)	Prec@1 35.352 (35.352)	Prec@5 60.547 (60.547)
 * Prec@1 26.632 Prec@5 51.306
Best accuracy: 29.17%

Epoch: [29][0/2503]	Loss 3.5662 (3.5662)	Prec@1 30.859 (30.859)	Prec@5 55.078 (55.078)	LR: 0.1
Epoch: [29][1000/2503]	Loss 3.4150 (3.4712)	Prec@1 33.398 (30.656)	Prec@5 58.008 (55.398)	LR: 0.1
Epoch: [29][2000/2503]	Loss 3.6815 (3.4724)	Prec@1 25.000 (30.606)	Prec@5 51.367 (55.373)	LR: 0.1
 * Prec@1 30.585 Prec@5 55.332
Best Train Accuracy: 30.58%

Test: [0/98]	Loss 2.4285 (2.4285)	Prec@1 45.703 (45.703)	Prec@5 74.805 (74.805)
 * Prec@1 30.362 Prec@5 55.330
Best accuracy: 30.36%

Epoch: [30][0/2503]	Loss 3.6456 (3.6456)	Prec@1 29.883 (29.883)	Prec@5 52.734 (52.734)	LR: 0.1
Epoch: [30][1000/2503]	Loss 3.3943 (3.4662)	Prec@1 30.859 (30.636)	Prec@5 56.836 (55.447)	LR: 0.1
Epoch: [30][2000/2503]	Loss 3.5057 (3.4671)	Prec@1 30.273 (30.597)	Prec@5 55.078 (55.419)	LR: 0.1
 * Prec@1 30.590 Prec@5 55.391
Best Train Accuracy: 30.59%

Test: [0/98]	Loss 3.0192 (3.0192)	Prec@1 36.523 (36.523)	Prec@5 66.211 (66.211)
 * Prec@1 23.804 Prec@5 47.006
Best accuracy: 30.36%

Epoch: [31][0/2503]	Loss 3.6229 (3.6229)	Prec@1 27.344 (27.344)	Prec@5 52.930 (52.930)	LR: 0.1
Epoch: [31][1000/2503]	Loss 3.4885 (3.4659)	Prec@1 30.078 (30.622)	Prec@5 56.836 (55.531)	LR: 0.1
Epoch: [31][2000/2503]	Loss 3.3813 (3.4675)	Prec@1 33.008 (30.622)	Prec@5 56.641 (55.448)	LR: 0.1
 * Prec@1 30.637 Prec@5 55.449
Best Train Accuracy: 30.64%

Test: [0/98]	Loss 2.5249 (2.5249)	Prec@1 44.336 (44.336)	Prec@5 72.461 (72.461)
 * Prec@1 30.624 Prec@5 56.112
Best accuracy: 30.62%

Epoch: [32][0/2503]	Loss 3.3134 (3.3134)	Prec@1 34.375 (34.375)	Prec@5 58.984 (58.984)	LR: 0.1
Epoch: [32][1000/2503]	Loss 3.6003 (3.4608)	Prec@1 26.953 (30.754)	Prec@5 52.539 (55.530)	LR: 0.1
Epoch: [32][2000/2503]	Loss 3.3686 (3.4657)	Prec@1 30.664 (30.664)	Prec@5 58.008 (55.438)	LR: 0.1
 * Prec@1 30.682 Prec@5 55.440
Best Train Accuracy: 30.68%

Test: [0/98]	Loss 3.2870 (3.2870)	Prec@1 32.812 (32.812)	Prec@5 61.133 (61.133)
 * Prec@1 23.362 Prec@5 45.914
Best accuracy: 30.62%

Epoch: [33][0/2503]	Loss 3.3789 (3.3789)	Prec@1 31.250 (31.250)	Prec@5 56.836 (56.836)	LR: 0.1
Epoch: [33][1000/2503]	Loss 3.4614 (3.4499)	Prec@1 30.469 (30.901)	Prec@5 54.883 (55.754)	LR: 0.1
Epoch: [33][2000/2503]	Loss 3.3944 (3.4613)	Prec@1 31.445 (30.721)	Prec@5 57.031 (55.545)	LR: 0.1
 * Prec@1 30.722 Prec@5 55.510
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.7292 (2.7292)	Prec@1 43.164 (43.164)	Prec@5 70.703 (70.703)
 * Prec@1 27.496 Prec@5 52.122
Best accuracy: 30.62%

Epoch: [34][0/2503]	Loss 3.3904 (3.3904)	Prec@1 33.008 (33.008)	Prec@5 57.031 (57.031)	LR: 0.1
Epoch: [34][1000/2503]	Loss 3.5428 (3.4607)	Prec@1 28.125 (30.683)	Prec@5 53.125 (55.545)	LR: 0.1
Epoch: [34][2000/2503]	Loss 3.3639 (3.4646)	Prec@1 32.227 (30.661)	Prec@5 57.422 (55.507)	LR: 0.1
 * Prec@1 30.636 Prec@5 55.483
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.8630 (2.8630)	Prec@1 37.109 (37.109)	Prec@5 67.188 (67.188)
 * Prec@1 26.050 Prec@5 49.862
Best accuracy: 30.62%

Epoch: [35][0/2503]	Loss 3.4634 (3.4634)	Prec@1 29.297 (29.297)	Prec@5 55.859 (55.859)	LR: 0.1
Epoch: [35][1000/2503]	Loss 3.3166 (3.4602)	Prec@1 31.250 (30.694)	Prec@5 59.961 (55.556)	LR: 0.1
Epoch: [35][2000/2503]	Loss 3.4342 (3.4634)	Prec@1 31.641 (30.648)	Prec@5 54.883 (55.484)	LR: 0.1
 * Prec@1 30.623 Prec@5 55.450
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.9458 (2.9458)	Prec@1 37.891 (37.891)	Prec@5 66.602 (66.602)
 * Prec@1 26.356 Prec@5 50.326
Best accuracy: 30.62%

Epoch: [36][0/2503]	Loss 3.5384 (3.5384)	Prec@1 31.836 (31.836)	Prec@5 57.031 (57.031)	LR: 0.1
Epoch: [36][1000/2503]	Loss 3.5671 (3.4602)	Prec@1 30.469 (30.758)	Prec@5 53.711 (55.490)	LR: 0.1
Epoch: [36][2000/2503]	Loss 3.4831 (3.4665)	Prec@1 30.273 (30.634)	Prec@5 53.906 (55.382)	LR: 0.1
 * Prec@1 30.604 Prec@5 55.364
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.7781 (2.7781)	Prec@1 41.016 (41.016)	Prec@5 71.875 (71.875)
 * Prec@1 28.960 Prec@5 53.714
Best accuracy: 30.62%

Epoch: [37][0/2503]	Loss 3.4571 (3.4571)	Prec@1 30.859 (30.859)	Prec@5 55.273 (55.273)	LR: 0.1
Epoch: [37][1000/2503]	Loss 3.2886 (3.4612)	Prec@1 34.180 (30.692)	Prec@5 59.766 (55.504)	LR: 0.1
Epoch: [37][2000/2503]	Loss 3.6753 (3.4656)	Prec@1 27.734 (30.621)	Prec@5 52.344 (55.453)	LR: 0.1
 * Prec@1 30.628 Prec@5 55.450
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.8062 (2.8062)	Prec@1 36.328 (36.328)	Prec@5 67.969 (67.969)
 * Prec@1 27.982 Prec@5 52.324
Best accuracy: 30.62%

Epoch: [38][0/2503]	Loss 3.5074 (3.5074)	Prec@1 29.883 (29.883)	Prec@5 55.664 (55.664)	LR: 0.1
Epoch: [38][1000/2503]	Loss 3.4332 (3.4596)	Prec@1 32.031 (30.782)	Prec@5 54.688 (55.563)	LR: 0.1
Epoch: [38][2000/2503]	Loss 3.5982 (3.4638)	Prec@1 26.758 (30.663)	Prec@5 52.344 (55.508)	LR: 0.1
 * Prec@1 30.630 Prec@5 55.485
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 2.9123 (2.9123)	Prec@1 37.109 (37.109)	Prec@5 69.922 (69.922)
 * Prec@1 28.166 Prec@5 52.360
Best accuracy: 30.62%

Epoch: [39][0/2503]	Loss 3.5930 (3.5930)	Prec@1 30.859 (30.859)	Prec@5 51.172 (51.172)	LR: 0.1
Epoch: [39][1000/2503]	Loss 3.3867 (3.4557)	Prec@1 31.445 (30.794)	Prec@5 57.031 (55.643)	LR: 0.1
Epoch: [39][2000/2503]	Loss 3.2877 (3.4591)	Prec@1 34.375 (30.734)	Prec@5 59.570 (55.596)	LR: 0.1
 * Prec@1 30.696 Prec@5 55.552
Best Train Accuracy: 30.72%

Test: [0/98]	Loss 4.0320 (4.0320)	Prec@1 22.656 (22.656)	Prec@5 49.805 (49.805)
 * Prec@1 20.600 Prec@5 42.412
Best accuracy: 30.62%

Epoch: [40][0/2503]	Loss 3.5296 (3.5296)	Prec@1 27.734 (27.734)	Prec@5 53.516 (53.516)	LR: 0.010000000000000002
Epoch: [40][1000/2503]	Loss 2.6964 (2.9816)	Prec@1 42.969 (38.317)	Prec@5 67.578 (63.804)	LR: 0.010000000000000002
Epoch: [40][2000/2503]	Loss 2.8595 (2.9044)	Prec@1 38.672 (39.625)	Prec@5 64.648 (65.071)	LR: 0.010000000000000002
 * Prec@1 40.074 Prec@5 65.499
Best Train Accuracy: 40.07%

Test: [0/98]	Loss 1.5095 (1.5095)	Prec@1 62.891 (62.891)	Prec@5 87.500 (87.500)
 * Prec@1 49.388 Prec@5 75.318
Best accuracy: 49.39%

Epoch: [41][0/2503]	Loss 2.6727 (2.6727)	Prec@1 45.703 (45.703)	Prec@5 68.164 (68.164)	LR: 0.010000000000000002
Epoch: [41][1000/2503]	Loss 2.5779 (2.7265)	Prec@1 43.555 (42.695)	Prec@5 71.094 (68.012)	LR: 0.010000000000000002
Epoch: [41][2000/2503]	Loss 2.6273 (2.7157)	Prec@1 41.797 (42.876)	Prec@5 70.312 (68.165)	LR: 0.010000000000000002
 * Prec@1 42.952 Prec@5 68.214
Best Train Accuracy: 42.95%

Test: [0/98]	Loss 1.5443 (1.5443)	Prec@1 64.062 (64.062)	Prec@5 86.328 (86.328)
 * Prec@1 50.218 Prec@5 75.736
Best accuracy: 50.22%

Epoch: [42][0/2503]	Loss 2.7037 (2.7037)	Prec@1 43.555 (43.555)	Prec@5 70.312 (70.312)	LR: 0.010000000000000002
Epoch: [42][1000/2503]	Loss 2.5107 (2.6640)	Prec@1 47.852 (43.855)	Prec@5 70.312 (69.042)	LR: 0.010000000000000002
Epoch: [42][2000/2503]	Loss 2.6072 (2.6635)	Prec@1 44.922 (43.902)	Prec@5 71.680 (69.033)	LR: 0.010000000000000002
 * Prec@1 43.876 Prec@5 69.018
Best Train Accuracy: 43.88%

Test: [0/98]	Loss 1.5370 (1.5370)	Prec@1 60.938 (60.938)	Prec@5 87.695 (87.695)
 * Prec@1 49.126 Prec@5 74.524
Best accuracy: 50.22%

Epoch: [43][0/2503]	Loss 2.7081 (2.7081)	Prec@1 41.602 (41.602)	Prec@5 66.406 (66.406)	LR: 0.010000000000000002
Epoch: [43][1000/2503]	Loss 2.6733 (2.6411)	Prec@1 45.117 (44.284)	Prec@5 67.773 (69.356)	LR: 0.010000000000000002
Epoch: [43][2000/2503]	Loss 2.6684 (2.6450)	Prec@1 44.727 (44.182)	Prec@5 68.945 (69.310)	LR: 0.010000000000000002
 * Prec@1 44.190 Prec@5 69.291
Best Train Accuracy: 44.19%

Test: [0/98]	Loss 1.5588 (1.5588)	Prec@1 60.156 (60.156)	Prec@5 87.305 (87.305)
 * Prec@1 50.666 Prec@5 76.280
Best accuracy: 50.67%

Epoch: [44][0/2503]	Loss 2.7104 (2.7104)	Prec@1 42.383 (42.383)	Prec@5 69.336 (69.336)	LR: 0.010000000000000002
Epoch: [44][1000/2503]	Loss 2.5544 (2.6274)	Prec@1 45.508 (44.579)	Prec@5 69.531 (69.618)	LR: 0.010000000000000002
Epoch: [44][2000/2503]	Loss 2.5999 (2.6354)	Prec@1 45.312 (44.394)	Prec@5 71.680 (69.483)	LR: 0.010000000000000002
 * Prec@1 44.387 Prec@5 69.455
Best Train Accuracy: 44.39%

Test: [0/98]	Loss 1.4235 (1.4235)	Prec@1 65.820 (65.820)	Prec@5 88.672 (88.672)
 * Prec@1 50.810 Prec@5 76.108
Best accuracy: 50.81%

Epoch: [45][0/2503]	Loss 2.5920 (2.5920)	Prec@1 46.680 (46.680)	Prec@5 69.531 (69.531)	LR: 0.010000000000000002
Epoch: [45][1000/2503]	Loss 2.5931 (2.6260)	Prec@1 42.969 (44.601)	Prec@5 70.312 (69.673)	LR: 0.010000000000000002
Epoch: [45][2000/2503]	Loss 2.8439 (2.6333)	Prec@1 37.500 (44.475)	Prec@5 66.406 (69.517)	LR: 0.010000000000000002
 * Prec@1 44.457 Prec@5 69.490
Best Train Accuracy: 44.46%

Test: [0/98]	Loss 1.6225 (1.6225)	Prec@1 58.203 (58.203)	Prec@5 87.891 (87.891)
 * Prec@1 49.928 Prec@5 75.586
Best accuracy: 50.81%

Epoch: [46][0/2503]	Loss 2.8567 (2.8567)	Prec@1 41.016 (41.016)	Prec@5 64.453 (64.453)	LR: 0.010000000000000002
Epoch: [46][1000/2503]	Loss 2.5535 (2.6248)	Prec@1 47.461 (44.600)	Prec@5 71.484 (69.694)	LR: 0.010000000000000002
Epoch: [46][2000/2503]	Loss 2.8178 (2.6302)	Prec@1 40.430 (44.517)	Prec@5 66.797 (69.589)	LR: 0.010000000000000002
 * Prec@1 44.502 Prec@5 69.541
Best Train Accuracy: 44.50%

Test: [0/98]	Loss 1.4093 (1.4093)	Prec@1 65.234 (65.234)	Prec@5 89.062 (89.062)
 * Prec@1 48.770 Prec@5 74.660
Best accuracy: 50.81%

Epoch: [47][0/2503]	Loss 2.5984 (2.5984)	Prec@1 43.359 (43.359)	Prec@5 70.508 (70.508)	LR: 0.010000000000000002
Epoch: [47][1000/2503]	Loss 2.5004 (2.6203)	Prec@1 45.703 (44.715)	Prec@5 71.289 (69.737)	LR: 0.010000000000000002
Epoch: [47][2000/2503]	Loss 2.7751 (2.6242)	Prec@1 43.164 (44.644)	Prec@5 66.602 (69.669)	LR: 0.010000000000000002
 * Prec@1 44.602 Prec@5 69.612
Best Train Accuracy: 44.60%

Test: [0/98]	Loss 1.6514 (1.6514)	Prec@1 59.766 (59.766)	Prec@5 84.180 (84.180)
 * Prec@1 48.712 Prec@5 74.368
Best accuracy: 50.81%

Epoch: [48][0/2503]	Loss 2.5235 (2.5235)	Prec@1 46.484 (46.484)	Prec@5 71.289 (71.289)	LR: 0.010000000000000002
Epoch: [48][1000/2503]	Loss 2.5024 (2.6197)	Prec@1 47.461 (44.784)	Prec@5 71.289 (69.736)	LR: 0.010000000000000002
Epoch: [48][2000/2503]	Loss 2.6867 (2.6217)	Prec@1 45.703 (44.721)	Prec@5 68.359 (69.655)	LR: 0.010000000000000002
 * Prec@1 44.654 Prec@5 69.602
Best Train Accuracy: 44.65%

Test: [0/98]	Loss 1.5706 (1.5706)	Prec@1 64.258 (64.258)	Prec@5 85.352 (85.352)
 * Prec@1 49.372 Prec@5 74.398
Best accuracy: 50.81%

Epoch: [49][0/2503]	Loss 2.6340 (2.6340)	Prec@1 40.820 (40.820)	Prec@5 71.289 (71.289)	LR: 0.010000000000000002
Epoch: [49][1000/2503]	Loss 2.6788 (2.6114)	Prec@1 44.727 (44.910)	Prec@5 68.750 (69.881)	LR: 0.010000000000000002
Epoch: [49][2000/2503]	Loss 2.6838 (2.6155)	Prec@1 42.773 (44.835)	Prec@5 67.188 (69.764)	LR: 0.010000000000000002
 * Prec@1 44.775 Prec@5 69.745
Best Train Accuracy: 44.78%

Test: [0/98]	Loss 1.5612 (1.5612)	Prec@1 62.891 (62.891)	Prec@5 85.352 (85.352)
 * Prec@1 47.920 Prec@5 73.322
Best accuracy: 50.81%

Epoch: [50][0/2503]	Loss 2.5685 (2.5685)	Prec@1 45.312 (45.312)	Prec@5 68.555 (68.555)	LR: 0.010000000000000002
Epoch: [50][1000/2503]	Loss 2.8430 (2.6088)	Prec@1 40.234 (45.000)	Prec@5 66.797 (69.860)	LR: 0.010000000000000002
Epoch: [50][2000/2503]	Loss 2.6783 (2.6141)	Prec@1 45.508 (44.861)	Prec@5 68.164 (69.786)	LR: 0.010000000000000002
 * Prec@1 44.829 Prec@5 69.758
Best Train Accuracy: 44.83%

Test: [0/98]	Loss 1.4451 (1.4451)	Prec@1 64.062 (64.062)	Prec@5 86.914 (86.914)
 * Prec@1 50.354 Prec@5 75.658
Best accuracy: 50.81%

Epoch: [51][0/2503]	Loss 2.6189 (2.6189)	Prec@1 43.750 (43.750)	Prec@5 66.797 (66.797)	LR: 0.010000000000000002
Epoch: [51][1000/2503]	Loss 2.6127 (2.6006)	Prec@1 45.898 (45.067)	Prec@5 69.922 (70.026)	LR: 0.010000000000000002
Epoch: [51][2000/2503]	Loss 2.4171 (2.6080)	Prec@1 46.680 (44.963)	Prec@5 73.633 (69.873)	LR: 0.010000000000000002
 * Prec@1 44.934 Prec@5 69.849
Best Train Accuracy: 44.93%

Test: [0/98]	Loss 1.6448 (1.6448)	Prec@1 58.789 (58.789)	Prec@5 84.570 (84.570)
 * Prec@1 48.704 Prec@5 73.910
Best accuracy: 50.81%

Epoch: [52][0/2503]	Loss 2.4056 (2.4056)	Prec@1 49.219 (49.219)	Prec@5 72.266 (72.266)	LR: 0.010000000000000002
Epoch: [52][1000/2503]	Loss 2.6883 (2.5952)	Prec@1 41.797 (45.172)	Prec@5 66.602 (70.095)	LR: 0.010000000000000002
Epoch: [52][2000/2503]	Loss 2.6401 (2.6011)	Prec@1 44.922 (45.032)	Prec@5 70.117 (70.034)	LR: 0.010000000000000002
 * Prec@1 45.011 Prec@5 70.009
Best Train Accuracy: 45.01%

Test: [0/98]	Loss 1.4237 (1.4237)	Prec@1 64.844 (64.844)	Prec@5 89.844 (89.844)
 * Prec@1 49.536 Prec@5 75.032
Best accuracy: 50.81%

Epoch: [53][0/2503]	Loss 2.4924 (2.4924)	Prec@1 47.266 (47.266)	Prec@5 72.461 (72.461)	LR: 0.010000000000000002
Epoch: [53][1000/2503]	Loss 2.4700 (2.5908)	Prec@1 48.633 (45.308)	Prec@5 72.266 (70.186)	LR: 0.010000000000000002
Epoch: [53][2000/2503]	Loss 2.5286 (2.5943)	Prec@1 47.266 (45.225)	Prec@5 70.312 (70.138)	LR: 0.010000000000000002
 * Prec@1 45.151 Prec@5 70.049
Best Train Accuracy: 45.15%

Test: [0/98]	Loss 1.5482 (1.5482)	Prec@1 63.086 (63.086)	Prec@5 86.914 (86.914)
 * Prec@1 50.546 Prec@5 75.992
Best accuracy: 50.81%

Epoch: [54][0/2503]	Loss 2.6431 (2.6431)	Prec@1 46.484 (46.484)	Prec@5 68.945 (68.945)	LR: 0.010000000000000002
Epoch: [54][1000/2503]	Loss 2.5099 (2.5863)	Prec@1 49.023 (45.288)	Prec@5 69.727 (70.229)	LR: 0.010000000000000002
Epoch: [54][2000/2503]	Loss 2.5901 (2.5898)	Prec@1 45.117 (45.246)	Prec@5 71.094 (70.181)	LR: 0.010000000000000002
 * Prec@1 45.200 Prec@5 70.140
Best Train Accuracy: 45.20%

Test: [0/98]	Loss 1.4637 (1.4637)	Prec@1 64.453 (64.453)	Prec@5 88.281 (88.281)
 * Prec@1 49.374 Prec@5 74.810
Best accuracy: 50.81%

Epoch: [55][0/2503]	Loss 2.6502 (2.6502)	Prec@1 43.750 (43.750)	Prec@5 68.164 (68.164)	LR: 0.010000000000000002
Epoch: [55][1000/2503]	Loss 2.6824 (2.5862)	Prec@1 40.430 (45.321)	Prec@5 69.336 (70.257)	LR: 0.010000000000000002
Epoch: [55][2000/2503]	Loss 2.6582 (2.5898)	Prec@1 41.602 (45.242)	Prec@5 69.922 (70.223)	LR: 0.010000000000000002
 * Prec@1 45.197 Prec@5 70.168
Best Train Accuracy: 45.20%

Test: [0/98]	Loss 1.4293 (1.4293)	Prec@1 64.844 (64.844)	Prec@5 90.430 (90.430)
 * Prec@1 48.408 Prec@5 73.786
Best accuracy: 50.81%

Epoch: [56][0/2503]	Loss 2.6770 (2.6770)	Prec@1 46.680 (46.680)	Prec@5 68.750 (68.750)	LR: 0.010000000000000002
Epoch: [56][1000/2503]	Loss 2.5194 (2.5772)	Prec@1 48.242 (45.572)	Prec@5 71.680 (70.362)	LR: 0.010000000000000002
Epoch: [56][2000/2503]	Loss 2.6535 (2.5837)	Prec@1 43.945 (45.434)	Prec@5 69.531 (70.292)	LR: 0.010000000000000002
 * Prec@1 45.384 Prec@5 70.268
Best Train Accuracy: 45.38%

Test: [0/98]	Loss 1.5091 (1.5091)	Prec@1 62.305 (62.305)	Prec@5 87.500 (87.500)
 * Prec@1 48.410 Prec@5 74.284
Best accuracy: 50.81%

Epoch: [57][0/2503]	Loss 2.7623 (2.7623)	Prec@1 41.211 (41.211)	Prec@5 66.016 (66.016)	LR: 0.010000000000000002
Epoch: [57][1000/2503]	Loss 2.5417 (2.5717)	Prec@1 47.852 (45.687)	Prec@5 69.727 (70.529)	LR: 0.010000000000000002
Epoch: [57][2000/2503]	Loss 2.6015 (2.5804)	Prec@1 46.289 (45.460)	Prec@5 71.484 (70.384)	LR: 0.010000000000000002
 * Prec@1 45.441 Prec@5 70.359
Best Train Accuracy: 45.44%

Test: [0/98]	Loss 1.6538 (1.6538)	Prec@1 61.914 (61.914)	Prec@5 87.109 (87.109)
 * Prec@1 50.662 Prec@5 76.064
Best accuracy: 50.81%

Epoch: [58][0/2503]	Loss 2.7256 (2.7256)	Prec@1 43.359 (43.359)	Prec@5 66.602 (66.602)	LR: 0.010000000000000002
Epoch: [58][1000/2503]	Loss 2.5906 (2.5713)	Prec@1 45.117 (45.573)	Prec@5 70.508 (70.532)	LR: 0.010000000000000002
Epoch: [58][2000/2503]	Loss 2.6264 (2.5788)	Prec@1 44.141 (45.518)	Prec@5 68.164 (70.359)	LR: 0.010000000000000002
 * Prec@1 45.513 Prec@5 70.359
Best Train Accuracy: 45.51%

Test: [0/98]	Loss 1.4166 (1.4166)	Prec@1 63.672 (63.672)	Prec@5 89.453 (89.453)
 * Prec@1 50.394 Prec@5 75.350
Best accuracy: 50.81%

Epoch: [59][0/2503]	Loss 2.4571 (2.4571)	Prec@1 48.633 (48.633)	Prec@5 71.289 (71.289)	LR: 0.010000000000000002
Epoch: [59][1000/2503]	Loss 2.3667 (2.5679)	Prec@1 48.438 (45.708)	Prec@5 75.000 (70.561)	LR: 0.010000000000000002
Epoch: [59][2000/2503]	Loss 2.5903 (2.5738)	Prec@1 46.094 (45.597)	Prec@5 72.070 (70.450)	LR: 0.010000000000000002
 * Prec@1 45.568 Prec@5 70.407
Best Train Accuracy: 45.57%

Test: [0/98]	Loss 1.7171 (1.7171)	Prec@1 59.961 (59.961)	Prec@5 83.594 (83.594)
 * Prec@1 46.360 Prec@5 71.872
Best accuracy: 50.81%

Epoch: [60][0/2503]	Loss 2.6057 (2.6057)	Prec@1 44.727 (44.727)	Prec@5 69.531 (69.531)	LR: 0.010000000000000002
Epoch: [60][1000/2503]	Loss 2.8107 (2.5640)	Prec@1 40.820 (45.761)	Prec@5 67.578 (70.613)	LR: 0.010000000000000002
Epoch: [60][2000/2503]	Loss 2.4177 (2.5709)	Prec@1 48.047 (45.626)	Prec@5 72.266 (70.505)	LR: 0.010000000000000002
 * Prec@1 45.589 Prec@5 70.455
Best Train Accuracy: 45.59%

Test: [0/98]	Loss 1.7654 (1.7654)	Prec@1 59.375 (59.375)	Prec@5 83.398 (83.398)
 * Prec@1 49.268 Prec@5 74.958
Best accuracy: 50.81%

Epoch: [61][0/2503]	Loss 2.5473 (2.5473)	Prec@1 44.141 (44.141)	Prec@5 70.117 (70.117)	LR: 0.010000000000000002
Epoch: [61][1000/2503]	Loss 2.6510 (2.5595)	Prec@1 45.508 (45.799)	Prec@5 68.750 (70.639)	LR: 0.010000000000000002
Epoch: [61][2000/2503]	Loss 2.6722 (2.5652)	Prec@1 44.336 (45.769)	Prec@5 66.992 (70.564)	LR: 0.010000000000000002
 * Prec@1 45.679 Prec@5 70.493
Best Train Accuracy: 45.68%

Test: [0/98]	Loss 1.5648 (1.5648)	Prec@1 61.133 (61.133)	Prec@5 86.328 (86.328)
 * Prec@1 50.854 Prec@5 76.260
Best accuracy: 50.85%

Epoch: [62][0/2503]	Loss 2.5384 (2.5384)	Prec@1 45.508 (45.508)	Prec@5 70.117 (70.117)	LR: 0.010000000000000002
Epoch: [62][1000/2503]	Loss 2.5125 (2.5601)	Prec@1 47.852 (45.885)	Prec@5 70.703 (70.688)	LR: 0.010000000000000002
Epoch: [62][2000/2503]	Loss 2.6860 (2.5667)	Prec@1 43.945 (45.749)	Prec@5 68.164 (70.547)	LR: 0.010000000000000002
 * Prec@1 45.716 Prec@5 70.513
Best Train Accuracy: 45.72%

Test: [0/98]	Loss 1.8079 (1.8079)	Prec@1 60.742 (60.742)	Prec@5 83.789 (83.789)
 * Prec@1 50.250 Prec@5 75.358
Best accuracy: 50.85%

Epoch: [63][0/2503]	Loss 2.5959 (2.5959)	Prec@1 48.438 (48.438)	Prec@5 71.289 (71.289)	LR: 0.010000000000000002
Epoch: [63][1000/2503]	Loss 2.5493 (2.5543)	Prec@1 47.852 (45.947)	Prec@5 69.727 (70.747)	LR: 0.010000000000000002
Epoch: [63][2000/2503]	Loss 2.6721 (2.5611)	Prec@1 43.164 (45.824)	Prec@5 69.336 (70.623)	LR: 0.010000000000000002
 * Prec@1 45.773 Prec@5 70.585
Best Train Accuracy: 45.77%

Test: [0/98]	Loss 1.6477 (1.6477)	Prec@1 60.352 (60.352)	Prec@5 85.156 (85.156)
 * Prec@1 49.704 Prec@5 74.762
Best accuracy: 50.85%

Epoch: [64][0/2503]	Loss 2.5387 (2.5387)	Prec@1 47.266 (47.266)	Prec@5 71.094 (71.094)	LR: 0.010000000000000002
Epoch: [64][1000/2503]	Loss 2.5781 (2.5545)	Prec@1 43.750 (45.857)	Prec@5 71.875 (70.705)	LR: 0.010000000000000002
Epoch: [64][2000/2503]	Loss 2.5759 (2.5593)	Prec@1 45.312 (45.790)	Prec@5 68.750 (70.688)	LR: 0.010000000000000002
 * Prec@1 45.777 Prec@5 70.640
Best Train Accuracy: 45.78%

Test: [0/98]	Loss 1.4241 (1.4241)	Prec@1 64.844 (64.844)	Prec@5 89.258 (89.258)
 * Prec@1 52.110 Prec@5 77.118
Best accuracy: 52.11%

Epoch: [65][0/2503]	Loss 2.3556 (2.3556)	Prec@1 49.805 (49.805)	Prec@5 75.586 (75.586)	LR: 0.010000000000000002
Epoch: [65][1000/2503]	Loss 2.4176 (2.5513)	Prec@1 50.000 (45.988)	Prec@5 74.023 (70.827)	LR: 0.010000000000000002
Epoch: [65][2000/2503]	Loss 2.4323 (2.5595)	Prec@1 44.922 (45.857)	Prec@5 73.438 (70.665)	LR: 0.010000000000000002
 * Prec@1 45.836 Prec@5 70.642
Best Train Accuracy: 45.84%

Test: [0/98]	Loss 1.3522 (1.3522)	Prec@1 66.406 (66.406)	Prec@5 89.062 (89.062)
 * Prec@1 49.576 Prec@5 75.048
Best accuracy: 52.11%

Epoch: [66][0/2503]	Loss 2.6023 (2.6023)	Prec@1 45.508 (45.508)	Prec@5 69.922 (69.922)	LR: 0.010000000000000002
Epoch: [66][1000/2503]	Loss 2.5701 (2.5502)	Prec@1 44.531 (46.022)	Prec@5 69.727 (70.784)	LR: 0.010000000000000002
Epoch: [66][2000/2503]	Loss 2.6001 (2.5547)	Prec@1 46.094 (45.963)	Prec@5 67.383 (70.759)	LR: 0.010000000000000002
 * Prec@1 45.897 Prec@5 70.701
Best Train Accuracy: 45.90%

Test: [0/98]	Loss 1.6753 (1.6753)	Prec@1 61.719 (61.719)	Prec@5 83.398 (83.398)
 * Prec@1 48.740 Prec@5 74.062
Best accuracy: 52.11%

Epoch: [67][0/2503]	Loss 2.2461 (2.2461)	Prec@1 51.562 (51.562)	Prec@5 77.539 (77.539)	LR: 0.010000000000000002
Epoch: [67][1000/2503]	Loss 2.3207 (2.5498)	Prec@1 50.195 (45.986)	Prec@5 76.562 (70.864)	LR: 0.010000000000000002
Epoch: [67][2000/2503]	Loss 2.5526 (2.5539)	Prec@1 46.094 (45.939)	Prec@5 69.141 (70.791)	LR: 0.010000000000000002
 * Prec@1 45.929 Prec@5 70.758
Best Train Accuracy: 45.93%

Test: [0/98]	Loss 1.5668 (1.5668)	Prec@1 62.305 (62.305)	Prec@5 86.133 (86.133)
 * Prec@1 49.062 Prec@5 74.274
Best accuracy: 52.11%

Epoch: [68][0/2503]	Loss 2.5777 (2.5777)	Prec@1 46.289 (46.289)	Prec@5 71.094 (71.094)	LR: 0.010000000000000002
Epoch: [68][1000/2503]	Loss 2.7279 (2.5456)	Prec@1 40.234 (46.106)	Prec@5 67.773 (70.891)	LR: 0.010000000000000002
Epoch: [68][2000/2503]	Loss 2.4562 (2.5516)	Prec@1 45.703 (46.011)	Prec@5 72.852 (70.775)	LR: 0.010000000000000002
 * Prec@1 45.984 Prec@5 70.730
Best Train Accuracy: 45.98%

Test: [0/98]	Loss 1.5672 (1.5672)	Prec@1 61.914 (61.914)	Prec@5 87.891 (87.891)
 * Prec@1 50.028 Prec@5 75.556
Best accuracy: 52.11%

Epoch: [69][0/2503]	Loss 2.4457 (2.4457)	Prec@1 48.047 (48.047)	Prec@5 71.484 (71.484)	LR: 0.010000000000000002
Epoch: [69][1000/2503]	Loss 2.5489 (2.5464)	Prec@1 47.266 (46.132)	Prec@5 72.656 (70.906)	LR: 0.010000000000000002
Epoch: [69][2000/2503]	Loss 2.5864 (2.5506)	Prec@1 45.898 (46.071)	Prec@5 70.898 (70.835)	LR: 0.010000000000000002
 * Prec@1 46.019 Prec@5 70.804
Best Train Accuracy: 46.02%

Test: [0/98]	Loss 1.5077 (1.5077)	Prec@1 63.086 (63.086)	Prec@5 87.500 (87.500)
 * Prec@1 50.016 Prec@5 75.584
Best accuracy: 52.11%

Epoch: [70][0/2503]	Loss 2.4984 (2.4984)	Prec@1 46.875 (46.875)	Prec@5 72.461 (72.461)	LR: 0.010000000000000002
Epoch: [70][1000/2503]	Loss 2.4864 (2.5391)	Prec@1 49.023 (46.235)	Prec@5 71.094 (71.034)	LR: 0.010000000000000002
Epoch: [70][2000/2503]	Loss 2.5434 (2.5461)	Prec@1 47.656 (46.122)	Prec@5 69.727 (70.906)	LR: 0.010000000000000002
 * Prec@1 46.081 Prec@5 70.825
Best Train Accuracy: 46.08%

Test: [0/98]	Loss 1.5190 (1.5190)	Prec@1 64.062 (64.062)	Prec@5 85.352 (85.352)
 * Prec@1 48.386 Prec@5 74.072
Best accuracy: 52.11%

Epoch: [71][0/2503]	Loss 2.5559 (2.5559)	Prec@1 44.727 (44.727)	Prec@5 70.703 (70.703)	LR: 0.010000000000000002
Epoch: [71][1000/2503]	Loss 2.5110 (2.5341)	Prec@1 45.508 (46.341)	Prec@5 69.727 (71.096)	LR: 0.010000000000000002
Epoch: [71][2000/2503]	Loss 2.4818 (2.5445)	Prec@1 47.266 (46.122)	Prec@5 72.070 (70.930)	LR: 0.010000000000000002
 * Prec@1 46.062 Prec@5 70.859
Best Train Accuracy: 46.08%

Test: [0/98]	Loss 1.5861 (1.5861)	Prec@1 58.398 (58.398)	Prec@5 87.500 (87.500)
 * Prec@1 50.264 Prec@5 75.714
Best accuracy: 52.11%

Epoch: [72][0/2503]	Loss 2.5863 (2.5863)	Prec@1 45.508 (45.508)	Prec@5 69.727 (69.727)	LR: 0.010000000000000002
Epoch: [72][1000/2503]	Loss 2.6720 (2.5389)	Prec@1 44.727 (46.317)	Prec@5 66.992 (70.960)	LR: 0.010000000000000002
Epoch: [72][2000/2503]	Loss 2.5026 (2.5444)	Prec@1 43.164 (46.165)	Prec@5 70.703 (70.895)	LR: 0.010000000000000002
 * Prec@1 46.121 Prec@5 70.869
Best Train Accuracy: 46.12%

Test: [0/98]	Loss 1.6091 (1.6091)	Prec@1 61.523 (61.523)	Prec@5 88.086 (88.086)
 * Prec@1 51.230 Prec@5 76.350
Best accuracy: 52.11%

Epoch: [73][0/2503]	Loss 2.5798 (2.5798)	Prec@1 45.312 (45.312)	Prec@5 71.484 (71.484)	LR: 0.010000000000000002
Epoch: [73][1000/2503]	Loss 2.6014 (2.5329)	Prec@1 46.680 (46.325)	Prec@5 70.508 (71.154)	LR: 0.010000000000000002
Epoch: [73][2000/2503]	Loss 2.5271 (2.5415)	Prec@1 45.703 (46.214)	Prec@5 71.484 (70.965)	LR: 0.010000000000000002
 * Prec@1 46.190 Prec@5 70.914
Best Train Accuracy: 46.19%

Test: [0/98]	Loss 1.4538 (1.4538)	Prec@1 64.844 (64.844)	Prec@5 88.672 (88.672)
 * Prec@1 50.714 Prec@5 75.720
Best accuracy: 52.11%

Epoch: [74][0/2503]	Loss 2.6214 (2.6214)	Prec@1 43.945 (43.945)	Prec@5 68.945 (68.945)	LR: 0.010000000000000002
Epoch: [74][1000/2503]	Loss 2.3742 (2.5374)	Prec@1 50.586 (46.262)	Prec@5 74.414 (71.072)	LR: 0.010000000000000002
Epoch: [74][2000/2503]	Loss 2.4348 (2.5450)	Prec@1 46.680 (46.111)	Prec@5 73.438 (70.894)	LR: 0.010000000000000002
 * Prec@1 46.092 Prec@5 70.857
Best Train Accuracy: 46.19%

Test: [0/98]	Loss 1.4089 (1.4089)	Prec@1 63.867 (63.867)	Prec@5 88.672 (88.672)
 * Prec@1 50.780 Prec@5 76.316
Best accuracy: 52.11%

Epoch: [75][0/2503]	Loss 2.4098 (2.4098)	Prec@1 49.219 (49.219)	Prec@5 73.242 (73.242)	LR: 0.010000000000000002
Epoch: [75][1000/2503]	Loss 2.7149 (2.5299)	Prec@1 40.625 (46.450)	Prec@5 67.188 (71.137)	LR: 0.010000000000000002
Epoch: [75][2000/2503]	Loss 2.4857 (2.5386)	Prec@1 50.195 (46.257)	Prec@5 70.117 (71.003)	LR: 0.010000000000000002
 * Prec@1 46.185 Prec@5 70.950
Best Train Accuracy: 46.19%

Test: [0/98]	Loss 1.5664 (1.5664)	Prec@1 63.086 (63.086)	Prec@5 88.281 (88.281)
 * Prec@1 51.002 Prec@5 76.448
Best accuracy: 52.11%

Epoch: [76][0/2503]	Loss 2.6960 (2.6960)	Prec@1 43.359 (43.359)	Prec@5 68.555 (68.555)	LR: 0.010000000000000002
Epoch: [76][1000/2503]	Loss 2.5200 (2.5298)	Prec@1 45.312 (46.434)	Prec@5 71.094 (71.101)	LR: 0.010000000000000002
Epoch: [76][2000/2503]	Loss 2.5953 (2.5389)	Prec@1 45.508 (46.273)	Prec@5 71.289 (70.955)	LR: 0.010000000000000002
 * Prec@1 46.221 Prec@5 70.927
Best Train Accuracy: 46.22%

Test: [0/98]	Loss 1.6802 (1.6802)	Prec@1 58.984 (58.984)	Prec@5 85.742 (85.742)
 * Prec@1 50.012 Prec@5 75.230
Best accuracy: 52.11%

Epoch: [77][0/2503]	Loss 2.4713 (2.4713)	Prec@1 48.242 (48.242)	Prec@5 72.656 (72.656)	LR: 0.010000000000000002
Epoch: [77][1000/2503]	Loss 2.5056 (2.5272)	Prec@1 45.312 (46.492)	Prec@5 72.461 (71.171)	LR: 0.010000000000000002
Epoch: [77][2000/2503]	Loss 2.6334 (2.5355)	Prec@1 42.578 (46.318)	Prec@5 67.188 (71.042)	LR: 0.010000000000000002
 * Prec@1 46.252 Prec@5 70.969
Best Train Accuracy: 46.25%

Test: [0/98]	Loss 1.6607 (1.6607)	Prec@1 59.570 (59.570)	Prec@5 85.742 (85.742)
 * Prec@1 50.402 Prec@5 75.328
Best accuracy: 52.11%

Epoch: [78][0/2503]	Loss 2.5089 (2.5089)	Prec@1 48.242 (48.242)	Prec@5 70.312 (70.312)	LR: 0.010000000000000002
Epoch: [78][1000/2503]	Loss 2.5283 (2.5341)	Prec@1 46.289 (46.447)	Prec@5 71.875 (71.082)	LR: 0.010000000000000002
Epoch: [78][2000/2503]	Loss 2.5795 (2.5401)	Prec@1 44.141 (46.338)	Prec@5 69.727 (70.970)	LR: 0.010000000000000002
 * Prec@1 46.271 Prec@5 70.935
Best Train Accuracy: 46.27%

Test: [0/98]	Loss 1.5486 (1.5486)	Prec@1 63.477 (63.477)	Prec@5 86.328 (86.328)
 * Prec@1 51.682 Prec@5 76.480
Best accuracy: 52.11%

Epoch: [79][0/2503]	Loss 2.5497 (2.5497)	Prec@1 45.898 (45.898)	Prec@5 69.336 (69.336)	LR: 0.010000000000000002
Epoch: [79][1000/2503]	Loss 2.4398 (2.5298)	Prec@1 49.414 (46.362)	Prec@5 74.414 (71.159)	LR: 0.010000000000000002
Epoch: [79][2000/2503]	Loss 2.4197 (2.5370)	Prec@1 50.000 (46.257)	Prec@5 73.047 (71.030)	LR: 0.010000000000000002
 * Prec@1 46.215 Prec@5 70.983
Best Train Accuracy: 46.27%

Test: [0/98]	Loss 1.5912 (1.5912)	Prec@1 62.109 (62.109)	Prec@5 85.547 (85.547)
 * Prec@1 50.440 Prec@5 75.574
Best accuracy: 52.11%

Epoch: [80][0/2503]	Loss 2.6398 (2.6398)	Prec@1 44.531 (44.531)	Prec@5 68.359 (68.359)	LR: 0.0010000000000000002
Epoch: [80][1000/2503]	Loss 2.2992 (2.3289)	Prec@1 51.758 (49.947)	Prec@5 74.219 (74.222)	LR: 0.0010000000000000002
Epoch: [80][2000/2503]	Loss 2.3316 (2.2954)	Prec@1 49.805 (50.589)	Prec@5 74.023 (74.737)	LR: 0.0010000000000000002
 * Prec@1 50.760 Prec@5 74.899
Best Train Accuracy: 50.76%

Test: [0/98]	Loss 1.0870 (1.0870)	Prec@1 73.242 (73.242)	Prec@5 91.211 (91.211)
 * Prec@1 59.136 Prec@5 82.678
Best accuracy: 59.14%

Epoch: [81][0/2503]	Loss 2.1578 (2.1578)	Prec@1 52.148 (52.148)	Prec@5 75.391 (75.391)	LR: 0.0010000000000000002
Epoch: [81][1000/2503]	Loss 2.1487 (2.2243)	Prec@1 53.320 (51.908)	Prec@5 75.977 (75.815)	LR: 0.0010000000000000002
Epoch: [81][2000/2503]	Loss 2.1961 (2.2183)	Prec@1 50.586 (52.001)	Prec@5 75.781 (75.904)	LR: 0.0010000000000000002
 * Prec@1 52.058 Prec@5 75.940
Best Train Accuracy: 52.06%

Test: [0/98]	Loss 1.1076 (1.1076)	Prec@1 72.461 (72.461)	Prec@5 92.188 (92.188)
 * Prec@1 59.654 Prec@5 83.010
Best accuracy: 59.65%

Epoch: [82][0/2503]	Loss 2.2054 (2.2054)	Prec@1 50.586 (50.586)	Prec@5 75.391 (75.391)	LR: 0.0010000000000000002
Epoch: [82][1000/2503]	Loss 2.2581 (2.1936)	Prec@1 52.734 (52.434)	Prec@5 75.781 (76.295)	LR: 0.0010000000000000002
Epoch: [82][2000/2503]	Loss 2.2047 (2.1934)	Prec@1 50.586 (52.475)	Prec@5 75.781 (76.319)	LR: 0.0010000000000000002
 * Prec@1 52.505 Prec@5 76.350
Best Train Accuracy: 52.50%

Test: [0/98]	Loss 1.1387 (1.1387)	Prec@1 72.266 (72.266)	Prec@5 90.820 (90.820)
 * Prec@1 60.070 Prec@5 83.324
Best accuracy: 60.07%

Epoch: [83][0/2503]	Loss 2.1303 (2.1303)	Prec@1 54.492 (54.492)	Prec@5 76.953 (76.953)	LR: 0.0010000000000000002
Epoch: [83][1000/2503]	Loss 2.2268 (2.1789)	Prec@1 50.195 (52.722)	Prec@5 76.367 (76.547)	LR: 0.0010000000000000002
Epoch: [83][2000/2503]	Loss 2.2425 (2.1774)	Prec@1 50.977 (52.780)	Prec@5 74.805 (76.554)	LR: 0.0010000000000000002
 * Prec@1 52.806 Prec@5 76.545
Best Train Accuracy: 52.81%

Test: [0/98]	Loss 0.9674 (0.9674)	Prec@1 74.609 (74.609)	Prec@5 93.555 (93.555)
 * Prec@1 59.986 Prec@5 83.226
Best accuracy: 60.07%

Epoch: [84][0/2503]	Loss 2.1376 (2.1376)	Prec@1 52.930 (52.930)	Prec@5 76.367 (76.367)	LR: 0.0010000000000000002
Epoch: [84][1000/2503]	Loss 2.0806 (2.1591)	Prec@1 55.664 (53.167)	Prec@5 77.344 (76.821)	LR: 0.0010000000000000002
Epoch: [84][2000/2503]	Loss 2.0973 (2.1637)	Prec@1 56.055 (53.084)	Prec@5 77.539 (76.752)	LR: 0.0010000000000000002
 * Prec@1 53.069 Prec@5 76.741
Best Train Accuracy: 53.07%

Test: [0/98]	Loss 1.0358 (1.0358)	Prec@1 73.242 (73.242)	Prec@5 92.188 (92.188)
 * Prec@1 60.374 Prec@5 83.454
Best accuracy: 60.37%

Epoch: [85][0/2503]	Loss 2.0521 (2.0521)	Prec@1 53.125 (53.125)	Prec@5 79.883 (79.883)	LR: 0.0010000000000000002
Epoch: [85][1000/2503]	Loss 2.1561 (2.1541)	Prec@1 53.516 (53.301)	Prec@5 77.148 (76.882)	LR: 0.0010000000000000002
Epoch: [85][2000/2503]	Loss 2.1805 (2.1581)	Prec@1 53.320 (53.166)	Prec@5 76.953 (76.831)	LR: 0.0010000000000000002
 * Prec@1 53.170 Prec@5 76.845
Best Train Accuracy: 53.17%

Test: [0/98]	Loss 1.0689 (1.0689)	Prec@1 72.852 (72.852)	Prec@5 92.773 (92.773)
 * Prec@1 60.396 Prec@5 83.332
Best accuracy: 60.40%

Epoch: [86][0/2503]	Loss 2.2113 (2.2113)	Prec@1 51.367 (51.367)	Prec@5 76.562 (76.562)	LR: 0.0010000000000000002
Epoch: [86][1000/2503]	Loss 2.0988 (2.1472)	Prec@1 54.297 (53.465)	Prec@5 76.758 (77.002)	LR: 0.0010000000000000002
Epoch: [86][2000/2503]	Loss 2.1642 (2.1490)	Prec@1 52.930 (53.408)	Prec@5 76.562 (76.983)	LR: 0.0010000000000000002
 * Prec@1 53.377 Prec@5 76.948
Best Train Accuracy: 53.38%

Test: [0/98]	Loss 1.0129 (1.0129)	Prec@1 73.828 (73.828)	Prec@5 92.773 (92.773)
 * Prec@1 60.478 Prec@5 83.416
Best accuracy: 60.48%

Epoch: [87][0/2503]	Loss 2.1003 (2.1003)	Prec@1 54.102 (54.102)	Prec@5 79.297 (79.297)	LR: 0.0010000000000000002
Epoch: [87][1000/2503]	Loss 2.1920 (2.1378)	Prec@1 53.516 (53.626)	Prec@5 75.195 (77.136)	LR: 0.0010000000000000002
Epoch: [87][2000/2503]	Loss 2.1371 (2.1434)	Prec@1 52.539 (53.446)	Prec@5 75.586 (77.057)	LR: 0.0010000000000000002
 * Prec@1 53.425 Prec@5 77.026
Best Train Accuracy: 53.42%

Test: [0/98]	Loss 1.1147 (1.1147)	Prec@1 72.656 (72.656)	Prec@5 91.406 (91.406)
 * Prec@1 60.356 Prec@5 83.456
Best accuracy: 60.48%

Epoch: [88][0/2503]	Loss 2.1883 (2.1883)	Prec@1 52.148 (52.148)	Prec@5 76.953 (76.953)	LR: 0.0010000000000000002
Epoch: [88][1000/2503]	Loss 2.1763 (2.1386)	Prec@1 50.977 (53.635)	Prec@5 75.000 (77.102)	LR: 0.0010000000000000002
Epoch: [88][2000/2503]	Loss 2.2369 (2.1409)	Prec@1 51.172 (53.557)	Prec@5 74.023 (77.069)	LR: 0.0010000000000000002
 * Prec@1 53.569 Prec@5 77.095
Best Train Accuracy: 53.57%

Test: [0/98]	Loss 1.1337 (1.1337)	Prec@1 71.875 (71.875)	Prec@5 91.992 (91.992)
 * Prec@1 60.210 Prec@5 83.234
Best accuracy: 60.48%

Epoch: [89][0/2503]	Loss 1.9873 (1.9873)	Prec@1 59.180 (59.180)	Prec@5 78.906 (78.906)	LR: 0.0010000000000000002
Epoch: [89][1000/2503]	Loss 2.3035 (2.1301)	Prec@1 49.805 (53.741)	Prec@5 75.781 (77.308)	LR: 0.0010000000000000002
Epoch: [89][2000/2503]	Loss 2.2344 (2.1324)	Prec@1 52.344 (53.700)	Prec@5 75.586 (77.241)	LR: 0.0010000000000000002
 * Prec@1 53.671 Prec@5 77.176
Best Train Accuracy: 53.67%

Test: [0/98]	Loss 1.0508 (1.0508)	Prec@1 73.633 (73.633)	Prec@5 92.773 (92.773)
 * Prec@1 60.748 Prec@5 83.672
Best accuracy: 60.75%

Epoch: [90][0/2503]	Loss 2.0864 (2.0864)	Prec@1 52.539 (52.539)	Prec@5 79.297 (79.297)	LR: 0.0010000000000000002
Epoch: [90][1000/2503]	Loss 2.1354 (2.1315)	Prec@1 56.641 (53.720)	Prec@5 79.102 (77.227)	LR: 0.0010000000000000002
Epoch: [90][2000/2503]	Loss 2.1458 (2.1324)	Prec@1 53.906 (53.722)	Prec@5 75.977 (77.222)	LR: 0.0010000000000000002
 * Prec@1 53.686 Prec@5 77.185
Best Train Accuracy: 53.69%

Test: [0/98]	Loss 1.0982 (1.0982)	Prec@1 72.656 (72.656)	Prec@5 91.797 (91.797)
 * Prec@1 60.464 Prec@5 83.430
Best accuracy: 60.75%

Epoch: [91][0/2503]	Loss 2.1983 (2.1983)	Prec@1 55.078 (55.078)	Prec@5 74.609 (74.609)	LR: 0.0010000000000000002
Epoch: [91][1000/2503]	Loss 1.9918 (2.1294)	Prec@1 56.445 (53.731)	Prec@5 79.883 (77.227)	LR: 0.0010000000000000002
Epoch: [91][2000/2503]	Loss 2.2745 (2.1310)	Prec@1 51.562 (53.721)	Prec@5 75.977 (77.214)	LR: 0.0010000000000000002
 * Prec@1 53.711 Prec@5 77.221
Best Train Accuracy: 53.71%

Test: [0/98]	Loss 1.1044 (1.1044)	Prec@1 73.047 (73.047)	Prec@5 91.602 (91.602)
 * Prec@1 60.528 Prec@5 83.634
Best accuracy: 60.75%

Epoch: [92][0/2503]	Loss 2.2071 (2.2071)	Prec@1 50.000 (50.000)	Prec@5 76.953 (76.953)	LR: 0.0010000000000000002
Epoch: [92][1000/2503]	Loss 2.0214 (2.1226)	Prec@1 57.812 (53.972)	Prec@5 77.930 (77.350)	LR: 0.0010000000000000002
Epoch: [92][2000/2503]	Loss 2.0444 (2.1250)	Prec@1 53.711 (53.878)	Prec@5 76.758 (77.334)	LR: 0.0010000000000000002
 * Prec@1 53.823 Prec@5 77.291
Best Train Accuracy: 53.82%

Test: [0/98]	Loss 1.0450 (1.0450)	Prec@1 75.195 (75.195)	Prec@5 93.555 (93.555)
 * Prec@1 59.948 Prec@5 83.054
Best accuracy: 60.75%

Epoch: [93][0/2503]	Loss 2.0454 (2.0454)	Prec@1 55.273 (55.273)	Prec@5 79.688 (79.688)	LR: 0.0010000000000000002
Epoch: [93][1000/2503]	Loss 2.0602 (2.1184)	Prec@1 56.445 (53.948)	Prec@5 79.102 (77.497)	LR: 0.0010000000000000002
Epoch: [93][2000/2503]	Loss 2.1891 (2.1245)	Prec@1 55.859 (53.833)	Prec@5 74.805 (77.379)	LR: 0.0010000000000000002
 * Prec@1 53.803 Prec@5 77.319
Best Train Accuracy: 53.82%

Test: [0/98]	Loss 1.0590 (1.0590)	Prec@1 73.438 (73.438)	Prec@5 92.383 (92.383)
 * Prec@1 60.576 Prec@5 83.528
Best accuracy: 60.75%

Epoch: [94][0/2503]	Loss 2.1391 (2.1391)	Prec@1 51.172 (51.172)	Prec@5 77.734 (77.734)	LR: 0.0010000000000000002
Epoch: [94][1000/2503]	Loss 2.1015 (2.1194)	Prec@1 52.344 (54.019)	Prec@5 79.102 (77.422)	LR: 0.0010000000000000002
Epoch: [94][2000/2503]	Loss 2.1717 (2.1201)	Prec@1 51.562 (53.972)	Prec@5 75.391 (77.444)	LR: 0.0010000000000000002
 * Prec@1 53.937 Prec@5 77.423
Best Train Accuracy: 53.94%

Test: [0/98]	Loss 0.9712 (0.9712)	Prec@1 74.219 (74.219)	Prec@5 92.578 (92.578)
 * Prec@1 60.664 Prec@5 83.658
Best accuracy: 60.75%

Epoch: [95][0/2503]	Loss 2.1174 (2.1174)	Prec@1 55.273 (55.273)	Prec@5 75.391 (75.391)	LR: 0.0010000000000000002
Epoch: [95][1000/2503]	Loss 2.1950 (2.1137)	Prec@1 53.711 (54.031)	Prec@5 75.977 (77.563)	LR: 0.0010000000000000002
Epoch: [95][2000/2503]	Loss 2.2742 (2.1208)	Prec@1 51.562 (53.890)	Prec@5 72.461 (77.418)	LR: 0.0010000000000000002
 * Prec@1 53.881 Prec@5 77.400
Best Train Accuracy: 53.94%

Test: [0/98]	Loss 0.9931 (0.9931)	Prec@1 74.023 (74.023)	Prec@5 93.359 (93.359)
 * Prec@1 60.360 Prec@5 83.324
Best accuracy: 60.75%

Epoch: [96][0/2503]	Loss 2.2533 (2.2533)	Prec@1 51.172 (51.172)	Prec@5 73.633 (73.633)	LR: 0.0010000000000000002
Epoch: [96][1000/2503]	Loss 2.1567 (2.1185)	Prec@1 53.516 (53.946)	Prec@5 77.539 (77.418)	LR: 0.0010000000000000002
Epoch: [96][2000/2503]	Loss 2.0805 (2.1218)	Prec@1 53.320 (53.874)	Prec@5 78.320 (77.358)	LR: 0.0010000000000000002
 * Prec@1 53.834 Prec@5 77.330
Best Train Accuracy: 53.94%

Test: [0/98]	Loss 1.0229 (1.0229)	Prec@1 72.656 (72.656)	Prec@5 91.992 (91.992)
 * Prec@1 60.828 Prec@5 83.558
Best accuracy: 60.83%

Epoch: [97][0/2503]	Loss 2.1629 (2.1629)	Prec@1 54.688 (54.688)	Prec@5 76.367 (76.367)	LR: 0.0010000000000000002
Epoch: [97][1000/2503]	Loss 2.1703 (2.1117)	Prec@1 51.953 (54.096)	Prec@5 76.367 (77.548)	LR: 0.0010000000000000002
Epoch: [97][2000/2503]	Loss 2.1230 (2.1167)	Prec@1 54.102 (54.010)	Prec@5 77.734 (77.443)	LR: 0.0010000000000000002
 * Prec@1 53.952 Prec@5 77.390
Best Train Accuracy: 53.95%

Test: [0/98]	Loss 1.0259 (1.0259)	Prec@1 74.219 (74.219)	Prec@5 93.945 (93.945)
 * Prec@1 60.774 Prec@5 83.526
Best accuracy: 60.83%

Epoch: [98][0/2503]	Loss 2.1308 (2.1308)	Prec@1 53.906 (53.906)	Prec@5 77.930 (77.930)	LR: 0.0010000000000000002
Epoch: [98][1000/2503]	Loss 1.9669 (2.1152)	Prec@1 56.250 (54.101)	Prec@5 77.734 (77.537)	LR: 0.0010000000000000002
Epoch: [98][2000/2503]	Loss 2.1279 (2.1198)	Prec@1 48.633 (53.987)	Prec@5 77.539 (77.453)	LR: 0.0010000000000000002
 * Prec@1 53.948 Prec@5 77.421
Best Train Accuracy: 53.95%

Test: [0/98]	Loss 1.0555 (1.0555)	Prec@1 72.070 (72.070)	Prec@5 92.188 (92.188)
 * Prec@1 60.982 Prec@5 83.728
Best accuracy: 60.98%

Epoch: [99][0/2503]	Loss 2.1760 (2.1760)	Prec@1 52.930 (52.930)	Prec@5 76.172 (76.172)	LR: 0.0010000000000000002
Epoch: [99][1000/2503]	Loss 1.9697 (2.1147)	Prec@1 56.055 (54.063)	Prec@5 77.734 (77.474)	LR: 0.0010000000000000002
Epoch: [99][2000/2503]	Loss 2.2289 (2.1189)	Prec@1 53.516 (54.000)	Prec@5 76.172 (77.439)	LR: 0.0010000000000000002
 * Prec@1 53.955 Prec@5 77.415
Best Train Accuracy: 53.95%

Test: [0/98]	Loss 1.0726 (1.0726)	Prec@1 72.852 (72.852)	Prec@5 91.406 (91.406)
 * Prec@1 60.270 Prec@5 83.348
Best accuracy: 60.98%

Epoch: [100][0/2503]	Loss 2.0622 (2.0622)	Prec@1 57.812 (57.812)	Prec@5 78.516 (78.516)	LR: 0.0010000000000000002
Epoch: [100][1000/2503]	Loss 2.0856 (2.1091)	Prec@1 55.273 (54.173)	Prec@5 76.562 (77.633)	LR: 0.0010000000000000002
Epoch: [100][2000/2503]	Loss 2.2803 (2.1159)	Prec@1 51.562 (54.045)	Prec@5 74.023 (77.528)	LR: 0.0010000000000000002
 * Prec@1 54.008 Prec@5 77.489
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0293 (1.0293)	Prec@1 73.438 (73.438)	Prec@5 92.383 (92.383)
 * Prec@1 60.524 Prec@5 83.428
Best accuracy: 60.98%

Epoch: [101][0/2503]	Loss 2.1655 (2.1655)	Prec@1 51.172 (51.172)	Prec@5 76.367 (76.367)	LR: 0.0010000000000000002
Epoch: [101][1000/2503]	Loss 2.0542 (2.1123)	Prec@1 53.711 (54.092)	Prec@5 78.516 (77.552)	LR: 0.0010000000000000002
Epoch: [101][2000/2503]	Loss 1.9908 (2.1195)	Prec@1 56.836 (53.974)	Prec@5 79.297 (77.425)	LR: 0.0010000000000000002
 * Prec@1 53.951 Prec@5 77.415
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0410 (1.0410)	Prec@1 73.047 (73.047)	Prec@5 92.578 (92.578)
 * Prec@1 60.848 Prec@5 83.700
Best accuracy: 60.98%

Epoch: [102][0/2503]	Loss 2.2354 (2.2354)	Prec@1 48.047 (48.047)	Prec@5 73.828 (73.828)	LR: 0.0010000000000000002
Epoch: [102][1000/2503]	Loss 2.1124 (2.1193)	Prec@1 55.469 (53.988)	Prec@5 77.734 (77.488)	LR: 0.0010000000000000002
Epoch: [102][2000/2503]	Loss 2.2884 (2.1202)	Prec@1 50.000 (53.945)	Prec@5 76.367 (77.477)	LR: 0.0010000000000000002
 * Prec@1 53.928 Prec@5 77.416
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0385 (1.0385)	Prec@1 75.195 (75.195)	Prec@5 92.188 (92.188)
 * Prec@1 60.768 Prec@5 83.594
Best accuracy: 60.98%

Epoch: [103][0/2503]	Loss 2.0796 (2.0796)	Prec@1 56.641 (56.641)	Prec@5 78.125 (78.125)	LR: 0.0010000000000000002
Epoch: [103][1000/2503]	Loss 2.0807 (2.1137)	Prec@1 53.906 (54.042)	Prec@5 78.125 (77.508)	LR: 0.0010000000000000002
Epoch: [103][2000/2503]	Loss 2.0272 (2.1200)	Prec@1 51.953 (53.966)	Prec@5 79.102 (77.431)	LR: 0.0010000000000000002
 * Prec@1 53.962 Prec@5 77.405
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0464 (1.0464)	Prec@1 73.242 (73.242)	Prec@5 91.992 (91.992)
 * Prec@1 60.682 Prec@5 83.588
Best accuracy: 60.98%

Epoch: [104][0/2503]	Loss 2.1595 (2.1595)	Prec@1 53.516 (53.516)	Prec@5 77.539 (77.539)	LR: 0.0010000000000000002
Epoch: [104][1000/2503]	Loss 2.0748 (2.1133)	Prec@1 55.273 (54.127)	Prec@5 76.758 (77.546)	LR: 0.0010000000000000002
Epoch: [104][2000/2503]	Loss 2.2139 (2.1169)	Prec@1 51.172 (54.042)	Prec@5 76.367 (77.463)	LR: 0.0010000000000000002
 * Prec@1 54.015 Prec@5 77.421
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0859 (1.0859)	Prec@1 73.242 (73.242)	Prec@5 92.773 (92.773)
 * Prec@1 60.596 Prec@5 83.398
Best accuracy: 60.98%

Epoch: [105][0/2503]	Loss 1.9977 (1.9977)	Prec@1 55.078 (55.078)	Prec@5 80.469 (80.469)	LR: 0.0010000000000000002
Epoch: [105][1000/2503]	Loss 2.0738 (2.1067)	Prec@1 57.812 (54.187)	Prec@5 80.078 (77.685)	LR: 0.0010000000000000002
Epoch: [105][2000/2503]	Loss 2.1554 (2.1147)	Prec@1 52.344 (54.062)	Prec@5 78.516 (77.530)	LR: 0.0010000000000000002
 * Prec@1 53.990 Prec@5 77.464
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0421 (1.0421)	Prec@1 73.047 (73.047)	Prec@5 92.383 (92.383)
 * Prec@1 60.190 Prec@5 83.288
Best accuracy: 60.98%

Epoch: [106][0/2503]	Loss 2.1680 (2.1680)	Prec@1 56.836 (56.836)	Prec@5 76.562 (76.562)	LR: 0.0010000000000000002
Epoch: [106][1000/2503]	Loss 2.0617 (2.1135)	Prec@1 53.516 (54.118)	Prec@5 78.906 (77.549)	LR: 0.0010000000000000002
Epoch: [106][2000/2503]	Loss 2.1184 (2.1176)	Prec@1 53.711 (54.027)	Prec@5 75.586 (77.477)	LR: 0.0010000000000000002
 * Prec@1 54.010 Prec@5 77.461
Best Train Accuracy: 54.01%

Test: [0/98]	Loss 1.0534 (1.0534)	Prec@1 73.828 (73.828)	Prec@5 92.773 (92.773)
 * Prec@1 59.302 Prec@5 82.582
Best accuracy: 60.98%

Epoch: [107][0/2503]	Loss 2.1534 (2.1534)	Prec@1 50.781 (50.781)	Prec@5 75.586 (75.586)	LR: 0.0010000000000000002
Epoch: [107][1000/2503]	Loss 1.9256 (2.1080)	Prec@1 56.055 (54.243)	Prec@5 80.469 (77.620)	LR: 0.0010000000000000002
Epoch: [107][2000/2503]	Loss 2.2278 (2.1159)	Prec@1 52.148 (54.086)	Prec@5 77.148 (77.494)	LR: 0.0010000000000000002
 * Prec@1 54.040 Prec@5 77.480
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0390 (1.0390)	Prec@1 73.438 (73.438)	Prec@5 92.383 (92.383)
 * Prec@1 59.838 Prec@5 82.972
Best accuracy: 60.98%

Epoch: [108][0/2503]	Loss 2.1347 (2.1347)	Prec@1 51.367 (51.367)	Prec@5 78.320 (78.320)	LR: 0.0010000000000000002
Epoch: [108][1000/2503]	Loss 2.1422 (2.1133)	Prec@1 54.492 (54.127)	Prec@5 76.367 (77.574)	LR: 0.0010000000000000002
Epoch: [108][2000/2503]	Loss 2.1926 (2.1183)	Prec@1 53.516 (54.027)	Prec@5 75.586 (77.461)	LR: 0.0010000000000000002
 * Prec@1 53.983 Prec@5 77.423
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.1179 (1.1179)	Prec@1 72.656 (72.656)	Prec@5 90.820 (90.820)
 * Prec@1 60.176 Prec@5 83.204
Best accuracy: 60.98%

Epoch: [109][0/2503]	Loss 2.1254 (2.1254)	Prec@1 53.516 (53.516)	Prec@5 77.930 (77.930)	LR: 0.0010000000000000002
Epoch: [109][1000/2503]	Loss 2.0941 (2.1193)	Prec@1 54.492 (54.045)	Prec@5 76.953 (77.415)	LR: 0.0010000000000000002
Epoch: [109][2000/2503]	Loss 2.1113 (2.1202)	Prec@1 54.883 (53.982)	Prec@5 75.977 (77.418)	LR: 0.0010000000000000002
 * Prec@1 53.960 Prec@5 77.414
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0312 (1.0312)	Prec@1 74.414 (74.414)	Prec@5 92.578 (92.578)
 * Prec@1 60.312 Prec@5 83.330
Best accuracy: 60.98%

Epoch: [110][0/2503]	Loss 2.2525 (2.2525)	Prec@1 50.781 (50.781)	Prec@5 75.781 (75.781)	LR: 0.0010000000000000002
Epoch: [110][1000/2503]	Loss 2.1463 (2.1129)	Prec@1 50.977 (54.084)	Prec@5 77.734 (77.641)	LR: 0.0010000000000000002
Epoch: [110][2000/2503]	Loss 1.9295 (2.1179)	Prec@1 57.617 (54.012)	Prec@5 82.617 (77.535)	LR: 0.0010000000000000002
 * Prec@1 53.986 Prec@5 77.477
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0859 (1.0859)	Prec@1 72.852 (72.852)	Prec@5 93.164 (93.164)
 * Prec@1 60.608 Prec@5 83.536
Best accuracy: 60.98%

Epoch: [111][0/2503]	Loss 2.1441 (2.1441)	Prec@1 53.711 (53.711)	Prec@5 76.172 (76.172)	LR: 0.0010000000000000002
Epoch: [111][1000/2503]	Loss 2.1235 (2.1109)	Prec@1 54.688 (54.171)	Prec@5 78.516 (77.621)	LR: 0.0010000000000000002
Epoch: [111][2000/2503]	Loss 2.0912 (2.1191)	Prec@1 54.688 (54.006)	Prec@5 77.930 (77.483)	LR: 0.0010000000000000002
 * Prec@1 53.968 Prec@5 77.442
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.1314 (1.1314)	Prec@1 72.070 (72.070)	Prec@5 91.211 (91.211)
 * Prec@1 60.018 Prec@5 83.324
Best accuracy: 60.98%

Epoch: [112][0/2503]	Loss 2.1773 (2.1773)	Prec@1 53.906 (53.906)	Prec@5 75.977 (75.977)	LR: 0.0010000000000000002
Epoch: [112][1000/2503]	Loss 2.1023 (2.1140)	Prec@1 53.125 (54.077)	Prec@5 77.930 (77.511)	LR: 0.0010000000000000002
Epoch: [112][2000/2503]	Loss 2.0824 (2.1187)	Prec@1 55.664 (53.958)	Prec@5 78.906 (77.458)	LR: 0.0010000000000000002
 * Prec@1 53.935 Prec@5 77.422
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0991 (1.0991)	Prec@1 72.070 (72.070)	Prec@5 92.578 (92.578)
 * Prec@1 60.320 Prec@5 83.280
Best accuracy: 60.98%

Epoch: [113][0/2503]	Loss 2.1719 (2.1719)	Prec@1 50.586 (50.586)	Prec@5 76.172 (76.172)	LR: 0.0010000000000000002
Epoch: [113][1000/2503]	Loss 1.9910 (2.1201)	Prec@1 55.664 (54.006)	Prec@5 79.883 (77.483)	LR: 0.0010000000000000002
Epoch: [113][2000/2503]	Loss 2.1767 (2.1232)	Prec@1 51.953 (53.966)	Prec@5 78.516 (77.405)	LR: 0.0010000000000000002
 * Prec@1 53.916 Prec@5 77.374
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0743 (1.0743)	Prec@1 73.242 (73.242)	Prec@5 91.992 (91.992)
 * Prec@1 60.410 Prec@5 83.624
Best accuracy: 60.98%

Epoch: [114][0/2503]	Loss 1.9175 (1.9175)	Prec@1 58.008 (58.008)	Prec@5 81.836 (81.836)	LR: 0.0010000000000000002
Epoch: [114][1000/2503]	Loss 2.1864 (2.1138)	Prec@1 51.758 (54.195)	Prec@5 78.320 (77.569)	LR: 0.0010000000000000002
Epoch: [114][2000/2503]	Loss 2.0945 (2.1229)	Prec@1 56.641 (53.983)	Prec@5 78.516 (77.415)	LR: 0.0010000000000000002
 * Prec@1 53.952 Prec@5 77.374
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.2580 (1.2580)	Prec@1 70.898 (70.898)	Prec@5 89.648 (89.648)
 * Prec@1 58.804 Prec@5 82.270
Best accuracy: 60.98%

Epoch: [115][0/2503]	Loss 2.0456 (2.0456)	Prec@1 56.641 (56.641)	Prec@5 78.516 (78.516)	LR: 0.0010000000000000002
Epoch: [115][1000/2503]	Loss 2.1346 (2.1140)	Prec@1 53.906 (54.178)	Prec@5 75.781 (77.514)	LR: 0.0010000000000000002
Epoch: [115][2000/2503]	Loss 2.1461 (2.1201)	Prec@1 53.711 (54.010)	Prec@5 77.539 (77.439)	LR: 0.0010000000000000002
 * Prec@1 53.974 Prec@5 77.401
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0551 (1.0551)	Prec@1 73.047 (73.047)	Prec@5 93.945 (93.945)
 * Prec@1 59.976 Prec@5 83.184
Best accuracy: 60.98%

Epoch: [116][0/2503]	Loss 2.0689 (2.0689)	Prec@1 56.445 (56.445)	Prec@5 78.125 (78.125)	LR: 0.0010000000000000002
Epoch: [116][1000/2503]	Loss 2.2655 (2.1145)	Prec@1 50.000 (54.143)	Prec@5 76.367 (77.603)	LR: 0.0010000000000000002
Epoch: [116][2000/2503]	Loss 2.0916 (2.1205)	Prec@1 57.617 (54.006)	Prec@5 78.906 (77.441)	LR: 0.0010000000000000002
 * Prec@1 53.966 Prec@5 77.400
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.1457 (1.1457)	Prec@1 70.117 (70.117)	Prec@5 91.797 (91.797)
 * Prec@1 59.240 Prec@5 82.500
Best accuracy: 60.98%

Epoch: [117][0/2503]	Loss 2.2207 (2.2207)	Prec@1 51.562 (51.562)	Prec@5 76.367 (76.367)	LR: 0.0010000000000000002
Epoch: [117][1000/2503]	Loss 2.2256 (2.1170)	Prec@1 49.414 (54.058)	Prec@5 76.367 (77.488)	LR: 0.0010000000000000002
Epoch: [117][2000/2503]	Loss 2.1631 (2.1202)	Prec@1 53.906 (53.983)	Prec@5 77.344 (77.447)	LR: 0.0010000000000000002
 * Prec@1 53.932 Prec@5 77.406
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.1262 (1.1262)	Prec@1 71.289 (71.289)	Prec@5 92.383 (92.383)
 * Prec@1 60.166 Prec@5 83.488
Best accuracy: 60.98%

Epoch: [118][0/2503]	Loss 2.0832 (2.0832)	Prec@1 53.125 (53.125)	Prec@5 78.320 (78.320)	LR: 0.0010000000000000002
Epoch: [118][1000/2503]	Loss 2.1289 (2.1153)	Prec@1 52.930 (54.156)	Prec@5 76.758 (77.445)	LR: 0.0010000000000000002
Epoch: [118][2000/2503]	Loss 2.0210 (2.1206)	Prec@1 56.836 (54.078)	Prec@5 80.078 (77.384)	LR: 0.0010000000000000002
 * Prec@1 54.015 Prec@5 77.346
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.0605 (1.0605)	Prec@1 73.633 (73.633)	Prec@5 92.578 (92.578)
 * Prec@1 60.498 Prec@5 83.556
Best accuracy: 60.98%

Epoch: [119][0/2503]	Loss 2.0398 (2.0398)	Prec@1 56.641 (56.641)	Prec@5 79.102 (79.102)	LR: 0.0010000000000000002
Epoch: [119][1000/2503]	Loss 2.0936 (2.1135)	Prec@1 53.906 (54.149)	Prec@5 75.391 (77.523)	LR: 0.0010000000000000002
Epoch: [119][2000/2503]	Loss 2.4235 (2.1221)	Prec@1 48.242 (53.999)	Prec@5 71.289 (77.378)	LR: 0.0010000000000000002
 * Prec@1 53.963 Prec@5 77.338
Best Train Accuracy: 54.04%

Test: [0/98]	Loss 1.1492 (1.1492)	Prec@1 70.703 (70.703)	Prec@5 90.625 (90.625)
 * Prec@1 60.142 Prec@5 83.146
Best accuracy: 60.98%

Epoch: [120][0/2503]	Loss 2.0973 (2.0973)	Prec@1 52.734 (52.734)	Prec@5 76.953 (76.953)	LR: 0.00010000000000000003
Epoch: [120][1000/2503]	Loss 1.9885 (2.0464)	Prec@1 56.055 (55.342)	Prec@5 79.297 (78.519)	LR: 0.00010000000000000003
Epoch: [120][2000/2503]	Loss 2.1335 (2.0339)	Prec@1 54.102 (55.604)	Prec@5 75.977 (78.706)	LR: 0.00010000000000000003
 * Prec@1 55.649 Prec@5 78.757
Best Train Accuracy: 55.65%

Test: [0/98]	Loss 0.9742 (0.9742)	Prec@1 75.391 (75.391)	Prec@5 93.359 (93.359)
 * Prec@1 62.290 Prec@5 85.030
Best accuracy: 62.29%

Epoch: [121][0/2503]	Loss 2.0452 (2.0452)	Prec@1 56.836 (56.836)	Prec@5 79.492 (79.492)	LR: 0.00010000000000000003
Epoch: [121][1000/2503]	Loss 1.8461 (2.0073)	Prec@1 61.914 (56.099)	Prec@5 81.250 (79.061)	LR: 0.00010000000000000003
Epoch: [121][2000/2503]	Loss 1.9963 (2.0076)	Prec@1 56.641 (56.052)	Prec@5 80.273 (79.086)	LR: 0.00010000000000000003
 * Prec@1 56.069 Prec@5 79.088
Best Train Accuracy: 56.07%

Test: [0/98]	Loss 0.9537 (0.9537)	Prec@1 75.586 (75.586)	Prec@5 93.750 (93.750)
 * Prec@1 62.518 Prec@5 85.050
Best accuracy: 62.52%

Epoch: [122][0/2503]	Loss 2.0314 (2.0314)	Prec@1 52.539 (52.539)	Prec@5 78.711 (78.711)	LR: 0.00010000000000000003
Epoch: [122][1000/2503]	Loss 1.8987 (1.9989)	Prec@1 58.789 (56.355)	Prec@5 81.250 (79.199)	LR: 0.00010000000000000003
Epoch: [122][2000/2503]	Loss 2.0644 (1.9997)	Prec@1 57.227 (56.299)	Prec@5 77.930 (79.154)	LR: 0.00010000000000000003
 * Prec@1 56.323 Prec@5 79.175
Best Train Accuracy: 56.32%

Test: [0/98]	Loss 0.9528 (0.9528)	Prec@1 75.195 (75.195)	Prec@5 93.359 (93.359)
 * Prec@1 62.718 Prec@5 85.242
Best accuracy: 62.72%

Epoch: [123][0/2503]	Loss 2.1289 (2.1289)	Prec@1 54.492 (54.492)	Prec@5 75.391 (75.391)	LR: 0.00010000000000000003
Epoch: [123][1000/2503]	Loss 2.1148 (1.9923)	Prec@1 56.836 (56.423)	Prec@5 77.539 (79.346)	LR: 0.00010000000000000003
Epoch: [123][2000/2503]	Loss 1.9982 (1.9931)	Prec@1 58.203 (56.401)	Prec@5 80.469 (79.341)	LR: 0.00010000000000000003
 * Prec@1 56.368 Prec@5 79.324
Best Train Accuracy: 56.37%

Test: [0/98]	Loss 0.9892 (0.9892)	Prec@1 76.172 (76.172)	Prec@5 93.359 (93.359)
 * Prec@1 62.820 Prec@5 85.172
Best accuracy: 62.82%

Epoch: [124][0/2503]	Loss 2.0920 (2.0920)	Prec@1 52.734 (52.734)	Prec@5 77.930 (77.930)	LR: 0.00010000000000000003
Epoch: [124][1000/2503]	Loss 2.0761 (1.9905)	Prec@1 56.055 (56.423)	Prec@5 76.367 (79.294)	LR: 0.00010000000000000003
Epoch: [124][2000/2503]	Loss 2.0230 (1.9915)	Prec@1 53.125 (56.402)	Prec@5 79.688 (79.295)	LR: 0.00010000000000000003
 * Prec@1 56.387 Prec@5 79.291
Best Train Accuracy: 56.39%

Test: [0/98]	Loss 0.9886 (0.9886)	Prec@1 74.805 (74.805)	Prec@5 92.188 (92.188)
 * Prec@1 62.946 Prec@5 85.138
Best accuracy: 62.95%

Epoch: [125][0/2503]	Loss 1.9706 (1.9706)	Prec@1 55.664 (55.664)	Prec@5 80.273 (80.273)	LR: 0.00010000000000000003
Epoch: [125][1000/2503]	Loss 2.0155 (1.9874)	Prec@1 57.031 (56.538)	Prec@5 79.688 (79.340)	LR: 0.00010000000000000003
Epoch: [125][2000/2503]	Loss 2.1864 (1.9886)	Prec@1 50.977 (56.530)	Prec@5 75.586 (79.332)	LR: 0.00010000000000000003
 * Prec@1 56.518 Prec@5 79.310
Best Train Accuracy: 56.52%

Test: [0/98]	Loss 0.9350 (0.9350)	Prec@1 78.320 (78.320)	Prec@5 93.359 (93.359)
 * Prec@1 62.796 Prec@5 85.218
Best accuracy: 62.95%

Epoch: [126][0/2503]	Loss 2.0546 (2.0546)	Prec@1 54.883 (54.883)	Prec@5 78.125 (78.125)	LR: 0.00010000000000000003
Epoch: [126][1000/2503]	Loss 2.1254 (1.9861)	Prec@1 54.297 (56.537)	Prec@5 77.148 (79.394)	LR: 0.00010000000000000003
Epoch: [126][2000/2503]	Loss 2.0147 (1.9874)	Prec@1 55.664 (56.533)	Prec@5 81.641 (79.366)	LR: 0.00010000000000000003
 * Prec@1 56.490 Prec@5 79.360
Best Train Accuracy: 56.52%

Test: [0/98]	Loss 0.9878 (0.9878)	Prec@1 75.195 (75.195)	Prec@5 93.359 (93.359)
 * Prec@1 62.992 Prec@5 85.176
Best accuracy: 62.99%

Epoch: [127][0/2503]	Loss 2.0983 (2.0983)	Prec@1 54.297 (54.297)	Prec@5 75.977 (75.977)	LR: 0.00010000000000000003
Epoch: [127][1000/2503]	Loss 2.1320 (1.9801)	Prec@1 53.320 (56.682)	Prec@5 76.758 (79.483)	LR: 0.00010000000000000003
Epoch: [127][2000/2503]	Loss 2.2557 (1.9830)	Prec@1 51.562 (56.605)	Prec@5 74.609 (79.445)	LR: 0.00010000000000000003
 * Prec@1 56.592 Prec@5 79.438
Best Train Accuracy: 56.59%

Test: [0/98]	Loss 0.9579 (0.9579)	Prec@1 75.781 (75.781)	Prec@5 93.164 (93.164)
 * Prec@1 62.904 Prec@5 85.222
Best accuracy: 62.99%

Epoch: [128][0/2503]	Loss 1.9725 (1.9725)	Prec@1 57.422 (57.422)	Prec@5 79.297 (79.297)	LR: 0.00010000000000000003
Epoch: [128][1000/2503]	Loss 2.1206 (1.9848)	Prec@1 52.539 (56.627)	Prec@5 77.930 (79.412)	LR: 0.00010000000000000003
Epoch: [128][2000/2503]	Loss 1.9595 (1.9860)	Prec@1 56.641 (56.544)	Prec@5 79.883 (79.381)	LR: 0.00010000000000000003
 * Prec@1 56.541 Prec@5 79.383
Best Train Accuracy: 56.59%

Test: [0/98]	Loss 1.0111 (1.0111)	Prec@1 75.977 (75.977)	Prec@5 92.188 (92.188)
 * Prec@1 62.880 Prec@5 85.370
Best accuracy: 62.99%

Epoch: [129][0/2503]	Loss 1.9308 (1.9308)	Prec@1 57.031 (57.031)	Prec@5 79.883 (79.883)	LR: 0.00010000000000000003
Epoch: [129][1000/2503]	Loss 1.9657 (1.9826)	Prec@1 57.422 (56.584)	Prec@5 79.492 (79.369)	LR: 0.00010000000000000003
Epoch: [129][2000/2503]	Loss 1.7798 (1.9845)	Prec@1 60.938 (56.559)	Prec@5 81.641 (79.361)	LR: 0.00010000000000000003
 * Prec@1 56.550 Prec@5 79.370
Best Train Accuracy: 56.59%

Test: [0/98]	Loss 0.9797 (0.9797)	Prec@1 77.148 (77.148)	Prec@5 92.578 (92.578)
 * Prec@1 62.902 Prec@5 85.270
Best accuracy: 62.99%

Epoch: [130][0/2503]	Loss 1.9576 (1.9576)	Prec@1 57.227 (57.227)	Prec@5 81.055 (81.055)	LR: 0.00010000000000000003
Epoch: [130][1000/2503]	Loss 2.0748 (1.9763)	Prec@1 53.320 (56.726)	Prec@5 76.758 (79.564)	LR: 0.00010000000000000003
Epoch: [130][2000/2503]	Loss 1.9273 (1.9801)	Prec@1 57.031 (56.669)	Prec@5 80.273 (79.485)	LR: 0.00010000000000000003
 * Prec@1 56.661 Prec@5 79.488
Best Train Accuracy: 56.66%

Test: [0/98]	Loss 0.9639 (0.9639)	Prec@1 76.367 (76.367)	Prec@5 92.578 (92.578)
 * Prec@1 62.872 Prec@5 85.232
Best accuracy: 62.99%

Epoch: [131][0/2503]	Loss 1.9074 (1.9074)	Prec@1 56.836 (56.836)	Prec@5 81.445 (81.445)	LR: 0.00010000000000000003
Epoch: [131][1000/2503]	Loss 1.8710 (1.9782)	Prec@1 57.227 (56.689)	Prec@5 82.812 (79.524)	LR: 0.00010000000000000003
Epoch: [131][2000/2503]	Loss 1.8609 (1.9793)	Prec@1 58.789 (56.681)	Prec@5 81.445 (79.496)	LR: 0.00010000000000000003
 * Prec@1 56.688 Prec@5 79.506
Best Train Accuracy: 56.69%

Test: [0/98]	Loss 0.9576 (0.9576)	Prec@1 76.562 (76.562)	Prec@5 92.578 (92.578)
 * Prec@1 63.172 Prec@5 85.352
Best accuracy: 63.17%

Epoch: [132][0/2503]	Loss 2.0585 (2.0585)	Prec@1 54.688 (54.688)	Prec@5 79.297 (79.297)	LR: 0.00010000000000000003
Epoch: [132][1000/2503]	Loss 2.0196 (1.9747)	Prec@1 56.445 (56.799)	Prec@5 78.711 (79.608)	LR: 0.00010000000000000003
Epoch: [132][2000/2503]	Loss 2.0282 (1.9783)	Prec@1 56.055 (56.709)	Prec@5 79.492 (79.536)	LR: 0.00010000000000000003
 * Prec@1 56.696 Prec@5 79.513
Best Train Accuracy: 56.70%

Test: [0/98]	Loss 0.9601 (0.9601)	Prec@1 76.953 (76.953)	Prec@5 92.969 (92.969)
 * Prec@1 63.034 Prec@5 85.238
Best accuracy: 63.17%

Epoch: [133][0/2503]	Loss 2.0638 (2.0638)	Prec@1 54.297 (54.297)	Prec@5 79.492 (79.492)	LR: 0.00010000000000000003
Epoch: [133][1000/2503]	Loss 2.0329 (1.9737)	Prec@1 55.664 (56.713)	Prec@5 77.930 (79.624)	LR: 0.00010000000000000003
Epoch: [133][2000/2503]	Loss 1.9328 (1.9762)	Prec@1 58.789 (56.660)	Prec@5 80.469 (79.538)	LR: 0.00010000000000000003
 * Prec@1 56.641 Prec@5 79.514
Best Train Accuracy: 56.70%

Test: [0/98]	Loss 0.9182 (0.9182)	Prec@1 77.539 (77.539)	Prec@5 94.141 (94.141)
 * Prec@1 63.036 Prec@5 85.238
Best accuracy: 63.17%

Epoch: [134][0/2503]	Loss 1.9275 (1.9275)	Prec@1 60.156 (60.156)	Prec@5 79.297 (79.297)	LR: 0.00010000000000000003
Epoch: [134][1000/2503]	Loss 2.0554 (1.9783)	Prec@1 56.250 (56.742)	Prec@5 78.906 (79.509)	LR: 0.00010000000000000003
Epoch: [134][2000/2503]	Loss 1.8458 (1.9797)	Prec@1 59.570 (56.677)	Prec@5 80.273 (79.464)	LR: 0.00010000000000000003
 * Prec@1 56.686 Prec@5 79.458
Best Train Accuracy: 56.70%

Test: [0/98]	Loss 0.9090 (0.9090)	Prec@1 76.562 (76.562)	Prec@5 93.750 (93.750)
 * Prec@1 63.106 Prec@5 85.364
Best accuracy: 63.17%

Epoch: [135][0/2503]	Loss 1.9057 (1.9057)	Prec@1 57.031 (57.031)	Prec@5 79.688 (79.688)	LR: 0.00010000000000000003
Epoch: [135][1000/2503]	Loss 1.9309 (1.9742)	Prec@1 55.664 (56.794)	Prec@5 80.664 (79.549)	LR: 0.00010000000000000003
Epoch: [135][2000/2503]	Loss 1.9055 (1.9779)	Prec@1 58.594 (56.702)	Prec@5 80.859 (79.487)	LR: 0.00010000000000000003
 * Prec@1 56.708 Prec@5 79.480
Best Train Accuracy: 56.71%

Test: [0/98]	Loss 0.9918 (0.9918)	Prec@1 75.586 (75.586)	Prec@5 93.164 (93.164)
 * Prec@1 63.058 Prec@5 85.234
Best accuracy: 63.17%

Epoch: [136][0/2503]	Loss 2.0070 (2.0070)	Prec@1 54.688 (54.688)	Prec@5 79.297 (79.297)	LR: 0.00010000000000000003
Epoch: [136][1000/2503]	Loss 2.2249 (1.9733)	Prec@1 54.102 (56.780)	Prec@5 74.805 (79.561)	LR: 0.00010000000000000003
Epoch: [136][2000/2503]	Loss 2.0138 (1.9774)	Prec@1 57.031 (56.706)	Prec@5 76.758 (79.512)	LR: 0.00010000000000000003
 * Prec@1 56.708 Prec@5 79.507
Best Train Accuracy: 56.71%

Test: [0/98]	Loss 1.0365 (1.0365)	Prec@1 74.023 (74.023)	Prec@5 91.797 (91.797)
 * Prec@1 62.888 Prec@5 85.254
Best accuracy: 63.17%

Epoch: [137][0/2503]	Loss 1.8990 (1.8990)	Prec@1 57.812 (57.812)	Prec@5 79.883 (79.883)	LR: 0.00010000000000000003
Epoch: [137][1000/2503]	Loss 1.9297 (1.9699)	Prec@1 59.766 (56.878)	Prec@5 80.469 (79.651)	LR: 0.00010000000000000003
Epoch: [137][2000/2503]	Loss 1.9012 (1.9745)	Prec@1 58.789 (56.793)	Prec@5 80.664 (79.571)	LR: 0.00010000000000000003
 * Prec@1 56.756 Prec@5 79.571
Best Train Accuracy: 56.76%

Test: [0/98]	Loss 0.9638 (0.9638)	Prec@1 76.172 (76.172)	Prec@5 93.164 (93.164)
 * Prec@1 62.954 Prec@5 85.350
Best accuracy: 63.17%

Epoch: [138][0/2503]	Loss 2.0224 (2.0224)	Prec@1 53.125 (53.125)	Prec@5 79.102 (79.102)	LR: 0.00010000000000000003
Epoch: [138][1000/2503]	Loss 1.9817 (1.9713)	Prec@1 55.859 (56.856)	Prec@5 79.688 (79.664)	LR: 0.00010000000000000003
Epoch: [138][2000/2503]	Loss 2.0462 (1.9736)	Prec@1 55.078 (56.784)	Prec@5 77.734 (79.643)	LR: 0.00010000000000000003
 * Prec@1 56.765 Prec@5 79.608
Best Train Accuracy: 56.76%

Test: [0/98]	Loss 1.0275 (1.0275)	Prec@1 75.391 (75.391)	Prec@5 92.969 (92.969)
 * Prec@1 63.090 Prec@5 85.202
Best accuracy: 63.17%

Epoch: [139][0/2503]	Loss 1.9736 (1.9736)	Prec@1 59.375 (59.375)	Prec@5 78.711 (78.711)	LR: 0.00010000000000000003
Epoch: [139][1000/2503]	Loss 2.1084 (1.9690)	Prec@1 55.859 (56.873)	Prec@5 76.562 (79.641)	LR: 0.00010000000000000003
Epoch: [139][2000/2503]	Loss 1.7752 (1.9702)	Prec@1 60.547 (56.881)	Prec@5 82.617 (79.659)	LR: 0.00010000000000000003
 * Prec@1 56.806 Prec@5 79.602
Best Train Accuracy: 56.81%

Test: [0/98]	Loss 0.9320 (0.9320)	Prec@1 76.172 (76.172)	Prec@5 93.555 (93.555)
 * Prec@1 63.130 Prec@5 85.308
Best accuracy: 63.17%

Epoch: [140][0/2503]	Loss 1.9265 (1.9265)	Prec@1 59.570 (59.570)	Prec@5 79.688 (79.688)	LR: 0.00010000000000000003
Epoch: [140][1000/2503]	Loss 2.0989 (1.9736)	Prec@1 52.148 (56.802)	Prec@5 78.906 (79.603)	LR: 0.00010000000000000003
Epoch: [140][2000/2503]	Loss 1.7854 (1.9745)	Prec@1 59.570 (56.789)	Prec@5 83.789 (79.583)	LR: 0.00010000000000000003
 * Prec@1 56.752 Prec@5 79.533
Best Train Accuracy: 56.81%

Test: [0/98]	Loss 0.9776 (0.9776)	Prec@1 74.805 (74.805)	Prec@5 93.164 (93.164)
 * Prec@1 63.264 Prec@5 85.304
Best accuracy: 63.26%

Epoch: [141][0/2503]	Loss 2.0655 (2.0655)	Prec@1 56.641 (56.641)	Prec@5 78.125 (78.125)	LR: 0.00010000000000000003
Epoch: [141][1000/2503]	Loss 1.8851 (1.9707)	Prec@1 59.375 (56.817)	Prec@5 80.469 (79.632)	LR: 0.00010000000000000003
Epoch: [141][2000/2503]	Loss 2.0664 (1.9723)	Prec@1 54.492 (56.786)	Prec@5 79.297 (79.587)	LR: 0.00010000000000000003
 * Prec@1 56.757 Prec@5 79.588
Best Train Accuracy: 56.81%

Test: [0/98]	Loss 1.0227 (1.0227)	Prec@1 75.000 (75.000)	Prec@5 93.164 (93.164)
 * Prec@1 63.082 Prec@5 85.362
Best accuracy: 63.26%

Epoch: [142][0/2503]	Loss 1.9687 (1.9687)	Prec@1 59.180 (59.180)	Prec@5 79.297 (79.297)	LR: 0.00010000000000000003
Epoch: [142][1000/2503]	Loss 2.2623 (1.9696)	Prec@1 51.758 (56.888)	Prec@5 75.195 (79.637)	LR: 0.00010000000000000003
Epoch: [142][2000/2503]	Loss 1.8770 (1.9713)	Prec@1 57.617 (56.837)	Prec@5 80.859 (79.598)	LR: 0.00010000000000000003
 * Prec@1 56.810 Prec@5 79.567
Best Train Accuracy: 56.81%

Test: [0/98]	Loss 0.9678 (0.9678)	Prec@1 76.367 (76.367)	Prec@5 94.531 (94.531)
 * Prec@1 62.944 Prec@5 85.242
Best accuracy: 63.26%

Epoch: [143][0/2503]	Loss 1.9043 (1.9043)	Prec@1 57.617 (57.617)	Prec@5 80.664 (80.664)	LR: 0.00010000000000000003
Epoch: [143][1000/2503]	Loss 1.9829 (1.9698)	Prec@1 56.445 (56.869)	Prec@5 79.102 (79.662)	LR: 0.00010000000000000003
Epoch: [143][2000/2503]	Loss 1.9867 (1.9735)	Prec@1 55.859 (56.833)	Prec@5 79.883 (79.581)	LR: 0.00010000000000000003
 * Prec@1 56.797 Prec@5 79.557
Best Train Accuracy: 56.81%

Test: [0/98]	Loss 0.9533 (0.9533)	Prec@1 76.172 (76.172)	Prec@5 93.945 (93.945)
 * Prec@1 63.032 Prec@5 85.342
Best accuracy: 63.26%

Epoch: [144][0/2503]	Loss 1.8381 (1.8381)	Prec@1 56.641 (56.641)	Prec@5 83.984 (83.984)	LR: 0.00010000000000000003
Epoch: [144][1000/2503]	Loss 2.0314 (1.9692)	Prec@1 56.836 (56.871)	Prec@5 78.320 (79.727)	LR: 0.00010000000000000003
Epoch: [144][2000/2503]	Loss 2.2570 (1.9713)	Prec@1 54.883 (56.824)	Prec@5 75.391 (79.648)	LR: 0.00010000000000000003
 * Prec@1 56.847 Prec@5 79.617
Best Train Accuracy: 56.85%

Test: [0/98]	Loss 0.9351 (0.9351)	Prec@1 77.148 (77.148)	Prec@5 92.578 (92.578)
 * Prec@1 63.126 Prec@5 85.276
Best accuracy: 63.26%

Epoch: [145][0/2503]	Loss 1.8579 (1.8579)	Prec@1 58.398 (58.398)	Prec@5 81.445 (81.445)	LR: 0.00010000000000000003
Epoch: [145][1000/2503]	Loss 2.0798 (1.9683)	Prec@1 52.344 (56.843)	Prec@5 77.148 (79.656)	LR: 0.00010000000000000003
Epoch: [145][2000/2503]	Loss 1.9991 (1.9720)	Prec@1 54.297 (56.796)	Prec@5 80.859 (79.602)	LR: 0.00010000000000000003
 * Prec@1 56.795 Prec@5 79.609
Best Train Accuracy: 56.85%

Test: [0/98]	Loss 0.9797 (0.9797)	Prec@1 75.195 (75.195)	Prec@5 92.383 (92.383)
 * Prec@1 63.282 Prec@5 85.320
Best accuracy: 63.28%

Epoch: [146][0/2503]	Loss 1.8850 (1.8850)	Prec@1 57.617 (57.617)	Prec@5 84.180 (84.180)	LR: 0.00010000000000000003
Epoch: [146][1000/2503]	Loss 1.8390 (1.9653)	Prec@1 60.547 (56.925)	Prec@5 82.812 (79.720)	LR: 0.00010000000000000003
Epoch: [146][2000/2503]	Loss 1.9704 (1.9709)	Prec@1 56.641 (56.827)	Prec@5 79.102 (79.632)	LR: 0.00010000000000000003
 * Prec@1 56.841 Prec@5 79.645
Best Train Accuracy: 56.85%

Test: [0/98]	Loss 0.9682 (0.9682)	Prec@1 76.758 (76.758)	Prec@5 93.555 (93.555)
 * Prec@1 62.942 Prec@5 85.310
Best accuracy: 63.28%

Epoch: [147][0/2503]	Loss 1.9470 (1.9470)	Prec@1 56.445 (56.445)	Prec@5 80.664 (80.664)	LR: 0.00010000000000000003
Epoch: [147][1000/2503]	Loss 2.0835 (1.9675)	Prec@1 54.102 (56.931)	Prec@5 76.172 (79.670)	LR: 0.00010000000000000003
Epoch: [147][2000/2503]	Loss 1.9024 (1.9687)	Prec@1 58.398 (56.911)	Prec@5 80.859 (79.639)	LR: 0.00010000000000000003
 * Prec@1 56.879 Prec@5 79.601
Best Train Accuracy: 56.88%

Test: [0/98]	Loss 0.9500 (0.9500)	Prec@1 77.148 (77.148)	Prec@5 93.164 (93.164)
 * Prec@1 62.996 Prec@5 85.496
Best accuracy: 63.28%

Epoch: [148][0/2503]	Loss 1.7863 (1.7863)	Prec@1 60.156 (60.156)	Prec@5 81.641 (81.641)	LR: 0.00010000000000000003
Epoch: [148][1000/2503]	Loss 1.9990 (1.9706)	Prec@1 55.273 (56.821)	Prec@5 78.125 (79.650)	LR: 0.00010000000000000003
Epoch: [148][2000/2503]	Loss 1.8901 (1.9734)	Prec@1 58.203 (56.815)	Prec@5 81.445 (79.572)	LR: 0.00010000000000000003
 * Prec@1 56.822 Prec@5 79.573
Best Train Accuracy: 56.88%

Test: [0/98]	Loss 0.9565 (0.9565)	Prec@1 75.977 (75.977)	Prec@5 92.969 (92.969)
 * Prec@1 62.952 Prec@5 85.290
Best accuracy: 63.28%

Epoch: [149][0/2503]	Loss 2.1066 (2.1066)	Prec@1 51.953 (51.953)	Prec@5 77.344 (77.344)	LR: 0.00010000000000000003
Epoch: [149][1000/2503]	Loss 1.9618 (1.9659)	Prec@1 57.031 (56.948)	Prec@5 79.297 (79.738)	LR: 0.00010000000000000003
Epoch: [149][2000/2503]	Loss 2.0675 (1.9722)	Prec@1 53.906 (56.787)	Prec@5 78.711 (79.629)	LR: 0.00010000000000000003
 * Prec@1 56.803 Prec@5 79.626
Best Train Accuracy: 56.88%

Test: [0/98]	Loss 0.9172 (0.9172)	Prec@1 75.781 (75.781)	Prec@5 93.750 (93.750)
 * Prec@1 63.064 Prec@5 85.402
Best accuracy: 63.28%

