
      ==> Arguments:
          dataset: imagenet
          model: resnet18
          workers: 8
          epochs: 150
          start_epoch: 0
          batch_size: 256
          lr: 0.0001
          momentum: 0.9
          weight_decay: 0.0001
          tag: qfp_i8b7f_w8b7f_4
          milestones: [40, 80, 120]
          gamma: 0.1
          input_size: None
          print_freq: 1000
          resume: ../pretrained_models/ideal/resnet18qfp_imagenet_full_qfp_i8b7f_w8b7f_3_best.pth.tar
          evaluate: False
          pretrained: None
          half: False
          savedir: ../pretrained_models/ideal/
          save_every: 10
          gpus: 0,1,2,3
DEVICE: cuda
GPU Id(s) being used: 0,1,2,3
==> Building model for resnet18 ...
=> loading checkpoint '../pretrained_models/ideal/resnet18qfp_imagenet_full_qfp_i8b7f_w8b7f_3_best.pth.tar'
Resumed model accuracy: 63.28199768066406
=> loaded checkpoint from ../pretrained_models/ideal/resnet18qfp_imagenet_full_qfp_i8b7f_w8b7f_3_best.pth.tar
Test: [0/196]	Loss 0.9684 (0.9684)	Prec@1 77.734 (77.734)	Prec@5 92.188 (92.188)
Test: [100/196]	Loss 2.1937 (1.3763)	Prec@1 46.875 (67.002)	Prec@5 79.688 (88.544)
 * Prec@1 63.282 Prec@5 85.320
Pretrained model accuracy: 63.28199768066406
Epoch: [0][0/5005]	Loss 2.4864 (2.4864)	Prec@1 51.172 (51.172)	Prec@5 73.438 (73.438)	LR: 0.0001
Epoch: [0][1000/5005]	Loss 4.5730 (4.5816)	Prec@1 16.797 (20.410)	Prec@5 31.250 (37.315)	LR: 0.0001
Epoch: [0][2000/5005]	Loss 4.5529 (4.5356)	Prec@1 16.406 (19.195)	Prec@5 34.375 (37.377)	LR: 0.0001
Epoch: [0][3000/5005]	Loss 4.2726 (4.5135)	Prec@1 21.094 (18.737)	Prec@5 44.141 (37.451)	LR: 0.0001
Epoch: [0][4000/5005]	Loss 4.8394 (4.6145)	Prec@1 14.844 (17.338)	Prec@5 31.641 (35.491)	LR: 0.0001
Epoch: [0][5000/5005]	Loss 4.5144 (4.6347)	Prec@1 14.844 (16.845)	Prec@5 35.938 (34.982)	LR: 0.0001
 * Prec@1 16.844 Prec@5 34.981
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.1993 (3.1993)	Prec@1 32.812 (32.812)	Prec@5 66.406 (66.406)
Test: [100/196]	Loss 5.0554 (4.0602)	Prec@1 8.203 (21.852)	Prec@5 26.172 (44.052)
 * Prec@1 21.302 Prec@5 42.370
Best accuracy: 63.28%

Epoch: [1][0/5005]	Loss 4.7557 (4.7557)	Prec@1 12.891 (12.891)	Prec@5 28.906 (28.906)	LR: 0.0001
Epoch: [1][1000/5005]	Loss 5.2778 (5.0090)	Prec@1 7.812 (12.517)	Prec@5 22.656 (28.122)	LR: 0.0001
Epoch: [1][2000/5005]	Loss 5.1004 (5.1095)	Prec@1 13.672 (11.045)	Prec@5 25.781 (25.934)	LR: 0.0001
Epoch: [1][3000/5005]	Loss 5.3313 (5.3029)	Prec@1 8.203 (9.324)	Prec@5 21.094 (22.640)	LR: 0.0001
Epoch: [1][4000/5005]	Loss 5.3338 (5.3187)	Prec@1 7.812 (9.010)	Prec@5 21.094 (22.181)	LR: 0.0001
Epoch: [1][5000/5005]	Loss 5.4018 (5.2977)	Prec@1 10.156 (9.117)	Prec@5 22.266 (22.472)	LR: 0.0001
 * Prec@1 9.117 Prec@5 22.473
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.9411 (3.9411)	Prec@1 21.094 (21.094)	Prec@5 51.953 (51.953)
Test: [100/196]	Loss 5.5370 (4.8708)	Prec@1 8.203 (11.931)	Prec@5 20.312 (29.084)
 * Prec@1 12.482 Prec@5 29.360
Best accuracy: 63.28%

Epoch: [2][0/5005]	Loss 5.3441 (5.3441)	Prec@1 12.891 (12.891)	Prec@5 23.438 (23.438)	LR: 0.0001
Epoch: [2][1000/5005]	Loss 5.2858 (5.1301)	Prec@1 10.938 (10.314)	Prec@5 22.656 (25.174)	LR: 0.0001
Epoch: [2][2000/5005]	Loss 5.1494 (5.1243)	Prec@1 8.984 (10.342)	Prec@5 24.609 (25.242)	LR: 0.0001
Epoch: [2][3000/5005]	Loss 5.1333 (5.1038)	Prec@1 13.672 (10.592)	Prec@5 25.781 (25.572)	LR: 0.0001
Epoch: [2][4000/5005]	Loss 4.7404 (5.0825)	Prec@1 14.062 (10.818)	Prec@5 30.859 (25.955)	LR: 0.0001
Epoch: [2][5000/5005]	Loss 4.9991 (5.0644)	Prec@1 10.938 (10.993)	Prec@5 24.609 (26.295)	LR: 0.0001
 * Prec@1 10.995 Prec@5 26.298
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.7821 (3.7821)	Prec@1 23.828 (23.828)	Prec@5 57.031 (57.031)
Test: [100/196]	Loss 5.4620 (4.7454)	Prec@1 6.641 (13.885)	Prec@5 20.312 (31.323)
 * Prec@1 14.568 Prec@5 31.960
Best accuracy: 63.28%

Epoch: [3][0/5005]	Loss 5.0999 (5.0999)	Prec@1 10.156 (10.156)	Prec@5 26.172 (26.172)	LR: 0.0001
Epoch: [3][1000/5005]	Loss 4.9370 (4.9664)	Prec@1 10.156 (11.926)	Prec@5 27.734 (27.908)	LR: 0.0001
Epoch: [3][2000/5005]	Loss 4.9200 (4.9553)	Prec@1 9.375 (12.113)	Prec@5 27.734 (28.164)	LR: 0.0001
Epoch: [3][3000/5005]	Loss 4.9554 (4.9451)	Prec@1 8.594 (12.252)	Prec@5 26.953 (28.384)	LR: 0.0001
Epoch: [3][4000/5005]	Loss 5.0799 (4.9337)	Prec@1 10.938 (12.389)	Prec@5 23.828 (28.618)	LR: 0.0001
Epoch: [3][5000/5005]	Loss 5.5126 (5.0809)	Prec@1 5.078 (11.170)	Prec@5 20.312 (26.222)	LR: 0.0001
 * Prec@1 11.167 Prec@5 26.216
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.9103 (3.9103)	Prec@1 21.484 (21.484)	Prec@5 56.250 (56.250)
Test: [100/196]	Loss 5.6951 (5.3543)	Prec@1 3.125 (8.377)	Prec@5 13.672 (21.674)
 * Prec@1 9.046 Prec@5 22.554
Best accuracy: 63.28%

Epoch: [4][0/5005]	Loss 5.7386 (5.7386)	Prec@1 3.906 (3.906)	Prec@5 14.062 (14.062)	LR: 0.0001
Epoch: [4][1000/5005]	Loss 5.2760 (5.3912)	Prec@1 6.250 (7.932)	Prec@5 24.609 (20.507)	LR: 0.0001
Epoch: [4][2000/5005]	Loss 5.1200 (5.3315)	Prec@1 8.984 (8.529)	Prec@5 22.266 (21.554)	LR: 0.0001
Epoch: [4][3000/5005]	Loss 5.2562 (5.2892)	Prec@1 10.156 (8.904)	Prec@5 21.875 (22.311)	LR: 0.0001
Epoch: [4][4000/5005]	Loss 5.2004 (5.2558)	Prec@1 8.594 (9.212)	Prec@5 21.875 (22.896)	LR: 0.0001
Epoch: [4][5000/5005]	Loss 4.9864 (5.2298)	Prec@1 12.109 (9.455)	Prec@5 27.344 (23.334)	LR: 0.0001
 * Prec@1 9.455 Prec@5 23.334
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.4701 (3.4701)	Prec@1 32.422 (32.422)	Prec@5 62.109 (62.109)
Test: [100/196]	Loss 5.4208 (4.7449)	Prec@1 5.469 (13.745)	Prec@5 21.875 (31.629)
 * Prec@1 14.348 Prec@5 32.108
Best accuracy: 63.28%

Epoch: [5][0/5005]	Loss 5.1405 (5.1405)	Prec@1 10.156 (10.156)	Prec@5 23.828 (23.828)	LR: 0.0001
Epoch: [5][1000/5005]	Loss 5.0019 (5.0906)	Prec@1 8.984 (10.820)	Prec@5 28.125 (25.789)	LR: 0.0001
Epoch: [5][2000/5005]	Loss 4.9503 (5.0802)	Prec@1 12.109 (10.904)	Prec@5 27.734 (25.955)	LR: 0.0001
Epoch: [5][3000/5005]	Loss 5.0575 (5.0785)	Prec@1 10.938 (10.936)	Prec@5 27.734 (26.001)	LR: 0.0001
Epoch: [5][4000/5005]	Loss 4.9094 (5.0718)	Prec@1 13.672 (11.004)	Prec@5 31.250 (26.131)	LR: 0.0001
Epoch: [5][5000/5005]	Loss 5.0277 (5.0660)	Prec@1 11.328 (11.063)	Prec@5 28.906 (26.246)	LR: 0.0001
 * Prec@1 11.063 Prec@5 26.246
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.6691 (3.6691)	Prec@1 23.828 (23.828)	Prec@5 57.422 (57.422)
Test: [100/196]	Loss 5.2514 (4.6175)	Prec@1 10.547 (14.801)	Prec@5 23.828 (33.397)
 * Prec@1 15.190 Prec@5 33.224
Best accuracy: 63.28%

Epoch: [6][0/5005]	Loss 5.1120 (5.1120)	Prec@1 9.766 (9.766)	Prec@5 24.219 (24.219)	LR: 0.0001
Epoch: [6][1000/5005]	Loss 5.1298 (5.0266)	Prec@1 9.766 (11.466)	Prec@5 25.391 (26.907)	LR: 0.0001
Epoch: [6][2000/5005]	Loss 5.0708 (5.0128)	Prec@1 15.234 (11.578)	Prec@5 26.562 (27.199)	LR: 0.0001
Epoch: [6][3000/5005]	Loss 4.9116 (5.0030)	Prec@1 10.938 (11.686)	Prec@5 30.078 (27.397)	LR: 0.0001
Epoch: [6][4000/5005]	Loss 4.9432 (4.9948)	Prec@1 14.453 (11.769)	Prec@5 28.125 (27.540)	LR: 0.0001
Epoch: [6][5000/5005]	Loss 4.7257 (4.9877)	Prec@1 12.891 (11.844)	Prec@5 32.031 (27.664)	LR: 0.0001
 * Prec@1 11.845 Prec@5 27.664
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.5282 (3.5282)	Prec@1 26.953 (26.953)	Prec@5 58.984 (58.984)
Test: [100/196]	Loss 5.1765 (4.5356)	Prec@1 8.594 (16.136)	Prec@5 26.172 (35.338)
 * Prec@1 16.670 Prec@5 35.546
Best accuracy: 63.28%

Epoch: [7][0/5005]	Loss 4.9931 (4.9931)	Prec@1 8.984 (8.984)	Prec@5 25.781 (25.781)	LR: 0.0001
Epoch: [7][1000/5005]	Loss 4.9326 (4.9395)	Prec@1 16.797 (12.355)	Prec@5 31.250 (28.598)	LR: 0.0001
Epoch: [7][2000/5005]	Loss 4.9530 (4.9384)	Prec@1 13.672 (12.383)	Prec@5 30.078 (28.581)	LR: 0.0001
Epoch: [7][3000/5005]	Loss 4.9921 (4.9347)	Prec@1 11.328 (12.461)	Prec@5 28.906 (28.665)	LR: 0.0001
Epoch: [7][4000/5005]	Loss 5.6830 (5.1098)	Prec@1 5.469 (10.969)	Prec@5 15.234 (25.762)	LR: 0.0001
Epoch: [7][5000/5005]	Loss 6.7032 (5.2736)	Prec@1 0.781 (9.757)	Prec@5 2.734 (23.274)	LR: 0.0001
 * Prec@1 9.750 Prec@5 23.260
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 5.5591 (5.5591)	Prec@1 7.422 (7.422)	Prec@5 23.438 (23.438)
Test: [100/196]	Loss 6.8005 (6.5759)	Prec@1 1.562 (1.180)	Prec@5 4.297 (4.494)
 * Prec@1 1.234 Prec@5 4.644
Best accuracy: 63.28%

Epoch: [8][0/5005]	Loss 6.6559 (6.6559)	Prec@1 1.172 (1.172)	Prec@5 4.297 (4.297)	LR: 0.0001
Epoch: [8][1000/5005]	Loss 6.3612 (6.4691)	Prec@1 1.172 (1.437)	Prec@5 5.859 (5.206)	LR: 0.0001
Epoch: [8][2000/5005]	Loss 6.2904 (6.3839)	Prec@1 2.734 (1.725)	Prec@5 6.641 (6.019)	LR: 0.0001
Epoch: [8][3000/5005]	Loss 6.0156 (6.3250)	Prec@1 2.734 (1.961)	Prec@5 10.547 (6.665)	LR: 0.0001
Epoch: [8][4000/5005]	Loss 6.1352 (6.2873)	Prec@1 2.344 (2.109)	Prec@5 9.766 (7.111)	LR: 0.0001
Epoch: [8][5000/5005]	Loss 6.0841 (6.2543)	Prec@1 3.516 (2.262)	Prec@5 12.500 (7.517)	LR: 0.0001
 * Prec@1 2.263 Prec@5 7.519
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 4.7806 (4.7806)	Prec@1 15.234 (15.234)	Prec@5 38.281 (38.281)
Test: [100/196]	Loss 6.3498 (5.9476)	Prec@1 1.953 (3.771)	Prec@5 5.469 (11.746)
 * Prec@1 4.082 Prec@5 11.918
Best accuracy: 63.28%

Epoch: [9][0/5005]	Loss 6.1519 (6.1519)	Prec@1 3.516 (3.516)	Prec@5 10.156 (10.156)	LR: 0.0001
Epoch: [9][1000/5005]	Loss 5.9709 (6.0654)	Prec@1 3.125 (3.179)	Prec@5 9.375 (9.788)	LR: 0.0001
Epoch: [9][2000/5005]	Loss 5.8423 (6.0488)	Prec@1 5.078 (3.305)	Prec@5 10.547 (10.074)	LR: 0.0001
Epoch: [9][3000/5005]	Loss 5.9525 (6.0270)	Prec@1 3.906 (3.440)	Prec@5 7.422 (10.365)	LR: 0.0001
Epoch: [9][4000/5005]	Loss 5.9968 (6.0066)	Prec@1 3.125 (3.559)	Prec@5 11.719 (10.703)	LR: 0.0001
Epoch: [9][5000/5005]	Loss 5.9260 (5.9891)	Prec@1 3.516 (3.654)	Prec@5 9.375 (10.956)	LR: 0.0001
 * Prec@1 3.654 Prec@5 10.957
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 4.6131 (4.6131)	Prec@1 16.016 (16.016)	Prec@5 45.703 (45.703)
Test: [100/196]	Loss 6.0551 (5.6871)	Prec@1 1.953 (5.287)	Prec@5 8.594 (15.474)
 * Prec@1 5.824 Prec@5 16.046
Best accuracy: 63.28%

Epoch: [10][0/5005]	Loss 5.9687 (5.9687)	Prec@1 2.344 (2.344)	Prec@5 12.109 (12.109)	LR: 0.0001
Epoch: [10][1000/5005]	Loss 5.8825 (5.8827)	Prec@1 3.516 (4.251)	Prec@5 14.844 (12.513)	LR: 0.0001
Epoch: [10][2000/5005]	Loss 5.8627 (5.8737)	Prec@1 5.078 (4.367)	Prec@5 11.719 (12.658)	LR: 0.0001
Epoch: [10][3000/5005]	Loss 5.7711 (5.8635)	Prec@1 6.250 (4.442)	Prec@5 17.188 (12.800)	LR: 0.0001
Epoch: [10][4000/5005]	Loss 5.7115 (5.8520)	Prec@1 6.641 (4.528)	Prec@5 17.969 (12.976)	LR: 0.0001
Epoch: [10][5000/5005]	Loss 5.7556 (5.8417)	Prec@1 4.688 (4.587)	Prec@5 16.016 (13.141)	LR: 0.0001
 * Prec@1 4.587 Prec@5 13.141
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 4.1847 (4.1847)	Prec@1 23.047 (23.047)	Prec@5 51.562 (51.562)
Test: [100/196]	Loss 6.0214 (5.4391)	Prec@1 3.516 (7.035)	Prec@5 12.891 (18.677)
 * Prec@1 7.112 Prec@5 18.332
Best accuracy: 63.28%

Epoch: [11][0/5005]	Loss 5.8695 (5.8695)	Prec@1 3.906 (3.906)	Prec@5 10.547 (10.547)	LR: 0.0001
Epoch: [11][1000/5005]	Loss 5.7728 (5.7800)	Prec@1 5.469 (4.980)	Prec@5 12.891 (14.140)	LR: 0.0001
Epoch: [11][2000/5005]	Loss 5.8862 (5.7680)	Prec@1 4.688 (5.071)	Prec@5 11.328 (14.317)	LR: 0.0001
Epoch: [11][3000/5005]	Loss 5.8478 (5.7612)	Prec@1 5.859 (5.113)	Prec@5 15.234 (14.394)	LR: 0.0001
Epoch: [11][4000/5005]	Loss 5.7634 (5.7540)	Prec@1 5.859 (5.184)	Prec@5 11.719 (14.518)	LR: 0.0001
Epoch: [11][5000/5005]	Loss 5.5976 (5.7474)	Prec@1 7.812 (5.251)	Prec@5 16.797 (14.652)	LR: 0.0001
 * Prec@1 5.251 Prec@5 14.653
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.9675 (3.9675)	Prec@1 23.438 (23.438)	Prec@5 55.859 (55.859)
Test: [100/196]	Loss 6.0069 (5.3406)	Prec@1 6.250 (7.789)	Prec@5 13.281 (20.417)
 * Prec@1 7.782 Prec@5 19.874
Best accuracy: 63.28%

Epoch: [12][0/5005]	Loss 5.6707 (5.6707)	Prec@1 6.641 (6.641)	Prec@5 15.625 (15.625)	LR: 0.0001
Epoch: [12][1000/5005]	Loss 5.6757 (5.6942)	Prec@1 5.078 (5.602)	Prec@5 16.016 (15.383)	LR: 0.0001
Epoch: [12][2000/5005]	Loss 5.4763 (5.6879)	Prec@1 9.375 (5.707)	Prec@5 18.750 (15.597)	LR: 0.0001
Epoch: [12][3000/5005]	Loss 5.7134 (5.6827)	Prec@1 4.297 (5.711)	Prec@5 14.844 (15.681)	LR: 0.0001
Epoch: [12][4000/5005]	Loss 5.5340 (5.6784)	Prec@1 5.078 (5.758)	Prec@5 17.188 (15.762)	LR: 0.0001
Epoch: [12][5000/5005]	Loss 5.5044 (5.6721)	Prec@1 7.031 (5.810)	Prec@5 21.484 (15.874)	LR: 0.0001
 * Prec@1 5.809 Prec@5 15.874
Best Train Accuracy: 56.88%

Test: [0/196]	Loss 3.9386 (3.9386)	Prec@1 22.266 (22.266)	Prec@5 57.031 (57.031)
Test: [100/196]	Loss 5.8310 (5.4180)	Prec@1 4.688 (7.526)	Prec@5 11.328 (19.787)
 * Prec@1 7.962 Prec@5 20.136
Best accuracy: 63.28%

